---
title: "EKS ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜ ê°€ì´ë“œ"
sidebar_label: "4. EKS ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜"
description: "Amazon EKS í™˜ê²½ì—ì„œ ê³ ê°€ìš©ì„±ê³¼ ì¥ì•  íšŒë³µë ¥ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ì•„í‚¤í…ì²˜ íŒ¨í„´ê³¼ ìš´ì˜ ì „ëµ ê°€ì´ë“œ"
tags: [eks, kubernetes, resiliency, high-availability, cell-architecture, chaos-engineering, multi-az]
category: "observability-monitoring"
last_update:
  date: 2026-02-10
  author: devfloor9
sidebar_position: 4
---

# EKS ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜ ê°€ì´ë“œ

> ğŸ“… **ì‘ì„±ì¼**: 2026-02-10 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 20ë¶„

> **ğŸ“Œ ê¸°ì¤€ í™˜ê²½**: EKS 1.30+, Karpenter v1.x, Istio 1.22+

## 1. ê°œìš”

ë ˆì§ˆë¦¬ì–¸ì‹œ(Resiliency)ëŠ” ì‹œìŠ¤í…œì´ ì¥ì• ì— ì§ë©´í–ˆì„ ë•Œ ì •ìƒ ìƒíƒœë¡œ ë³µêµ¬í•˜ê±°ë‚˜, ì¥ì•  ì˜í–¥ì„ ìµœì†Œí™”í•˜ë©´ì„œ ì„œë¹„ìŠ¤ë¥¼ ìœ ì§€í•˜ëŠ” ëŠ¥ë ¥ì…ë‹ˆë‹¤. í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ í™˜ê²½ì—ì„œ ë ˆì§ˆë¦¬ì–¸ì‹œì˜ í•µì‹¬ ì›ì¹™ì€ ë‹¨ìˆœí•©ë‹ˆë‹¤: **ì¥ì• ëŠ” ë°˜ë“œì‹œ ë°œìƒí•œë‹¤ â€” ì„¤ê³„ë¡œ ëŒ€ë¹„í•œë‹¤.**

ë‹¨ì¼ Pod ì¥ì• ë¶€í„° ë¦¬ì „ ì „ì²´ ì¥ì• ê¹Œì§€, ê° ê³„ì¸µì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” Failure Domainì„ ì´í•´í•˜ê³  ê·¸ì— ë§ëŠ” ë°©ì–´ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ê²ƒì´ EKS ìš´ì˜ì˜ í•µì‹¬ì…ë‹ˆë‹¤.

### Failure Domain ê³„ì¸µ êµ¬ì¡°

```mermaid
graph TB
    subgraph "Failure Domain ê³„ì¸µ"
        POD[Pod ì¥ì• <br/>ì»¨í…Œì´ë„ˆ í¬ë˜ì‹œ, OOM]
        NODE[Node ì¥ì• <br/>ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ, í•˜ë“œì›¨ì–´ ê²°í•¨]
        AZ[AZ ì¥ì• <br/>ë°ì´í„°ì„¼í„° ì „ì›, ë„¤íŠ¸ì›Œí¬ ë‹¨ì ˆ]
        REGION[Region ì¥ì• <br/>ë¦¬ì „ ìˆ˜ì¤€ ì„œë¹„ìŠ¤ ì¤‘ë‹¨]
        GLOBAL[Global ì¥ì• <br/>ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ì¥ì• ]
    end

    subgraph "ëŒ€ì‘ ì „ëµ"
        S1[Liveness/Readiness Probe<br/>PDB, ìë™ ì¬ì‹œì‘]
        S2[Topology Spread<br/>Pod Anti-Affinity]
        S3[Multi-AZ ë°°í¬<br/>ARC Zonal Shift]
        S4[Multi-Region ì•„í‚¤í…ì²˜<br/>Global Accelerator]
        S5[Multi-Cloud / CDN<br/>DNS Failover]
    end

    subgraph "ì˜í–¥ ë²”ìœ„"
        I1[ë‹¨ì¼ ì„œë¹„ìŠ¤ ì¼ë¶€ ì €í•˜]
        I2[í•´ë‹¹ ë…¸ë“œì˜ ì „ì²´ Pod]
        I3[AZ ë‚´ ì „ì²´ ì›Œí¬ë¡œë“œ]
        I4[ë¦¬ì „ ë‚´ ì „ì²´ ì„œë¹„ìŠ¤]
        I5[ì „ì²´ ì„œë¹„ìŠ¤ ë¶ˆê°€]
    end

    POD --> S1
    NODE --> S2
    AZ --> S3
    REGION --> S4
    GLOBAL --> S5

    S1 --> I1
    S2 --> I2
    S3 --> I3
    S4 --> I4
    S5 --> I5

    style POD fill:#34a853,stroke:#2a8642,color:#fff
    style NODE fill:#fbbc04,stroke:#c99603,color:#000
    style AZ fill:#ff9900,stroke:#cc7a00,color:#fff
    style REGION fill:#ff4444,stroke:#cc3636,color:#fff
    style GLOBAL fill:#ff4444,stroke:#cc3636,color:#fff
    style S1 fill:#4286f4,stroke:#2a6acf,color:#fff
    style S2 fill:#4286f4,stroke:#2a6acf,color:#fff
    style S3 fill:#4286f4,stroke:#2a6acf,color:#fff
    style S4 fill:#4286f4,stroke:#2a6acf,color:#fff
    style S5 fill:#4286f4,stroke:#2a6acf,color:#fff
```

### ë ˆì§ˆë¦¬ì–¸ì‹œ ì„±ìˆ™ë„ ëª¨ë¸

ì¡°ì§ì˜ ë ˆì§ˆë¦¬ì–¸ì‹œ ìˆ˜ì¤€ì„ 4ë‹¨ê³„ë¡œ ë¶„ë¥˜í•˜ê³ , í˜„ì¬ ìœ„ì¹˜ì—ì„œ ì ì§„ì ìœ¼ë¡œ ë°œì „ì‹œì¼œ ë‚˜ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

| Level | ë‹¨ê³„ | í•µì‹¬ ì—­ëŸ‰ | êµ¬í˜„ í•­ëª© | ë³µì¡ì„± | ë¹„ìš© ì˜í–¥ |
|-------|------|-----------|-----------|--------|-----------|
| **1** | ê¸°ë³¸ (Basic) | Pod ìˆ˜ì¤€ ë³µì›ë ¥ | Probe ì„¤ì •, PDB, Graceful Shutdown, ë¦¬ì†ŒìŠ¤ Limits | ë‚®ìŒ | ìµœì†Œ |
| **2** | Multi-AZ | AZ ì¥ì•  ë‚´ì„± | Topology Spread, Multi-AZ NodePool, ARC Zonal Shift | ì¤‘ê°„ | Cross-AZ íŠ¸ë˜í”½ ë¹„ìš© |
| **3** | Cell-Based | Blast Radius ê²©ë¦¬ | Cell Architecture, Shuffle Sharding, ë…ë¦½ ë°°í¬ | ë†’ìŒ | Cell ë³„ ì˜¤ë²„í—¤ë“œ |
| **4** | Multi-Region | ë¦¬ì „ ì¥ì•  ë‚´ì„± | Active-Active ì•„í‚¤í…ì²˜, Global Accelerator, ë°ì´í„° ë³µì œ | ë§¤ìš° ë†’ìŒ | ë¦¬ì „ ë³„ ì¸í”„ë¼ ë¹„ìš© |

:::info ì¥ì•  ì§„ë‹¨ ë° ëŒ€ì‘ ê°€ì´ë“œ ì°¸ì¡°
ìš´ì˜ ì¤‘ ì¥ì•  ì§„ë‹¨ ë° í•´ê²°ì€ [EKS ì¥ì•  ì§„ë‹¨ ë° ëŒ€ì‘ ê°€ì´ë“œ](./eks-debugging-guide.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. ë³¸ ë¬¸ì„œëŠ” ì¥ì•  **ì˜ˆë°©**ê³¼ **ì„¤ê³„**ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©°, ì‹¤ì‹œê°„ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…ì€ ì¥ì•  ì§„ë‹¨ ë° ëŒ€ì‘ ê°€ì´ë“œì—ì„œ ë‹¤ë£¹ë‹ˆë‹¤.
:::

---

## 2. Multi-AZ ì „ëµ

Multi-AZ ë°°í¬ëŠ” EKS ë ˆì§ˆë¦¬ì–¸ì‹œì˜ ê°€ì¥ ê¸°ë³¸ì ì´ë©´ì„œë„ ê°•ë ¥í•œ ì „ëµì…ë‹ˆë‹¤. ë‹¨ì¼ AZ ì¥ì• ê°€ ì„œë¹„ìŠ¤ ì „ì²´ë¥¼ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•Šë„ë¡ ì›Œí¬ë¡œë“œë¥¼ ì—¬ëŸ¬ ê°€ìš© ì˜ì—­ì— ë¶„ì‚°í•©ë‹ˆë‹¤.

### Pod Topology Spread Constraints

Topology Spread ConstraintsëŠ” Podë¥¼ AZ, ë…¸ë“œ, ì»¤ìŠ¤í…€ í† í´ë¡œì§€ ë„ë©”ì¸ì— ê±¸ì³ ê· ë“±í•˜ê²Œ ë¶„ì‚°ì‹œí‚µë‹ˆë‹¤. K8s 1.30+ì—ì„œëŠ” `minDomains` íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ìµœì†Œ ë¶„ì‚° ë„ë©”ì¸ ìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¶Œì¥ê°’ |
|----------|------|--------|
| `maxSkew` | ë„ë©”ì¸ ê°„ Pod ìˆ˜ ìµœëŒ€ ì°¨ì´ | AZ: 1, ë…¸ë“œ: 2 |
| `topologyKey` | ë¶„ì‚° ê¸°ì¤€ ë ˆì´ë¸” | `topology.kubernetes.io/zone` |
| `whenUnsatisfiable` | ì¡°ê±´ ë¶ˆì¶©ì¡± ì‹œ ë™ì‘ | `DoNotSchedule` (hard) ë˜ëŠ” `ScheduleAnyway` (soft) |
| `minDomains` | ìµœì†Œ ë¶„ì‚° ë„ë©”ì¸ ìˆ˜ | AZ ìˆ˜ì™€ ë™ì¼ (ì˜ˆ: 3) |
| `labelSelector` | ëŒ€ìƒ Pod ì„ íƒ | Deploymentì˜ matchLabelsì™€ ë™ì¼ |

**Hard + Soft ì¡°í•© ì „ëµ** (ê¶Œì¥):

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: critical-app
  template:
    metadata:
      labels:
        app: critical-app
    spec:
      topologySpreadConstraints:
      # Hard: AZ ê°„ ê· ë“± ë¶„ì‚° (ë°˜ë“œì‹œ ë³´ì¥)
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: critical-app
        minDomains: 3
      # Soft: ë…¸ë“œ ê°„ ë¶„ì‚° (ê°€ëŠ¥í•œ í•œ ë³´ì¥)
      - maxSkew: 2
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: critical-app
```

:::tip maxSkew ì„¤ì • íŒ
`maxSkew: 1`ì€ ê°€ì¥ ì—„ê²©í•œ ê· ë“± ë¶„ì‚°ì„ ë³´ì¥í•©ë‹ˆë‹¤. 6ê°œ replicaë¥¼ 3 AZì— ë°°í¬í•˜ë©´ ê° AZì— ì •í™•íˆ 2ê°œì”© ë°°ì¹˜ë©ë‹ˆë‹¤. ìŠ¤ì¼€ì¼ë§ ì†ë„ê°€ ì¤‘ìš”í•œ ê²½ìš° `maxSkew: 2`ë¡œ ëŠìŠ¨í•˜ê²Œ ì„¤ì •í•˜ì—¬ ìŠ¤ì¼€ì¤„ë§ ìœ ì—°ì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

### AZ-aware Karpenter ì„¤ì •

Karpenter v1 GAì—ì„œëŠ” NodePool ë‹¨ìœ„ë¡œ Multi-AZ ë¶„ì‚°, Disruption budget, Spot + On-Demand í˜¼í•© ì „ëµì„ ì„ ì–¸ì ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: multi-az-pool
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m
    # Disruption budget: ë™ì‹œì— 20% ì´ìƒì˜ ë…¸ë“œê°€ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡ ì œí•œ
    budgets:
    - nodes: "20%"
  template:
    spec:
      requirements:
      # 3ê°œ AZì— ê±¸ì³ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
      - key: topology.kubernetes.io/zone
        operator: In
        values: ["us-east-1a", "us-east-1b", "us-east-1c"]
      # Spot + On-Demand í˜¼í•©ìœ¼ë¡œ ë¹„ìš© ìµœì í™” + ì•ˆì •ì„± í™•ë³´
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]
      - key: node.kubernetes.io/instance-type
        operator: In
        values:
          - c6i.xlarge
          - c6i.2xlarge
          - c6i.4xlarge
          - c7i.xlarge
          - c7i.2xlarge
          - c7i.4xlarge
          - m6i.xlarge
          - m6i.2xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: multi-az
  limits:
    cpu: "1000"
    memory: 2000Gi
```

:::warning Spot ì¸ìŠ¤í„´ìŠ¤ì™€ Multi-AZ
Spot ì¸ìŠ¤í„´ìŠ¤ëŠ” AZë³„ë¡œ ê°€ìš© í’€ì´ ë‹¤ë¦…ë‹ˆë‹¤. 15ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ ì¸ìŠ¤í„´ìŠ¤ ìœ í˜•ì„ ì§€ì •í•˜ë©´ Spot ìš©ëŸ‰ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ í”„ë¡œë¹„ì €ë‹ ì‹¤íŒ¨ë¥¼ ìµœì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬ ì›Œí¬ë¡œë“œì˜ base capacityëŠ” ë°˜ë“œì‹œ On-Demandë¡œ ìš´ì˜í•˜ì„¸ìš”.
:::

### Node Readiness ê¸°ë°˜ ì•ˆì „í•œ ì›Œí¬ë¡œë“œ ë°°ì¹˜

Multi-AZ í™˜ê²½ì—ì„œ ìƒˆ ë…¸ë“œê°€ í”„ë¡œë¹„ì €ë‹ë  ë•Œ, ë…¸ë“œê°€ `Ready` ìƒíƒœê°€ ë˜ë”ë¼ë„ ì‹¤ì œë¡œ ì›Œí¬ë¡œë“œë¥¼ ìˆ˜ìš©í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ Kubernetes readiness ë©”ì»¤ë‹ˆì¦˜ë“¤ì„ í™œìš©í•©ë‹ˆë‹¤.

#### Node Readiness Controller (2026ë…„ 2ì›” ë°œí‘œ)

[Node Readiness Controller](https://github.com/kubernetes-sigs/node-readiness-controller)ëŠ” ë…¸ë“œ ë¶€íŠ¸ìŠ¤íŠ¸ë© ê³¼ì •ì—ì„œ ì»¤ìŠ¤í…€ taintë¥¼ ì„ ì–¸ì ìœ¼ë¡œ ê´€ë¦¬í•˜ì—¬, GPU ë“œë¼ì´ë²„, CNI í”ŒëŸ¬ê·¸ì¸, CSI ë“œë¼ì´ë²„, ë³´ì•ˆ ì—ì´ì „íŠ¸ ë“± ëª¨ë“  ì¸í”„ë¼ ìš”êµ¬ì‚¬í•­ì´ ì¶©ì¡±ë  ë•Œê¹Œì§€ ì›Œí¬ë¡œë“œ ìŠ¤ì¼€ì¤„ë§ì„ ì§€ì—°ì‹œí‚µë‹ˆë‹¤.

```mermaid
flowchart TD
    subgraph "ë…¸ë“œ ë¶€íŠ¸ìŠ¤íŠ¸ë© ë‹¨ê³„"
        NP[ë…¸ë“œ í”„ë¡œë¹„ì €ë‹<br/>kubelet ì‹œì‘] --> NR[Node Ready ìƒíƒœ]
        NR --> T1[Taint: node.readiness/gpu=NotReady]
        NR --> T2[Taint: node.readiness/cni=NotReady]
        NR --> T3[Taint: node.readiness/security=NotReady]
    end

    subgraph "í—¬ìŠ¤ ì‹œê·¸ë„ ìˆ˜ì§‘"
        T1 --> G[GPU ë“œë¼ì´ë²„ ë¡œë”© ì™„ë£Œ]
        T2 --> C[CNI ì´ˆê¸°í™” ì™„ë£Œ]
        T3 --> S[ë³´ì•ˆ ì—ì´ì „íŠ¸ ì„¤ì¹˜ ì™„ë£Œ]
    end

    subgraph "Taint ì œê±°"
        G --> R1[GPU Taint ì œê±° âœ…]
        C --> R2[CNI Taint ì œê±° âœ…]
        S --> R3[Security Taint ì œê±° âœ…]
    end

    R1 --> WS[ì›Œí¬ë¡œë“œ ìŠ¤ì¼€ì¤„ë§ ì‹œì‘]
    R2 --> WS
    R3 --> WS
```

**ë ˆì§ˆë¦¬ì–¸ì‹œ ê´€ì ì˜ ì´ì :**

- **AZ ì¥ì•  ë³µêµ¬ ì‹œ**: Karpenterê°€ ìƒˆ AZì— ë…¸ë“œë¥¼ í”„ë¡œë¹„ì €ë‹í•  ë•Œ, ë…¸ë“œê°€ ì™„ì „íˆ ì¤€ë¹„ëœ í›„ì—ë§Œ íŠ¸ë˜í”½ì„ ìˆ˜ìš©
- **Scale-out ì´ë²¤íŠ¸**: ê¸‰ê²©í•œ í™•ì¥ ì‹œì—ë„ ë¯¸ì™„ì„± ë…¸ë“œì— ì›Œí¬ë¡œë“œê°€ ë°°ì¹˜ë˜ì§€ ì•ŠìŒ
- **GPU/ML ì›Œí¬ë¡œë“œ**: ë“œë¼ì´ë²„ ë¡œë”© ì™„ë£Œ ì „ ìŠ¤ì¼€ì¤„ë§ì„ ë°©ì§€í•˜ì—¬ `CrashLoopBackOff` ë°©ì§€

#### Pod Scheduling Readiness (K8s 1.30 GA)

`schedulingGates`ë¥¼ ì‚¬ìš©í•˜ë©´ Pod ì¸¡ì—ì„œ ìŠ¤ì¼€ì¤„ë§ íƒ€ì´ë°ì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™¸ë¶€ ì‹œìŠ¤í…œì´ ì¤€ë¹„ ìƒíƒœë¥¼ í™•ì¸í•œ í›„ gateë¥¼ ì œê±°í•˜ì—¬ ìŠ¤ì¼€ì¤„ë§ì„ í—ˆìš©í•©ë‹ˆë‹¤:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: validated-pod
spec:
  schedulingGates:
    - name: "example.com/capacity-validation"
    - name: "example.com/security-clearance"
  containers:
    - name: app
      image: app:latest
      resources:
        requests:
          cpu: "4"
          memory: "8Gi"
```

**í™œìš© ì‚¬ë¡€:**

- ë¦¬ì†ŒìŠ¤ ì¿¼í„° ì‚¬ì „ ê²€ì¦ í›„ ìŠ¤ì¼€ì¤„ë§ í—ˆìš©
- ë³´ì•ˆ ìŠ¹ì¸ ì™„ë£Œ í›„ ìŠ¤ì¼€ì¤„ë§ í—ˆìš©
- ì»¤ìŠ¤í…€ ì–´ë“œë¯¸ì…˜ ì²´í¬ í†µê³¼ í›„ ìŠ¤ì¼€ì¤„ë§ í—ˆìš©

#### Pod Readiness Gates (AWS LB Controller)

AWS Load Balancer Controllerì˜ Pod Readiness GatesëŠ” ë¡¤ë§ ì—…ë°ì´íŠ¸ ì‹œ **ë¬´ì¤‘ë‹¨ ë°°í¬**ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    elbv2.k8s.aws/pod-readiness-gate-inject: enabled  # ìë™ ì£¼ì… í™œì„±í™”
```

ìƒˆ Podê°€ ALB/NLB íƒ€ê²Ÿìœ¼ë¡œ ë“±ë¡ë˜ê³  í—¬ìŠ¤ ì²´í¬ë¥¼ í†µê³¼í•  ë•Œê¹Œì§€ ì´ì „ Podê°€ ì¢…ë£Œë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, íŠ¸ë˜í”½ ìœ ì‹¤ ì—†ëŠ” ë°°í¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

:::tip Readiness ê¸°ëŠ¥ ì„ íƒ ê°€ì´ë“œ

| ìš”êµ¬ì‚¬í•­ | ì¶”ì²œ ê¸°ëŠ¥ | ì ìš© ë ˆë²¨ |
|----------|-----------|-----------|
| ë…¸ë“œ ë¶€íŠ¸ìŠ¤íŠ¸ë© ì™„ë£Œ ë³´ì¥ | Node Readiness Controller | Node |
| Pod ìŠ¤ì¼€ì¤„ë§ ì „ ì™¸ë¶€ ê²€ì¦ | Pod Scheduling Readiness | Pod |
| LB ë“±ë¡ ì™„ë£Œ í›„ íŠ¸ë˜í”½ ìˆ˜ì‹  | Pod Readiness Gates | Pod |
| GPU/íŠ¹ìˆ˜ í•˜ë“œì›¨ì–´ ì¤€ë¹„ ë³´ì¥ | Node Readiness Controller | Node |
| ë¬´ì¤‘ë‹¨ ë¡¤ë§ ë°°í¬ | Pod Readiness Gates | Pod |
:::

### AZ íšŒí”¼ ë°°í¬ ì „ëµ (ARC Zonal Shift)

AWS Application Recovery Controller(ARC) Zonal ShiftëŠ” íŠ¹ì • AZì— ë¬¸ì œê°€ ê°ì§€ë˜ì—ˆì„ ë•Œ í•´ë‹¹ AZë¡œì˜ íŠ¸ë˜í”½ì„ ìë™ ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ ì „í™˜í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. EKSëŠ” 2024ë…„ 11ì›”ë¶€í„° ARC Zonal Shiftë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph "AZ ì¥ì•  ê°ì§€ ë° ëŒ€ì‘"
        HD[AWS Health Dashboard<br/>ì¥ì•  ì´ë²¤íŠ¸ ê°ì§€]
        EB[EventBridge Rule<br/>ì´ë²¤íŠ¸ í•„í„°ë§]
        LM[Lambda Function<br/>ìë™ ëŒ€ì‘]
    end

    subgraph "ARC Zonal Shift"
        ZA[Zonal Autoshift<br/>AWS ìë™ íŠ¸ë˜í”½ ì „í™˜]
        ZS[Manual Zonal Shift<br/>ìš´ì˜ì ìˆ˜ë™ ì „í™˜]
    end

    subgraph "EKS í´ëŸ¬ìŠ¤í„°"
        AZ1[AZ-1a<br/>ì •ìƒ]
        AZ2[AZ-1b<br/>ì¥ì•  ë°œìƒ]
        AZ3[AZ-1c<br/>ì •ìƒ]
    end

    HD --> EB
    EB --> LM
    LM --> ZS
    ZA --> AZ2

    AZ2 -.->|íŠ¸ë˜í”½ ì°¨ë‹¨| AZ1
    AZ2 -.->|íŠ¸ë˜í”½ ì°¨ë‹¨| AZ3

    style AZ2 fill:#ff4444,stroke:#cc3636,color:#fff
    style AZ1 fill:#34a853,stroke:#2a8642,color:#fff
    style AZ3 fill:#34a853,stroke:#2a8642,color:#fff
    style ZA fill:#ff9900,stroke:#cc7a00,color:#fff
    style LM fill:#ff9900,stroke:#cc7a00,color:#fff
```

**ARC Zonal Shift í™œì„±í™” ë° ì‚¬ìš©:**

```bash
# EKS í´ëŸ¬ìŠ¤í„°ì— Zonal Shift í™œì„±í™”
aws eks update-cluster-config \
  --name my-cluster \
  --zonal-shift-config enabled=true

# ìˆ˜ë™ Zonal Shift ì‹œì‘ (íŠ¹ì • AZì—ì„œ íŠ¸ë˜í”½ ìš°íšŒ)
aws arc-zonal-shift start-zonal-shift \
  --resource-identifier arn:aws:eks:us-east-1:123456789012:cluster/my-cluster \
  --away-from us-east-1b \
  --expires-in 3h \
  --comment "AZ-b impairment detected via Health Dashboard"

# Zonal Shift ìƒíƒœ í™•ì¸
aws arc-zonal-shift list-zonal-shifts \
  --resource-identifier arn:aws:eks:us-east-1:123456789012:cluster/my-cluster
```

:::info Zonal Shift ì œí•œì‚¬í•­
Zonal Shiftì˜ ìµœëŒ€ ì§€ì† ì‹œê°„ì€ **3ì¼**ì´ë©°, í•„ìš” ì‹œ ì—°ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Zonal Autoshiftë¥¼ í™œì„±í™”í•˜ë©´ AWSê°€ AZ ìˆ˜ì¤€ì˜ ì¥ì• ë¥¼ ê°ì§€í•˜ì—¬ ìë™ìœ¼ë¡œ íŠ¸ë˜í”½ì„ ì „í™˜í•©ë‹ˆë‹¤.
:::

**ê¸´ê¸‰ AZ Evacuation ìŠ¤í¬ë¦½íŠ¸:**

```bash
#!/bin/bash
# az-evacuation.sh - ì¥ì•  AZì˜ ëª¨ë“  ì›Œí¬ë¡œë“œë¥¼ ì•ˆì „í•˜ê²Œ ëŒ€í”¼
IMPAIRED_AZ=$1

if [ -z "$IMPAIRED_AZ" ]; then
  echo "Usage: $0 <az-name>"
  echo "Example: $0 us-east-1b"
  exit 1
fi

echo "=== AZ Evacuation: ${IMPAIRED_AZ} ==="

# 1. í•´ë‹¹ AZì˜ ë…¸ë“œ Cordon (ìƒˆ Pod ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨)
echo "[Step 1] Cordoning nodes in ${IMPAIRED_AZ}..."
kubectl get nodes -l topology.kubernetes.io/zone=${IMPAIRED_AZ} -o name | \
  xargs -I {} kubectl cordon {}

# 2. í•´ë‹¹ AZì˜ ë…¸ë“œ Drain (ê¸°ì¡´ Pod ì•ˆì „í•˜ê²Œ ì´ë™)
echo "[Step 2] Draining nodes in ${IMPAIRED_AZ}..."
kubectl get nodes -l topology.kubernetes.io/zone=${IMPAIRED_AZ} -o name | \
  xargs -I {} kubectl drain {} \
    --ignore-daemonsets \
    --delete-emptydir-data \
    --grace-period=30 \
    --timeout=120s

# 3. ëŒ€í”¼ ê²°ê³¼ í™•ì¸
echo "[Step 3] Verifying evacuation..."
echo "Remaining pods in ${IMPAIRED_AZ}:"
kubectl get pods --all-namespaces -o wide | grep ${IMPAIRED_AZ} | grep -v DaemonSet

echo "=== Evacuation complete ==="
```

### EBS AZ-Pinning ëŒ€ì‘

EBS ë³¼ë¥¨ì€ íŠ¹ì • AZì— ê³ ì •(pinned)ë©ë‹ˆë‹¤. í•´ë‹¹ AZì— ì¥ì• ê°€ ë°œìƒí•˜ë©´ ë³¼ë¥¨ì„ ì‚¬ìš©í•˜ëŠ” Podê°€ ë‹¤ë¥¸ AZë¡œ ì´ë™í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**WaitForFirstConsumer StorageClass** (ê¶Œì¥):

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: topology-aware-ebs
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  encrypted: "true"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
```

`WaitForFirstConsumer`ëŠ” Podê°€ ìŠ¤ì¼€ì¤„ë§ë  ë•Œê¹Œì§€ ë³¼ë¥¨ ìƒì„±ì„ ì§€ì—°ì‹œì¼œ, Podì™€ ê°™ì€ AZì— ë³¼ë¥¨ì´ ìƒì„±ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

**EFS Cross-AZ ëŒ€ì•ˆ**: AZ ì¥ì•  ì‹œì—ë„ ìŠ¤í† ë¦¬ì§€ ì ‘ê·¼ì´ í•„ìš”í•œ ì›Œí¬ë¡œë“œì—ëŠ” Amazon EFSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. EFSëŠ” ëª¨ë“  AZì—ì„œ ë™ì‹œ ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ AZ-Pinning ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.

| ìŠ¤í† ë¦¬ì§€ | AZ ì¢…ì†ì„± | ì¥ì•  ì‹œ ë™ì‘ | ì í•©í•œ ì›Œí¬ë¡œë“œ |
|----------|-----------|-------------|----------------|
| EBS (gp3) | ë‹¨ì¼ AZ ê³ ì • | AZ ì¥ì•  ì‹œ ì ‘ê·¼ ë¶ˆê°€ | ë°ì´í„°ë² ì´ìŠ¤, ìƒíƒœ ì €ì¥ ì•± |
| EFS | Cross-AZ | AZ ì¥ì• ì—ë„ ì ‘ê·¼ ê°€ëŠ¥ | ê³µìœ  íŒŒì¼, CMS, ë¡œê·¸ |
| Instance Store | ë…¸ë“œ ì¢…ì† | ë…¸ë“œ ì¢…ë£Œ ì‹œ ë°ì´í„° ì†Œì‹¤ | ì„ì‹œ ìºì‹œ, ìŠ¤í¬ë˜ì¹˜ |

### Cross-AZ ë¹„ìš© ìµœì í™”

Multi-AZ ë°°í¬ì˜ ì£¼ìš” ë¹„ìš© ìš”ì¸ì€ Cross-AZ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ì…ë‹ˆë‹¤. AWSì—ì„œ ê°™ì€ ë¦¬ì „ ë‚´ AZ ê°„ ë°ì´í„° ì „ì†¡ì€ ì–‘ë°©í–¥ ê° $0.01/GBê°€ ë¶€ê³¼ë©ë‹ˆë‹¤.

**Istio Locality-Aware ë¼ìš°íŒ…**ìœ¼ë¡œ Cross-AZ íŠ¸ë˜í”½ì„ ìµœì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: locality-aware-routing
spec:
  host: backend-service
  trafficPolicy:
    connectionPool:
      http:
        http2MaxRequests: 1000
    outlierDetection:
      consecutive5xxErrors: 5
      interval: 10s
      baseEjectionTime: 30s
    loadBalancer:
      localityLbSetting:
        enabled: true
        # ê°™ì€ AZ ìš°ì„ , ì¥ì•  ì‹œ ë‹¤ë¥¸ AZë¡œ failover
        distribute:
        - from: "us-east-1/us-east-1a/*"
          to:
            "us-east-1/us-east-1a/*": 80
            "us-east-1/us-east-1b/*": 10
            "us-east-1/us-east-1c/*": 10
        - from: "us-east-1/us-east-1b/*"
          to:
            "us-east-1/us-east-1b/*": 80
            "us-east-1/us-east-1a/*": 10
            "us-east-1/us-east-1c/*": 10
```

:::tip Cross-AZ ë¹„ìš© ì ˆê° íš¨ê³¼
Locality-Aware ë¼ìš°íŒ…ì„ ì ìš©í•˜ë©´ ê°™ì€ AZ ë‚´ íŠ¸ë˜í”½ì„ 80% ì´ìƒ ìœ ì§€í•˜ì—¬ Cross-AZ ë°ì´í„° ì „ì†¡ ë¹„ìš©ì„ í¬ê²Œ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì›” ìˆ˜ì²œ ë‹¬ëŸ¬ì˜ ë¹„ìš© ì ˆê°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
:::

---

## 3. Cell-Based Architecture

Cell-Based ArchitectureëŠ” AWS Well-Architected Frameworkì—ì„œ ê¶Œì¥í•˜ëŠ” ê³ ê¸‰ ë ˆì§ˆë¦¬ì–¸ì‹œ íŒ¨í„´ìœ¼ë¡œ, ì‹œìŠ¤í…œì„ ë…ë¦½ì ì¸ Cellë¡œ ë¶„í• í•˜ì—¬ ì¥ì•  ì˜í–¥ ë²”ìœ„(Blast Radius)ë¥¼ ê²©ë¦¬í•©ë‹ˆë‹¤.

### Cell ê°œë…ê³¼ ì„¤ê³„ ì›ì¹™

Cellì€ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•  ìˆ˜ ìˆëŠ” ìê¸° ì™„ê²°ì (self-contained) ì„œë¹„ìŠ¤ ë‹¨ìœ„ì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ Cellì´ ì¥ì• ë¥¼ ê²ªì–´ë„ ë‹¤ë¥¸ Cellì€ ì˜í–¥ì„ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph "Control Plane"
        CR[Cell Router<br/>íŠ¸ë˜í”½ ë¼ìš°íŒ…]
        REG[Cell Registry<br/>Cell ìƒíƒœ ê´€ë¦¬]
        HC[Health Checker<br/>Cell ëª¨ë‹ˆí„°ë§]
    end

    subgraph "Data Plane"
        subgraph "Cell 1 (ê³ ê° A-H)"
            C1_LB[Load Balancer]
            C1_APP[Application Pods]
            C1_DB[(Database)]
            C1_CACHE[(Cache)]
        end

        subgraph "Cell 2 (ê³ ê° I-P)"
            C2_LB[Load Balancer]
            C2_APP[Application Pods]
            C2_DB[(Database)]
            C2_CACHE[(Cache)]
        end

        subgraph "Cell 3 (ê³ ê° Q-Z)"
            C3_LB[Load Balancer]
            C3_APP[Application Pods]
            C3_DB[(Database)]
            C3_CACHE[(Cache)]
        end
    end

    CR --> C1_LB
    CR --> C2_LB
    CR --> C3_LB
    REG --> HC
    HC --> C1_APP
    HC --> C2_APP
    HC --> C3_APP

    style CR fill:#4286f4,stroke:#2a6acf,color:#fff
    style REG fill:#4286f4,stroke:#2a6acf,color:#fff
    style HC fill:#4286f4,stroke:#2a6acf,color:#fff
    style C1_LB fill:#34a853,stroke:#2a8642,color:#fff
    style C2_LB fill:#34a853,stroke:#2a8642,color:#fff
    style C3_LB fill:#34a853,stroke:#2a8642,color:#fff
```

**Cell ì„¤ê³„ í•µì‹¬ ì›ì¹™:**

1. **ë…ë¦½ì„±(Independence)**: ê° Cellì€ ìì²´ ë°ì´í„° ìŠ¤í† ì–´, ìºì‹œ, íë¥¼ ë³´ìœ 
2. **ê²©ë¦¬(Isolation)**: Cell ê°„ ì§ì ‘ í†µì‹  ì—†ìŒ â€” Control Planeì„ í†µí•´ì„œë§Œ ì¡°ìœ¨
3. **ê· ì¼ì„±(Homogeneity)**: ëª¨ë“  Cellì€ ë™ì¼í•œ ì½”ë“œì™€ êµ¬ì„±ì„ ì‹¤í–‰
4. **í™•ì¥ì„±(Scalability)**: ìˆ˜ìš” ì¦ê°€ ì‹œ ê¸°ì¡´ Cell í™•ì¥ì´ ì•„ë‹Œ ìƒˆ Cell ì¶”ê°€

### EKSì—ì„œì˜ Cell êµ¬í˜„

| êµ¬í˜„ ë°©ì‹ | Namespace ê¸°ë°˜ Cell | Cluster ê¸°ë°˜ Cell |
|-----------|-------------------|------------------|
| **ê²©ë¦¬ ìˆ˜ì¤€** | ë…¼ë¦¬ì  ê²©ë¦¬ (soft) | ë¬¼ë¦¬ì  ê²©ë¦¬ (hard) |
| **ë¦¬ì†ŒìŠ¤ ê²©ë¦¬** | ResourceQuota, LimitRange | ì™„ì „í•œ í´ëŸ¬ìŠ¤í„° ê²©ë¦¬ |
| **ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬** | NetworkPolicy | VPC/Subnet ìˆ˜ì¤€ |
| **Blast Radius** | ê°™ì€ í´ëŸ¬ìŠ¤í„° ë‚´ ì ì¬ì  ì˜í–¥ | Cell ê°„ ì™„ì „í•œ ê²©ë¦¬ |
| **ìš´ì˜ ë³µì¡ì„±** | ë‚®ìŒ (ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°) | ë†’ìŒ (ë©€í‹° í´ëŸ¬ìŠ¤í„°) |
| **ë¹„ìš©** | ë‚®ìŒ | ë†’ìŒ (Control Plane ë¹„ìš© Ã— Cell ìˆ˜) |
| **ì í•©í•œ í™˜ê²½** | ì†Œ~ì¤‘ê·œëª¨, ë‚´ë¶€ ì„œë¹„ìŠ¤ | ëŒ€ê·œëª¨, ê·œì œ ì¤€ìˆ˜ í•„ìš” |

**Namespace ê¸°ë°˜ Cell êµ¬í˜„ ì˜ˆì‹œ:**

```yaml
# Cell-1 Namespace ë° ResourceQuota
apiVersion: v1
kind: Namespace
metadata:
  name: cell-1
  labels:
    cell-id: "cell-1"
    partition: "customers-a-h"
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cell-1-quota
  namespace: cell-1
spec:
  hard:
    requests.cpu: "20"
    requests.memory: 40Gi
    limits.cpu: "40"
    limits.memory: 80Gi
    pods: "100"
---
# Cell-aware Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
  namespace: cell-1
  labels:
    cell-id: "cell-1"
spec:
  replicas: 4
  selector:
    matchLabels:
      app: api-server
      cell-id: "cell-1"
  template:
    metadata:
      labels:
        app: api-server
        cell-id: "cell-1"
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: api-server
            cell-id: "cell-1"
      containers:
      - name: api-server
        image: myapp/api-server:v2.1
        env:
        - name: CELL_ID
          value: "cell-1"
        - name: PARTITION_RANGE
          value: "A-H"
        resources:
          requests:
            cpu: "500m"
            memory: 1Gi
          limits:
            cpu: "1"
            memory: 2Gi
```

### Cell Router êµ¬í˜„

Cell RouterëŠ” ë“¤ì–´ì˜¤ëŠ” ìš”ì²­ì„ ì ì ˆí•œ Cellë¡œ ë¼ìš°íŒ…í•˜ëŠ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤. ì„¸ ê°€ì§€ êµ¬í˜„ ë°©ì‹ì´ ìˆìŠµë‹ˆë‹¤.

**1. Route 53 ARC Routing Control ê¸°ë°˜:**

DNS ìˆ˜ì¤€ì—ì„œ Cell ë¼ìš°íŒ…ì„ ì œì–´í•©ë‹ˆë‹¤. ê° Cellì— ëŒ€í•œ Health Checkì™€ Routing Controlì„ ì„¤ì •í•˜ì—¬, Cell ì¥ì•  ì‹œ DNS ë ˆë²¨ì—ì„œ íŠ¸ë˜í”½ì„ ì°¨ë‹¨í•©ë‹ˆë‹¤.

**2. ALB Target Group ê¸°ë°˜:**

ALBì˜ Weighted Target Groupì„ í™œìš©í•˜ì—¬ Cellë³„ íŠ¸ë˜í”½ì„ ë¶„ë°°í•©ë‹ˆë‹¤. í—¤ë” ê¸°ë°˜ ë¼ìš°íŒ… ê·œì¹™ìœ¼ë¡œ ê³ ê°ë³„ Cell ë§¤í•‘ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

**3. Service Mesh ê¸°ë°˜ (Istio):**

Istio VirtualServiceì˜ header-based ë¼ìš°íŒ…ì„ ì‚¬ìš©í•˜ì—¬ Cell ë¼ìš°íŒ…ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ê°€ì¥ ìœ ì—°í•˜ì§€ë§Œ Istio ìš´ì˜ ë³µì¡ì„±ì´ ì¶”ê°€ë©ë‹ˆë‹¤.

### Blast Radius ê²©ë¦¬ ì „ëµ

| ì „ëµ | ì„¤ëª… | ê²©ë¦¬ ê¸°ì¤€ | ì‚¬ìš© ì‚¬ë¡€ |
|------|------|-----------|-----------|
| **Customer Partitioning** | ê³ ê° ID í•´ì‹œ ê¸°ë°˜ Cell ë°°ì • | ê³ ê° ê·¸ë£¹ | SaaS í”Œë«í¼ |
| **Geographic** | ì§€ë¦¬ì  ìœ„ì¹˜ ê¸°ë°˜ Cell ë°°ì • | ë¦¬ì „/êµ­ê°€ | ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ |
| **Capacity-Based** | Cell ìš©ëŸ‰ ê¸°ë°˜ ë™ì  ë°°ì • | ê°€ìš© ë¦¬ì†ŒìŠ¤ | íŠ¸ë˜í”½ ë³€ë™ í° ì„œë¹„ìŠ¤ |
| **Tier-Based** | ê³ ê° ë“±ê¸‰ ê¸°ë°˜ Cell ë°°ì • | ì„œë¹„ìŠ¤ ë ˆë²¨ | í”„ë¦¬ë¯¸ì—„/ìŠ¤íƒ ë‹¤ë“œ ë¶„ë¦¬ |

### Shuffle Sharding íŒ¨í„´

Shuffle Shardingì€ ê° ê³ ê°(ë˜ëŠ” í…Œë„ŒíŠ¸)ì„ ì „ì²´ Cell í’€ì—ì„œ ëœë¤í•˜ê²Œ ì„ íƒí•œ ì†Œìˆ˜ì˜ Cellì— í• ë‹¹í•˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í•˜ë‚˜ì˜ Cell ì¥ì• ê°€ ì†Œìˆ˜ì˜ ê³ ê°ì—ê²Œë§Œ ì˜í–¥ì„ ë¯¸ì¹˜ë„ë¡ í•©ë‹ˆë‹¤.

**ì›ë¦¬**: 8ê°œì˜ Cellì´ ìˆê³ , ê° ê³ ê°ì—ê²Œ 2ê°œì˜ Cellì„ í• ë‹¹í•˜ë©´, ê°€ëŠ¥í•œ ì¡°í•©ì€ C(8,2) = 28ê°œì…ë‹ˆë‹¤. íŠ¹ì • Cell í•˜ë‚˜ê°€ ì¥ì• ë¥¼ ê²ªì–´ë„ í•´ë‹¹ Cellì„ ì‚¬ìš©í•˜ëŠ” ê³ ê°ë§Œ ì˜í–¥ì„ ë°›ìœ¼ë©°, ë‚˜ë¨¸ì§€ Cellë¡œ ìë™ failoverë©ë‹ˆë‹¤.

```yaml
# Shuffle Sharding ConfigMap ì˜ˆì‹œ
apiVersion: v1
kind: ConfigMap
metadata:
  name: shuffle-sharding-config
data:
  sharding-config.yaml: |
    totalCells: 8
    shardsPerTenant: 2
    tenantAssignments:
      tenant-acme:
        cells: ["cell-1", "cell-5"]
        primary: "cell-1"
      tenant-globex:
        cells: ["cell-3", "cell-7"]
        primary: "cell-3"
      tenant-initech:
        cells: ["cell-2", "cell-6"]
        primary: "cell-2"
```

:::warning Cell Architectureì˜ Trade-off
Cell ArchitectureëŠ” ê°•ë ¥í•œ ê²©ë¦¬ë¥¼ ì œê³µí•˜ì§€ë§Œ, ìš´ì˜ ë³µì¡ì„±ê³¼ ë¹„ìš©ì´ ì¦ê°€í•©ë‹ˆë‹¤. ê° Cellì´ ë…ë¦½ì ì¸ ë°ì´í„° ìŠ¤í† ì–´ë¥¼ ê°€ì§€ë¯€ë¡œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜, Cross-Cell ì¿¼ë¦¬, Cell ê°„ ì¼ê´€ì„± ìœ ì§€ì— ì¶”ê°€ì ì¸ ì„¤ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤. SLA 99.99% ì´ìƒì´ ìš”êµ¬ë˜ëŠ” ì„œë¹„ìŠ¤ë¶€í„° ë„ì…ì„ ê²€í† í•˜ì„¸ìš”.
:::

---

## 4. Multi-Cluster / Multi-Region

ë¦¬ì „ ìˆ˜ì¤€ì˜ ì¥ì• ì— ëŒ€ë¹„í•˜ê¸° ìœ„í•œ Multi-Cluster ë° Multi-Region ì „ëµì…ë‹ˆë‹¤.

### ì•„í‚¤í…ì²˜ íŒ¨í„´ ë¹„êµ

| íŒ¨í„´ | ì„¤ëª… | RTO | RPO | ë¹„ìš© | ë³µì¡ì„± | ì í•©í•œ í™˜ê²½ |
|------|------|-----|-----|------|--------|------------|
| **Active-Active** | ëª¨ë“  ë¦¬ì „ì—ì„œ ë™ì‹œì— íŠ¸ë˜í”½ ì²˜ë¦¬ | ~0 | ~0 | ë§¤ìš° ë†’ìŒ | ë§¤ìš° ë†’ìŒ | ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤, ê·¹í•œ SLA |
| **Active-Passive** | í•˜ë‚˜ì˜ ë¦¬ì „ë§Œ í™œì„±, ë‚˜ë¨¸ì§€ ëŒ€ê¸° | ë¶„~ì‹œê°„ | ë¶„ | ë†’ìŒ | ë†’ìŒ | ëŒ€ë¶€ë¶„ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì•± |
| **Regional Isolation** | ë¦¬ì „ë³„ ë…ë¦½ ìš´ì˜, ë°ì´í„° ê²©ë¦¬ | ë¦¬ì „ë³„ ë…ë¦½ | N/A | ì¤‘ê°„ | ì¤‘ê°„ | ê·œì œ ì¤€ìˆ˜, ë°ì´í„° ì£¼ê¶Œ |
| **Hub-Spoke** | ì¤‘ì•™ Hubì—ì„œ ê´€ë¦¬, Spokeì—ì„œ ì„œë¹™ | ë¶„ | ì´ˆ~ë¶„ | ì¤‘ê°„~ë†’ìŒ | ì¤‘ê°„ | ê´€ë¦¬ íš¨ìœ¨ ì¤‘ì‹œ |

### Global Accelerator + EKS

AWS Global AcceleratorëŠ” AWS ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ê°€ê¹Œìš´ ë¦¬ì „ì˜ EKS í´ëŸ¬ìŠ¤í„°ë¡œ íŠ¸ë˜í”½ì„ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph "ì‚¬ìš©ì"
        U1[ì•„ì‹œì•„ ì‚¬ìš©ì]
        U2[ìœ ëŸ½ ì‚¬ìš©ì]
        U3[ë¯¸ì£¼ ì‚¬ìš©ì]
    end

    GA[AWS Global Accelerator<br/>Anycast IP]

    subgraph "ap-northeast-2"
        EKS1[EKS Cluster<br/>ì„œìš¸]
        ALB1[ALB]
    end

    subgraph "eu-west-1"
        EKS2[EKS Cluster<br/>ì•„ì¼ëœë“œ]
        ALB2[ALB]
    end

    subgraph "us-east-1"
        EKS3[EKS Cluster<br/>ë²„ì§€ë‹ˆì•„]
        ALB3[ALB]
    end

    U1 --> GA
    U2 --> GA
    U3 --> GA

    GA -->|ê°€ì¤‘ì¹˜ ë¼ìš°íŒ…| ALB1
    GA -->|ê°€ì¤‘ì¹˜ ë¼ìš°íŒ…| ALB2
    GA -->|ê°€ì¤‘ì¹˜ ë¼ìš°íŒ…| ALB3

    ALB1 --> EKS1
    ALB2 --> EKS2
    ALB3 --> EKS3

    style GA fill:#ff9900,stroke:#cc7a00,color:#fff
    style EKS1 fill:#4286f4,stroke:#2a6acf,color:#fff
    style EKS2 fill:#4286f4,stroke:#2a6acf,color:#fff
    style EKS3 fill:#4286f4,stroke:#2a6acf,color:#fff
```

### ArgoCD Multi-Cluster GitOps

ArgoCD ApplicationSet Generatorë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ì— ì¼ê´€ëœ ë°°í¬ë¥¼ ìë™í™”í•©ë‹ˆë‹¤.

```yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: multi-cluster-app
  namespace: argocd
spec:
  generators:
  # í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸” ê¸°ë°˜ ë™ì  ë°°í¬
  - clusters:
      selector:
        matchLabels:
          environment: production
          resiliency-tier: "high"
  template:
    metadata:
      name: 'myapp-{{name}}'
    spec:
      project: default
      source:
        repoURL: https://github.com/myorg/k8s-manifests.git
        targetRevision: main
        path: 'overlays/{{metadata.labels.region}}'
      destination:
        server: '{{server}}'
        namespace: production
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
        - CreateNamespace=true
        retry:
          limit: 5
          backoff:
            duration: 5s
            factor: 2
            maxDuration: 3m
```

### Istio Multi-Cluster Federation

Istio Multi-Primary ì„¤ì •ì€ ê° í´ëŸ¬ìŠ¤í„°ì— ë…ë¦½ì ì¸ Istio Control Planeì„ ìš´ì˜í•˜ë©´ì„œ, í´ëŸ¬ìŠ¤í„° ê°„ ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬ì™€ ë¡œë“œ ë°¸ëŸ°ì‹±ì„ ì œê³µí•©ë‹ˆë‹¤.

```yaml
# Istio Locality-Aware ë¼ìš°íŒ… (Multi-Region)
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: multi-region-routing
spec:
  host: backend-service
  trafficPolicy:
    loadBalancer:
      localityLbSetting:
        enabled: true
        # ê°™ì€ ë¦¬ì „ ìš°ì„ , ì¥ì•  ì‹œ ë‹¤ë¥¸ ë¦¬ì „ìœ¼ë¡œ failover
        failover:
        - from: us-east-1
          to: eu-west-1
        - from: eu-west-1
          to: us-east-1
        - from: ap-northeast-2
          to: ap-southeast-1
    outlierDetection:
      consecutive5xxErrors: 3
      interval: 10s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
```

:::info Istio API Version ì°¸ê³ 
Istio 1.22+ì—ì„œëŠ” `networking.istio.io/v1`ê³¼ `networking.istio.io/v1beta1` ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. ì‹ ê·œ ë°°í¬ì—ì„œëŠ” `v1`ì„ ê¶Œì¥í•˜ë©°, ê¸°ì¡´ `v1beta1` ì„¤ì •ë„ ì—¬ì „íˆ ìœ íš¨í•©ë‹ˆë‹¤.
:::

---

## 5. ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆì§ˆë¦¬ì–¸ì‹œ íŒ¨í„´

ì¸í”„ë¼ ìˆ˜ì¤€ì˜ ë ˆì§ˆë¦¬ì–¸ì‹œì™€ í•¨ê»˜, ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ì˜ ì¥ì•  ë‚´ì„± íŒ¨í„´ì„ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.

### PodDisruptionBudgets (PDB)

PDBëŠ” ìë°œì  ì¤‘ë‹¨(Voluntary Disruption) ì‹œ â€” ë…¸ë“œ Drain, í´ëŸ¬ìŠ¤í„° ì—…ê·¸ë ˆì´ë“œ, Karpenter í†µí•© ë“± â€” ìµœì†Œí•œì˜ Pod ê°€ìš©ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

| ì„¤ì • | ë™ì‘ | ì í•©í•œ ìƒí™© |
|------|------|------------|
| `minAvailable: 2` | í•­ìƒ ìµœì†Œ 2ê°œ Pod ìœ ì§€ | replica ìˆ˜ê°€ ì ì€ ì„œë¹„ìŠ¤ (3-5ê°œ) |
| `minAvailable: "50%"` | ì „ì²´ì˜ 50% ì´ìƒ ìœ ì§€ | replica ìˆ˜ê°€ ë§ì€ ì„œë¹„ìŠ¤ |
| `maxUnavailable: 1` | ë™ì‹œì— ìµœëŒ€ 1ê°œë§Œ ì¤‘ë‹¨ | ë¡¤ë§ ì—…ë°ì´íŠ¸ ì¤‘ ì•ˆì •ì„± |
| `maxUnavailable: "25%"` | ì „ì²´ì˜ 25%ê¹Œì§€ ë™ì‹œ ì¤‘ë‹¨ í—ˆìš© | ë¹ ë¥¸ ë°°í¬ê°€ í•„ìš”í•œ ê²½ìš° |

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: api-server
---
# ëŒ€ê·œëª¨ Deploymentì— ì í•©í•œ ë¹„ìœ¨ ê¸°ë°˜ PDB
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: worker-pdb
spec:
  maxUnavailable: "25%"
  selector:
    matchLabels:
      app: worker
```

:::warning PDBì™€ Karpenter ìƒí˜¸ì‘ìš©
Karpenterì˜ Disruption budget(`budgets: - nodes: "20%"`)ê³¼ PDBëŠ” í•¨ê»˜ ë™ì‘í•©ë‹ˆë‹¤. KarpenterëŠ” ë…¸ë“œ í†µí•©(consolidation) ì‹œ PDBë¥¼ ì¡´ì¤‘í•©ë‹ˆë‹¤. PDBê°€ ë„ˆë¬´ ì—„ê²©í•˜ë©´ (ì˜ˆ: minAvailableì´ replica ìˆ˜ì™€ ê°™ìŒ) ë…¸ë“œ ë“œë ˆì¸ì´ ì˜êµ¬ì ìœ¼ë¡œ ì°¨ë‹¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜í•˜ì„¸ìš”.
:::

### Graceful Shutdown

Pod ì¢…ë£Œ ì‹œ ì§„í–‰ ì¤‘ì¸ ìš”ì²­ì„ ì•ˆì „í•˜ê²Œ ì™„ë£Œí•˜ê³ , ìƒˆë¡œìš´ ìš”ì²­ ìˆ˜ì‹ ì„ ì¤‘ë‹¨í•˜ëŠ” Graceful Shutdown íŒ¨í„´ì…ë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  template:
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: web
        image: myapp/web:v2.0
        ports:
        - containerPort: 8080
        lifecycle:
          preStop:
            exec:
              # 1. sleepìœ¼ë¡œ Endpoint ì œê±° ëŒ€ê¸° (Kubeletê³¼ Endpoint Controller ê²½í•© ë°©ì§€)
              # 2. SIGTERM ì „ì†¡ìœ¼ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ Graceful Shutdown ì‹œì‘
              command: ["/bin/sh", "-c", "sleep 5 && kill -TERM 1"]
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          periodSeconds: 5
          failureThreshold: 1
```

**Graceful Shutdown íƒ€ì´ë° ì„¤ê³„:**

```mermaid
sequenceDiagram
    participant K8s as Kubernetes
    participant EP as Endpoint Controller
    participant Pod as Pod
    participant App as Application

    K8s->>Pod: Pod ì‚­ì œ ìš”ì²­
    K8s->>EP: Endpoint ì œê±° ì‹œì‘

    par preStop Hook ì‹¤í–‰
        Pod->>Pod: sleep 5 (EP ì œê±° ëŒ€ê¸°)
    and Endpoint ì—…ë°ì´íŠ¸
        EP->>EP: Endpointì—ì„œ Pod IP ì œê±°
    end

    Pod->>App: SIGTERM ì „ì†¡
    App->>App: ìƒˆ ìš”ì²­ ìˆ˜ì‹  ì¤‘ë‹¨
    App->>App: ì§„í–‰ ì¤‘ì¸ ìš”ì²­ ì™„ë£Œ (ìµœëŒ€ 55ì´ˆ)
    App->>K8s: ì •ìƒ ì¢…ë£Œ

    Note over K8s,App: terminationGracePeriodSeconds: 60
    Note over Pod,App: preStop(5ì´ˆ) + Shutdown(ìµœëŒ€ 55ì´ˆ) = 60ì´ˆ ì´ë‚´
```

:::tip preStop sleepì´ í•„ìš”í•œ ì´ìœ 
Kubernetesì—ì„œ Pod ì‚­ì œ ì‹œ preStop Hook ì‹¤í–‰ê³¼ Endpoint ì œê±°ê°€ **ë¹„ë™ê¸°ì ìœ¼ë¡œ** ë°œìƒí•©ë‹ˆë‹¤. preStopì— 5ì´ˆ sleepì„ ì¶”ê°€í•˜ë©´, Endpoint Controllerê°€ ì„œë¹„ìŠ¤ì—ì„œ Pod IPë¥¼ ì œê±°í•  ì‹œê°„ì„ í™•ë³´í•˜ì—¬ ì¢…ë£Œ ì¤‘ì¸ Podë¡œì˜ íŠ¸ë˜í”½ ìœ ì…ì„ ë°©ì§€í•©ë‹ˆë‹¤.
:::

### Circuit Breaker (Istio DestinationRule)

Circuit BreakerëŠ” ì¥ì• ê°€ ë°œìƒí•œ ì„œë¹„ìŠ¤ë¡œì˜ ìš”ì²­ì„ ì°¨ë‹¨í•˜ì—¬ ì—°ì‡„ ì¥ì• (Cascading Failure)ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤. Istioì˜ DestinationRuleì„ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•©ë‹ˆë‹¤.

```yaml
# Istio 1.22+: v1ê³¼ v1beta1 ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: backend-circuit-breaker
spec:
  host: backend-service
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
        connectTimeout: 5s
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
        maxRequestsPerConnection: 10
        maxRetries: 3
    outlierDetection:
      # 5íšŒ ì—°ì† 5xx ì—ëŸ¬ ì‹œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í’€ì—ì„œ ì œê±°
      consecutive5xxErrors: 5
      # 30ì´ˆë§ˆë‹¤ ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ ì ê²€
      interval: 30s
      # ì œê±°ëœ ì¸ìŠ¤í„´ìŠ¤ì˜ ìµœì†Œ ê²©ë¦¬ ì‹œê°„
      baseEjectionTime: 30s
      # ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì˜ ìµœëŒ€ 50%ê¹Œì§€ ì œê±° í—ˆìš©
      maxEjectionPercent: 50
```

### Retry / Timeout (Istio VirtualService)

```yaml
apiVersion: networking.istio.io/v1
kind: VirtualService
metadata:
  name: backend-retry
spec:
  hosts:
  - backend-service
  http:
  - route:
    - destination:
        host: backend-service
    timeout: 10s
    retries:
      attempts: 3
      perTryTimeout: 3s
      retryOn: "5xx,reset,connect-failure,retriable-4xx"
      retryRemoteLocalities: true
```

**Retry Best Practices:**

| ì„¤ì • | ê¶Œì¥ê°’ | ì´ìœ  |
|------|--------|------|
| `attempts` | 2-3 | ë„ˆë¬´ ë§ì€ retryëŠ” ë¶€í•˜ë¥¼ ì¦í­ì‹œí‚´ |
| `perTryTimeout` | ì „ì²´ timeoutì˜ 1/3 | 3íšŒ retryê°€ ì „ì²´ timeout ë‚´ì— ì™„ë£Œ |
| `retryOn` | `5xx,connect-failure` | ì¼ì‹œì  ì¥ì• ë§Œ retry |
| `retryRemoteLocalities` | `true` | ë‹¤ë¥¸ AZì˜ ì¸ìŠ¤í„´ìŠ¤ì—ë„ retry |

:::warning Rate Limiting ë„ì… ì‹œ ì£¼ì˜
Rate Limitingì€ Circuit Breaker, Retryì™€ í•¨ê»˜ ë ˆì§ˆë¦¬ì–¸ì‹œì˜ í•µì‹¬ ìš”ì†Œì´ì§€ë§Œ, ì˜ëª»ëœ ì„¤ì •ì€ ì •ìƒ íŠ¸ë˜í”½ì„ ì°¨ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Istioì˜ EnvoyFilter ë˜ëŠ” ì™¸ë¶€ Rate Limiter(ì˜ˆ: Redis ê¸°ë°˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•˜ë˜, **ë°˜ë“œì‹œ ë‹¨ê³„ì ìœ¼ë¡œ ë„ì…**í•˜ì„¸ìš”: ëª¨ë‹ˆí„°ë§ ëª¨ë“œ â†’ ê²½ê³  ëª¨ë“œ â†’ ì°¨ë‹¨ ëª¨ë“œ ìˆœì„œë¡œ ì§„í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
:::

---

## 6. Chaos Engineering

Chaos Engineeringì€ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‹œìŠ¤í…œì˜ ë ˆì§ˆë¦¬ì–¸ì‹œë¥¼ ê²€ì¦í•˜ëŠ” ì‹¤ì²œì  ë°©ë²•ë¡ ì…ë‹ˆë‹¤. "ëª¨ë“  ê²ƒì´ ì •ìƒì¼ ë•Œ" í…ŒìŠ¤íŠ¸í•˜ì—¬ "ì¥ì• ê°€ ë°œìƒí–ˆì„ ë•Œ" ëŒ€ë¹„í•©ë‹ˆë‹¤.

### AWS Fault Injection Service (FIS)

AWS FISëŠ” ê´€ë¦¬í˜• Chaos Engineering ì„œë¹„ìŠ¤ë¡œ, EC2, EKS, RDS ë“± AWS ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì¥ì• ë¥¼ ì£¼ì…í•©ë‹ˆë‹¤.

**ì‹œë‚˜ë¦¬ì˜¤ 1: Pod ì‚­ì œ (ì• í”Œë¦¬ì¼€ì´ì…˜ ë³µì›ë ¥ í…ŒìŠ¤íŠ¸)**

```json
{
  "description": "EKS Pod termination test",
  "targets": {
    "eks-pods": {
      "resourceType": "aws:eks:pod",
      "resourceTags": {
        "app": "critical-api"
      },
      "selectionMode": "COUNT(3)",
      "parameters": {
        "clusterIdentifier": "arn:aws:eks:us-east-1:123456789012:cluster/prod-cluster",
        "namespace": "production"
      }
    }
  },
  "actions": {
    "terminate-pods": {
      "actionId": "aws:eks:pod-delete",
      "targets": {
        "Pods": "eks-pods"
      }
    }
  },
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:us-east-1:123456789012:alarm:HighErrorRate"
    }
  ]
}
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: AZ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜**

```json
{
  "description": "Simulate AZ failure for EKS",
  "targets": {
    "eks-nodes-az1a": {
      "resourceType": "aws:ec2:instance",
      "resourceTags": {
        "kubernetes.io/cluster/my-cluster": "owned"
      },
      "filters": [
        {
          "path": "Placement.AvailabilityZone",
          "values": ["us-east-1a"]
        }
      ],
      "selectionMode": "ALL"
    }
  },
  "actions": {
    "stop-instances": {
      "actionId": "aws:ec2:stop-instances",
      "parameters": {
        "startInstancesAfterDuration": "PT10M"
      },
      "targets": {
        "Instances": "eks-nodes-az1a"
      }
    }
  },
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:us-east-1:123456789012:alarm:CriticalServiceDown"
    }
  ]
}
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì£¼ì…**

```json
{
  "description": "Inject network latency to EKS nodes",
  "targets": {
    "eks-nodes": {
      "resourceType": "aws:ec2:instance",
      "resourceTags": {
        "kubernetes.io/cluster/my-cluster": "owned",
        "app-tier": "backend"
      },
      "selectionMode": "PERCENT(50)"
    }
  },
  "actions": {
    "inject-latency": {
      "actionId": "aws:ssm:send-command",
      "parameters": {
        "documentArn": "arn:aws:ssm:us-east-1::document/AWSFIS-Run-Network-Latency",
        "documentParameters": "{\"DurationSeconds\":\"300\",\"DelayMilliseconds\":\"200\",\"Interface\":\"eth0\"}",
        "duration": "PT5M"
      },
      "targets": {
        "Instances": "eks-nodes"
      }
    }
  }
}
```

### Litmus Chaos on EKS

LitmusëŠ” CNCF ì¸íë² ì´íŒ… í”„ë¡œì íŠ¸ë¡œ, Kubernetes ë„¤ì´í‹°ë¸Œ Chaos Engineering í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

**ì„¤ì¹˜:**

```bash
# Litmus ChaosCenter ì„¤ì¹˜
helm repo add litmuschaos https://litmuschaos.github.io/litmus-helm/
helm repo update

helm install litmus litmuschaos/litmus \
  --namespace litmus --create-namespace \
  --set portal.frontend.service.type=LoadBalancer
```

**ChaosEngine ì˜ˆì‹œ (Pod Delete):**

```yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: pod-delete-chaos
  namespace: production
spec:
  appinfo:
    appns: production
    applabel: "app=api-server"
    appkind: deployment
  engineState: active
  chaosServiceAccount: litmus-admin
  experiments:
  - name: pod-delete
    spec:
      components:
        env:
        - name: TOTAL_CHAOS_DURATION
          value: "60"
        - name: CHAOS_INTERVAL
          value: "10"
        - name: FORCE
          value: "false"
        - name: PODS_AFFECTED_PERC
          value: "50"
```

### Chaos Mesh

Chaos MeshëŠ” CNCF ì¸íë² ì´íŒ… í”„ë¡œì íŠ¸ë¡œ, ë‹¤ì–‘í•œ ì¥ì•  ìœ í˜•ì„ ì§€ì›í•˜ëŠ” Kubernetes ì „ìš© Chaos Engineering í”Œë«í¼ì…ë‹ˆë‹¤.

**ì„¤ì¹˜:**

```bash
# Chaos Mesh ì„¤ì¹˜
helm repo add chaos-mesh https://charts.chaos-mesh.org
helm repo update

helm install chaos-mesh chaos-mesh/chaos-mesh \
  --namespace chaos-mesh --create-namespace \
  --set chaosDaemon.runtime=containerd \
  --set chaosDaemon.socketPath=/run/containerd/containerd.sock
```

**NetworkChaos ì˜ˆì‹œ (ë„¤íŠ¸ì›Œí¬ íŒŒí‹°ì…˜):**

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-partition
  namespace: chaos-mesh
spec:
  action: partition
  mode: all
  selector:
    namespaces:
    - production
    labelSelectors:
      "app": "frontend"
  direction: both
  target:
    selector:
      namespaces:
      - production
      labelSelectors:
        "app": "backend"
    mode: all
  duration: "5m"
  scheduler:
    cron: "@every 24h"
```

**PodChaos ì˜ˆì‹œ (Pod Kill):**

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: pod-kill-test
  namespace: chaos-mesh
spec:
  action: pod-kill
  mode: fixed-percent
  value: "30"
  selector:
    namespaces:
    - production
    labelSelectors:
      "app": "api-server"
  duration: "1m"
  gracePeriod: 0
```

### Chaos Engineering ë„êµ¬ ë¹„êµ

| íŠ¹ì„± | AWS FIS | Litmus Chaos | Chaos Mesh |
|------|---------|-------------|------------|
| **ìœ í˜•** | ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ | ì˜¤í”ˆì†ŒìŠ¤ (CNCF) | ì˜¤í”ˆì†ŒìŠ¤ (CNCF) |
| **ë²”ìœ„** | AWS ì¸í”„ë¼ + K8s | Kubernetes ì „ìš© | Kubernetes ì „ìš© |
| **ì¥ì•  ìœ í˜•** | EC2, EKS, RDS, ë„¤íŠ¸ì›Œí¬ | Pod, Node, ë„¤íŠ¸ì›Œí¬, DNS | Pod, ë„¤íŠ¸ì›Œí¬, I/O, ì‹œê°„, JVM |
| **AZ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜** | ë„¤ì´í‹°ë¸Œ ì§€ì› | ì œí•œì  (Pod/Node ë ˆë²¨) | ì œí•œì  (Pod/Node ë ˆë²¨) |
| **ëŒ€ì‹œë³´ë“œ** | AWS Console | Litmus Portal (ì›¹ UI) | Chaos Dashboard (ì›¹ UI) |
| **ë¹„ìš©** | ì‹¤í–‰ ë‹¹ ê³¼ê¸ˆ | ë¬´ë£Œ (ì¸í”„ë¼ ë¹„ìš©ë§Œ) | ë¬´ë£Œ (ì¸í”„ë¼ ë¹„ìš©ë§Œ) |
| **Stop Condition** | CloudWatch Alarm ì—°ë™ | ìˆ˜ë™ / API | ìˆ˜ë™ / API |
| **ìš´ì˜ ë³µì¡ì„±** | ë‚®ìŒ | ì¤‘ê°„ | ì¤‘ê°„ |
| **GitOps í†µí•©** | CloudFormation / CDK | CRD ê¸°ë°˜ (ArgoCD í˜¸í™˜) | CRD ê¸°ë°˜ (ArgoCD í˜¸í™˜) |
| **ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤** | ì¸í”„ë¼ ìˆ˜ì¤€ ì¥ì•  í…ŒìŠ¤íŠ¸ | K8s ë„¤ì´í‹°ë¸Œ í…ŒìŠ¤íŠ¸ | ì„¸ë°€í•œ ì¥ì•  ì£¼ì… í•„ìš” ì‹œ |

:::tip ë„êµ¬ ì„ íƒ ê°€ì´ë“œ
AWS FISë¡œ ì‹œì‘í•˜ì—¬ ì¸í”„ë¼ ìˆ˜ì¤€ì˜ ì¥ì• (AZ, ë„¤íŠ¸ì›Œí¬)ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³ , Litmus ë˜ëŠ” Chaos Meshë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ì¤€ì˜ ì„¸ë°€í•œ ì¥ì• ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” **í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼**ì„ ê¶Œì¥í•©ë‹ˆë‹¤. AWS FISì˜ Stop Condition(CloudWatch Alarm ê¸°ë°˜)ì€ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ì•ˆì „í•œ í…ŒìŠ¤íŠ¸ì— í•µì‹¬ì ì¸ ê¸°ëŠ¥ì…ë‹ˆë‹¤.
:::

### Game Day ëŸ°ë¶ í…œí”Œë¦¿

Game DayëŠ” íŒ€ì´ í•¨ê»˜ ëª¨ì—¬ ê³„íšëœ ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹¤í–‰í•˜ê³ , ì‹œìŠ¤í…œê³¼ í”„ë¡œì„¸ìŠ¤ì˜ ì·¨ì•½ì ì„ ë°œê²¬í•˜ëŠ” ì—°ìŠµì…ë‹ˆë‹¤.

**5ë‹¨ê³„ Game Day ì‹¤í–‰ í”„ë ˆì„ì›Œí¬:**

```mermaid
flowchart LR
    subgraph "Phase 1: ì¤€ë¹„"
        P1[ê°€ì„¤ ìˆ˜ë¦½<br/>ì˜ˆ: AZ ì¥ì•  ì‹œ ìë™ ë³µêµ¬]
        P2[ì„±ê³µ ê¸°ì¤€ ì •ì˜<br/>ì˜ˆ: 5ë¶„ ë‚´ ë³µêµ¬]
        P3[ì¤‘ë‹¨ ê¸°ì¤€ ì„¤ì •<br/>CloudWatch Alarm]
    end

    subgraph "Phase 2: ì‹¤í–‰"
        E1[Steady State í™•ì¸<br/>í˜„ì¬ ë©”íŠ¸ë¦­ ê¸°ë¡]
        E2[ì¥ì•  ì£¼ì…<br/>FIS ì‹¤í—˜ ì‹œì‘]
        E3[ê´€ì°° ë° ê¸°ë¡<br/>ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§]
    end

    subgraph "Phase 3: ë¶„ì„"
        A1[ë³µêµ¬ ì‹œê°„ ì¸¡ì •<br/>RTO ì‹¤ì¸¡]
        A2[ë°ì´í„° ì†ì‹¤ í‰ê°€<br/>RPO ì‹¤ì¸¡]
        A3[ì‚¬ìš©ì ì˜í–¥ ë¶„ì„]
    end

    subgraph "Phase 4: ê°œì„ "
        I1[ë°œê²¬ëœ ì·¨ì•½ì  ëª©ë¡í™”]
        I2[ê°œì„  ì‘ì—… í‹°ì¼“ ìƒì„±]
        I3[ëŸ°ë¶ ì—…ë°ì´íŠ¸]
    end

    subgraph "Phase 5: ë°˜ë³µ"
        R1[ë‹¤ìŒ Game Day ì¼ì •]
        R2[ì‹œë‚˜ë¦¬ì˜¤ í™•ëŒ€]
        R3[ìë™í™” í™•ëŒ€]
    end

    P1 --> P2 --> P3
    P3 --> E1 --> E2 --> E3
    E3 --> A1 --> A2 --> A3
    A3 --> I1 --> I2 --> I3
    I3 --> R1 --> R2 --> R3

    style P1 fill:#4286f4,stroke:#2a6acf,color:#fff
    style E2 fill:#ff4444,stroke:#cc3636,color:#fff
    style A1 fill:#fbbc04,stroke:#c99603,color:#000
    style I1 fill:#34a853,stroke:#2a8642,color:#fff
    style R1 fill:#4286f4,stroke:#2a6acf,color:#fff
```

**Game Day ìë™í™” ìŠ¤í¬ë¦½íŠ¸:**

```bash
#!/bin/bash
# game-day.sh - Game Day ì‹¤í–‰ ìë™í™”
set -euo pipefail

CLUSTER_NAME=$1
SCENARIO=$2
NAMESPACE=${3:-production}

echo "============================================"
echo " Game Day: ${SCENARIO}"
echo " Cluster: ${CLUSTER_NAME}"
echo " Namespace: ${NAMESPACE}"
echo " Time: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
echo "============================================"

# Phase 1: Steady State ê¸°ë¡
echo ""
echo "[Phase 1] Recording Steady State..."
echo "--- Pod Status ---"
kubectl get pods -n ${NAMESPACE} -o wide | head -20

echo "--- Node Status ---"
kubectl get nodes -o custom-columns=\
NAME:.metadata.name,\
STATUS:.status.conditions[-1].type,\
AZ:.metadata.labels.topology\\.kubernetes\\.io/zone

echo "--- Service Endpoints ---"
kubectl get endpoints -n ${NAMESPACE}

# Phase 2: ì¥ì•  ì£¼ì… (ì‹œë‚˜ë¦¬ì˜¤ë³„)
echo ""
echo "[Phase 2] Injecting failure: ${SCENARIO}..."

case ${SCENARIO} in
  "az-failure")
    echo "Simulating AZ failure with ARC Zonal Shift..."
    # ARC Zonal Shift ì‹¤í–‰ (1ì‹œê°„)
    aws arc-zonal-shift start-zonal-shift \
      --resource-identifier arn:aws:eks:us-east-1:$(aws sts get-caller-identity --query Account --output text):cluster/${CLUSTER_NAME} \
      --away-from us-east-1a \
      --expires-in 1h \
      --comment "Game Day: AZ failure simulation"
    ;;

  "pod-delete")
    echo "Deleting 30% of pods in ${NAMESPACE}..."
    TOTAL=$(kubectl get pods -n ${NAMESPACE} -l app=api-server --no-headers | wc -l)
    DELETE_COUNT=$(( TOTAL * 30 / 100 ))
    DELETE_COUNT=$(( DELETE_COUNT < 1 ? 1 : DELETE_COUNT ))
    kubectl get pods -n ${NAMESPACE} -l app=api-server -o name | \
      shuf | head -n ${DELETE_COUNT} | \
      xargs kubectl delete -n ${NAMESPACE}
    ;;

  "node-drain")
    echo "Draining a random node..."
    NODE=$(kubectl get nodes --no-headers | shuf -n 1 | awk '{print $1}')
    kubectl cordon ${NODE}
    kubectl drain ${NODE} --ignore-daemonsets --delete-emptydir-data --timeout=120s
    ;;

  *)
    echo "Unknown scenario: ${SCENARIO}"
    echo "Available: az-failure, pod-delete, node-drain"
    exit 1
    ;;
esac

# Phase 3: ë³µêµ¬ ê´€ì°°
echo ""
echo "[Phase 3] Observing recovery..."
echo "Waiting 60 seconds for recovery..."
sleep 60

echo "--- Post-Failure Pod Status ---"
kubectl get pods -n ${NAMESPACE} -o wide | head -20

echo "--- Pod Restart Counts ---"
kubectl get pods -n ${NAMESPACE} -o custom-columns=\
NAME:.metadata.name,\
RESTARTS:.status.containerStatuses[0].restartCount,\
STATUS:.status.phase

echo ""
echo "============================================"
echo " Game Day Phase 3 Complete"
echo " Review results and proceed to analysis"
echo "============================================"
```

---

## 7. ë ˆì§ˆë¦¬ì–¸ì‹œ ì²´í¬ë¦¬ìŠ¤íŠ¸ & ì°¸ê³  ìë£Œ

### ë ˆì§ˆë¦¬ì–¸ì‹œ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì•„ë˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ í˜„ì¬ ë ˆì§ˆë¦¬ì–¸ì‹œ ìˆ˜ì¤€ì„ í‰ê°€í•˜ê³ , ë‹¤ìŒ ë‹¨ê³„ì˜ êµ¬í˜„ í•­ëª©ì„ í™•ì¸í•˜ì„¸ìš”.

**Level 1 â€” ê¸°ë³¸ (Basic)**

| í•­ëª© | ì„¤ëª… | í™•ì¸ |
|------|------|------|
| Liveness/Readiness Probe ì„¤ì • | ëª¨ë“  Deploymentì— ì ì ˆí•œ Probe êµ¬ì„± | [ ] |
| Resource Requests/Limits ì„¤ì • | CPU, Memory ë¦¬ì†ŒìŠ¤ ì œí•œ ëª…ì‹œ | [ ] |
| PodDisruptionBudget ì„¤ì • | ìµœì†Œ ê°€ìš© Pod ìˆ˜ ë³´ì¥ | [ ] |
| Graceful Shutdown êµ¬í˜„ | preStop Hook + terminationGracePeriodSeconds | [ ] |
| Startup Probe ì„¤ì • | ëŠë¦° ì‹œì‘ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì´ˆê¸°í™” ë³´í˜¸ | [ ] |
| ìë™ ì¬ì‹œì‘ ì •ì±… | restartPolicy: Always í™•ì¸ | [ ] |

**Level 2 â€” Multi-AZ**

| í•­ëª© | ì„¤ëª… | í™•ì¸ |
|------|------|------|
| Topology Spread Constraints | AZ ê°„ Pod ê· ë“± ë¶„ì‚° | [ ] |
| Multi-AZ Karpenter NodePool | 3ê°œ ì´ìƒ AZì— ê±¸ì¹œ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ | [ ] |
| WaitForFirstConsumer StorageClass | EBS AZ-Pinning ë°©ì§€ | [ ] |
| ARC Zonal Shift í™œì„±í™” | AZ ì¥ì•  ì‹œ ìë™ íŠ¸ë˜í”½ ì „í™˜ | [ ] |
| Cross-AZ íŠ¸ë˜í”½ ìµœì í™” | Locality-Aware ë¼ìš°íŒ… êµ¬ì„± | [ ] |
| AZ Evacuation ëŸ°ë¶ ì¤€ë¹„ | ê¸´ê¸‰ AZ ëŒ€í”¼ ì ˆì°¨ ë¬¸ì„œí™” | [ ] |

**Level 3 â€” Cell-Based**

| í•­ëª© | ì„¤ëª… | í™•ì¸ |
|------|------|------|
| Cell ê²½ê³„ ì •ì˜ | Namespace ë˜ëŠ” Cluster ê¸°ë°˜ Cell êµ¬ì„± | [ ] |
| Cell Router êµ¬í˜„ | ìš”ì²­ì„ ì ì ˆí•œ Cellë¡œ ë¼ìš°íŒ… | [ ] |
| Cell ê°„ ê²©ë¦¬ í™•ì¸ | NetworkPolicy ë˜ëŠ” VPC ìˆ˜ì¤€ ê²©ë¦¬ | [ ] |
| Shuffle Sharding ì ìš© | í…Œë„ŒíŠ¸ë³„ Cell í• ë‹¹ ë‹¤ì–‘í™” | [ ] |
| Cell Health Monitoring | ê°œë³„ Cell ìƒíƒœ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ | [ ] |
| Cell Failover í…ŒìŠ¤íŠ¸ | Chaos Engineeringìœ¼ë¡œ Cell ì¥ì•  ê²€ì¦ | [ ] |

**Level 4 â€” Multi-Region**

| í•­ëª© | ì„¤ëª… | í™•ì¸ |
|------|------|------|
| Multi-Region ì•„í‚¤í…ì²˜ ì„¤ê³„ | Active-Active ë˜ëŠ” Active-Passive ê²°ì • | [ ] |
| Global Accelerator êµ¬ì„± | ë¦¬ì „ ê°„ íŠ¸ë˜í”½ ë¼ìš°íŒ… | [ ] |
| ë°ì´í„° ë³µì œ ì „ëµ | Cross-Region ë°ì´í„° ë™ê¸°í™” | [ ] |
| ArgoCD Multi-Cluster GitOps | ApplicationSet ê¸°ë°˜ ë©€í‹° í´ëŸ¬ìŠ¤í„° ë°°í¬ | [ ] |
| Multi-Region Chaos Test | ë¦¬ì „ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜ Game Day | [ ] |
| RTO/RPO ì‹¤ì¸¡ ë° ê²€ì¦ | ëª©í‘œ ëŒ€ë¹„ ì‹¤ì œ ë³µêµ¬ ì‹œê°„/ë°ì´í„° ì†ì‹¤ ê²€ì¦ | [ ] |

### ë¹„ìš© ìµœì í™” íŒ

| ìµœì í™” ì˜ì—­ | ì „ëµ | ì˜ˆìƒ ì ˆê° |
|-------------|------|-----------|
| **Cross-AZ íŠ¸ë˜í”½** | Istio Locality-Aware ë¼ìš°íŒ…ìœ¼ë¡œ ë™ì¼ AZ íŠ¸ë˜í”½ 80%+ ìœ ì§€ | AZê°„ ì „ì†¡ ë¹„ìš© 60-80% ì ˆê° |
| **Spot ì¸ìŠ¤í„´ìŠ¤** | Non-critical ì›Œí¬ë¡œë“œì— Spot í™œìš© (Karpenter capacity-type í˜¼í•©) | ì»´í“¨íŒ… ë¹„ìš© 60-90% ì ˆê° |
| **Cell í™œìš©ë¥ ** | Cell í¬ê¸°ë¥¼ ì ì ˆíˆ ì„¤ê³„í•˜ì—¬ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ìµœì†Œí™” | ì˜¤ë²„í”„ë¡œë¹„ì €ë‹ 20-40% ì ˆê° |
| **Multi-Region** | Active-Passiveì—ì„œ Passive ë¦¬ì „ì€ ìµœì†Œ ìš©ëŸ‰ìœ¼ë¡œ ìš´ì˜ | Passive ë¦¬ì „ ë¹„ìš© 50-70% ì ˆê° |
| **Karpenter í†µí•©** | WhenEmptyOrUnderutilized ì •ì±…ìœ¼ë¡œ ë¯¸ì‚¬ìš© ë…¸ë“œ ìë™ ì œê±° | ìœ íœ´ ë¦¬ì†ŒìŠ¤ ë¹„ìš© ì œê±° |
| **EFS ì„ íƒì  ì‚¬ìš©** | ë°˜ë“œì‹œ Cross-AZ í•„ìš” ì‹œë§Œ EFS, ê·¸ ì™¸ EBS gp3 ì‚¬ìš© | ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì ˆê° |

:::danger ë¹„ìš© vs ë ˆì§ˆë¦¬ì–¸ì‹œ Trade-off
ë ˆì§ˆë¦¬ì–¸ì‹œ ìˆ˜ì¤€ì´ ë†’ì•„ì§ˆìˆ˜ë¡ ë¹„ìš©ì´ ì¦ê°€í•©ë‹ˆë‹¤. Multi-Region Active-ActiveëŠ” ë‹¨ì¼ ë¦¬ì „ ëŒ€ë¹„ 2ë°° ì´ìƒì˜ ì¸í”„ë¼ ë¹„ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­(SLA, ê·œì œ)ê³¼ ë¹„ìš©ì„ ê· í˜• ìˆê²Œ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ë ˆì§ˆë¦¬ì–¸ì‹œ ìˆ˜ì¤€ì„ ì„ íƒí•˜ì„¸ìš”. ëª¨ë“  ì„œë¹„ìŠ¤ê°€ Level 4ì¼ í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.
:::

### ê´€ë ¨ ë¬¸ì„œ

- [EKS ì¥ì•  ì§„ë‹¨ ë° ëŒ€ì‘ ê°€ì´ë“œ](./eks-debugging-guide.md) â€” ìš´ì˜ ì¤‘ ì¥ì•  ì§„ë‹¨ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…
- [GitOps ê¸°ë°˜ EKS í´ëŸ¬ìŠ¤í„° ìš´ì˜](./gitops-cluster-operation.md) â€” ArgoCD, KRO ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬
- [Karpenterë¥¼ í™œìš©í•œ ì´ˆê³ ì† ì˜¤í† ìŠ¤ì¼€ì¼ë§](/docs/infrastructure-optimization/karpenter-autoscaling.md) â€” Karpenter ì‹¬ì¸µ ì„¤ì • ë° HPA ìµœì í™”

### ì™¸ë¶€ ì°¸ì¡°

- [AWS Well-Architected â€” Cell-Based Architecture](https://docs.aws.amazon.com/wellarchitected/latest/reducing-scope-of-impact-with-cell-based-architecture/reducing-scope-of-impact-with-cell-based-architecture.html)
- [AWS Cell-Based Architecture Guidance](https://aws.amazon.com/solutions/guidance/cell-based-architecture-on-aws/)
- [AWS Shuffle Sharding](https://aws.amazon.com/blogs/architecture/shuffle-sharding-massive-and-magical-fault-isolation/)
- [EKS Reliability Best Practices](https://docs.aws.amazon.com/eks/latest/best-practices/reliability.html)
- [EKS + ARC Zonal Shift](https://docs.aws.amazon.com/eks/latest/userguide/zone-shift.html)
- [Kubernetes PDB](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)
- [Kubernetes Topology Spread Constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)
- [Istio Circuit Breaking](https://istio.io/latest/docs/tasks/traffic-management/circuit-breaking/)
- [Karpenter ê³µì‹ ë¬¸ì„œ](https://karpenter.sh/docs/)
- [AWS FIS](https://aws.amazon.com/fis/)
- [Litmus Chaos](https://litmuschaos.io/)
- [Chaos Mesh](https://chaos-mesh.org/)
- [Route 53 ARC](https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.html)
