---
title: "EKS Pod ìŠ¤ì¼€ì¤„ë§ & ê°€ìš©ì„± íŒ¨í„´"
sidebar_label: "6. Pod ìŠ¤ì¼€ì¤„ë§ & ê°€ìš©ì„±"
description: "Kubernetes Pod ìŠ¤ì¼€ì¤„ë§ ì „ëµ, Affinity/Anti-Affinity, PDB, Priority/Preemption, Taints/Tolerations ëª¨ë²” ì‚¬ë¡€"
tags: [eks, kubernetes, scheduling, affinity, pdb, priority, taints, tolerations, descheduler]
category: "operations"
last_update:
  date: 2026-02-14
  author: devfloor9
  changes: "Section 10.5 ì¶”ê°€ (Node Readiness Controller), Section 10 ì¶”ê°€ (2025-2026 AWS í˜ì‹ ), Section 8.4.1 ì¶”ê°€ (Descheduler+Karpenter ì¡°í•©), Section 9.2 ì¶”ê°€ (AI/ML ì›Œí¬ë¡œë“œ)"
sidebar_position: 6
---

# EKS Pod ìŠ¤ì¼€ì¤„ë§ & ê°€ìš©ì„± íŒ¨í„´

> ğŸ“… **ì‘ì„±ì¼**: 2026-02-12 | **ìˆ˜ì •ì¼**: 2026-02-14 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 54ë¶„

> **ğŸ“Œ ê¸°ì¤€ í™˜ê²½**: EKS 1.30+, Karpenter v1.x, Kubernetes 1.30+

## 1. ê°œìš”

Kubernetesì˜ Pod ìŠ¤ì¼€ì¤„ë§ì€ ì„œë¹„ìŠ¤ ê°€ìš©ì„±, ì„±ëŠ¥, ë¹„ìš© íš¨ìœ¨ì„±ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ìŠ¤ì¼€ì¤„ë§ ì „ëµì„ ì ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ê³ ê°€ìš©ì„±**: ì¥ì•  ë„ë©”ì¸ ë¶„ë¦¬ë¥¼ í†µí•œ ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ìµœì†Œí™”
- **ì„±ëŠ¥ ìµœì í™”**: ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë§ëŠ” ë…¸ë“œ ë°°ì¹˜ë¡œ ì‘ë‹µ ì‹œê°„ ê°œì„ 
- **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨**: ë…¸ë“œ ë¦¬ì†ŒìŠ¤ì˜ ê· í˜• ìˆëŠ” í™œìš©ìœ¼ë¡œ ë¹„ìš© ì ˆê°
- **ì•ˆì •ì  ìš´ì˜**: ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ë³´ì¥ ë° Preemption ì œì–´

ë³¸ ë¬¸ì„œëŠ” Pod ìŠ¤ì¼€ì¤„ë§ì˜ í•µì‹¬ ê°œë…ë¶€í„° ê³ ê¸‰ íŒ¨í„´ê¹Œì§€ ë‹¤ë£¨ë©°, EKS í™˜ê²½ì—ì„œ ì‹¤ì „ ì ìš© ê°€ëŠ¥í•œ YAML ì˜ˆì‹œì™€ ì˜ì‚¬ê²°ì • ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

:::info ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜ ì°¸ê³ 
ë³¸ ë¬¸ì„œëŠ” **Pod ìˆ˜ì¤€**ì˜ ìŠ¤ì¼€ì¤„ë§ íŒ¨í„´ì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„° ì „ì²´ì˜ ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜(Multi-AZ ì „ëµ, Topology Spread, Cell Architecture)ëŠ” [EKS ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜ ê°€ì´ë“œ](/docs/operations-observability/eks-resiliency-guide)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
:::

### ìŠ¤ì¼€ì¤„ë§ì´ ì¤‘ìš”í•œ ì´ìœ 

| ì‹œë‚˜ë¦¬ì˜¤ | ì˜ëª»ëœ ìŠ¤ì¼€ì¤„ë§ | ì˜¬ë°”ë¥¸ ìŠ¤ì¼€ì¤„ë§ |
|---------|----------------|----------------|
| **ì¥ì•  ê²©ë¦¬** | ëª¨ë“  replicaê°€ ê°™ì€ ë…¸ë“œ â†’ ë…¸ë“œ ì¥ì•  ì‹œ ì „ì²´ ì¤‘ë‹¨ | Anti-Affinityë¡œ ë…¸ë“œ ë¶„ì‚° â†’ ë¶€ë¶„ ì¥ì• ë§Œ ë°œìƒ |
| **ë¦¬ì†ŒìŠ¤ ê²½í•©** | CPU ì§‘ì•½ì  Podë“¤ì´ í•œ ë…¸ë“œì— ì§‘ì¤‘ â†’ ì„±ëŠ¥ ì €í•˜ | Node Affinityë¡œ ì›Œí¬ë¡œë“œ ë¶„ë¦¬ â†’ ì•ˆì •ì  ì„±ëŠ¥ |
| **ë¹„ìš© ìµœì í™”** | GPU í•„ìš” ì—†ëŠ” Podê°€ GPU ë…¸ë“œì— ë°°ì¹˜ â†’ ë¹„ìš© ë‚­ë¹„ | Taints/Tolerationsë¡œ ì „ìš© ë…¸ë“œ ê²©ë¦¬ â†’ ë¹„ìš© ì ˆê° |
| **ì—…ê·¸ë ˆì´ë“œ ì•ˆì „ì„±** | PDB ë¯¸ì„¤ì • â†’ ë¡¤ë§ ì—…ë°ì´íŠ¸ ì¤‘ ì„œë¹„ìŠ¤ ì¤‘ë‹¨ | PDB ì„¤ì • â†’ ìµœì†Œ ê°€ìš© Pod ë³´ì¥ |
| **ê¸´ê¸‰ ëŒ€ì‘** | ìš°ì„ ìˆœìœ„ ë¯¸ì„¤ì • â†’ ì¤‘ìš” ì›Œí¬ë¡œë“œ Pending | PriorityClass ì„¤ì • â†’ ì¤‘ìš” Pod ìš°ì„  ìŠ¤ì¼€ì¤„ë§ |

---

## 2. Kubernetes ìŠ¤ì¼€ì¤„ë§ ê¸°ë³¸ ì›ë¦¬

### 2.1 ìŠ¤ì¼€ì¤„ë§ í”„ë¡œì„¸ìŠ¤

Kubernetes ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¥¼ ê±°ì³ Podë¥¼ ë…¸ë“œì— ë°°ì¹˜í•©ë‹ˆë‹¤:

```mermaid
flowchart TB
    subgraph "Phase 1: Filtering"
        P1[ìƒˆ Pod ìƒì„± ìš”ì²­]
        P2[ëª¨ë“  ë…¸ë“œ ëª©ë¡ ì¡°íšŒ]
        P3{ë…¸ë“œ í•„í„°ë§<br/>Predicates}
        P4[ë¦¬ì†ŒìŠ¤ ë¶€ì¡±]
        P5[Taint ë¶ˆì¼ì¹˜]
        P6[Node Selector ë¶ˆì¼ì¹˜]
        P7[ì í•©í•œ ë…¸ë“œ ëª©ë¡]
    end

    subgraph "Phase 2: Scoring"
        S1[ê° ë…¸ë“œ ì ìˆ˜ ê³„ì‚°<br/>Priorities]
        S2[ë¦¬ì†ŒìŠ¤ ê· í˜•]
        S3[Affinity/Anti-Affinity]
        S4[ì´ë¯¸ì§€ ìºì‹œ ì—¬ë¶€]
        S5[ìµœê³  ì ìˆ˜ ë…¸ë“œ ì„ íƒ]
    end

    subgraph "Phase 3: Binding"
        B1[ë…¸ë“œì— Pod í• ë‹¹<br/>Bind]
        B2[Kubeletì— í†µë³´]
        B3[ì»¨í…Œì´ë„ˆ ì‹œì‘]
    end

    P1 --> P2
    P2 --> P3
    P3 -->|í†µê³¼ ëª»í•œ ë…¸ë“œ ì œê±°| P4
    P3 --> P5
    P3 --> P6
    P3 -->|ì í•©í•œ ë…¸ë“œë§Œ| P7
    P7 --> S1
    S1 --> S2
    S1 --> S3
    S1 --> S4
    S2 --> S5
    S3 --> S5
    S4 --> S5
    S5 --> B1
    B1 --> B2
    B2 --> B3

    style P1 fill:#4286f4,stroke:#2a6acf,color:#fff
    style P3 fill:#fbbc04,stroke:#c99603,color:#000
    style P7 fill:#34a853,stroke:#2a8642,color:#fff
    style S5 fill:#34a853,stroke:#2a8642,color:#fff
    style B3 fill:#34a853,stroke:#2a8642,color:#fff
```

**1. Filtering (Predicates)**: ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ì§€ ëª»í•˜ëŠ” ë…¸ë“œë¥¼ ì œì™¸
- ë¦¬ì†ŒìŠ¤ ë¶€ì¡± (CPU, Memory)
- Taints/Tolerations ë¶ˆì¼ì¹˜
- Node Selector ì¡°ê±´ ë¯¸ì¶©ì¡±
- Volume í† í´ë¡œì§€ ì œì•½ (EBS AZ-Pinning)
- Port ì¶©ëŒ

**2. Scoring (Priorities)**: ë‚¨ì€ ë…¸ë“œë“¤ì— ì ìˆ˜ë¥¼ ë§¤ê²¨ ìµœì ì˜ ë…¸ë“œ ì„ íƒ
- ë¦¬ì†ŒìŠ¤ ë°¸ëŸ°ìŠ¤ (ê· ë“± ì‚¬ìš©)
- Pod Affinity/Anti-Affinity ë§Œì¡±ë„
- ì´ë¯¸ì§€ ìºì‹œ ì¡´ì¬ ì—¬ë¶€
- Topology Spread ê· ë“±ë„
- ë…¸ë“œ Preference (PreferredDuringScheduling)

**3. Binding**: ìµœê³  ì ìˆ˜ ë…¸ë“œì— Podë¥¼ í• ë‹¹í•˜ê³  Kubeletì— í†µë³´

:::tip ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨ ë””ë²„ê¹…
Podê°€ `Pending` ìƒíƒœë¡œ ë‚¨ì•„ìˆë‹¤ë©´, `kubectl describe pod <pod-name>`ìœ¼ë¡œ Events ì„¹ì…˜ì„ í™•ì¸í•˜ì„¸ìš”. `Insufficient cpu`, `No nodes available`, `Taint not tolerated` ë“±ì˜ ë©”ì‹œì§€ë¡œ ì‹¤íŒ¨ ì›ì¸ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

### 2.2 ìŠ¤ì¼€ì¤„ë§ì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì†Œ

| ìš”ì†Œ | íƒ€ì… | ì˜í–¥ ë‹¨ê³„ | ê°•ì œì„± | ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€ |
|------|------|-----------|--------|---------------|
| **Node Selector** | Pod | Filtering | Hard | íŠ¹ì • ë…¸ë“œ íƒ€ì… ì§€ì • (GPU, ARM) |
| **Node Affinity** | Pod | Filtering/Scoring | Hard/Soft | ì„¸ë°€í•œ ë…¸ë“œ ì„ íƒ ì¡°ê±´ |
| **Pod Affinity** | Pod | Scoring | Hard/Soft | ê´€ë ¨ Podë¥¼ ê°€ê¹Œì´ ë°°ì¹˜ |
| **Pod Anti-Affinity** | Pod | Filtering/Scoring | Hard/Soft | Podë¥¼ ì„œë¡œ ë©€ë¦¬ ë°°ì¹˜ |
| **Taints/Tolerations** | Node + Pod | Filtering | Hard | ì „ìš© ë…¸ë“œ ê²©ë¦¬ |
| **Topology Spread** | Pod | Scoring | Hard/Soft | AZ/ë…¸ë“œ ê°„ ê· ë“± ë¶„ì‚° |
| **PriorityClass** | Pod | Preemption | Hard | ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ì„ ì  |
| **Resource Requests** | Pod | Filtering | Hard | ìµœì†Œ ë¦¬ì†ŒìŠ¤ ë³´ì¥ |
| **PDB** | Pod Group | Eviction | Hard | ìµœì†Œ ê°€ìš© Pod ë³´ì¥ |

**Hard vs Soft ì œì•½:**
- **Hard (Required)**: ì¡°ê±´ì„ ì¶©ì¡±í•˜ì§€ ëª»í•˜ë©´ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨ â†’ `Pending` ìƒíƒœ
- **Soft (Preferred)**: ì¡°ê±´ì„ ì„ í˜¸í•˜ì§€ë§Œ ì¶©ì¡±í•˜ì§€ ëª»í•´ë„ ìŠ¤ì¼€ì¤„ë§ ì§„í–‰ â†’ ì°¨ì„ ì±… í—ˆìš©

---

## 3. Node Affinity & Anti-Affinity

### 3.1 Node Selector (ê¸°ë³¸)

Node SelectorëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë…¸ë“œ ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ, ë ˆì´ë¸” ê¸°ë°˜ ì •í™•í•œ ì¼ì¹˜(exact match)ë§Œ ì§€ì›í•©ë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-workload
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ml-training
  template:
    metadata:
      labels:
        app: ml-training
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: g5.2xlarge
        workload-type: gpu
      containers:
      - name: trainer
        image: ml/trainer:v2.0
        resources:
          requests:
            nvidia.com/gpu: 1
```

**ì œí•œì‚¬í•­**: Node SelectorëŠ” `AND` ì¡°ê±´ë§Œ ì§€ì›í•˜ë©°, `OR`, `NOT`, ë¹„êµ ì—°ì‚°ì ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë³µì¡í•œ ì¡°ê±´ì´ í•„ìš”í•˜ë©´ Node Affinityë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

### 3.2 Node Affinity ìƒì„¸

Node AffinityëŠ” Node Selectorì˜ í™•ì¥ ë²„ì „ìœ¼ë¡œ, ë³µì¡í•œ ë…¼ë¦¬ ì¡°ê±´ê³¼ ì„ í˜¸ë„(preference)ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Required vs Preferred

| íƒ€ì… | ë™ì‘ | ì‚¬ìš© ì‹œê¸° |
|------|------|----------|
| `requiredDuringSchedulingIgnoredDuringExecution` | ì¡°ê±´ ì¶©ì¡± í•„ìˆ˜ (Hard) | ë°˜ë“œì‹œ íŠ¹ì • ë…¸ë“œì— ë°°ì¹˜í•´ì•¼ í•  ë•Œ |
| `preferredDuringSchedulingIgnoredDuringExecution` | ì¡°ê±´ ì„ í˜¸ (Soft, ê°€ì¤‘ì¹˜ ê¸°ë°˜) | ì„ í˜¸í•˜ì§€ë§Œ ëŒ€ì•ˆ í—ˆìš©í•  ë•Œ |

:::info IgnoredDuringExecutionì˜ ì˜ë¯¸
`IgnoredDuringExecution`ì€ Podê°€ **ì´ë¯¸ ì‹¤í–‰ ì¤‘**ì¼ ë•Œ ë…¸ë“œ ë ˆì´ë¸”ì´ ë³€ê²½ë˜ì–´ë„ Podë¥¼ Evictí•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ë¯¸ë˜ì— `RequiredDuringExecution`ì´ ë„ì…ë˜ë©´ ì‹¤í–‰ ì¤‘ì—ë„ ì¡°ê±´ ë¶ˆì¶©ì¡± ì‹œ ì¬ë°°ì¹˜ë©ë‹ˆë‹¤.
:::

#### ì—°ì‚°ì ì¢…ë¥˜

| ì—°ì‚°ì | ì„¤ëª… | ì˜ˆì‹œ |
|--------|------|------|
| `In` | ê°’ì´ ëª©ë¡ì— í¬í•¨ë¨ | `values: ["t3.xlarge", "t3.2xlarge"]` |
| `NotIn` | ê°’ì´ ëª©ë¡ì— í¬í•¨ë˜ì§€ ì•ŠìŒ | `values: ["t2.micro", "t2.small"]` |
| `Exists` | í‚¤ê°€ ì¡´ì¬í•¨ (ê°’ ë¬´ê´€) | ë ˆì´ë¸” ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸ |
| `DoesNotExist` | í‚¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ | íŠ¹ì • ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œ ì„ íƒ |
| `Gt` | ê°’ì´ í¬ë‹¤ (ìˆ«ì) | `values: ["100"]` (CPU ì½”ì–´ ìˆ˜ ë“±) |
| `Lt` | ê°’ì´ ì‘ë‹¤ (ìˆ«ì) | `values: ["10"]` |

#### ì‚¬ìš© ì‚¬ë¡€ë³„ YAML ì˜ˆì‹œ

**ì˜ˆì‹œ 1: GPU ë…¸ë“œì— ML ì›Œí¬ë¡œë“œ ë°°ì¹˜ (Hard)**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-training
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-training
  template:
    metadata:
      labels:
        app: ml-training
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - g5.xlarge
                - g5.2xlarge
                - g5.4xlarge
              - key: karpenter.sh/capacity-type
                operator: NotIn
                values:
                - spot  # GPU ì›Œí¬ë¡œë“œëŠ” Spot ì œì™¸
      containers:
      - name: trainer
        image: ml/trainer:v3.0
        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: "4"
            memory: 16Gi
```

**ì˜ˆì‹œ 2: ì¸ìŠ¤í„´ìŠ¤ íŒ¨ë°€ë¦¬ ì„ í˜¸ (Soft, ê°€ì¤‘ì¹˜)**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 6
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      affinity:
        nodeAffinity:
          # í•„ìˆ˜: On-Demand ë…¸ë“œë§Œ ì‚¬ìš©
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: karpenter.sh/capacity-type
                operator: In
                values:
                - on-demand
          # ì„ í˜¸: c7i > c6i > m6i ìˆœì„œ
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - c7i.xlarge
                - c7i.2xlarge
          - weight: 80
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - c6i.xlarge
                - c6i.2xlarge
          - weight: 50
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - m6i.xlarge
                - m6i.2xlarge
      containers:
      - name: api
        image: api-server:v2.5
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
```

**ì˜ˆì‹œ 3: íŠ¹ì • AZ ì§€ì • (ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸)**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: db-client
spec:
  replicas: 4
  selector:
    matchLabels:
      app: db-client
  template:
    metadata:
      labels:
        app: db-client
    spec:
      affinity:
        nodeAffinity:
          # RDS ì¸ìŠ¤í„´ìŠ¤ì™€ ê°™ì€ AZ (us-east-1a)ì— ë°°ì¹˜í•˜ì—¬ Cross-AZ ë¹„ìš© ì ˆê°
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-east-1a
      containers:
      - name: client
        image: db-client:v1.2
        env:
        - name: DB_ENDPOINT
          value: "mydb.us-east-1a.rds.amazonaws.com"
```

### 3.3 Node Anti-Affinity

Node Anti-AffinityëŠ” ëª…ì‹œì ì¸ ë¬¸ë²•ì´ ì—†ì§€ë§Œ, Node Affinityì˜ `NotIn`, `DoesNotExist` ì—°ì‚°ìë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: avoid-spot
spec:
  replicas: 3
  selector:
    matchLabels:
      app: critical-service
  template:
    metadata:
      labels:
        app: critical-service
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              # Spot ë…¸ë“œ íšŒí”¼
              - key: karpenter.sh/capacity-type
                operator: NotIn
                values:
                - spot
              # ARM ì•„í‚¤í…ì²˜ íšŒí”¼
              - key: kubernetes.io/arch
                operator: NotIn
                values:
                - arm64
      containers:
      - name: app
        image: critical-service:v1.0
```

---

## 4. Pod Affinity & Anti-Affinity

Pod Affinityì™€ Anti-AffinityëŠ” **Pod ê°„ì˜ ê´€ê³„**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ì¼€ì¤„ë§ ê²°ì •ì„ ë‚´ë¦½ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê´€ë ¨ëœ Podë“¤ì„ ê°€ê¹Œì´ ë°°ì¹˜í•˜ê±°ë‚˜(Affinity), ë©€ë¦¬ ë°°ì¹˜(Anti-Affinity)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 4.1 Pod Affinity

Pod AffinityëŠ” íŠ¹ì • Podê°€ ìˆëŠ” í† í´ë¡œì§€ ë„ë©”ì¸(ë…¸ë“œ, AZ, ë¦¬ì „)ì— ë‹¤ë¥¸ Podë¥¼ í•¨ê»˜ ë°°ì¹˜í•©ë‹ˆë‹¤.

**ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€:**
- **Cache Locality**: ìºì‹œ ì„œë²„ì™€ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°™ì€ ë…¸ë“œì— ë°°ì¹˜í•˜ì—¬ ë ˆì´í„´ì‹œ ìµœì†Œí™”
- **Data Locality**: ë°ì´í„° ì²˜ë¦¬ ì›Œí¬ë¡œë“œë¥¼ ë°ì´í„° ì†ŒìŠ¤ì™€ ê°€ê¹Œì´ ë°°ì¹˜
- **Communication Intensive**: ë¹ˆë²ˆí•˜ê²Œ í†µì‹ í•˜ëŠ” ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë¥¼ ê°™ì€ AZì— ë°°ì¹˜

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cache-client
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cache-client
  template:
    metadata:
      labels:
        app: cache-client
    spec:
      affinity:
        podAffinity:
          # Hard: Redis Podì™€ ê°™ì€ ë…¸ë“œì— ë°°ì¹˜ (ì´ˆì €ì§€ì—° ìš”êµ¬ì‚¬í•­)
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis
            topologyKey: kubernetes.io/hostname
      containers:
      - name: client
        image: cache-client:v1.0
```

**topologyKey ì„¤ëª…:**

| topologyKey | ë²”ìœ„ | ì„¤ëª… |
|-------------|------|------|
| `kubernetes.io/hostname` | ë…¸ë“œ | ê°™ì€ ë…¸ë“œì— ë°°ì¹˜ (ê°€ì¥ ê°•ë ¥í•œ co-location) |
| `topology.kubernetes.io/zone` | AZ | ê°™ì€ AZì— ë°°ì¹˜ |
| `topology.kubernetes.io/region` | ë¦¬ì „ | ê°™ì€ ë¦¬ì „ì— ë°°ì¹˜ |
| ì»¤ìŠ¤í…€ ë ˆì´ë¸” | ì‚¬ìš©ì ì •ì˜ | ì˜ˆ: `rack`, `datacenter` |

**Soft Affinity ì˜ˆì‹œ (ì„ í˜¸, ëŒ€ì•ˆ í—ˆìš©):**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
spec:
  replicas: 6
  selector:
    matchLabels:
      app: web-frontend
  template:
    metadata:
      labels:
        app: web-frontend
    spec:
      affinity:
        podAffinity:
          # Soft: API ì„œë²„ì™€ ê°™ì€ AZ ì„ í˜¸ (Cross-AZ ë¹„ìš© ì ˆê°)
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - api-server
              topologyKey: topology.kubernetes.io/zone
      containers:
      - name: frontend
        image: web-frontend:v2.0
```

### 4.2 Pod Anti-Affinity

Pod Anti-AffinityëŠ” íŠ¹ì • Podê°€ ìˆëŠ” í† í´ë¡œì§€ ë„ë©”ì¸ì— ë‹¤ë¥¸ Podë¥¼ ë°°ì¹˜í•˜ì§€ **ì•Šë„ë¡** í•©ë‹ˆë‹¤. ê³ ê°€ìš©ì„± í™•ë³´ì˜ í•µì‹¬ íŒ¨í„´ì…ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph "ë…¸ë“œ 1 (AZ-1a)"
        N1P1[replica-1<br/>app=api-server]
        N1P2[...]
    end

    subgraph "ë…¸ë“œ 2 (AZ-1b)"
        N2P1[replica-2<br/>app=api-server]
        N2P2[...]
    end

    subgraph "ë…¸ë“œ 3 (AZ-1c)"
        N3P1[replica-3<br/>app=api-server]
        N3P2[...]
    end

    subgraph "Pod Anti-Affinity ê·œì¹™"
        RULE[topologyKey: topology.kubernetes.io/zone<br/>app=api-server Podë¼ë¦¬ ë‹¤ë¥¸ AZì— ë°°ì¹˜]
    end

    RULE -.->|ì ìš©| N1P1
    RULE -.->|ì ìš©| N2P1
    RULE -.->|ì ìš©| N3P1

    style N1P1 fill:#34a853,stroke:#2a8642,color:#fff
    style N2P1 fill:#4286f4,stroke:#2a6acf,color:#fff
    style N3P1 fill:#fbbc04,stroke:#c99603,color:#000
    style RULE fill:#ff9900,stroke:#cc7a00,color:#fff
```

#### Hard Anti-Affinity (ì¥ì•  ë„ë©”ì¸ ê²©ë¦¬)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 6
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      affinity:
        podAntiAffinity:
          # Hard: ê° ë…¸ë“œì— ìµœëŒ€ 1ê°œ replicaë§Œ ë°°ì¹˜ (ë…¸ë“œ ì¥ì•  ê²©ë¦¬)
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - api-server
            topologyKey: kubernetes.io/hostname
      containers:
      - name: api
        image: api-server:v3.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
```

:::warning Hard Anti-Affinity ì£¼ì˜ì‚¬í•­
Hard Anti-Affinityë¥¼ `kubernetes.io/hostname`ì— ì ìš©í•˜ë©´, replica ìˆ˜ê°€ ë…¸ë“œ ìˆ˜ë³´ë‹¤ ë§ì„ ë•Œ ì¼ë¶€ Podê°€ `Pending` ìƒíƒœë¡œ ë‚¨ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë…¸ë“œ 3ê°œì— replica 5ê°œë¥¼ ë°°í¬í•˜ë©´ 2ê°œê°€ ìŠ¤ì¼€ì¤„ë§ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ê²½ìš° Soft Anti-Affinityë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
:::

#### Soft Anti-Affinity (ê¶Œì¥ íŒ¨í„´)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker
spec:
  replicas: 10
  selector:
    matchLabels:
      app: worker
  template:
    metadata:
      labels:
        app: worker
    spec:
      affinity:
        podAntiAffinity:
          # Soft: ê°€ëŠ¥í•œ í•œ ë‹¤ë¥¸ ë…¸ë“œì— ë¶„ì‚° ë°°ì¹˜ (ìœ ì—°ì„± í™•ë³´)
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - worker
              topologyKey: kubernetes.io/hostname
      containers:
      - name: worker
        image: worker:v2.1
        resources:
          requests:
            cpu: "500m"
            memory: 1Gi
```

#### Hard vs Soft ì„ íƒ ê¸°ì¤€

| ì‹œë‚˜ë¦¬ì˜¤ | ê¶Œì¥ | ì´ìœ  |
|---------|------|------|
| replica ìˆ˜ â‰¤ ë…¸ë“œ ìˆ˜ | Hard | ê° ë…¸ë“œì— ì •í™•íˆ 1ê°œì”© ë°°ì¹˜ ê°€ëŠ¥ |
| replica ìˆ˜ > ë…¸ë“œ ìˆ˜ | Soft | ì¼ë¶€ ë…¸ë“œì— 2ê°œ ì´ìƒ ë°°ì¹˜ í—ˆìš© |
| ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬ ì„œë¹„ìŠ¤ | Hard (AZ ë ˆë²¨) | ì¥ì•  ë„ë©”ì¸ ì™„ì „ ê²©ë¦¬ |
| ì¼ë°˜ ì›Œí¬ë¡œë“œ | Soft | ìŠ¤ì¼€ì¤„ë§ ìœ ì—°ì„± í™•ë³´ |
| ë¹ ë¥¸ ìŠ¤ì¼€ì¼ë§ í•„ìš” | Soft | Pending ìƒíƒœ ë°©ì§€ |

### 4.3 Affinity/Anti-Affinity vs Topology Spread ë¹„êµ

| ë¹„êµ í•­ëª© | Pod Anti-Affinity | Topology Spread Constraints |
|----------|-------------------|----------------------------|
| **ëª©ì ** | Pod ê°„ ë¶„ë¦¬ | Pod ê· ë“± ë¶„ì‚° |
| **ì„¸ë°€í•¨** | Pod ë‹¨ìœ„ ì œì–´ | ë„ë©”ì¸ ê°„ ê· í˜• ì œì–´ |
| **ë³µì¡ì„±** | ë‚®ìŒ | ì¤‘ê°„ |
| **ìœ ì—°ì„±** | Hard/Soft ì„ íƒ | maxSkewë¡œ í—ˆìš© ë²”ìœ„ ì œì–´ |
| **ì£¼ìš” ì‚¬ìš©** | ê°™ì€ ì•± replica ë¶„ë¦¬ | ì—¬ëŸ¬ ì•±ì˜ ì „ì²´ ê· í˜• |
| **AZ ë¶„ì‚°** | ê°€ëŠ¥ | ë” ì •êµí•¨ (minDomains) |
| **ë…¸ë“œ ë¶„ì‚°** | ê°€ëŠ¥ | ë” ì •êµí•¨ (maxSkew) |
| **ê¶Œì¥ ì¡°í•©** | Topology Spread (AZ) + Anti-Affinity (ë…¸ë“œ) | |

:::info Topology Spread Constraints ì°¸ê³ 
Topology Spread ConstraintsëŠ” Pod Anti-Affinityë³´ë‹¤ ë” ì •êµí•œ ë¶„ì‚° ì œì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ê³¼ YAML ì˜ˆì‹œëŠ” [EKS ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜ ê°€ì´ë“œ](/docs/operations-observability/eks-resiliency-guide#pod-topology-spread-constraints)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
:::

#### 4.3.1 Topology Spread Constraints ì‹¤ì „ íŒ¨í„´

Topology Spread ConstraintsëŠ” ë³µì¡í•œ ë¶„ì‚° ìš”êµ¬ì‚¬í•­ì„ ìš°ì•„í•˜ê²Œ í•´ê²°í•©ë‹ˆë‹¤. ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” íŒ¨í„´ì„ YAMLê³¼ í•¨ê»˜ ì†Œê°œí•©ë‹ˆë‹¤.

##### íŒ¨í„´ 1: Multi-AZ ê· ë“± ë¶„ë°° (ê¸°ë³¸)

ê°€ì¥ ì¼ë°˜ì ì¸ íŒ¨í„´ìœ¼ë¡œ, ëª¨ë“  replicaë¥¼ AZ ê°„ì— ê· ë“±í•˜ê²Œ ë¶„ì‚°ì‹œí‚µë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-az-app
  namespace: production
spec:
  replicas: 9
  selector:
    matchLabels:
      app: multi-az-app
  template:
    metadata:
      labels:
        app: multi-az-app
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: multi-az-app
      containers:
      - name: app
        image: myapp:v1.0
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
```

**ë™ì‘ ë°©ì‹:**
- `maxSkew: 1`: AZ ê°„ Pod ìˆ˜ ì°¨ì´ê°€ ìµœëŒ€ 1ê°œê¹Œì§€ í—ˆìš©
- 9ê°œ replica â†’ us-east-1a(3), us-east-1b(3), us-east-1c(3)
- `whenUnsatisfiable: DoNotSchedule`: ì¡°ê±´ ìœ„ë°˜ ì‹œ Podë¥¼ Pending ìƒíƒœë¡œ ìœ ì§€

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬ ì„œë¹„ìŠ¤ì˜ AZ ì¥ì•  ëŒ€ì‘
- í´ë¼ì´ì–¸íŠ¸ íŠ¸ë˜í”½ì´ ëª¨ë“  AZì—ì„œ ê· ë“±í•˜ê²Œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°
- ë°ì´í„°ì„¼í„° ìˆ˜ì¤€ì˜ ì¥ì•  ê²©ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°

##### íŒ¨í„´ 2: minDomains í™œìš© (ìµœì†Œ AZ ë³´ì¥)

`minDomains`ëŠ” Podê°€ ë°˜ë“œì‹œ ë¶„ì‚°ë˜ì–´ì•¼ í•˜ëŠ” ìµœì†Œ ë„ë©”ì¸(AZ) ìˆ˜ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤. AZ ì¶•ì†Œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ Podê°€ í•œ ê³³ìœ¼ë¡œ ë°€ë¦¬ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ha-critical-service
  namespace: production
spec:
  replicas: 6
  selector:
    matchLabels:
      app: ha-critical-service
      tier: critical
  template:
    metadata:
      labels:
        app: ha-critical-service
        tier: critical
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        minDomains: 3  # ë°˜ë“œì‹œ 3ê°œ AZì— ë¶„ì‚°
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: ha-critical-service
      containers:
      - name: service
        image: critical-service:v2.5
        resources:
          requests:
            cpu: "1"
            memory: 1Gi
          limits:
            cpu: "2"
            memory: 2Gi
```

**ë™ì‘ ë°©ì‹:**
- `minDomains: 3`: ìµœì†Œ 3ê°œ AZì— Pod ë¶„ì‚° ë³´ì¥
- 6ê°œ replica â†’ ê° AZì— ìµœì†Œ 2ê°œì”© ë°°ì¹˜
- íŠ¹ì • AZê°€ ë¦¬ì†ŒìŠ¤ ë¶€ì¡±ì´ì–´ë„, ë‹¤ë¥¸ AZë¡œë§Œ ëª°ë¦¬ì§€ ì•ŠìŒ

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- ê¸ˆìœµ, ê²°ì œ ì‹œìŠ¤í…œ ë“± ì´ˆê³ ê°€ìš©ì„± ìš”êµ¬ ì„œë¹„ìŠ¤
- SLA 99.99% ì´ìƒ ë³´ì¥ í•„ìš” ì‹œ
- AZ ì¶•ì†Œ(Zonal Shift) ì¤‘ì—ë„ ìµœì†Œ ê°€ìš©ì„± ìœ ì§€

:::warning minDomains ì„¤ì • ì‹œ ì£¼ì˜ì‚¬í•­
`minDomains`ë¥¼ ì„¤ì •í•˜ë©´ í•´ë‹¹ ìˆ˜ë§Œí¼ì˜ ë„ë©”ì¸ì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë¦¬ì†ŒìŠ¤ê°€ ë¶€ì¡±í•  ê²½ìš°, Podê°€ Pending ìƒíƒœë¡œ ë‚¨ìŠµë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ì— ì‹¤ì œë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ AZ ìˆ˜ë¥¼ í™•ì¸ í›„ ì„¤ì •í•˜ì„¸ìš”.
:::

##### íŒ¨í„´ 3: Anti-Affinity + Topology Spread ì¡°í•©

ê°™ì€ ë…¸ë“œì— replicaë¥¼ 2ê°œ ì´ìƒ ë°°ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ì„œ, ë™ì‹œì— AZ ê°„ ê· ë“± ë¶„ë°°ë¥¼ ë³´ì¥í•˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: combined-constraints-app
  namespace: production
spec:
  replicas: 12
  selector:
    matchLabels:
      app: combined-app
  template:
    metadata:
      labels:
        app: combined-app
        version: v3.0
    spec:
      # 1. Topology Spread: AZ ê°„ ê· ë“± ë¶„ì‚° (Hard)
      topologySpreadConstraints:
      - maxSkew: 1
        minDomains: 3
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: combined-app

      # 2. Anti-Affinity: ë…¸ë“œ ê°„ ë¶„ì‚° (Hard)
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - combined-app
            topologyKey: kubernetes.io/hostname

      containers:
      - name: app
        image: combined-app:v3.0
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
```

**ë™ì‘ ë°©ì‹:**
- **Level 1 (AZ)**: 12ê°œ replica â†’ ê° AZì— 4ê°œì”© ê· ë“± ë°°ì¹˜
- **Level 2 (Node)**: ê° ë…¸ë“œì— ìµœëŒ€ 1ê°œ Podë§Œ ë°°ì¹˜

**íš¨ê³¼:**
- ë…¸ë“œ ì¥ì•  ì‹œ ìµœëŒ€ 1ê°œ Podë§Œ ì˜í–¥
- AZ ì¥ì•  ì‹œ ìµœëŒ€ 4ê°œ Podë§Œ ì˜í–¥
- ì´ 12ê°œ ì¤‘ 8ê°œ(66.7%) í•­ìƒ ê°€ìš©

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- ë‹¨ì¼ ì¥ì• ì (Single Point of Failure) ì™„ì „ ì œê±°
- í•˜ë“œì›¨ì–´ ì¥ì• ì™€ ë°ì´í„°ì„¼í„° ì¥ì•  ëª¨ë‘ ëŒ€ì‘
- ê³ íŠ¸ë˜í”½ API ì„œë²„, ê²°ì œ ê²Œì´íŠ¸ì›¨ì´

##### íŒ¨í„´ 4: ë‹¤ì¤‘ Topology Spread (Zone + Node)

í•˜ë‚˜ì˜ Pod Specì—ì„œ ì—¬ëŸ¬ í† í´ë¡œì§€ ë ˆë²¨ì˜ ë¶„ì‚°ì„ ë™ì‹œì— ì œì–´í•©ë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-level-spread
  namespace: production
spec:
  replicas: 18
  selector:
    matchLabels:
      app: multi-level-app
  template:
    metadata:
      labels:
        app: multi-level-app
    spec:
      topologySpreadConstraints:
      # ì œì•½ 1: AZ ë ˆë²¨ ë¶„ì‚° (Hard)
      - maxSkew: 1
        minDomains: 3
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: multi-level-app

      # ì œì•½ 2: ë…¸ë“œ ë ˆë²¨ ë¶„ì‚° (Soft)
      - maxSkew: 2
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: multi-level-app

      containers:
      - name: app
        image: multi-level-app:v1.5
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
```

**ë™ì‘ ë°©ì‹:**
- **1ë‹¨ê³„ (AZ)**: 18ê°œ â†’ us-east-1a(6), us-east-1b(6), us-east-1c(6)
- **2ë‹¨ê³„ (Node)**: ê° AZ ë‚´ì—ì„œ ë…¸ë“œë‹¹ Pod ìˆ˜ ì°¨ì´ ìµœëŒ€ 2ê°œ
- Node ì œì•½ì€ Soft(`ScheduleAnyway`)ë¡œ ì„¤ì •í•˜ì—¬ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨ ë°©ì§€

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- ëŒ€ê·œëª¨ replica(10ê°œ ì´ìƒ) ë°°í¬
- ë…¸ë“œ ìˆ˜ê°€ ìœ ë™ì ì¸ í™˜ê²½ (Karpenter ì˜¤í† ìŠ¤ì¼€ì¼ë§)
- AZ ë¶„ì‚°ì€ í•„ìˆ˜, ë…¸ë“œ ë¶„ì‚°ì€ ì„ í˜¸í•˜ëŠ” ê²½ìš°

##### íŒ¨í„´ ë¹„êµí‘œ

| íŒ¨í„´ | maxSkew | minDomains | whenUnsatisfiable | ì¶”ê°€ ì œì•½ | ë³µì¡ë„ | ê¶Œì¥ Replica ìˆ˜ |
|------|---------|------------|-------------------|----------|--------|----------------|
| **íŒ¨í„´ 1: ê¸°ë³¸ Multi-AZ** | 1 | - | DoNotSchedule | ì—†ìŒ | ë‚®ìŒ | 3~12 |
| **íŒ¨í„´ 2: minDomains** | 1 | 3 | DoNotSchedule | ì—†ìŒ | ì¤‘ê°„ | 6~20 |
| **íŒ¨í„´ 3: Anti-Affinity ì¡°í•©** | 1 | 3 | DoNotSchedule | Hard Anti-Affinity | ë†’ìŒ | 12~50 |
| **íŒ¨í„´ 4: ë‹¤ì¤‘ Spread** | 1, 2 | 3 | Mixed | 2ë‹¨ê³„ Topology | ë†’ìŒ | 15+ |

##### íŠ¸ëŸ¬ë¸”ìŠˆíŒ…: Topology Spread ì‹¤íŒ¨ ì›ì¸

| ì¦ìƒ | ì›ì¸ | í•´ê²° ë°©ë²• |
|------|------|----------|
| Podê°€ Pending ìƒíƒœ | `maxSkew` ì´ˆê³¼ ë˜ëŠ” `minDomains` ë¯¸ì¶©ì¡± | `kubectl describe pod`ë¡œ Events í™•ì¸, replica ìˆ˜ ì¡°ì • ë˜ëŠ” ë…¸ë“œ ì¶”ê°€ |
| íŠ¹ì • AZì—ë§Œ Pod ì§‘ì¤‘ | `whenUnsatisfiable: ScheduleAnyway` ì‚¬ìš© | `DoNotSchedule`ë¡œ ë³€ê²½í•˜ì—¬ Hard ì œì•½ ì ìš© |
| ì‹ ê·œ AZ ì¶”ê°€ ì‹œ ì¬ë°°ì¹˜ ì•ˆë¨ | ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ê¸°ì¡´ Pod ì¬ë°°ì¹˜ ì•ˆí•¨ | Descheduler ì‚¬ìš© ë˜ëŠ” Rolling Restart |
| `minDomains` ì„¤ì • í›„ ëª¨ë“  Pod Pending | í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹ ìˆ˜ì˜ AZ ì—†ìŒ | ì‹¤ì œ AZ ìˆ˜ì— ë§ì¶° `minDomains` ì¡°ì • |

:::tip Topology Spread ë””ë²„ê¹… ëª…ë ¹ì–´
```bash
# Podê°€ ë°°ì¹˜ëœ AZ ë¶„í¬ í™•ì¸
kubectl get pods -n production -l app=multi-az-app \
  -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,ZONE:.spec.nodeSelector.topology\.kubernetes\.io/zone

# ë…¸ë“œë³„ Pod ìˆ˜ í™•ì¸
kubectl get pods -A -o wide --no-headers | \
  awk '{print $8}' | sort | uniq -c | sort -rn
```
:::

**ê¶Œì¥ ì¡°í•© íŒ¨í„´:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: best-practice-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: best-practice-app
  template:
    metadata:
      labels:
        app: best-practice-app
    spec:
      # Topology Spread: AZ ê°„ ê· ë“± ë¶„ì‚° (Hard)
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: best-practice-app
        minDomains: 3
      # Anti-Affinity: ë…¸ë“œ ê°„ ë¶„ì‚° (Soft)
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - best-practice-app
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: app:v1.0
```

---

## 5. Taints & Tolerations

Taintsì™€ TolerationsëŠ” **ë…¸ë“œ ìˆ˜ì¤€ì˜ íšŒí”¼(repel) ë©”ì»¤ë‹ˆì¦˜**ì…ë‹ˆë‹¤. ë…¸ë“œì— Taintë¥¼ ì ìš©í•˜ë©´, í•´ë‹¹ Taintë¥¼ Tolerateí•˜ëŠ” Podë§Œ ìŠ¤ì¼€ì¤„ë§ë©ë‹ˆë‹¤.

**ê°œë…:**
- **Taint**: ë…¸ë“œì— ì ìš© (ì˜ˆ: "ì´ ë…¸ë“œëŠ” GPU ì „ìš©ì…ë‹ˆë‹¤")
- **Toleration**: Podì— ì ìš© (ì˜ˆ: "ë‚˜ëŠ” GPU ë…¸ë“œë¥¼ Tolerateí•©ë‹ˆë‹¤")

### 5.1 Taint íš¨ê³¼ (Effect)

| Effect | ë™ì‘ | ê¸°ì¡´ Pod ì˜í–¥ | ì‚¬ìš© ì‹œê¸° |
|--------|------|--------------|----------|
| `NoSchedule` | ìƒˆ Pod ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨ | ê¸°ì¡´ Pod ìœ ì§€ | ì‹ ê·œ ì „ìš© ë…¸ë“œ ìƒì„± ì‹œ |
| `PreferNoSchedule` | ê°€ëŠ¥í•˜ë©´ ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨ (Soft) | ê¸°ì¡´ Pod ìœ ì§€ | ì„ í˜¸ íšŒí”¼ (ëŒ€ì•ˆ í—ˆìš©) |
| `NoExecute` | ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨ + ê¸°ì¡´ Pod Evict | ê¸°ì¡´ Pod ì¦‰ì‹œ Evict | ë…¸ë“œ ìœ ì§€ë³´ìˆ˜, ê¸´ê¸‰ ëŒ€í”¼ |

**Taint ì ìš© ëª…ë ¹ì–´:**

```bash
# NoSchedule: ì‹ ê·œ Pod ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨
kubectl taint nodes node1 workload-type=gpu:NoSchedule

# NoExecute: ì‹ ê·œ ì°¨ë‹¨ + ê¸°ì¡´ Pod Evict
kubectl taint nodes node1 maintenance=true:NoExecute

# Taint ì œê±° (ë§ˆì§€ë§‰ì— '-' ì¶”ê°€)
kubectl taint nodes node1 workload-type=gpu:NoSchedule-
```

### 5.2 ì¼ë°˜ì ì¸ Taint íŒ¨í„´

#### íŒ¨í„´ 1: ì „ìš© ë…¸ë“œ ê·¸ë£¹ (GPU, High-Memory)

```yaml
# ë…¸ë“œì— Taint ì ìš© (kubectl ë˜ëŠ” Karpenter)
# kubectl taint nodes gpu-node-1 nvidia.com/gpu=present:NoSchedule

# GPU Podê°€ Toleration ì„ ì–¸
apiVersion: v1
kind: Pod
metadata:
  name: gpu-job
spec:
  tolerations:
  - key: nvidia.com/gpu
    operator: Equal
    value: present
    effect: NoSchedule
  nodeSelector:
    node.kubernetes.io/instance-type: g5.2xlarge
  containers:
  - name: trainer
    image: ml/trainer:v1.0
    resources:
      limits:
        nvidia.com/gpu: 1
```

#### íŒ¨í„´ 2: ì‹œìŠ¤í…œ ì›Œí¬ë¡œë“œ ê²©ë¦¬

```yaml
# Karpenterë¡œ ì‹œìŠ¤í…œ ì „ìš© NodePool ìƒì„±
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: system-pool
spec:
  template:
    spec:
      requirements:
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["c6i.large", "c6i.xlarge"]
      taints:
      - key: workload-type
        value: system
        effect: NoSchedule
  limits:
    cpu: "20"
---
# ì‹œìŠ¤í…œ DaemonSet (ëª¨ë‹ˆí„°ë§ ì—ì´ì „íŠ¸)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-agent
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      tolerations:
      - key: workload-type
        operator: Equal
        value: system
        effect: NoSchedule
      # ëª¨ë“  ë…¸ë“œì— ë°°í¬ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ê¸°ë³¸ Taintsë„ Tolerate
      - key: node.kubernetes.io/not-ready
        operator: Exists
        effect: NoExecute
      - key: node.kubernetes.io/unreachable
        operator: Exists
        effect: NoExecute
      containers:
      - name: agent
        image: monitoring-agent:v2.0
```

#### íŒ¨í„´ 3: ë…¸ë“œ ìœ ì§€ë³´ìˆ˜ (Drain ì¤€ë¹„)

```bash
# Step 1: ë…¸ë“œì— NoExecute Taint ì ìš©
kubectl taint nodes node-1 maintenance=true:NoExecute

# ê²°ê³¼: Toleration ì—†ëŠ” ëª¨ë“  Podê°€ ì¦‰ì‹œ Evictë˜ê³  ë‹¤ë¥¸ ë…¸ë“œë¡œ ì´ë™
# PDBê°€ ì„¤ì •ëœ ê²½ìš°, minAvailableì„ ì¡´ì¤‘í•˜ë©° ìˆœì°¨ì ìœ¼ë¡œ Evict

# Step 2: ìœ ì§€ë³´ìˆ˜ ì™„ë£Œ í›„ Taint ì œê±°
kubectl taint nodes node-1 maintenance=true:NoExecute-
kubectl uncordon node-1
```

### 5.3 Toleration ì„¤ì •

#### Operator: Equal vs Exists

```yaml
# Equal: ì •í™•í•œ key=value ì¼ì¹˜ í•„ìš”
tolerations:
- key: workload-type
  operator: Equal
  value: gpu
  effect: NoSchedule

# Exists: keyë§Œ ì¡´ì¬í•˜ë©´ ë¨ (value ë¬´ì‹œ)
tolerations:
- key: workload-type
  operator: Exists
  effect: NoSchedule

# ëª¨ë“  Taint Tolerate (DaemonSet ë“±)
tolerations:
- operator: Exists
```

#### tolerationSeconds (NoExecute ì „ìš©)

`NoExecute` Taintê°€ ì ìš©ë˜ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ì¦‰ì‹œ Evictë˜ì§€ë§Œ, `tolerationSeconds`ë¡œ ìœ ì˜ˆ ì‹œê°„ì„ ë¶€ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resilient-app
spec:
  tolerations:
  # ë…¸ë“œê°€ NotReady ìƒíƒœê°€ ë˜ì–´ë„ 300ì´ˆ ë™ì•ˆ ìœ ì§€ (ì¼ì‹œì  ì¥ì•  ëŒ€ì‘)
  - key: node.kubernetes.io/not-ready
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 300
  # ë…¸ë“œê°€ Unreachable ìƒíƒœê°€ ë˜ì–´ë„ 300ì´ˆ ë™ì•ˆ ìœ ì§€
  - key: node.kubernetes.io/unreachable
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 300
  containers:
  - name: app
    image: app:v1.0
```

**ê¸°ë³¸ê°’**: KubernetesëŠ” `tolerationSeconds` ë¯¸ì§€ì • ì‹œ ë‹¤ìŒ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
- `node.kubernetes.io/not-ready`: 300ì´ˆ
- `node.kubernetes.io/unreachable`: 300ì´ˆ

### 5.4 EKS ê¸°ë³¸ Taints

EKSëŠ” íŠ¹ì • ë…¸ë“œì— ìë™ìœ¼ë¡œ Taintë¥¼ ì ìš©í•©ë‹ˆë‹¤:

| Taint | ì ìš© ëŒ€ìƒ | íš¨ê³¼ | ëŒ€ì‘ ë°©ë²• |
|-------|----------|------|----------|
| `node.kubernetes.io/not-ready` | ì¤€ë¹„ë˜ì§€ ì•Šì€ ë…¸ë“œ | NoExecute | ìë™ Toleration (kubelet) |
| `node.kubernetes.io/unreachable` | ì—°ê²° ë¶ˆê°€ ë…¸ë“œ | NoExecute | ìë™ Toleration (kubelet) |
| `node.kubernetes.io/disk-pressure` | ë””ìŠ¤í¬ ë¶€ì¡± ë…¸ë“œ | NoSchedule | DaemonSetë§Œ Tolerate |
| `node.kubernetes.io/memory-pressure` | ë©”ëª¨ë¦¬ ë¶€ì¡± ë…¸ë“œ | NoSchedule | DaemonSetë§Œ Tolerate |
| `node.kubernetes.io/pid-pressure` | PID ë¶€ì¡± ë…¸ë“œ | NoSchedule | DaemonSetë§Œ Tolerate |
| `node.kubernetes.io/network-unavailable` | ë„¤íŠ¸ì›Œí¬ ë¯¸êµ¬ì„± ë…¸ë“œ | NoSchedule | CNI í”ŒëŸ¬ê·¸ì¸ì´ ì œê±° |

### 5.5 Karpenterì—ì„œ Taint ê´€ë¦¬

KarpenterëŠ” NodePoolì—ì„œ ì„ ì–¸ì ìœ¼ë¡œ Taintë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤:

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-pool
spec:
  template:
    spec:
      requirements:
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["g5.xlarge", "g5.2xlarge"]
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand"]
      # ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì‹œ ìë™ìœ¼ë¡œ Taint ì ìš©
      taints:
      - key: nvidia.com/gpu
        value: present
        effect: NoSchedule
      - key: workload-type
        value: ml
        effect: NoSchedule
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodes
  limits:
    cpu: "100"
    memory: 500Gi
```

Karpenterê°€ í”„ë¡œë¹„ì €ë‹í•˜ëŠ” ëª¨ë“  ë…¸ë“œì— ìë™ìœ¼ë¡œ Taintê°€ ì ìš©ë˜ë¯€ë¡œ, ìˆ˜ë™ìœ¼ë¡œ `kubectl taint` ëª…ë ¹ì„ ì‹¤í–‰í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.

### 5.6 Cluster Autoscalerì—ì„œ Karpenterë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜

Cluster Autoscalerì™€ KarpenterëŠ” ëª¨ë‘ ë…¸ë“œ ì˜¤í† ìŠ¤ì¼€ì¼ë§ì„ ì œê³µí•˜ì§€ë§Œ, ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œ ìŠ¤ì¼€ì¤„ë§ ë™ì‘ì˜ ì°¨ì´ì™€ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

#### 5.6.1 ìŠ¤ì¼€ì¤„ë§ ë™ì‘ ì°¨ì´

Cluster Autoscalerì™€ Karpenterì˜ í•µì‹¬ ì°¨ì´ëŠ” **ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ë°©ì‹**ê³¼ **Pod ìŠ¤ì¼€ì¤„ë§ê³¼ì˜ í†µí•© ìˆ˜ì¤€**ì…ë‹ˆë‹¤.

##### ë™ì‘ ë¹„êµ

| ë¹„êµ í•­ëª© | Cluster Autoscaler | Karpenter |
|----------|-------------------|-----------|
| **íŠ¸ë¦¬ê±° ë°©ì‹** | Pending Pod ê°ì§€ â†’ ASG í™•ì¥ ìš”ì²­ | Pending Pod ê°ì§€ â†’ ì¦‰ì‹œ EC2 í”„ë¡œë¹„ì €ë‹ |
| **í™•ì¥ ì†ë„** | ìˆ˜ì‹­ ì´ˆ ~ ìˆ˜ ë¶„ (ASG ëŒ€ê¸° ì‹œê°„) | ìˆ˜ ì´ˆ (ì§ì ‘ EC2 API í˜¸ì¶œ) |
| **ë…¸ë“œ ì„ íƒ** | ë¯¸ë¦¬ ì •ì˜ëœ ASG ê·¸ë£¹ ì¤‘ ì„ íƒ | Pod ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ì‹¤ì‹œê°„ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ |
| **ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë‹¤ì–‘ì„±** | ASGë‹¹ ê³ ì •ëœ íƒ€ì… (LaunchTemplate) | 100+ íƒ€ì… ì¤‘ ìµœì  ì„ íƒ (NodePool ìš”êµ¬ì‚¬í•­) |
| **ë¹„ìš© ìµœì í™”** | ìˆ˜ë™ ASG ì„¤ì • í•„ìš” | ìë™ Spot/On-Demand ë¯¹ìŠ¤, ìµœì €ê°€ ì„ íƒ |
| **Bin Packing** | ì œí•œì  (ASG ë‹¨ìœ„) | ê³ ê¸‰ (Pod ìš”êµ¬ì‚¬í•­ ì¸ì‹) |
| **Taints/Tolerations ì¸ì‹** | ì œí•œì  | ë„¤ì´í‹°ë¸Œ í†µí•© |
| **Topology Spread ì¸ì‹** | ì œí•œì  | ë„¤ì´í‹°ë¸Œ í†µí•© |
| **í†µí•© ìˆ˜ì¤€** | Kubernetes ì™¸ë¶€ ë„êµ¬ | Kubernetes ë„¤ì´í‹°ë¸Œ (CRD ê¸°ë°˜) |

##### í™•ì¥ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ

**ì‹œë‚˜ë¦¬ì˜¤: GPUë¥¼ ìš”ì²­í•˜ëŠ” Pod 3ê°œ ìƒì„±**

**Cluster Autoscaler ë™ì‘:**
```
1. Pod 3ê°œ Pending ìƒíƒœ (GPU ìš”ì²­)
2. Cluster Autoscalerê°€ 10ì´ˆë§ˆë‹¤ Pending Pod ìŠ¤ìº”
3. GPU ASGë¥¼ ì°¾ì•„ í™•ì¥ ìš”ì²­ (ì˜ˆ: g5.2xlarge ASG)
4. AWS ASGê°€ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì‹œì‘ (30~90ì´ˆ)
5. ë…¸ë“œ Ready í›„ kubeletì´ Pod ìŠ¤ì¼€ì¤„ë§
6. ì´ ì†Œìš” ì‹œê°„: 1~2ë¶„
```

**Karpenter ë™ì‘:**
```
1. Pod 3ê°œ Pending ìƒíƒœ (GPU ìš”ì²­)
2. Karpenterê°€ ì¦‰ì‹œ ê°ì§€ (1~2ì´ˆ)
3. NodePool ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ìµœì  ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ (g5.xlarge, g5.2xlarge ì¤‘)
4. ì§ì ‘ EC2 RunInstances API í˜¸ì¶œ
5. ë…¸ë“œ Ready í›„ Pod ìŠ¤ì¼€ì¤„ë§
6. ì´ ì†Œìš” ì‹œê°„: 30~45ì´ˆ
```

##### ë¹„ìš© ìµœì í™” ì°¨ì´

**Cluster Autoscaler:**
- ASGë³„ë¡œ Spot/On-Demand ë¶„ë¦¬ ì„¤ì • í•„ìš”
- ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë³€ê²½ ì‹œ LaunchTemplate ìˆ˜ë™ ì—…ë°ì´íŠ¸
- ê³¼ë„í•œ í”„ë¡œë¹„ì €ë‹(over-provisioning) ë°œìƒ ê°€ëŠ¥

**Karpenter:**
- NodePoolì—ì„œ Spot/On-Demand ìš°ì„ ìˆœìœ„ ì„ ì–¸ì  ì„¤ì •
- ì‹¤ì‹œê°„ìœ¼ë¡œ ê°€ì¥ ì €ë ´í•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ
- Pod ìš”êµ¬ì‚¬í•­ì— ì •í™•íˆ ë§ëŠ” ë…¸ë“œ í”„ë¡œë¹„ì €ë‹

**ë¹„ìš© ì ˆê° ì˜ˆì‹œ (ì‹¤ì¸¡ ë°ì´í„°):**
```yaml
# Cluster Autoscaler: ê³ ì • ASG
# m5.2xlarge (8 vCPU, 32GB) â†’ $0.384/ì‹œê°„
# â†’ Podê°€ 2 vCPUë§Œ ìš”ì²­í•´ë„ ì „ì²´ ë…¸ë“œ ë¹„ìš© ë¶€ë‹´

# Karpenter: ìœ ì—°í•œ ì„ íƒ
# m5.large (2 vCPU, 8GB) â†’ $0.096/ì‹œê°„
# â†’ Pod ìš”êµ¬ì‚¬í•­ì— ë§ì¶° ì‘ì€ ë…¸ë“œ ì„ íƒ
# â†’ 75% ë¹„ìš© ì ˆê°
```

#### 5.6.2 ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

Cluster Autoscalerì—ì„œ Karpenterë¡œì˜ ì•ˆì „í•œ ì „í™˜ì„ ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œì…ë‹ˆë‹¤.

##### 1ë‹¨ê³„: NodePool ì •ì˜ (ASG â†’ NodePool ë§¤í•‘)

ê¸°ì¡´ ASG ì„¤ì •ì„ Karpenter NodePool CRDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

**ê¸°ì¡´ Cluster Autoscaler ì„¤ì •:**
```yaml
# ASG: eks-general-purpose-asg
# - ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: m5.xlarge, m5.2xlarge
# - ìš©ëŸ‰ íƒ€ì…: On-Demand
# - AZ: us-east-1a, us-east-1b, us-east-1c
```

**Karpenter NodePool ë³€í™˜:**
```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general-purpose
spec:
  template:
    spec:
      requirements:
      # ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: ASG LaunchTemplateì—ì„œ ê°€ì ¸ì˜´
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["m5.xlarge", "m5.2xlarge", "m5a.xlarge", "m5a.2xlarge"]

      # ìš©ëŸ‰ íƒ€ì…: On-Demand ìš°ì„ , Spot í—ˆìš©
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]

      # AZ: ê¸°ì¡´ ASG AZ ìœ ì§€
      - key: topology.kubernetes.io/zone
        operator: In
        values: ["us-east-1a", "us-east-1b", "us-east-1c"]

      # ì•„í‚¤í…ì²˜: x86_64ë§Œ (ARM ì œì™¸)
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]

      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default

  # ë¦¬ì†ŒìŠ¤ ì œí•œ: ASG Max Size ê¸°ë°˜
  limits:
    cpu: "1000"
    memory: 1000Gi

  # í†µí•© ì •ì±…: Consolidation í™œì„±í™”
  disruption:
    consolidationPolicy: WhenUnderutilized
    expireAfter: 720h  # 30ì¼
```

**ë³€í™˜ ê°€ì´ë“œ:**

| ASG ì„¤ì • | NodePool í•„ë“œ | ë¹„ê³  |
|---------|--------------|------|
| LaunchTemplate ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | `requirements[instance-type]` | ë” ë„“ì€ ë²”ìœ„ ê¶Œì¥ (ë¹„ìš© ìµœì í™”) |
| Spot/On-Demand | `requirements[capacity-type]` | ìš°ì„ ìˆœìœ„ ë°°ì—´ë¡œ ë³€ê²½ |
| Subnets (AZ) | `requirements[zone]` | SubnetSelectorë¡œë„ ê°€ëŠ¥ |
| Max Size | `limits.cpu`, `limits.memory` | vCPU/ë©”ëª¨ë¦¬ ì´í•©ìœ¼ë¡œ í™˜ì‚° |
| Tags | `EC2NodeClass.tags` | ë³´ì•ˆ, ë¹„ìš© ì¶”ì ìš© íƒœê·¸ |

##### 2ë‹¨ê³„: Taints/Tolerations í˜¸í™˜ì„± í™•ì¸

ê¸°ì¡´ ASGì— ì ìš©ëœ Taintsë¥¼ NodePoolì—ì„œë„ ë™ì¼í•˜ê²Œ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

**ê¸°ì¡´ ASG Taint (UserData ìŠ¤í¬ë¦½íŠ¸):**
```bash
# /etc/eks/bootstrap.sh ì˜µì…˜
--kubelet-extra-args '--register-with-taints=workload-type=batch:NoSchedule'
```

**Karpenter NodePool Taint:**
```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: batch-workload
spec:
  template:
    spec:
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot"]  # BatchëŠ” Spot ì‚¬ìš©

      # Taint ì ìš©: ê¸°ì¡´ ASGì™€ ë™ì¼í•˜ê²Œ
      taints:
      - key: workload-type
        value: batch
        effect: NoSchedule
```

**ê²€ì¦ ëª…ë ¹ì–´:**
```bash
# ê¸°ì¡´ ASG ë…¸ë“œì˜ Taints í™•ì¸
kubectl get nodes -l eks.amazonaws.com/nodegroup=batch-asg \
  -o jsonpath='{.items[*].spec.taints}' | jq

# Karpenter ë…¸ë“œì˜ Taints í™•ì¸
kubectl get nodes -l karpenter.sh/nodepool=batch-workload \
  -o jsonpath='{.items[*].spec.taints}' | jq

# ì¼ì¹˜ ì—¬ë¶€ í™•ì¸
```

##### 3ë‹¨ê³„: PDB ê²€ì¦ (ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì¤‘ë‹¨ ìµœì†Œí™”)

ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ Pod ì¤‘ë‹¨ì„ ìµœì†Œí™”í•˜ë ¤ë©´ PodDisruptionBudgetì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

**PDB ì„¤ì • í™•ì¸:**
```bash
# ëª¨ë“  PDB ì¡°íšŒ
kubectl get pdb -A

# íŠ¹ì • PDB ìƒì„¸ í™•ì¸
kubectl describe pdb api-server-pdb -n production
```

**ê¶Œì¥ PDB ì„¤ì • (ë§ˆì´ê·¸ë ˆì´ì…˜ìš©):**
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
  namespace: production
spec:
  minAvailable: 2  # ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ìµœì†Œ 2ê°œ ìœ ì§€
  selector:
    matchLabels:
      app: critical-app
```

**ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] ëª¨ë“  í”„ë¡œë•ì…˜ ì›Œí¬ë¡œë“œì— PDB ì„¤ì • í™•ì¸
- [ ] `minAvailable` ë˜ëŠ” `maxUnavailable` ì ì ˆíˆ ì„¤ì •
- [ ] StatefulSetì€ ì¶”ê°€ ì£¼ì˜ (ìˆœì°¨ ì¢…ë£Œ í™•ì¸)

##### 4ë‹¨ê³„: Topology Spread ì¬ê²€ì¦

KarpenterëŠ” Topology Spread Constraintsë¥¼ ë„¤ì´í‹°ë¸Œ ì§€ì›í•˜ì§€ë§Œ, ê¸°ì¡´ ì„¤ì •ì„ ì¬ê²€ì¦í•´ì•¼ í•©ë‹ˆë‹¤.

**ê²€ì¦ í¬ì¸íŠ¸:**

| í•­ëª© | í™•ì¸ ì‚¬í•­ |
|------|----------|
| **maxSkew** | Karpenterê°€ ìƒˆ ë…¸ë“œë¥¼ ì–´ëŠ AZì— ìƒì„±í• ì§€ ê²°ì •í•  ë•Œ ì˜í–¥ |
| **minDomains** | í´ëŸ¬ìŠ¤í„°ì˜ ì‹¤ì œ AZ ìˆ˜ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ |
| **whenUnsatisfiable** | `DoNotSchedule` ì‚¬ìš© ì‹œ Karpenterê°€ ë…¸ë“œë¥¼ ìƒì„±í•´ë„ Podê°€ Pending ê°€ëŠ¥ |

**ì˜ˆì‹œ: Topology Spread ë¬¸ì œ ë””ë²„ê¹…**
```bash
# Podê°€ Pendingì¸ ì´ìœ  í™•ì¸
kubectl describe pod my-app-xyz -n production

# Events ì„¹ì…˜ì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ë©”ì‹œì§€:
# "0/10 nodes are available: 3 node(s) didn't match pod topology spread constraints."

# í•´ê²°: maxSkew ì™„í™” ë˜ëŠ” replica ìˆ˜ ì¡°ì •
```

##### 5ë‹¨ê³„: ëª¨ë‹ˆí„°ë§ ì „í™˜ (ë©”íŠ¸ë¦­ ë³€ê²½)

Cluster Autoscalerì™€ KarpenterëŠ” ë‹¤ë¥¸ ë©”íŠ¸ë¦­ì„ ì œê³µí•©ë‹ˆë‹¤.

**Cluster Autoscaler ë©”íŠ¸ë¦­:**
```promql
# ê¸°ì¡´ ë©”íŠ¸ë¦­ ì˜ˆì‹œ
cluster_autoscaler_scaled_up_nodes_total
cluster_autoscaler_scaled_down_nodes_total
cluster_autoscaler_unschedulable_pods_count
```

**Karpenter ë©”íŠ¸ë¦­:**
```promql
# ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ ì˜ˆì‹œ
karpenter_nodes_created
karpenter_nodes_terminated
karpenter_pods_startup_duration_seconds
karpenter_disruption_queue_depth
karpenter_nodepool_usage
```

**CloudWatch ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸:**
```yaml
# CloudWatch Container Insights ìœ„ì ¯ ì˜ˆì‹œ
{
  "type": "metric",
  "properties": {
    "metrics": [
      [ "AWS/Karpenter", "NodesCreated", { "stat": "Sum" } ],
      [ ".", "NodesTerminated", { "stat": "Sum" } ],
      [ ".", "PendingPods", { "stat": "Average" } ]
    ],
    "period": 300,
    "stat": "Average",
    "region": "us-east-1",
    "title": "Karpenter ë…¸ë“œ ì˜¤í† ìŠ¤ì¼€ì¼ë§"
  }
}
```

**ì•ŒëŒ ì „í™˜ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Cluster Autoscaler ì•ŒëŒ ë¹„í™œì„±í™”
- [ ] Karpenter ë©”íŠ¸ë¦­ ê¸°ë°˜ ìƒˆ ì•ŒëŒ ìƒì„±
- [ ] ë…¸ë“œ ìƒì„± ì‹¤íŒ¨ ì•ŒëŒ (`karpenter_nodeclaims_created{reason="failed"}`)
- [ ] Pending Pod ì§€ì† ì•ŒëŒ (`karpenter_pods_state{state="pending"} > 5`)

##### 6ë‹¨ê³„: ë‹¨ê³„ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

ì›Œí¬ë¡œë“œë³„ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì „í™˜í•˜ì—¬ ë¦¬ìŠ¤í¬ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.

**Phase 1: ë¹„í”„ë¡œë•ì…˜ ì›Œí¬ë¡œë“œ (Week 1-2)**
```yaml
# ê°œë°œ/ìŠ¤í…Œì´ì§• ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¶€í„° ì‹œì‘
# 1. Karpenter NodePool ìƒì„± (dev-workload)
# 2. ê¸°ì¡´ ASG ë…¸ë“œì— Taint ì¶”ê°€ (ì‹ ê·œ Pod ì°¨ë‹¨)
kubectl taint nodes -l eks.amazonaws.com/nodegroup=dev-asg \
  migration=in-progress:NoSchedule

# 3. ê°œë°œ ì›Œí¬ë¡œë“œ Rolling Restart
kubectl rollout restart deployment -n dev --all

# 4. ìƒˆ Podê°€ Karpenter ë…¸ë“œì— ìŠ¤ì¼€ì¤„ë§ í™•ì¸
kubectl get pods -n dev -o wide

# 5. ê¸°ì¡´ ASG ìŠ¤ì¼€ì¼ ë‹¤ìš´
```

**Phase 2: í”„ë¡œë•ì…˜ ì›Œí¬ë¡œë“œ (Week 3-4)**
```yaml
# Canary ë°°í¬ ë°©ì‹: ì¼ë¶€ replicaë§Œ Karpenterë¡œ ì´ë™
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server-karpenter
  namespace: production
spec:
  replicas: 2  # ê¸°ì¡´ 10ê°œ ì¤‘ 2ê°œë§Œ
  selector:
    matchLabels:
      app: api-server
      migration: karpenter
  template:
    metadata:
      labels:
        app: api-server
        migration: karpenter
    spec:
      # NodeSelector ì œê±° (Karpenterê°€ ìë™ ì„ íƒ)
      # nodeSelector:
      #   eks.amazonaws.com/nodegroup: prod-asg  # ì œê±°
      containers:
      - name: api
        image: api-server:v3.0
```

**Phase 3: ë³‘í–‰ ìš´ì˜ ê²€ì¦ (Week 5-6)**
- Cluster Autoscalerì™€ Karpenterê°€ ë™ì‹œì— ì‹¤í–‰
- íŠ¸ë˜í”½ íŒ¨í„´ ëª¨ë‹ˆí„°ë§
- ë¹„ìš© ë¹„êµ ë¶„ì„
- ìŠ¤ì¼€ì¼ë§ ì†ë„ ë¹„êµ

**Phase 4: ì™„ì „ ì „í™˜ (Week 7-8)**
```bash
# 1. ëª¨ë“  ì›Œí¬ë¡œë“œê°€ Karpenter ë…¸ë“œì—ì„œ ì‹¤í–‰ í™•ì¸
kubectl get pods -A -o wide | grep -v karpenter

# 2. Cluster Autoscaler ë¹„í™œì„±í™”
kubectl scale deployment cluster-autoscaler \
  -n kube-system --replicas=0

# 3. ê¸°ì¡´ ASG ì‚­ì œ
aws autoscaling delete-auto-scaling-group \
  --auto-scaling-group-name eks-prod-asg \
  --force-delete

# 4. Cluster Autoscaler Deployment ì‚­ì œ
kubectl delete deployment cluster-autoscaler -n kube-system
```

#### 5.6.3 ë³‘í–‰ ìš´ì˜ íŒ¨í„´ (Cluster Autoscaler + Karpenter)

ë§ˆì´ê·¸ë ˆì´ì…˜ ê¸°ê°„ ë™ì•ˆ ë‘ ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì•ˆì „í•˜ê²Œ ë³‘í–‰ ìš´ì˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

##### ì¶©ëŒ ë°©ì§€ ì„¤ì •

**1. NodePoolì— ë…¸ë“œ ê·¸ë£¹ ì œì™¸ ì„¤ì •**

Karpenterê°€ Cluster Autoscaler ê´€ë¦¬ ë…¸ë“œë¥¼ ê±´ë“œë¦¬ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: karpenter-only
spec:
  template:
    spec:
      requirements:
      # Cluster Autoscaler ê´€ë¦¬ ë…¸ë“œ ì œì™¸
      - key: eks.amazonaws.com/nodegroup
        operator: DoesNotExist  # NodeGroup ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œë§Œ ê´€ë¦¬

      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]
```

**2. Cluster Autoscalerì— ë…¸ë“œ ì œì™¸ ì„¤ì •**

Cluster Autoscalerê°€ Karpenter ê´€ë¦¬ ë…¸ë“œë¥¼ ìŠ¤ì¼€ì¼ ë‹¤ìš´í•˜ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  template:
    spec:
      containers:
      - name: cluster-autoscaler
        image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.30.0
        command:
        - ./cluster-autoscaler
        - --v=4
        - --cloud-provider=aws
        - --skip-nodes-with-system-pods=false
        # Karpenter ë…¸ë“œ ì œì™¸
        - --skip-nodes-with-local-storage=false
        - --balance-similar-node-groups
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/my-cluster
```

**3. Pod NodeSelectorë¡œ ëª…ì‹œì  ë¶„ë¦¬**

íŠ¹ì • ì›Œí¬ë¡œë“œë¥¼ ì–´ëŠ ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ê°€ ê´€ë¦¬í•˜ëŠ” ë…¸ë“œì— ë°°ì¹˜í• ì§€ ëª…ì‹œí•©ë‹ˆë‹¤.

```yaml
# Cluster Autoscaler ë…¸ë“œë¡œ ë°°ì¹˜
apiVersion: apps/v1
kind: Deployment
metadata:
  name: legacy-app
spec:
  template:
    spec:
      nodeSelector:
        eks.amazonaws.com/nodegroup: prod-asg  # ASG ë…¸ë“œë§Œ
---
# Karpenter ë…¸ë“œë¡œ ë°°ì¹˜
apiVersion: apps/v1
kind: Deployment
metadata:
  name: new-app
spec:
  template:
    spec:
      nodeSelector:
        karpenter.sh/nodepool: general-purpose  # Karpenter ë…¸ë“œë§Œ
```

##### ë³‘í–‰ ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] NodePoolì— `eks.amazonaws.com/nodegroup: DoesNotExist` ì„¤ì •
- [ ] Cluster Autoscalerì— Karpenter ë…¸ë“œ ì œì™¸ í”Œë˜ê·¸ ì¶”ê°€
- [ ] ì›Œí¬ë¡œë“œë³„ NodeSelector ë˜ëŠ” NodeAffinity ì„¤ì •
- [ ] ë‘ ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ì˜ ë©”íŠ¸ë¦­ ë™ì‹œ ëª¨ë‹ˆí„°ë§
- [ ] ë¹„ìš© ë¹„êµ ëŒ€ì‹œë³´ë“œ ìƒì„±
- [ ] ë¡¤ë°± ê³„íš ìˆ˜ë¦½ (Karpenter ë¬¸ì œ ì‹œ ASGë¡œ ë³µê·€)

:::warning ë³‘í–‰ ìš´ì˜ ì‹œ ì£¼ì˜ì‚¬í•­
Cluster Autoscalerì™€ Karpenterë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ë©´ ë‹¤ìŒ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ê²½ìŸ (ê°™ì€ ì›Œí¬ë¡œë“œë¥¼ ë‘ ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ê°€ ë™ì‹œì— ì²˜ë¦¬)
- ë¹„ìš© ì˜ˆì¸¡ ì–´ë ¤ì›€ (ì–´ëŠ ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ê°€ ë…¸ë“œë¥¼ ìƒì„±í–ˆëŠ”ì§€ ì¶”ì  í•„ìš”)
- ë””ë²„ê¹… ë³µì¡ì„± ì¦ê°€

**ê¶Œì¥ ì ‘ê·¼:**
- ë³‘í–‰ ìš´ì˜ ê¸°ê°„ì€ ìµœëŒ€ 2ì£¼ë¡œ ì œí•œ
- ëª…í™•í•œ ì›Œí¬ë¡œë“œ ë¶„ë¦¬ (NodeSelector í•„ìˆ˜)
- ë‹¨ê³„ë³„ ì „í™˜ ì¼ì • ìˆ˜ë¦½
:::

##### ë¡¤ë°± ì ˆì°¨

Karpenterë¡œ ì „í™˜ í›„ ë¬¸ì œ ë°œìƒ ì‹œ Cluster Autoscalerë¡œ ë³µê·€í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```bash
# 1. Karpenter NodePool ì‚­ì œ (ë…¸ë“œëŠ” ìœ ì§€)
kubectl delete nodepool --all

# 2. Cluster Autoscaler ì¬í™œì„±í™”
kubectl scale deployment cluster-autoscaler \
  -n kube-system --replicas=1

# 3. ê¸°ì¡´ ASG ìŠ¤ì¼€ì¼ ì—…
aws autoscaling set-desired-capacity \
  --auto-scaling-group-name eks-prod-asg \
  --desired-capacity 10

# 4. Karpenter ë…¸ë“œì— Taint ì¶”ê°€ (ì‹ ê·œ Pod ì°¨ë‹¨)
kubectl taint nodes -l karpenter.sh/nodepool \
  rollback=true:NoSchedule

# 5. ì›Œí¬ë¡œë“œ Rolling Restart
kubectl rollout restart deployment -n production --all

# 6. Karpenter ë…¸ë“œ ì œê±°
kubectl delete nodes -l karpenter.sh/nodepool
```

---

## 6. PodDisruptionBudget (PDB) ê³ ê¸‰ íŒ¨í„´

PodDisruptionBudgetì€ **ìë°œì  ì¤‘ë‹¨(Voluntary Disruption)** ì‹œ ìµœì†Œí•œì˜ Pod ê°€ìš©ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

### 6.1 PDB ê¸°ë³¸ ë³µìŠµ

:::info ê¸°ë³¸ PDB ê°œë…
PDBì˜ ê¸°ë³¸ ê°œë…ê³¼ Karpenterì™€ì˜ ìƒí˜¸ì‘ìš©ì€ [EKS ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜ ê°€ì´ë“œ](/docs/operations-observability/eks-resiliency-guide#poddisruptionbudgets-pdb)ì—ì„œ ë‹¤ë£¹ë‹ˆë‹¤. ë³¸ ì„¹ì…˜ì€ ê³ ê¸‰ íŒ¨í„´ê³¼ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…ì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤.
:::

**ìë°œì  vs ë¹„ìë°œì  ì¤‘ë‹¨:**

| ì¤‘ë‹¨ ìœ í˜• | ì˜ˆì‹œ | PDB ì ìš© | ëŒ€ì‘ ë°©ë²• |
|----------|------|---------|----------|
| **ìë°œì ** | ë…¸ë“œ Drain, í´ëŸ¬ìŠ¤í„° ì—…ê·¸ë ˆì´ë“œ, Karpenter í†µí•© | âœ… ì ìš© | PDB ì„¤ì • |
| **ë¹„ìë°œì ** | ë…¸ë“œ í¬ë˜ì‹œ, OOM Kill, í•˜ë“œì›¨ì–´ ì¥ì• , AZ ì¥ì•  | âŒ ë¯¸ì ìš© | Replica ì¦ê°€, Anti-Affinity |

### 6.2 PDB ê³ ê¸‰ ì „ëµ

#### ì „ëµ 1: Rolling Update + PDB ì¡°í•©

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2         # ìµœëŒ€ 12ê°œê¹Œì§€ ì¦ê°€ í—ˆìš©
      maxUnavailable: 0   # ë™ì‹œì— ì‚¬ìš© ë¶ˆê°€ Pod 0ê°œ (ë¬´ì¤‘ë‹¨ ë°°í¬)
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      containers:
      - name: api
        image: api-server:v3.0
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-server-pdb
spec:
  minAvailable: 8  # í•­ìƒ ìµœì†Œ 8ê°œ ìœ ì§€ (80% ê°€ìš©ì„±)
  selector:
    matchLabels:
      app: api-server
```

**íš¨ê³¼:**
- Rolling Update ì¤‘: `maxUnavailable: 0`ìœ¼ë¡œ ê¸°ì¡´ Podê°€ ìƒˆ Podê°€ Readyë  ë•Œê¹Œì§€ ìœ ì§€
- ë…¸ë“œ Drain ì¤‘: PDBê°€ ìµœì†Œ 8ê°œ ë³´ì¥ â†’ ë™ì‹œì— ìµœëŒ€ 2ê°œë§Œ Evict í—ˆìš©

#### ì „ëµ 2: StatefulSet + PDB (ë°ì´í„°ë² ì´ìŠ¤ í´ëŸ¬ìŠ¤í„°)

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
spec:
  serviceName: cassandra
  replicas: 5
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      containers:
      - name: cassandra
        image: cassandra:4.1
        ports:
        - containerPort: 9042
          name: cql
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: cassandra-pdb
spec:
  maxUnavailable: 1  # ë™ì‹œì— ìµœëŒ€ 1ê°œ ë…¸ë“œë§Œ ì¤‘ë‹¨ í—ˆìš© (ì¿¼ëŸ¼ ìœ ì§€)
  selector:
    matchLabels:
      app: cassandra
```

**íš¨ê³¼:**
- Cassandra ì¿¼ëŸ¼(5ê°œ ì¤‘ 3ê°œ ì´ìƒ)ì„ ìœ ì§€í•˜ë©´ì„œ ì•ˆì „í•˜ê²Œ ë…¸ë“œ Drain ê°€ëŠ¥
- Karpenter í†µí•© ì‹œ ë…¸ë“œê°€ í•œ ë²ˆì— í•˜ë‚˜ì”©ë§Œ ì œê±°ë¨

#### ì „ëµ 3: ë¹„ìœ¨ ê¸°ë°˜ PDB (ëŒ€ê·œëª¨ Deployment)

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: worker-pdb
spec:
  maxUnavailable: "25%"  # ë™ì‹œì— ìµœëŒ€ 25% ì¤‘ë‹¨ í—ˆìš©
  selector:
    matchLabels:
      app: worker
```

| Replica ìˆ˜ | maxUnavailable: "25%" | ë™ì‹œ Evict ê°€ëŠ¥ ìˆ˜ |
|-----------|---------------------|------------------|
| 4 | 1ê°œ | 1 |
| 10 | 2.5 â†’ 2ê°œ | 2 |
| 100 | 25ê°œ | 25 |

**ë¹„ìœ¨ ê¸°ë°˜ì˜ ì¥ì :**
- ìŠ¤ì¼€ì¼ë§ ì‹œ ìë™ìœ¼ë¡œ ë¹„ìœ¨ ì¡°ì •
- Cluster Autoscaler / Karpenterì™€ ìì—°ìŠ¤ëŸ½ê²Œ í˜‘ì—…

### 6.3 PDB íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### ë¬¸ì œ 1: Drainì´ ì˜êµ¬ì ìœ¼ë¡œ ì°¨ë‹¨ë¨

**ì¦ìƒ:**
```bash
$ kubectl drain node-1 --ignore-daemonsets
error: cannot delete Pods with local storage (use --delete-emptydir-data to override)
Cannot evict pod as it would violate the pod's disruption budget.
```

**ì›ì¸:** PDBì˜ `minAvailable`ì´ í˜„ì¬ `replicas`ì™€ ë™ì¼í•˜ê±°ë‚˜, ë…¸ë“œì— PDB ëŒ€ìƒ Podê°€ ê³¼ë„í•˜ê²Œ ì§‘ì¤‘ë¨

```yaml
# ì˜ëª»ëœ ì„¤ì • ì˜ˆì‹œ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
spec:
  replicas: 3  # âš ï¸ ë¬¸ì œ: minAvailableê³¼ ê°™ìŒ
  # ...
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
spec:
  minAvailable: 3  # âš ï¸ ë¬¸ì œ: replica ìˆ˜ì™€ ê°™ìŒ
  selector:
    matchLabels:
      app: critical-app
```

**í•´ê²° ë°©ë²•:**

```yaml
# ì˜¬ë°”ë¥¸ ì„¤ì • ì˜ˆì‹œ
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
spec:
  minAvailable: 2  # âœ… replica ìˆ˜(3)ë³´ë‹¤ ì‘ê²Œ ì„¤ì •
  selector:
    matchLabels:
      app: critical-app
```

ë˜ëŠ” ë¹„ìœ¨ ì‚¬ìš©:

```yaml
spec:
  minAvailable: "67%"  # 3ê°œ ì¤‘ 2ê°œ (67%)
```

:::warning PDB ì„¤ì • ì‹œ ì£¼ì˜ì‚¬í•­
`minAvailable: replicas`ë¡œ ì„¤ì •í•˜ë©´ **ì–´ë–¤ ë…¸ë“œë„ Drainí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤**. í•­ìƒ `minAvailable < replicas` ë˜ëŠ” `maxUnavailable â‰¥ 1`ë¡œ ì„¤ì •í•˜ì—¬ ìµœì†Œ 1ê°œì˜ Pod Evictë¥¼ í—ˆìš©í•˜ì„¸ìš”.
:::

#### ë¬¸ì œ 2: PDBê°€ ì ìš©ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ:** ë…¸ë“œ Drain ì‹œ PDB ë¬´ì‹œë˜ê³  ëª¨ë“  Podê°€ ë™ì‹œì— Evictë¨

**ì›ì¸:**
1. PDBì˜ `selector`ê°€ Pod `labels`ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ
2. PDBê°€ ë‹¤ë¥¸ namespaceì— ìƒì„±ë¨
3. PDBì˜ `minAvailable: 0` ë˜ëŠ” `maxUnavailable: "100%"`

**í™•ì¸ ë°©ë²•:**

```bash
# PDB ìƒíƒœ í™•ì¸
kubectl get pdb -A
kubectl describe pdb <pdb-name>

# PDBê°€ ì„ íƒí•˜ëŠ” Pod ìˆ˜ í™•ì¸
# ALLOWED DISRUPTIONS ì»¬ëŸ¼ì´ 0ì´ë©´ Drain ì°¨ë‹¨, 1 ì´ìƒì´ë©´ í—ˆìš©
```

#### ë¬¸ì œ 3: Karpenter í†µí•©ê³¼ PDB ì¶©ëŒ

**ì¦ìƒ:** Karpenterê°€ ë…¸ë“œë¥¼ ì œê±°í•˜ë ¤ í•˜ì§€ë§Œ PDB ë•Œë¬¸ì— ì‹¤íŒ¨í•˜ê³ , ë…¸ë“œê°€ `cordoned` ìƒíƒœë¡œ ë‚¨ìŒ

**ì›ì¸:** PDBê°€ ë„ˆë¬´ ì—„ê²©í•˜ì—¬ Karpenterì˜ Disruption budgetê³¼ ì¶©ëŒ

**í•´ê²° ë°©ë²•:**

```yaml
# Karpenter NodePoolì— Disruption budget ì„¤ì •
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general-pool
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m
    # ë™ì‹œì— ìµœëŒ€ 20% ë…¸ë“œ ì¤‘ë‹¨ í—ˆìš©
    budgets:
    - nodes: "20%"
  # ...
```

**ê· í˜• ì¡íŒ PDB ì˜ˆì‹œ:**

```yaml
# ì• í”Œë¦¬ì¼€ì´ì…˜ PDB: ìµœì†Œ ê°€ìš©ì„± ë³´ì¥
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: app-pdb
spec:
  maxUnavailable: "33%"  # ë™ì‹œì— 33%ê¹Œì§€ ì¤‘ë‹¨ í—ˆìš©
  selector:
    matchLabels:
      app: my-app
```

ì´ë ‡ê²Œ ì„¤ì •í•˜ë©´ Karpenterê°€ ë…¸ë“œë¥¼ í†µí•©í•  ë•Œ PDBë¥¼ ì¡´ì¤‘í•˜ë©´ì„œë„ ìœ ì—°í•˜ê²Œ í†µí•©ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 7. Priority & Preemption

PriorityClassëŠ” Podì˜ ìš°ì„ ìˆœìœ„ë¥¼ ì •ì˜í•˜ë©°, ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì‹œ ë‚®ì€ ìš°ì„ ìˆœìœ„ Podë¥¼ Evict(Preemption)í•˜ì—¬ ë†’ì€ ìš°ì„ ìˆœìœ„ Podë¥¼ ìŠ¤ì¼€ì¤„ë§í•©ë‹ˆë‹¤.

### 7.1 PriorityClass ì •ì˜

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000  # ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ (ìµœëŒ€ 10ì–µ)
globalDefault: false
description: "High priority for mission-critical services"
```

**ì£¼ìš” ì†ì„±:**

| ì†ì„± | ì„¤ëª… | ê¶Œì¥ê°’ |
|------|------|--------|
| `value` | ìš°ì„ ìˆœìœ„ ê°’ (ì •ìˆ˜) | 0 ~ 1,000,000,000 |
| `globalDefault` | ê¸°ë³¸ PriorityClass ì—¬ë¶€ | `false` (ëª…ì‹œì  ì§€ì • ê¶Œì¥) |
| `preemptionPolicy` | Preemption ì •ì±… | `PreemptLowerPriority` (ê¸°ë³¸) ë˜ëŠ” `Never` |
| `description` | ì„¤ëª… | ì‚¬ìš© ëª©ì  ëª…ì‹œ |

:::warning System PriorityClass ì˜ˆì•½ ë²”ìœ„
10ì–µ ì´ìƒì˜ ê°’ì€ Kubernetes ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸(kube-system)ìš©ìœ¼ë¡œ ì˜ˆì•½ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ì •ì˜ PriorityClassëŠ” 10ì–µ ë¯¸ë§Œì˜ ê°’ì„ ì‚¬ìš©í•˜ì„¸ìš”.
:::

### 7.2 í”„ë¡œë•ì…˜ 4-Tier ìš°ì„ ìˆœìœ„ ì²´ê³„

**ê¶Œì¥ ìš°ì„ ìˆœìœ„ ê³„ì¸µ:**

```yaml
# Tier 1: Critical System (10ì–µ ë¯¸ë§Œ ìµœê³ ê°’)
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: system-critical
value: 999999000
globalDefault: false
description: "Critical system components (DNS, CNI, monitoring)"
---
# Tier 2: Business Critical (100ë§Œ)
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: business-critical
value: 1000000
globalDefault: false
description: "Revenue-impacting services (payment, checkout, auth)"
---
# Tier 3: High Priority (10ë§Œ)
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 100000
globalDefault: false
description: "Important services (API, web frontend)"
---
# Tier 4: Standard (1ë§Œ, ê¸°ë³¸ê°’)
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: standard-priority
value: 10000
globalDefault: true  # PriorityClass ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ê°’
description: "Standard workloads"
---
# Tier 5: Low Priority (1ì²œ)
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 1000
globalDefault: false
preemptionPolicy: Never  # ë‹¤ë¥¸ Podë¥¼ Preemptí•˜ì§€ ì•ŠìŒ
description: "Batch jobs, non-critical background tasks"
```

**ì ìš© ì˜ˆì‹œ:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-service
spec:
  replicas: 5
  selector:
    matchLabels:
      app: payment-service
  template:
    metadata:
      labels:
        app: payment-service
    spec:
      priorityClassName: business-critical  # ìµœìš°ì„  ë³´ì¥
      containers:
      - name: payment
        image: payment-service:v2.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: data-cleanup
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          priorityClassName: low-priority  # ë°°ì¹˜ ì‘ì—…ì€ ë‚®ì€ ìš°ì„ ìˆœìœ„
          containers:
          - name: cleanup
            image: data-cleanup:v1.0
```

### 7.3 Preemption ë™ì‘ ì´í•´

Preemptionì€ ë†’ì€ ìš°ì„ ìˆœìœ„ Podê°€ ìŠ¤ì¼€ì¤„ë§ë˜ì§€ ëª»í•  ë•Œ, ë‚®ì€ ìš°ì„ ìˆœìœ„ Podë¥¼ Evictí•˜ì—¬ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ë³´í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.

```mermaid
flowchart TB
    START[ë†’ì€ ìš°ì„ ìˆœìœ„ Pod<br/>ìŠ¤ì¼€ì¤„ë§ ìš”ì²­]
    CHECK{ë¦¬ì†ŒìŠ¤ ì¶©ë¶„?}
    SCHEDULE[ì¦‰ì‹œ ìŠ¤ì¼€ì¤„ë§]
    FIND[Preemption í›„ë³´<br/>ë…¸ë“œ íƒìƒ‰]
    CANDIDATE{ë‚®ì€ ìš°ì„ ìˆœìœ„<br/>Pod ì¡´ì¬?}
    EVICT[ë‚®ì€ ìš°ì„ ìˆœìœ„ Pod<br/>Evict]
    WAIT[Evict ì™„ë£Œ ëŒ€ê¸°<br/>gracePeriod]
    BIND[ë†’ì€ ìš°ì„ ìˆœìœ„ Pod<br/>ìŠ¤ì¼€ì¤„ë§]
    PENDING[Pending ìƒíƒœ ìœ ì§€<br/>Cluster Autoscaler ëŒ€ê¸°]

    START --> CHECK
    CHECK -->|ì˜ˆ| SCHEDULE
    CHECK -->|ì•„ë‹ˆì˜¤| FIND
    FIND --> CANDIDATE
    CANDIDATE -->|ì˜ˆ| EVICT
    CANDIDATE -->|ì•„ë‹ˆì˜¤| PENDING
    EVICT --> WAIT
    WAIT --> BIND

    style START fill:#4286f4,stroke:#2a6acf,color:#fff
    style EVICT fill:#ff4444,stroke:#cc3636,color:#fff
    style BIND fill:#34a853,stroke:#2a8642,color:#fff
    style PENDING fill:#fbbc04,stroke:#c99603,color:#000
```

**Preemption ì˜ì‚¬ê²°ì • ê³¼ì •:**

1. **ë†’ì€ ìš°ì„ ìˆœìœ„ Pod ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨**
2. **Preemption í›„ë³´ ë…¸ë“œ íƒìƒ‰**: ë‚®ì€ ìš°ì„ ìˆœìœ„ Podë¥¼ ì œê±°í•˜ë©´ ìŠ¤ì¼€ì¤„ë§ ê°€ëŠ¥í•œ ë…¸ë“œ ì°¾ê¸°
3. **Victim Pod ì„ íƒ**: ê°€ì¥ ë‚®ì€ ìš°ì„ ìˆœìœ„ë¶€í„° ì œê±° ëŒ€ìƒ ì„ ì •
4. **PDB í™•ì¸**: Victim Podê°€ PDBë¡œ ë³´í˜¸ë˜ëŠ”ì§€ í™•ì¸ â†’ PDB ìœ„ë°˜ ì‹œ ë‹¤ë¥¸ ë…¸ë“œ íƒìƒ‰
5. **Graceful Eviction**: `terminationGracePeriodSeconds` ì¡´ì¤‘í•˜ë©° Evict
6. **ë¦¬ì†ŒìŠ¤ í™•ë³´ í›„ ìŠ¤ì¼€ì¤„ë§**: ë†’ì€ ìš°ì„ ìˆœìœ„ Pod ë°°ì¹˜

:::tip Preemptionê³¼ PDBì˜ ê´€ê³„
Preemptionì€ PDBë¥¼ **ì¡´ì¤‘í•©ë‹ˆë‹¤**. PDBì˜ `minAvailable`ì„ ìœ„ë°˜í•˜ëŠ” Evictionì€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¦‰, PDBê°€ ì„¤ì •ëœ ë‚®ì€ ìš°ì„ ìˆœìœ„ Podë„ ë³´í˜¸ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

**Preemption ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤:**

```yaml
# í˜„ì¬ í´ëŸ¬ìŠ¤í„° ìƒíƒœ: ë…¸ë“œ ë¦¬ì†ŒìŠ¤ê°€ ê±°ì˜ ê°€ë“ ì°¸
# Node-1: low-priority-pod (CPU: 2, Memory: 4Gi)
# Node-2: standard-priority-pod (CPU: 2, Memory: 4Gi)

# ë†’ì€ ìš°ì„ ìˆœìœ„ Pod ìƒì„± ìš”ì²­
apiVersion: v1
kind: Pod
metadata:
  name: critical-payment
spec:
  priorityClassName: business-critical  # ìš°ì„ ìˆœìœ„: 1000000
  containers:
  - name: payment
    image: payment:v1.0
    resources:
      requests:
        cpu: "2"
        memory: 4Gi

# ê²°ê³¼:
# 1. ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ê°ì§€
# 2. low-priority-pod (ìš°ì„ ìˆœìœ„: 1000)ë¥¼ Victimìœ¼ë¡œ ì„ íƒ
# 3. low-priority-pod Evict (graceful shutdown)
# 4. critical-payment Pod ìŠ¤ì¼€ì¤„ë§
```

### 7.4 PreemptionPolicy: Never

íŠ¹ì • ì›Œí¬ë¡œë“œê°€ ë‹¤ë¥¸ Podë¥¼ Preemptí•˜ì§€ ì•Šë„ë¡ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: batch-job
value: 5000
globalDefault: false
preemptionPolicy: Never  # ë‹¤ë¥¸ Podë¥¼ Preemptí•˜ì§€ ì•ŠìŒ
description: "Batch jobs that wait for available resources"
```

**ì‚¬ìš© ì‚¬ë¡€:**
- **ë°°ì¹˜ ì‘ì—…**: ë¦¬ì†ŒìŠ¤ê°€ ìƒê¸¸ ë•Œê¹Œì§€ ëŒ€ê¸°í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ê²½ìš°
- **í…ŒìŠ¤íŠ¸/ê°œë°œ ì›Œí¬ë¡œë“œ**: í”„ë¡œë•ì…˜ ì›Œí¬ë¡œë“œë¥¼ ë°©í•´í•˜ì§€ ì•Šì•„ì•¼ í•  ë•Œ
- **ë‚®ì€ ê¸´ê¸‰ì„±**: ì¦‰ì‹œ ì‹¤í–‰ë˜ì§€ ì•Šì•„ë„ ê´œì°®ì€ ì‘ì—…

### 7.5 Priority + QoS Class ì¡°í•© ê³ ê¸‰ íŒ¨í„´

PriorityClassì™€ QoS ClassëŠ” ì„œë¡œ ë‹¤ë¥¸ ëª©ì ì„ ê°€ì§„ ë©”ì»¤ë‹ˆì¦˜ì´ì§€ë§Œ, í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ìƒí™©ì—ì„œ ë”ìš± ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë™ì‘ì„ ë³´ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” ë‘ ê°œë…ì˜ ìƒí˜¸ì‘ìš©ê³¼ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ê²€ì¦ëœ ì¡°í•© íŒ¨í„´ì„ ì†Œê°œí•©ë‹ˆë‹¤.

#### QoS Class ë³µìŠµ

KubernetesëŠ” Podì˜ ë¦¬ì†ŒìŠ¤ ìš”ì²­(requests)ê³¼ ì œí•œ(limits) ì„¤ì •ì— ë”°ë¼ ìë™ìœ¼ë¡œ QoS Classë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.

| QoS Class | ì¡°ê±´ | CPU ìŠ¤ë¡œí‹€ë§ | OOM ì‹œ Eviction ìˆœì„œ | ì¼ë°˜ì  ì‚¬ìš© |
|-----------|------|-------------|-------------------|------------|
| **Guaranteed** | ëª¨ë“  ì»¨í…Œì´ë„ˆì˜ requests = limits | ì œí•œ ë„ë‹¬ ì‹œë§Œ | ë§ˆì§€ë§‰ (ê°€ì¥ ì•ˆì „) | ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬, DB |
| **Burstable** | ìµœì†Œ í•˜ë‚˜ì˜ ì»¨í…Œì´ë„ˆì— requests ì„¤ì •, requests < limits | ì œí•œ ë„ë‹¬ ì‹œë§Œ | ì¤‘ê°„ | ì¼ë°˜ ì›¹ ì•±, API |
| **BestEffort** | requests/limits ëª¨ë‘ ë¯¸ì„¤ì • | ì œí•œ ì—†ìŒ | ê°€ì¥ ë¨¼ì € (ìœ„í—˜) | ë°°ì¹˜ ì‘ì—…, í…ŒìŠ¤íŠ¸ |

**QoS Class ê²°ì • ê·œì¹™:**

```yaml
# Guaranteed: requests = limits (ëª¨ë“  ì»¨í…Œì´ë„ˆ)
resources:
  requests:
    cpu: "1"
    memory: 2Gi
  limits:
    cpu: "1"      # requestsì™€ ë™ì¼
    memory: 2Gi   # requestsì™€ ë™ì¼

# Burstable: requests < limits
resources:
  requests:
    cpu: "500m"
    memory: 1Gi
  limits:
    cpu: "2"      # requestsë³´ë‹¤ í¼
    memory: 4Gi   # requestsë³´ë‹¤ í¼

# BestEffort: ì•„ë¬´ê²ƒë„ ì„¤ì • ì•ˆí•¨
resources: {}
```

**QoS Class í™•ì¸:**
```bash
# Podì˜ QoS Class í™•ì¸
kubectl get pod my-pod -o jsonpath='{.status.qosClass}'

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì „ì²´ Podì˜ QoS ë¶„í¬
kubectl get pods -n production \
  -o custom-columns=NAME:.metadata.name,QOS:.status.qosClass
```

#### ê¶Œì¥ ì¡°í•© ë§¤íŠ¸ë¦­ìŠ¤

Priorityì™€ QoSë¥¼ ì–´ë–»ê²Œ ì¡°í•©í• ì§€ì— ë”°ë¼ ë¦¬ì†ŒìŠ¤ ë³´ì¥ ìˆ˜ì¤€ê³¼ ë¹„ìš©ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

| ì¡°í•© | Priority | QoS | ìŠ¤ì¼€ì¤„ë§ ìš°ì„ ìˆœìœ„ | OOM ì‹œ ìƒì¡´ìœ¨ | ë¹„ìš© | ê¶Œì¥ ì›Œí¬ë¡œë“œ | ì˜ˆì‹œ |
|------|----------|-----|-----------------|-------------|------|-------------|------|
| **Tier 1** | critical (10000) | Guaranteed | ìµœìš°ì„  | ìµœê³  | ë†’ìŒ | ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬ | ê²°ì œ ì‹œìŠ¤í…œ, DB |
| **Tier 2** | high (5000) | Guaranteed | ë†’ìŒ | ë†’ìŒ | ì¤‘ìƒ | í•µì‹¬ ì„œë¹„ìŠ¤ | API ê²Œì´íŠ¸ì›¨ì´ |
| **Tier 3** | standard (1000) | Burstable | ë³´í†µ | ì¤‘ê°„ | ì¤‘ê°„ | ì¼ë°˜ ì›¹ ì•± | í”„ë¡ íŠ¸ì—”ë“œ, ë°±ì˜¤í”¼ìŠ¤ |
| **Tier 4** | low (500) | Burstable | ë‚®ìŒ | ë‚®ìŒ | ì €ë ´ | ë‚´ë¶€ ë„êµ¬ | ëª¨ë‹ˆí„°ë§, ë¡œê¹… |
| **Tier 5** | batch (100) | BestEffort | ìµœí•˜ìœ„ | ë§¤ìš° ë‚®ìŒ | ë§¤ìš° ì €ë ´ | ë°°ì¹˜, CI/CD | ë°ì´í„° íŒŒì´í”„ë¼ì¸ |

**ì¡°í•©ë³„ ìƒì„¸ ì„¤ëª…:**

##### Tier 1: Guaranteed + critical-priority (ìµœê³  ë³´ì¥)

**íŠ¹ì§•:**
- ìŠ¤ì¼€ì¤„ë§ ì‹œ ë‹¤ë¥¸ Podë¥¼ Preemptí•˜ì—¬ ì¦‰ì‹œ ë°°ì¹˜
- CPU/ë©”ëª¨ë¦¬ ë³´ì¥ (requests = limits)
- OOM ë°œìƒ ì‹œ ê°€ì¥ ë§ˆì§€ë§‰ì— ì¢…ë£Œ
- ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ì••ë°• ì‹œì—ë„ ì ˆëŒ€ Evictë˜ì§€ ì•ŠìŒ

**ì‹¤ì „ YAML:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-gateway
  namespace: production
spec:
  replicas: 6
  selector:
    matchLabels:
      app: payment-gateway
      tier: critical
  template:
    metadata:
      labels:
        app: payment-gateway
        tier: critical
    spec:
      priorityClassName: critical-priority  # Priority: 10000
      containers:
      - name: gateway
        image: payment-gateway:v3.5
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
          limits:
            cpu: "2"       # requestsì™€ ë™ì¼ â†’ Guaranteed
            memory: 4Gi    # requestsì™€ ë™ì¼ â†’ Guaranteed
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: payment-gateway-pdb
  namespace: production
spec:
  minAvailable: 4  # 6ê°œ ì¤‘ ìµœì†Œ 4ê°œ í•­ìƒ ìœ ì§€
  selector:
    matchLabels:
      app: payment-gateway
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- ê¸ˆìœµ ê±°ë˜ ì‹œìŠ¤í…œ (ê²°ì œ, ì†¡ê¸ˆ)
- ì‹¤ì‹œê°„ ì£¼ë¬¸ ì²˜ë¦¬
- ë°ì´í„°ë² ì´ìŠ¤ (MySQL, PostgreSQL)
- ë©”ì‹œì§€ í (Kafka, RabbitMQ)

##### Tier 2: Guaranteed + high-priority (í•µì‹¬ ì„œë¹„ìŠ¤)

**íŠ¹ì§•:**
- critical ë‹¤ìŒ ìš°ì„ ìˆœìœ„
- CPU/ë©”ëª¨ë¦¬ ë³´ì¥
- OOM ì‹œ BestEffort, Burstable ë‹¤ìŒìœ¼ë¡œ ì¢…ë£Œ
- ì¼ë°˜ì ì¸ í”„ë¡œë•ì…˜ ì„œë¹„ìŠ¤ì˜ ê¶Œì¥ ì„¤ì •

**ì‹¤ì „ YAML:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
  namespace: production
spec:
  replicas: 10
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      priorityClassName: high-priority  # Priority: 5000
      containers:
      - name: api
        image: api-server:v2.8
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
          limits:
            cpu: "1"       # Guaranteed
            memory: 2Gi    # Guaranteed
        env:
        - name: MAX_CONNECTIONS
          value: "1000"
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- REST API ì„œë²„
- GraphQL ì„œë²„
- ì¸ì¦/ì¸ê°€ ì„œë¹„ìŠ¤
- ì„¸ì…˜ ê´€ë¦¬ ì„œë¹„ìŠ¤

##### Tier 3: Burstable + standard-priority (ì¼ë°˜ ì›¹ ì•±)

**íŠ¹ì§•:**
- ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ë³´ì¥ (requests)
- ìœ íœ´ ì‹œ ì¶”ê°€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ê°€ëŠ¥ (limits > requests)
- ë¹„ìš© íš¨ìœ¨ì ì´ë©´ì„œ ì•ˆì •ì 
- ëŒ€ë¶€ë¶„ì˜ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©

**ì‹¤ì „ YAML:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
  namespace: production
spec:
  replicas: 8
  selector:
    matchLabels:
      app: web-frontend
  template:
    metadata:
      labels:
        app: web-frontend
    spec:
      priorityClassName: standard-priority  # Priority: 1000
      containers:
      - name: frontend
        image: web-frontend:v1.12
        resources:
          requests:
            cpu: "500m"    # ìµœì†Œ ë³´ì¥
            memory: 1Gi    # ìµœì†Œ ë³´ì¥
          limits:
            cpu: "2"       # ìµœëŒ€ 4ë°° ë²„ìŠ¤íŠ¸ í—ˆìš©
            memory: 4Gi    # ìµœëŒ€ 4ë°° ë²„ìŠ¤íŠ¸ í—ˆìš©
        env:
        - name: NODE_ENV
          value: "production"
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- ì›¹ í”„ë¡ íŠ¸ì—”ë“œ (React, Vue, Angular)
- ë°±ì˜¤í”¼ìŠ¤ ì• í”Œë¦¬ì¼€ì´ì…˜
- ë‚´ë¶€ ëŒ€ì‹œë³´ë“œ
- CMS (Content Management System)

##### Tier 4: Burstable + low-priority (ë‚´ë¶€ ë„êµ¬)

**íŠ¹ì§•:**
- ìµœì†Œí•œì˜ ë¦¬ì†ŒìŠ¤ ë³´ì¥
- ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì‹œ Preempt ëŒ€ìƒ
- ë¹„ìš© ìµœì†Œí™”
- ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì‹œ ì˜í–¥ ì œí•œì 

**ì‹¤ì „ YAML:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-agent
  namespace: monitoring
spec:
  replicas: 3
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      priorityClassName: low-priority  # Priority: 500
      containers:
      - name: agent
        image: monitoring-agent:v2.1
        resources:
          requests:
            cpu: "100m"    # ìµœì†Œí•œì˜ ë³´ì¥
            memory: 256Mi
          limits:
            cpu: "500m"
            memory: 1Gi
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- ëª¨ë‹ˆí„°ë§ ì—ì´ì „íŠ¸
- ë¡œê·¸ ìˆ˜ì§‘ê¸° (Fluent Bit, Fluentd)
- ë©”íŠ¸ë¦­ Exporter
- ê°œë°œ ë„êµ¬

##### Tier 5: BestEffort + batch-priority (ë°°ì¹˜ ì‘ì—…)

**íŠ¹ì§•:**
- ë¦¬ì†ŒìŠ¤ ë³´ì¥ ì—†ìŒ (ìœ íœ´ ë¦¬ì†ŒìŠ¤ë§Œ ì‚¬ìš©)
- OOM ë°œìƒ ì‹œ ê°€ì¥ ë¨¼ì € ì¢…ë£Œ
- ë¹„ìš© ìµœì†Œí™” (Spot ì¸ìŠ¤í„´ìŠ¤ í™œìš© ê°€ëŠ¥)
- ì¬ì‹œë„ ê°€ëŠ¥í•œ ì‘ì—…ì— ì í•©

**ì‹¤ì „ YAML:**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: data-pipeline
  namespace: batch
spec:
  schedule: "0 2 * * *"  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
  jobTemplate:
    spec:
      template:
        spec:
          priorityClassName: batch-priority  # Priority: 100
          restartPolicy: OnFailure
          containers:
          - name: etl
            image: data-pipeline:v1.8
            resources: {}  # BestEffort: requests/limits ì—†ìŒ
            env:
            - name: BATCH_SIZE
              value: "10000"
          # Spot ì¸ìŠ¤í„´ìŠ¤ì— ë°°ì¹˜
          nodeSelector:
            karpenter.sh/capacity-type: spot
          tolerations:
          - key: karpenter.sh/capacity-type
            operator: Equal
            value: spot
            effect: NoSchedule
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- ETL íŒŒì´í”„ë¼ì¸
- ë°ì´í„° ë¶„ì„ ì‘ì—…
- CI/CD ë¹Œë“œ
- ì´ë¯¸ì§€/ë™ì˜ìƒ ì²˜ë¦¬

#### Eviction ìˆœì„œ (OOM ë°œìƒ ì‹œ)

ë…¸ë“œì—ì„œ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ë•Œ, Kubeletì€ ë‹¤ìŒ ìˆœì„œë¡œ Podë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤:

```mermaid
flowchart TD
    A[ë…¸ë“œ ë©”ëª¨ë¦¬ ë¶€ì¡±]
    B[1ë‹¨ê³„: BestEffort Pod ì¢…ë£Œ<br/>Priority ë‚®ì€ ìˆœ]
    C[2ë‹¨ê³„: Burstable Pod ì¢…ë£Œ<br/>ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼ í° ìˆœ]
    D[3ë‹¨ê³„: Guaranteed Pod ì¢…ë£Œ<br/>Priority ë‚®ì€ ìˆœ]
    E[ë©”ëª¨ë¦¬ í™•ë³´ ì™„ë£Œ]

    A --> B
    B -->|ì—¬ì „íˆ ë¶€ì¡±| C
    C -->|ì—¬ì „íˆ ë¶€ì¡±| D
    D --> E

    style A fill:#ea4335,stroke:#c5221f,color:#fff
    style B fill:#fbbc04,stroke:#f9ab00,color:#000
    style C fill:#ff9800,stroke:#f57c00,color:#fff
    style D fill:#f44336,stroke:#d32f2f,color:#fff
    style E fill:#34a853,stroke:#2a8642,color:#fff
```

**Eviction ê²°ì • ìš”ì†Œ:**

1. **QoS Class** (1ì°¨ ê¸°ì¤€)
   - BestEffort â†’ Burstable â†’ Guaranteed ìˆœì„œ

2. **Priority** (2ì°¨ ê¸°ì¤€, QoS ë™ì¼ ì‹œ)
   - ë‚®ì€ Priority ë¨¼ì € ì¢…ë£Œ

3. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** (3ì°¨ ê¸°ì¤€, QoS + Priority ë™ì¼ ì‹œ)
   - requests ëŒ€ë¹„ ì´ˆê³¼ ì‚¬ìš©ëŸ‰ì´ í° Pod ë¨¼ì € ì¢…ë£Œ

**ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤:**

```yaml
# ë…¸ë“œ ìƒí™©: ë©”ëª¨ë¦¬ 32GB ì¤‘ 31GB ì‚¬ìš©, OOM ì„ë°•

# Pod 1: BestEffort + low-priority (500)
# - ì‚¬ìš© ì¤‘: 4GB
# â†’ Eviction ìˆœì„œ: 1ìœ„

# Pod 2: Burstable + standard-priority (1000)
# - requests: 2GB, limits: 8GB
# - ì‚¬ìš© ì¤‘: 6GB (requests ëŒ€ë¹„ +4GB ì´ˆê³¼)
# â†’ Eviction ìˆœì„œ: 2ìœ„

# Pod 3: Burstable + high-priority (5000)
# - requests: 4GB, limits: 8GB
# - ì‚¬ìš© ì¤‘: 5GB (requests ëŒ€ë¹„ +1GB ì´ˆê³¼)
# â†’ Eviction ìˆœì„œ: 3ìœ„

# Pod 4: Guaranteed + critical-priority (10000)
# - requests = limits: 8GB
# - ì‚¬ìš© ì¤‘: 8GB (ì´ˆê³¼ ì—†ìŒ)
# â†’ Eviction ìˆœì„œ: 4ìœ„ (ë§ˆì§€ë§‰)
```

#### Kubelet Eviction ì„¤ì •

Kubeletì˜ Eviction ì„ê³„ê°’ì€ ë…¸ë“œ ìˆ˜ì¤€ì—ì„œ ì„¤ì •ë©ë‹ˆë‹¤. EKSì—ì„œëŠ” User Data ìŠ¤í¬ë¦½íŠ¸ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥í•©ë‹ˆë‹¤.

**ê¸°ë³¸ ì„¤ì • (EKS):**
```yaml
# /etc/kubernetes/kubelet/kubelet-config.json
{
  "evictionHard": {
    "memory.available": "100Mi",
    "nodefs.available": "10%",
    "imagefs.available": "15%"
  },
  "evictionSoft": {
    "memory.available": "500Mi",
    "nodefs.available": "15%"
  },
  "evictionSoftGracePeriod": {
    "memory.available": "1m30s",
    "nodefs.available": "2m"
  }
}
```

**ì»¤ìŠ¤í„°ë§ˆì´ì§• ì˜ˆì‹œ (Karpenter EC2NodeClass):**
```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: custom-eviction
spec:
  amiFamily: AL2023
  userData: |
    #!/bin/bash
    # Kubelet ì„¤ì • ìˆ˜ì •
    cat <<EOF > /etc/kubernetes/kubelet/kubelet-config.json
    {
      "evictionHard": {
        "memory.available": "200Mi",  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
        "nodefs.available": "10%"
      },
      "evictionSoft": {
        "memory.available": "1Gi",    # Soft ì„ê³„ê°’ ìƒí–¥
        "nodefs.available": "15%"
      },
      "evictionSoftGracePeriod": {
        "memory.available": "2m",     # ìœ ì˜ˆ ì‹œê°„ ì¦ê°€
        "nodefs.available": "3m"
      }
    }
    EOF

    systemctl restart kubelet
```

**Eviction ì„ê³„ê°’ ì„¤ëª…:**

| ì„¤ì • | ì˜ë¯¸ | ê¸°ë³¸ê°’ | ê¶Œì¥ê°’ (í”„ë¡œë•ì…˜) |
|------|------|--------|-----------------|
| `evictionHard.memory.available` | ì´ ìˆ˜ì¤€ ì´í•˜ ì‹œ ì¦‰ì‹œ Eviction | 100Mi | 200~500Mi |
| `evictionSoft.memory.available` | ì´ ìˆ˜ì¤€ ì´í•˜ë¡œ ì¼ì • ì‹œê°„ ìœ ì§€ ì‹œ Eviction | 500Mi | 1Gi |
| `evictionSoftGracePeriod.memory.available` | Soft ì„ê³„ê°’ ìœ ì˜ˆ ì‹œê°„ | 1m30s | 2~5m |

:::warning Eviction ì„¤ì • ì‹œ ì£¼ì˜ì‚¬í•­
`evictionHard` ì„ê³„ê°’ì„ ë„ˆë¬´ ë‚®ê²Œ ì„¤ì •í•˜ë©´ OOM Killerê°€ ë¨¼ì € ë™ì‘í•˜ì—¬ Kubeletì˜ graceful evictionì´ ë¬´ìš©ì§€ë¬¼ì´ ë©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ ë„ˆë¬´ ë†’ê²Œ ì„¤ì •í•˜ë©´ ë…¸ë“œ ë¦¬ì†ŒìŠ¤ í™œìš©ë¥ ì´ ë‚®ì•„ì ¸ ë¹„ìš©ì´ ì¦ê°€í•©ë‹ˆë‹¤.

**ê¶Œì¥ ì ‘ê·¼:**
- ì¼ë°˜ ì›Œí¬ë¡œë“œ: `evictionHard: 200Mi`, `evictionSoft: 1Gi`
- ë©”ëª¨ë¦¬ ì§‘ì•½ì : `evictionHard: 500Mi`, `evictionSoft: 2Gi`
- ëª¨ë‹ˆí„°ë§: `kube_node_status_condition{condition="MemoryPressure"}` ë©”íŠ¸ë¦­ ì¶”ì 
:::

#### ì‹¤ì „ ì¡°í•© íŒ¨í„´ ê²€ì¦

**íŒ¨í„´ 1: Multi-Tier ì•„í‚¤í…ì²˜**

```yaml
# Tier 1: Database (Guaranteed + critical)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 3
  template:
    spec:
      priorityClassName: critical-priority
      containers:
      - name: postgres
        image: postgres:16
        resources:
          requests:
            cpu: "4"
            memory: 16Gi
          limits:
            cpu: "4"
            memory: 16Gi
---
# Tier 2: API Server (Guaranteed + high)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 10
  template:
    spec:
      priorityClassName: high-priority
      containers:
      - name: api
        resources:
          requests: { cpu: "1", memory: 2Gi }
          limits: { cpu: "1", memory: 2Gi }
---
# Tier 3: Frontend (Burstable + standard)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 8
  template:
    spec:
      priorityClassName: standard-priority
      containers:
      - name: frontend
        resources:
          requests: { cpu: "500m", memory: 1Gi }
          limits: { cpu: "2", memory: 4Gi }
---
# Tier 4: Monitoring (Burstable + low)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
spec:
  template:
    spec:
      priorityClassName: low-priority
      containers:
      - name: exporter
        resources:
          requests: { cpu: "100m", memory: 128Mi }
          limits: { cpu: "200m", memory: 256Mi }
```

**ê²€ì¦ ëª…ë ¹ì–´:**
```bash
# QoS + Priority ë¶„í¬ í™•ì¸
kubectl get pods -A -o custom-columns=\
NAME:.metadata.name,\
NAMESPACE:.metadata.namespace,\
QOS:.status.qosClass,\
PRIORITY:.spec.priorityClassName,\
CPU_REQ:.spec.containers[0].resources.requests.cpu,\
MEM_REQ:.spec.containers[0].resources.requests.memory

# ë…¸ë“œë³„ QoS ë¶„í¬ í™•ì¸
kubectl describe node <node-name> | grep -A 10 "Non-terminated Pods"
```

#### íŠ¸ëŸ¬ë¸”ìŠˆíŒ…: QoS + Priority ì¡°í•© ë¬¸ì œ

| ì¦ìƒ | ì›ì¸ | í•´ê²° ë°©ë²• |
|------|------|----------|
| Guaranteed Podê°€ OOM Killë¨ | limits ì„¤ì •ì´ ë„ˆë¬´ ë‚®ìŒ | ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ í›„ limits ìƒí–¥ |
| Burstable Podê°€ CPU ìŠ¤ë¡œí‹€ë§ | limits ë„ë‹¬, ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± | requests ìƒí–¥ ë˜ëŠ” ë…¸ë“œ ì¶”ê°€ |
| Low-priority Podê°€ ê³„ì† Pending | High-priority Podê°€ ë¦¬ì†ŒìŠ¤ ë…ì  | ë…¸ë“œ ì¶”ê°€ ë˜ëŠ” Priority ì¬ì¡°ì • |
| BestEffort Podê°€ ì¦‰ì‹œ ì¢…ë£Œë¨ | Eviction ì„ê³„ê°’ ë„ë‹¬ | Burstableë¡œ ì „í™˜, requests ì„¤ì • |

:::tip QoS + Priority ìµœì í™” íŒ
1. **ëª¨ë‹ˆí„°ë§**: Prometheusì˜ `container_memory_working_set_bytes`, `container_cpu_usage_seconds_total` ë©”íŠ¸ë¦­ìœ¼ë¡œ ì‹¤ì œ ì‚¬ìš©ëŸ‰ ì¶”ì 
2. **Rightsizing**: VPA(Vertical Pod Autoscaler) ê¶Œì¥ê°’ ì°¸ê³ 
3. **ë‹¨ê³„ì  ì „í™˜**: BestEffort â†’ Burstable â†’ Guaranteed ìˆœìœ¼ë¡œ ì ì§„ì  ì ìš©
4. **ë¹„ìš© ê· í˜•**: ëª¨ë“  Podë¥¼ Guaranteedë¡œ ì„¤ì •í•˜ë©´ ë¹„ìš© ì¦ê°€, ì›Œí¬ë¡œë“œ ì¤‘ìš”ë„ì— ë”°ë¼ ì°¨ë“± ì ìš©
:::

---

## 8. Descheduler

DeschedulerëŠ” ì´ë¯¸ ìŠ¤ì¼€ì¤„ë§ëœ Podë¥¼ **ì¬ë°°ì¹˜**í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. Kubernetes ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì´ˆê¸° ë°°ì¹˜ë§Œ ë‹´ë‹¹í•˜ë¯€ë¡œ, ì‹œê°„ì´ ì§€ë‚˜ë©´ ë…¸ë“œ ê°„ ë¶ˆê· í˜•ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 8.1 Deschedulerê°€ í•„ìš”í•œ ì´ìœ 

**ì‹œë‚˜ë¦¬ì˜¤ 1: ë…¸ë“œ ì¶”ê°€ í›„ ë¶ˆê· í˜•**
- ê¸°ì¡´ ë…¸ë“œì— Podê°€ ëª°ë ¤ìˆê³ , ìƒˆë¡œ ì¶”ê°€ëœ ë…¸ë“œëŠ” ë¹„ì–´ìˆìŒ
- Deschedulerê°€ ì˜¤ë˜ëœ Podë¥¼ Evict â†’ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ìƒˆ ë…¸ë“œì— ì¬ë°°ì¹˜

**ì‹œë‚˜ë¦¬ì˜¤ 2: Affinity/Anti-Affinity ìœ„ë°˜**
- Pod ë°°ì¹˜ í›„ ë…¸ë“œ ë ˆì´ë¸”ì´ ë³€ê²½ë˜ì–´ Affinity ì¡°ê±´ ìœ„ë°˜
- Deschedulerê°€ ìœ„ë°˜ Podë¥¼ Evict â†’ ì¡°ê±´ì— ë§ëŠ” ë…¸ë“œì— ì¬ë°°ì¹˜

**ì‹œë‚˜ë¦¬ì˜¤ 3: ë¦¬ì†ŒìŠ¤ íŒŒí¸í™”**
- ì¼ë¶€ ë…¸ë“œëŠ” CPU ê³¼ë‹¤ ì‚¬ìš©, ì¼ë¶€ëŠ” ìœ íœ´ ìƒíƒœ
- Deschedulerê°€ ë¶ˆê· í˜• í•´ì†Œ

### 8.2 Descheduler ì„¤ì¹˜ (Helm)

```bash
# Descheduler Helm Chart ì¶”ê°€
helm repo add descheduler https://kubernetes-sigs.github.io/descheduler/
helm repo update

# ê¸°ë³¸ ì„¤ì¹˜
helm install descheduler descheduler/descheduler \
  --namespace kube-system \
  --set cronJobApiVersion="batch/v1" \
  --set schedule="*/15 * * * *"  # 15ë¶„ë§ˆë‹¤ ì‹¤í–‰
```

**CronJob vs Deployment ëª¨ë“œ:**

| ëª¨ë“œ | ì‹¤í–‰ ì£¼ê¸° | ë¦¬ì†ŒìŠ¤ ì‚¬ìš© | ê¶Œì¥ í™˜ê²½ |
|------|----------|------------|----------|
| **CronJob** | ì£¼ê¸°ì  (ì˜ˆ: 15ë¶„) | ì‹¤í–‰ ì‹œë§Œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© | ì†Œ~ì¤‘ê·œëª¨ í´ëŸ¬ìŠ¤í„° (ê¶Œì¥) |
| **Deployment** | ì§€ì†ì  ì‹¤í–‰ | í•­ìƒ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© | ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„° (1000+ ë…¸ë“œ) |

### 8.3 ì£¼ìš” Descheduler ì „ëµ

#### ì „ëµ 1: RemoveDuplicates

**ëª©ì **: ê°™ì€ Controller(ReplicaSet, Deployment)ì˜ Podê°€ í•œ ë…¸ë“œì— ì¤‘ë³µ ë°°ì¹˜ëœ ê²½ìš°, ë¶„ì‚°ì‹œí‚´

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: RemoveDuplicates
          args:
            # ë…¸ë“œë‹¹ ê°™ì€ Controllerì˜ Pod 1ê°œë§Œ ìœ ì§€
            excludeOwnerKinds:
            - "ReplicaSet"
            - "StatefulSet"
        plugins:
          balance:
            enabled:
            - RemoveDuplicates
```

**íš¨ê³¼**: ê°™ì€ Deploymentì˜ replicaê°€ ì—¬ëŸ¬ ê°œ í•œ ë…¸ë“œì— ìˆìœ¼ë©´, ì¼ë¶€ë¥¼ Evictí•˜ì—¬ ë‹¤ë¥¸ ë…¸ë“œë¡œ ë¶„ì‚°

#### ì „ëµ 2: LowNodeUtilization

**ëª©ì **: ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ ì´ ë‚®ì€ ë…¸ë“œì™€ ë†’ì€ ë…¸ë“œ ê°„ ê· í˜• ì¡°ì •

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: LowNodeUtilization
          args:
            # ë‚®ì€ ì‚¬ìš©ë¥  ê¸°ì¤€ (ì´ ì´í•˜ë©´ underutilized)
            thresholds:
              cpu: 20
              memory: 20
              pods: 20
            # ë†’ì€ ì‚¬ìš©ë¥  ê¸°ì¤€ (ì´ ì´ìƒì´ë©´ overutilized)
            targetThresholds:
              cpu: 50
              memory: 50
              pods: 50
        plugins:
          balance:
            enabled:
            - LowNodeUtilization
```

**ë™ì‘:**
1. CPU/Memory/Pod ìˆ˜ê°€ 20% ë¯¸ë§Œì¸ ë…¸ë“œ ì‹ë³„ (underutilized)
2. 50% ì´ìƒì¸ ë…¸ë“œ ì‹ë³„ (overutilized)
3. overutilized ë…¸ë“œì—ì„œ Podë¥¼ Evict
4. Kubernetes ìŠ¤ì¼€ì¤„ëŸ¬ê°€ underutilized ë…¸ë“œì— ì¬ë°°ì¹˜

#### ì „ëµ 3: RemovePodsViolatingNodeAffinity

**ëª©ì **: Node Affinity ì¡°ê±´ì„ ìœ„ë°˜í•˜ëŠ” Pod ì œê±° (ë…¸ë“œ ë ˆì´ë¸” ë³€ê²½ í›„)

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: RemovePodsViolatingNodeAffinity
          args:
            nodeAffinityType:
            - requiredDuringSchedulingIgnoredDuringExecution
        plugins:
          deschedule:
            enabled:
            - RemovePodsViolatingNodeAffinity
```

**ì‹œë‚˜ë¦¬ì˜¤**: GPU ë…¸ë“œì—ì„œ `gpu=true` ë ˆì´ë¸” ì œê±° â†’ GPU ìš”êµ¬ Podê°€ ë ˆì´ë¸” ì—†ëŠ” ë…¸ë“œì— ë‚¨ìŒ â†’ Deschedulerê°€ Evict â†’ GPU ë…¸ë“œì— ì¬ë°°ì¹˜

#### ì „ëµ 4: RemovePodsViolatingInterPodAntiAffinity

**ëª©ì **: Pod Anti-Affinity ì¡°ê±´ì„ ìœ„ë°˜í•˜ëŠ” Pod ì œê±°

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        plugins:
          deschedule:
            enabled:
            - RemovePodsViolatingInterPodAntiAffinity
```

**ì‹œë‚˜ë¦¬ì˜¤**: ì´ˆê¸°ì—ëŠ” ë…¸ë“œê°€ ì¶©ë¶„í•˜ì—¬ Anti-Affinity ë§Œì¡± â†’ ë…¸ë“œ ì¶•ì†Œë¡œ ê°™ì€ ë…¸ë“œì— ìœ„ë°˜ Pod ë°°ì¹˜ â†’ ë…¸ë“œ ì¶”ê°€ í›„ Deschedulerê°€ ì¬ë°°ì¹˜

#### ì „ëµ 5: RemovePodsHavingTooManyRestarts

**ëª©ì **: ê³¼ë„í•˜ê²Œ ì¬ì‹œì‘ë˜ëŠ” ë¬¸ì œ ìˆëŠ” Pod ì œê±° (ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ì¬ì‹œë„)

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: RemovePodsHavingTooManyRestarts
          args:
            podRestartThreshold: 10  # 10íšŒ ì´ìƒ ì¬ì‹œì‘ ì‹œ Evict
            includingInitContainers: true
        plugins:
          deschedule:
            enabled:
            - RemovePodsHavingTooManyRestarts
```

#### ì „ëµ 6: PodLifeTime

**ëª©ì **: ì˜¤ë˜ëœ Podë¥¼ ì œê±°í•˜ì—¬ ìµœì‹  ì´ë¯¸ì§€/ì„¤ì •ìœ¼ë¡œ êµì²´

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: PodLifeTime
          args:
            maxPodLifeTimeSeconds: 604800  # 7ì¼ (7 * 24 * 3600)
            # íŠ¹ì • ìƒíƒœì˜ Podë§Œ ëŒ€ìƒ
            states:
            - Running
            # íŠ¹ì • ë ˆì´ë¸”ì˜ Pod ì œì™¸
            labelSelector:
              matchExpressions:
              - key: app
                operator: NotIn
                values:
                - stateful-db
        plugins:
          deschedule:
            enabled:
            - PodLifeTime
```

### 8.4 Descheduler vs Karpenter Consolidation ë¹„êµ

| ê¸°ëŠ¥ | Descheduler | Karpenter Consolidation |
|------|------------|------------------------|
| **ëª©ì ** | Pod ì¬ë°°ì¹˜ (ê· í˜•) | ë…¸ë“œ ì œê±° (ë¹„ìš© ì ˆê°) |
| **ë²”ìœ„** | Pod ë ˆë²¨ | ë…¸ë“œ ë ˆë²¨ |
| **ì‹¤í–‰ ì£¼ê¸°** | CronJob (ì˜ˆ: 15ë¶„) | ì§€ì†ì  ê°ì‹œ (ì‹¤ì‹œê°„) |
| **ì „ëµ** | ë‹¤ì–‘í•œ ì „ëµ (6+ê°œ) | Empty / Underutilized ë…¸ë“œ |
| **PDB ì¡´ì¤‘** | âœ… ì˜ˆ | âœ… ì˜ˆ |
| **ë…¸ë“œ ì¶”ê°€/ì œê±°** | âŒ ì•„ë‹ˆì˜¤ | âœ… ì˜ˆ |
| **Cluster Autoscaler í˜¸í™˜** | âœ… ì˜ˆ | N/A (ëŒ€ì²´ì¬) |
| **ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€** | ë¶ˆê· í˜• í•´ì†Œ, Affinity ìœ„ë°˜ í•´ê²° | ë¹„ìš© ìµœì í™”, ë…¸ë“œ í†µí•© |
| **í•¨ê»˜ ì‚¬ìš© ê°€ëŠ¥** | âœ… Karpenterì™€ ë³‘í–‰ ê°€ëŠ¥ | âœ… Deschedulerì™€ ë³‘í–‰ ê°€ëŠ¥ |

:::tip Descheduler + Karpenter ì¡°í•© ê¶Œì¥
DeschedulerëŠ” Pod ì¬ë°°ì¹˜ì— íŠ¹í™”ë˜ê³ , KarpenterëŠ” ë…¸ë“œ ê´€ë¦¬ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‘ ë„êµ¬ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì‹œë„ˆì§€ê°€ ë°œìƒí•©ë‹ˆë‹¤:
- Deschedulerê°€ ë¶ˆê· í˜• Podë¥¼ Evict
- Kubernetes ìŠ¤ì¼€ì¤„ëŸ¬ê°€ Podë¥¼ ë‹¤ë¥¸ ë…¸ë“œì— ì¬ë°°ì¹˜
- Karpenterê°€ ë¹„ì–´ìˆëŠ” ë…¸ë“œë¥¼ ì œê±°í•˜ì—¬ ë¹„ìš© ì ˆê°
:::

**í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ì„¤ì • ì˜ˆì‹œ:**

```yaml
# Descheduler: 15ë¶„ë§ˆë‹¤ ê· í˜• ì¡°ì •
apiVersion: batch/v1
kind: CronJob
metadata:
  name: descheduler
  namespace: kube-system
spec:
  schedule: "*/15 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: descheduler
            image: registry.k8s.io/descheduler/descheduler:v0.29.0
            command:
            - /bin/descheduler
            - --policy-config-file=/policy/policy.yaml
---
# Karpenter: ì§€ì†ì  ë…¸ë“œ í†µí•©
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m  # 5ë¶„ í›„ í†µí•© ì‹œì‘
    budgets:
    - nodes: "20%"
```

#### 8.4.1 Descheduler + Karpenter ì‹¤ì „ ì¡°í•© íŒ¨í„´

Deschedulerì™€ Karpenterë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ Pod ì¬ë°°ì¹˜ì™€ ë…¸ë“œ í†µí•©ì´ ìë™ìœ¼ë¡œ ì¡°ìœ¨ë˜ì–´ í´ëŸ¬ìŠ¤í„° íš¨ìœ¨ì„±ê³¼ ë¹„ìš© ì ˆê°ì„ ë™ì‹œì— ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì¡°í•©ì˜ ì‹œë„ˆì§€ ì›ë¦¬:**

1. **1ë‹¨ê³„ (Descheduler)**: ë¦¬ì†ŒìŠ¤ ë¶ˆê· í˜• ê°ì§€ ë° Pod ì¬ë°°ì¹˜
   - `LowNodeUtilization` ì „ëµìœ¼ë¡œ ê³¼ë„í•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ë…¸ë“œì—ì„œ Pod Evict
   - `RemoveDuplicates`, `RemovePodsViolatingNodeAffinity` ë“±ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ Pod ì¬ë°°ì¹˜

2. **2ë‹¨ê³„ (Kubernetes Scheduler)**: Evictëœ Podë¥¼ ìµœì ì˜ ë…¸ë“œì— ì¬ìŠ¤ì¼€ì¤„ë§
   - ë¦¬ì†ŒìŠ¤ ì—¬ìœ ê°€ ìˆëŠ” ë…¸ë“œ ì„ íƒ
   - Affinity/Anti-Affinity, Topology Spread ì¡°ê±´ ë§Œì¡±

3. **3ë‹¨ê³„ (Karpenter)**: ë¹„ì–´ìˆê±°ë‚˜ ì €í™œìš© ë…¸ë“œ ì œê±°
   - `consolidateAfter` ì‹œê°„ ê²½ê³¼ í›„ ë¹ˆ ë…¸ë“œ í†µí•©
   - ì—¬ëŸ¬ ì €í™œìš© ë…¸ë“œì˜ Podë¥¼ ë” ì‘ì€ ìˆ˜ì˜ ë…¸ë“œë¡œ í†µí•©
   - ë¶ˆí•„ìš”í•œ ë…¸ë“œ ì¢…ë£Œë¡œ ë¹„ìš© ì ˆê°

**íƒ€ì´ë° ì¡°ìœ¨ ì˜ˆì‹œ:**

```yaml
# Descheduler: 15ë¶„ ì£¼ê¸°ë¡œ LowNodeUtilization ì‹¤í–‰
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: LowNodeUtilization
          args:
            thresholds:
              cpu: 20
              memory: 20
              pods: 20
            targetThresholds:
              cpu: 50
              memory: 50
              pods: 50
        plugins:
          balance:
            enabled:
            - LowNodeUtilization
---
# Karpenter: 5ë¶„ í›„ ë¹ˆ ë…¸ë“œ í†µí•© (Descheduler ì‹¤í–‰ í›„ ì¶©ë¶„í•œ ì‹œê°„ ë¶€ì—¬)
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general-pool
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m  # Deschedulerê°€ Podë¥¼ ì´ë™ì‹œí‚¨ í›„ 5ë¶„ ëŒ€ê¸°
    budgets:
    - nodes: "20%"  # ë™ì‹œì— ìµœëŒ€ 20% ë…¸ë“œë§Œ í†µí•©
  template:
    spec:
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]
```

**ì‹¤ì œ ë™ì‘ ì‹œë‚˜ë¦¬ì˜¤:**

```
ì‹œê°„: 00:00 - Descheduler ì‹¤í–‰ (15ë¶„ ì£¼ê¸°)
  â””â”€ Node-A (CPU 80%, Memory 85%) â†’ overutilized ê°ì§€
  â””â”€ Node-B (CPU 15%, Memory 10%) â†’ underutilized ê°ì§€
  â””â”€ Node-Aì—ì„œ Pod-1, Pod-2 Evict

ì‹œê°„: 00:01 - Kubernetes Scheduler ì¬ë°°ì¹˜
  â””â”€ Pod-1 â†’ Node-Bë¡œ ìŠ¤ì¼€ì¤„ë§
  â””â”€ Pod-2 â†’ Node-Cë¡œ ìŠ¤ì¼€ì¤„ë§
  â””â”€ Node-AëŠ” ì´ì œ CPU 50%, Memory 55% (ì •ìƒ ë²”ìœ„)

ì‹œê°„: 00:06 - Karpenter í†µí•© (5ë¶„ ê²½ê³¼)
  â””â”€ Node-B: ì—¬ì „íˆ ì €í™œìš© ìƒíƒœì§€ë§Œ Pod ì‹¤í–‰ ì¤‘ â†’ ìœ ì§€
  â””â”€ Node-D: ë¹ˆ ë…¸ë“œ ê°ì§€ (ì´ì „ì— Podê°€ ìˆì—ˆìœ¼ë‚˜ ì´ë™) â†’ ì¢…ë£Œ
  â””â”€ ë¹„ìš© ì ˆê° ë‹¬ì„±
```

**PDBì™€ì˜ ìƒí˜¸ì‘ìš©:**

Deschedulerì™€ KarpenterëŠ” ëª¨ë‘ PDBë¥¼ ì¡´ì¤‘í•˜ë¯€ë¡œ, ì•ˆì „í•œ ì¡°í•©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:

```yaml
# ì• í”Œë¦¬ì¼€ì´ì…˜ PDB ì„¤ì •
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-server-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: api-server
---
# Deschedulerì™€ KarpenterëŠ” ëª¨ë‘ ì´ PDBë¥¼ ì¡´ì¤‘í•˜ë©° ë™ì‘
# - Descheduler: minAvailableì„ ìœ„ë°˜í•˜ëŠ” Eviction ì°¨ë‹¨
# - Karpenter: ë…¸ë“œ ì œê±° ì‹œ minAvailable ë³´ì¥
```

**ì£¼ì˜ì‚¬í•­: Pod Flapping ë°©ì§€**

Deschedulerì™€ Karpenterì˜ íƒ€ì´ë°ì´ ì¶©ëŒí•˜ë©´ Podê°€ ë°˜ë³µì ìœ¼ë¡œ ì´ë™í•˜ëŠ” í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

:::warning Pod Flapping ë°©ì§€
Descheduler ì‹¤í–‰ ì£¼ê¸°ì™€ Karpenter `consolidateAfter` ê°„ê²©ì„ ì ì ˆíˆ ì¡°ìœ¨í•˜ì„¸ìš”:
- **ê¶Œì¥ íŒ¨í„´**: Descheduler 15ë¶„ ì£¼ê¸° + Karpenter 5ë¶„ `consolidateAfter`
- **ìœ„í—˜ íŒ¨í„´**: Descheduler 5ë¶„ ì£¼ê¸° + Karpenter 1ë¶„ `consolidateAfter` (ë„ˆë¬´ ë¹ˆë²ˆ)
- **ì•ˆì „ì¥ì¹˜**: Karpenter `budgets`ë¡œ ë™ì‹œ í†µí•© ë…¸ë“œ ìˆ˜ ì œí•œ
:::

**ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦:**

```bash
# 1. Descheduler ë¡œê·¸ í™•ì¸
kubectl logs -n kube-system -l app=descheduler --tail=50

# 2. Karpenter í†µí•© ì´ë²¤íŠ¸ í™•ì¸
kubectl logs -n karpenter -l app.kubernetes.io/name=karpenter --tail=50 | grep consolidation

# 3. ë…¸ë“œë³„ Pod ë¶„í¬ í™•ì¸
kubectl get pods -A -o wide | awk '{print $8}' | sort | uniq -c

# 4. ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  í™•ì¸
kubectl top nodes

# 5. PDB ìƒíƒœ í™•ì¸ (Eviction ì°¨ë‹¨ ì—¬ë¶€)
kubectl get pdb -A
```

**ì¡°í•©ì˜ ì¥ì :**

| ì¥ì  | ì„¤ëª… |
|------|------|
| **ìë™ ê· í˜• ì¡°ì •** | Deschedulerê°€ ë¦¬ì†ŒìŠ¤ ë¶ˆê· í˜• ìë™ í•´ì†Œ |
| **ë¹„ìš© ìµœì í™”** | Karpenterê°€ ë¶ˆí•„ìš”í•œ ë…¸ë“œ ì œê±° |
| **ì•ˆì „ì„± ë³´ì¥** | PDB ì¡´ì¤‘ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë°©ì§€ |
| **ìš´ì˜ ë¶€ë‹´ ê°ì†Œ** | ìˆ˜ë™ ê°œì… ì—†ì´ ìë™ ì¡°ìœ¨ |
| **í™•ì¥ì„±** | í´ëŸ¬ìŠ¤í„° ê·œëª¨ì— ê´€ê³„ì—†ì´ ë™ì‘ |

---

## 9. EKS ìŠ¤ì¼€ì¤„ë§ ì¢…í•© ì „ëµ

### 9.1 ì›Œí¬ë¡œë“œ ìœ í˜•ë³„ ìŠ¤ì¼€ì¤„ë§ ì„¤ì • ë§¤íŠ¸ë¦­ìŠ¤

ì•„ë˜ í‘œëŠ” ë‹¤ì–‘í•œ ì›Œí¬ë¡œë“œ ìœ í˜•ì— ëŒ€í•œ ê¶Œì¥ ìŠ¤ì¼€ì¤„ë§ ì„¤ì •ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.

| ì›Œí¬ë¡œë“œ ìœ í˜• | Node Selector/Affinity | Pod Anti-Affinity | Topology Spread | Taints/Tolerations | PriorityClass | PDB | ì¶”ê°€ ê³ ë ¤ì‚¬í•­ |
|-------------|----------------------|-------------------|-----------------|-------------------|---------------|-----|-------------|
| **API ì„œë²„** | On-Demand ë…¸ë“œ | Soft (ë…¸ë“œ ë¶„ì‚°) | Hard (AZ ë¶„ì‚°) | - | `high-priority` | `minAvailable: "67%"` | Readiness Probe í•„ìˆ˜ |
| **ê²°ì œ ì„œë¹„ìŠ¤** | On-Demand, íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | Hard (ë…¸ë“œ ë¶„ì‚°) | Hard (AZ ë¶„ì‚°, minDomains: 3) | - | `business-critical` | `minAvailable: 2` | PCI-DSS ì¤€ìˆ˜ ë…¸ë“œ |
| **ML í•™ìŠµ** | GPU ë…¸ë“œ (g5.xlarge+) | Soft (ë…¸ë“œ ë¶„ì‚°) | - | GPU Taint Tolerate | `high-priority` | `maxUnavailable: 1` | Spot ê°€ëŠ¥ (checkpointing ìˆì„ ë•Œ) |
| **ML ì¶”ë¡ ** | GPU ë…¸ë“œ | Hard (AZ ë¶„ì‚°) | Hard (AZ ë¶„ì‚°) | GPU Taint Tolerate | `high-priority` | `minAvailable: 2` | On-Demand ê¶Œì¥ |
| **ë°ì´í„°ë² ì´ìŠ¤ (StatefulSet)** | EBS ê°€ìš© ë…¸ë“œ, WaitForFirstConsumer | Hard (ë…¸ë“œ ë¶„ì‚°) | Hard (AZ ë¶„ì‚°) | - | `business-critical` | `maxUnavailable: 1` | PVC ë°±ì—… í•„ìˆ˜ |
| **ìºì‹œ (Redis)** | Memory ìµœì í™” ë…¸ë“œ (r6i) | Hard (ë…¸ë“œ ë¶„ì‚°) | Hard (AZ ë¶„ì‚°) | - | `high-priority` | `minAvailable: 2` | Persistence ì„¤ì • |
| **ë°°ì¹˜ ì‘ì—…** | Spot ë…¸ë“œ í—ˆìš© | - | - | Spot Tolerate | `low-priority`, `preemptionPolicy: Never` | - | ì¬ì‹œì‘ ê°€ëŠ¥ ì„¤ê³„ |
| **CI/CD Runner** | Spot ë…¸ë“œ ì„ í˜¸ | - | - | Spot Tolerate | `low-priority` | - | Ephemeral ì‘ì—… |
| **ë¡œê·¸ ìˆ˜ì§‘ (DaemonSet)** | ëª¨ë“  ë…¸ë“œ | - | - | ëª¨ë“  Taint Tolerate | `system-critical` | - | `hostPath` ì‚¬ìš© |
| **Ingress Controller** | On-Demand | Hard (ë…¸ë“œ ë¶„ì‚°) | Hard (AZ ë¶„ì‚°) | - | `high-priority` | `minAvailable: 2` | NodePort / LB êµ¬ì„± |
| **ëª¨ë‹ˆí„°ë§ (Prometheus)** | ì „ìš© ëª¨ë‹ˆí„°ë§ ë…¸ë“œ | Soft (ë…¸ë“œ ë¶„ì‚°) | Soft (AZ ë¶„ì‚°) | ëª¨ë‹ˆí„°ë§ Taint Tolerate | `high-priority` | `minAvailable: 1` | ëŒ€ìš©ëŸ‰ ìŠ¤í† ë¦¬ì§€ |
| **ì›¹ í”„ë¡ íŠ¸ì—”ë“œ** | ARM ë…¸ë“œ ê°€ëŠ¥ | Soft (ë…¸ë“œ ë¶„ì‚°) | Hard (AZ ë¶„ì‚°) | - | `standard-priority` | `minAvailable: "50%"` | CDN í†µí•© |
| **ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤** | Spot ë…¸ë“œ | - | Soft (AZ ë¶„ì‚°) | Spot Tolerate | `standard-priority` | `maxUnavailable: "50%"` | ì¬ì‹œë„ ë¡œì§ í•„ìˆ˜ |
| **Serverless (Knative)** | Spot + On-Demand í˜¼í•© | - | Soft (AZ ë¶„ì‚°) | - | `standard-priority` | - | Scale-to-zero ì„¤ì • |
| **AI/ML í•™ìŠµ** | GPU ë…¸ë“œ (g5.xlarge+) | Soft (ë…¸ë“œ ë¶„ì‚°) | Soft (AZ ë¶„ì‚°) | GPU Taint Tolerate | `high-priority` | `maxUnavailable: 1` | Checkpointing, Spot ê°€ëŠ¥ |
| **AI/ML ì¶”ë¡ ** | GPU/Inferentia ë…¸ë“œ | Hard (ë…¸ë“œ ë¶„ì‚°) | Hard (AZ ë¶„ì‚°) | GPU Taint Tolerate | `high-priority` | `minAvailable: 2` | On-Demand ê¶Œì¥ |

### 9.2 AI/ML ì›Œí¬ë¡œë“œ ìŠ¤ì¼€ì¤„ë§ íŒ¨í„´

AI/ML ì›Œí¬ë¡œë“œëŠ” GPU, ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬, íŠ¹ìˆ˜ ê°€ì†ê¸°(Inferentia, Trainium) ë“±ì˜ ë¦¬ì†ŒìŠ¤ë¥¼ í•„ìš”ë¡œ í•˜ë©°, í•™ìŠµê³¼ ì¶”ë¡ ì˜ ìš”êµ¬ì‚¬í•­ì´ í¬ê²Œ ë‹¤ë¦…ë‹ˆë‹¤.

#### 9.2.1 GPU ì›Œí¬ë¡œë“œ ìŠ¤ì¼€ì¤„ë§

GPU ì›Œí¬ë¡œë“œëŠ” ì „ìš© ë…¸ë“œ ê²©ë¦¬, ë¦¬ì†ŒìŠ¤ ìš”ì²­, Node Affinityë¥¼ ì¡°í•©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ìŠ¤ì¼€ì¤„ë§í•©ë‹ˆë‹¤.

**GPU ë¦¬ì†ŒìŠ¤ ìš”ì²­ íŒ¨í„´:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-training-job
spec:
  containers:
  - name: trainer
    image: ml/trainer:v3.0
    resources:
      requests:
        nvidia.com/gpu: 1  # GPU 1ê°œ ìš”ì²­
        cpu: "4"
        memory: 16Gi
      limits:
        nvidia.com/gpu: 1  # limitsëŠ” requestsì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        cpu: "4"
        memory: 16Gi
```

:::info GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
`nvidia.com/gpu`ëŠ” ì •ìˆ˜ ë‹¨ìœ„ë¡œë§Œ ìš”ì²­ ê°€ëŠ¥í•˜ë©°, limitsëŠ” requestsì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤. GPUëŠ” ì˜¤ë²„ì»¤ë°‹(overcommit)ì´ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ fractional GPUê°€ í•„ìš”í•˜ë©´ Multi-Instance GPU(MIG) ë˜ëŠ” Time-Slicingì„ ê³ ë ¤í•˜ì„¸ìš”.
:::

**GPU ì „ìš© NodePool + ì›Œí¬ë¡œë“œ ë°°í¬:**

```yaml
# Karpenter NodePool: GPU ì „ìš© ë…¸ë“œ ê·¸ë£¹
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-pool
spec:
  template:
    spec:
      requirements:
      - key: node.kubernetes.io/instance-type
        operator: In
        values:
        - g5.xlarge   # 1x NVIDIA A10G, 4 vCPU, 16 GiB
        - g5.2xlarge  # 1x NVIDIA A10G, 8 vCPU, 32 GiB
        - g5.4xlarge  # 1x NVIDIA A10G, 16 vCPU, 64 GiB
        - g5.12xlarge # 4x NVIDIA A10G, 48 vCPU, 192 GiB
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand  # í•™ìŠµ ì›Œí¬ë¡œë“œëŠ” On-Demand ê¶Œì¥
      # GPU ì „ìš© ë…¸ë“œ ê²©ë¦¬
      taints:
      - key: nvidia.com/gpu
        value: present
        effect: NoSchedule
      - key: workload-type
        value: ml-training
        effect: NoSchedule
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodes
  limits:
    cpu: "200"
    memory: 800Gi
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 10m  # ë¹ˆ GPU ë…¸ë“œëŠ” 10ë¶„ í›„ ì œê±° (ë¹„ìš© ì ˆê°)
---
# EC2NodeClass: GPU ë…¸ë“œ êµ¬ì„±
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu-nodes
spec:
  amiFamily: AL2
  amiSelectorTerms:
  - alias: al2@latest  # GPU ë“œë¼ì´ë²„ í¬í•¨ëœ EKS-optimized AMI
  role: KarpenterNodeRole
  subnetSelectorTerms:
  - tags:
      karpenter.sh/discovery: my-cluster
  securityGroupSelectorTerms:
  - tags:
      karpenter.sh/discovery: my-cluster
  userData: |
    #!/bin/bash
    # NVIDIA ì»¨í…Œì´ë„ˆ ëŸ°íƒ€ì„ ì„¤ì • (ì´ë¯¸ AMIì— í¬í•¨ë¨)
    echo "GPU node initialized"
---
# ML í•™ìŠµ ì›Œí¬ë¡œë“œ: GPU ë…¸ë“œì— ìŠ¤ì¼€ì¤„ë§
apiVersion: batch/v1
kind: Job
metadata:
  name: model-training
spec:
  parallelism: 4  # 4ê°œ ë³‘ë ¬ í•™ìŠµ
  completions: 4
  template:
    metadata:
      labels:
        app: model-training
    spec:
      # GPU Taint Tolerate
      tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: present
        effect: NoSchedule
      - key: workload-type
        operator: Equal
        value: ml-training
        effect: NoSchedule
      # GPU ë…¸ë“œ ì„ íƒ
      nodeSelector:
        node.kubernetes.io/instance-type: g5.2xlarge
      # Pod Anti-Affinity: ê° Job Podë¥¼ ë‹¤ë¥¸ ë…¸ë“œì— ë°°ì¹˜
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - model-training
              topologyKey: kubernetes.io/hostname
      containers:
      - name: trainer
        image: ml/pytorch-trainer:v2.0
        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: "7"
            memory: 28Gi
          limits:
            nvidia.com/gpu: 1
            cpu: "7"
            memory: 28Gi
        env:
        - name: NCCL_DEBUG
          value: "INFO"
        volumeMounts:
        - name: data
          mountPath: /data
        - name: checkpoints
          mountPath: /checkpoints
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: training-data
      - name: checkpoints
        persistentVolumeClaim:
          claimName: model-checkpoints
      restartPolicy: OnFailure
```

**Multi-Instance GPU (MIG) í™œìš©:**

NVIDIA A100, A30 ë“±ì˜ GPUëŠ” MIGë¥¼ ì§€ì›í•˜ì—¬ í•˜ë‚˜ì˜ GPUë¥¼ ì—¬ëŸ¬ ë…ë¦½ì ì¸ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¶„í• í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
# MIG í”„ë¡œíŒŒì¼ ìš”ì²­ ì˜ˆì‹œ (A100 GPU)
apiVersion: v1
kind: Pod
metadata:
  name: mig-inference
spec:
  containers:
  - name: inference
    image: ml/inference:v1.0
    resources:
      requests:
        nvidia.com/mig-1g.5gb: 1  # 1/7 A100 (1 GPU slice, 5GB memory)
      limits:
        nvidia.com/mig-1g.5gb: 1
```

**MIG í”„ë¡œíŒŒì¼:**

| MIG í”„ë¡œíŒŒì¼ | GPU Slice | ë©”ëª¨ë¦¬ | ì‚¬ìš© ì‚¬ë¡€ |
|-------------|-----------|--------|----------|
| `mig-1g.5gb` | 1/7 | 5GB | ì†Œí˜• ì¶”ë¡  |
| `mig-2g.10gb` | 2/7 | 10GB | ì¤‘í˜• ì¶”ë¡  |
| `mig-3g.20gb` | 3/7 | 20GB | ëŒ€í˜• ì¶”ë¡  |
| `mig-7g.40gb` | 7/7 | 40GB | ì „ì²´ GPU (í•™ìŠµ) |

#### 9.2.2 DRA (Dynamic Resource Allocation) ì†Œê°œ

Kubernetes 1.34+ì—ì„œëŠ” Dynamic Resource Allocation(DRA)ì„ í†µí•´ GPU ë“±ì˜ íŠ¹ìˆ˜ ë¦¬ì†ŒìŠ¤ë¥¼ ë” ìœ ì—°í•˜ê²Œ í• ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**DRAì˜ ì¥ì :**

| ê¸°ì¡´ ë°©ì‹ (Device Plugin) | DRA (K8s 1.34+) |
|-------------------------|----------------|
| ì •ì  ë¦¬ì†ŒìŠ¤ ì´ë¦„ (`nvidia.com/gpu`) | ë™ì  ë¦¬ì†ŒìŠ¤ í´ë ˆì„ |
| ë…¸ë“œ ë ˆë²¨ í• ë‹¹ | Pod ë ˆë²¨ ì„¸ë°€í•œ ì œì–´ |
| ë‹¨ìˆœ ì¹´ìš´íŒ… (1, 2, 3...) | ë¦¬ì†ŒìŠ¤ ì†ì„± ê¸°ë°˜ ì„ íƒ |
| ì œí•œì  ê³µìœ  | ë™ì  ê³µìœ /ë¶„í•  |
| ë…¸ë“œ ì¬ì‹œì‘ í•„ìš” | ëŸ°íƒ€ì„ ì¬êµ¬ì„± |

**DRA ResourceClass ë° ResourceClaim ì˜ˆì‹œ:**

```yaml
# ResourceClass: GPU ë¦¬ì†ŒìŠ¤ í´ë˜ìŠ¤ ì •ì˜
apiVersion: resource.k8s.io/v1alpha4
kind: ResourceClass
metadata:
  name: nvidia-a100-gpu
spec:
  driverName: gpu.nvidia.com
  parameters:
    apiVersion: gpu.nvidia.com/v1alpha1
    kind: GpuConfig
    memory: "40Gi"
    computeCapability: "8.0"  # A100
    migEnabled: true
---
# ResourceClaim: GPU ë¦¬ì†ŒìŠ¤ ìš”ì²­
apiVersion: resource.k8s.io/v1alpha4
kind: ResourceClaim
metadata:
  name: ml-training-gpu
  namespace: ml-team
spec:
  resourceClassName: nvidia-a100-gpu
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClaimParameters
    name: training-params
---
# GpuClaimParameters: ì„¸ë¶€ ìš”êµ¬ì‚¬í•­
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: training-params
  namespace: ml-team
spec:
  count: 1  # GPU 1ê°œ
  migProfile: "mig-3g.20gb"  # MIG í”„ë¡œíŒŒì¼ ì§€ì •
  sharing: "TimeSlicing"  # ì‹œê°„ ë¶„í•  ê³µìœ  í—ˆìš©
---
# Pod: ResourceClaim ì‚¬ìš©
apiVersion: v1
kind: Pod
metadata:
  name: dra-training-pod
  namespace: ml-team
spec:
  resourceClaims:
  - name: gpu-claim
    resourceClaimName: ml-training-gpu
  containers:
  - name: trainer
    image: ml/trainer:v3.0
    resources:
      claims:
      - name: gpu-claim
    env:
    - name: CUDA_VISIBLE_DEVICES
      value: "0"
```

:::info DRA ë„ì… ì‹œê¸°
DRAëŠ” Kubernetes 1.34ì—ì„œ Alpha, 1.36ì—ì„œ Betaë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ ì‚¬ìš©ì€ 1.38+ Stable ì´í›„ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. í˜„ì¬ëŠ” ê¸°ì¡´ Device Plugin ë°©ì‹ì´ ì•ˆì •ì ì…ë‹ˆë‹¤.
:::

#### 9.2.3 AI í•™ìŠµ vs ì¶”ë¡  ìŠ¤ì¼€ì¤„ë§ ì „ëµ

AI/ML ì›Œí¬ë¡œë“œëŠ” í•™ìŠµ(Training)ê³¼ ì¶”ë¡ (Inference)ì˜ ìš”êµ¬ì‚¬í•­ì´ í¬ê²Œ ë‹¤ë¥´ë¯€ë¡œ, ê°ê°ì— ë§ëŠ” ìŠ¤ì¼€ì¤„ë§ ì „ëµì´ í•„ìš”í•©ë‹ˆë‹¤.

**í•™ìŠµ vs ì¶”ë¡  ë¹„êµ:**

| ë¹„êµ í•­ëª© | í•™ìŠµ (Training) | ì¶”ë¡  (Inference) |
|----------|----------------|-----------------|
| **GPU ìš”êµ¬** | ëŒ€ê·œëª¨ (4-8+ GPU) | ì†Œê·œëª¨ (1-2 GPU) ë˜ëŠ” Inferentia |
| **ì‹¤í–‰ ì‹œê°„** | ì¥ì‹œê°„ (ìˆ˜ ì‹œê°„~ìˆ˜ ì¼) | ì§§ì€ ì§€ì—° (ms~ì´ˆ) |
| **ì›Œí¬ë¡œë“œ íƒ€ì…** | ë°°ì¹˜ ì‘ì—… (Job) | ìƒì‹œ ì„œë¹„ìŠ¤ (Deployment) |
| **ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…** | g5, p4d, p5 (NVIDIA) | g5 (ì†Œí˜•), inf2 (Inferentia), c7g (Graviton) |
| **Spot ì‚¬ìš©** | âœ… ê°€ëŠ¥ (Checkpointing í•„ìˆ˜) | âš ï¸ ì‹ ì¤‘ (ê³ ê°€ìš©ì„± í•„ìš”) |
| **PriorityClass** | `standard-priority` | `high-priority` |
| **PDB** | `maxUnavailable: 1` (ì¬ì‹œì‘ í—ˆìš©) | `minAvailable: 2` (ê°€ìš©ì„± ë³´ì¥) |
| **ìŠ¤ì¼€ì¤„ë§ ì „ëµ** | Soft Anti-Affinity (ë¶„ì‚° ì„ í˜¸) | Hard Anti-Affinity (ì¥ì•  ê²©ë¦¬) |
| **ë¹„ìš© ìµœì í™”** | Spot + Reserved Instances | On-Demand + Savings Plans |

**í•™ìŠµ ì›Œí¬ë¡œë“œ ìŠ¤ì¼€ì¤„ë§ ì˜ˆì‹œ:**

```yaml
# ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµ: 8-GPU Job
apiVersion: batch/v1
kind: Job
metadata:
  name: distributed-training
spec:
  parallelism: 8
  completions: 8
  template:
    metadata:
      labels:
        app: distributed-training
    spec:
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      - key: karpenter.sh/capacity-type
        operator: Equal
        value: spot
        effect: NoSchedule  # Spot ë…¸ë“œ í—ˆìš©
      nodeSelector:
        node.kubernetes.io/instance-type: g5.12xlarge  # 4x A10G per node
      affinity:
        # Soft Anti-Affinity: ê°€ëŠ¥í•˜ë©´ ë‹¤ë¥¸ ë…¸ë“œì— ë¶„ì‚°
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: distributed-training
              topologyKey: kubernetes.io/hostname
      containers:
      - name: trainer
        image: ml/pytorch-distributed:v2.0
        resources:
          requests:
            nvidia.com/gpu: 4  # ë…¸ë“œë‹¹ 4 GPU
            cpu: "45"
            memory: 180Gi
        env:
        - name: MASTER_ADDR
          value: "distributed-training-master"
        - name: WORLD_SIZE
          value: "8"  # ì´ 8ê°œ í”„ë¡œì„¸ìŠ¤
        - name: RANK
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: checkpoints
          mountPath: /checkpoints
      volumes:
      - name: checkpoints
        persistentVolumeClaim:
          claimName: training-checkpoints
      restartPolicy: OnFailure
```

#### 9.2.4 Setu: Kueue + Karpenter í”„ë¡œì•¡í‹°ë¸Œ ìŠ¤ì¼€ì¤„ë§

ë¶„ì‚° AI í•™ìŠµ ì›Œí¬ë¡œë“œ(ì˜ˆ: PyTorch DDP, JAX)ëŠ” **Gang Scheduling** ìš”êµ¬ì‚¬í•­ì„ ê°€ì§‘ë‹ˆë‹¤. ëª¨ë“  GPU ë…¸ë“œê°€ ë™ì‹œì— ì¤€ë¹„ë˜ì§€ ì•Šìœ¼ë©´ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ê°€ ë°œìƒí•˜ê±°ë‚˜ í•™ìŠµì´ ì‹œì‘ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ì¡´ KarpenterëŠ” **ë°˜ì‘í˜•(reactive)** í”„ë¡œë¹„ì €ë‹ë§Œ ì§€ì›í•˜ì—¬, Podê°€ Pending ìƒíƒœê°€ ëœ í›„ì—ì•¼ ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤:

**ê¸°ì¡´ Karpenterì˜ í•œê³„:**

| ë¬¸ì œ | ì„¤ëª… | ì˜í–¥ |
|------|------|------|
| **ë¶€ë¶„ í• ë‹¹ ë¦¬ìŠ¤í¬** | 4-GPU ë…¸ë“œ 4ëŒ€ê°€ í•„ìš”í•œë° 2ëŒ€ë§Œ í”„ë¡œë¹„ì €ë‹ ì„±ê³µ | 2ëŒ€ëŠ” ìœ íœ´ ìƒíƒœ ìœ ì§€, ë¹„ìš© ë‚­ë¹„ |
| **ìŠ¤ì¼€ì¤„ë§ ì§€ì—°** | Pod Pending â†’ Karpenter ê°ì§€ â†’ EC2 í”„ë¡œë¹„ì €ë‹ (ìˆœì°¨ í”„ë¡œì„¸ìŠ¤) | ë¶„ì‚° í•™ìŠµ ì‹œì‘ê¹Œì§€ ìˆ˜ ë¶„ ì†Œìš” |
| **ì›ìì„± ë¶€ì¬** | ì¼ë¶€ ë…¸ë“œë§Œ ìƒì„±ë˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìš©ëŸ‰ ë¶€ì¡±ìœ¼ë¡œ ì‹¤íŒ¨ | ì›Œí¬ë¡œë“œê°€ ë¬´í•œ ëŒ€ê¸° ìƒíƒœ |

**Setu ì†”ë£¨ì…˜:**

SetuëŠ” Kueueì˜ **AdmissionCheck**ì™€ Karpenterì˜ **NodeClaim v1 API**ë¥¼ ë¸Œë¦¿ì§€í•˜ì—¬, ì›Œí¬ë¡œë“œ ìŠ¹ì¸ ì „ì— í•„ìš”í•œ ëª¨ë“  ë…¸ë“œë¥¼ **í”„ë¡œì•¡í‹°ë¸Œí•˜ê²Œ** í”„ë¡œë¹„ì €ë‹í•©ë‹ˆë‹¤.

**ë™ì‘ íë¦„:**

```mermaid
sequenceDiagram
    participant User
    participant Kueue
    participant Setu
    participant Karpenter
    participant EC2

    User->>Kueue: Job ì œì¶œ (4-GPU ë…¸ë“œ 4ëŒ€ í•„ìš”)
    Kueue->>Kueue: Workload ìƒì„± (Pending)
    Kueue->>Setu: AdmissionCheck ìš”ì²­
    Setu->>Karpenter: NodeClaim 4ê°œ ìƒì„± (ì›ìì )
    Karpenter->>EC2: EC2 ì¸ìŠ¤í„´ìŠ¤ 4ëŒ€ í”„ë¡œë¹„ì €ë‹
    EC2-->>Karpenter: ëª¨ë“  ë…¸ë“œ Ready
    Karpenter-->>Setu: NodeClaim ìŠ¹ì¸ ì™„ë£Œ
    Setu-->>Kueue: AdmissionCheck í†µê³¼
    Kueue->>Kueue: Workload ìŠ¹ì¸ (Active)
    Kueue->>User: Pod ìŠ¤ì¼€ì¤„ë§ ì‹œì‘ (ì¦‰ì‹œ ë°°ì¹˜)
```

**Gang Schedulingê³¼ ìŠ¤ì¼€ì¤„ë§ ì•ˆì „ì„±:**

Setuì˜ í•µì‹¬ ê°€ì¹˜ëŠ” **All-or-Nothing** ë³´ì¥ì…ë‹ˆë‹¤. ë¶„ì‚° í•™ìŠµ ì›Œí¬ë¡œë“œëŠ” ëª¨ë“  replicaê°€ ë™ì‹œì— ì‹¤í–‰ë˜ì–´ì•¼ ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤.

| ì‹œë‚˜ë¦¬ì˜¤ | ê¸°ì¡´ Karpenter | Setu + Kueue |
|---------|---------------|-------------|
| **4-GPU ë…¸ë“œ 4ëŒ€ í•„ìš”** | 2ëŒ€ë§Œ ìƒì„± â†’ 2ëŒ€ ìœ íœ´ â†’ ë¹„ìš© ë‚­ë¹„ | 4ëŒ€ ëª¨ë‘ Ready í™•ì¸ í›„ ìŠ¹ì¸ â†’ ë‚­ë¹„ ì œë¡œ |
| **ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì‹¤íŒ¨** | ì¼ë¶€ Podë§Œ Running, ë‚˜ë¨¸ì§€ ë¬´í•œ Pending | ìë™ ë¡¤ë°± + ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ (5s-80s, ìµœëŒ€ 5íšŒ) |
| **ìŠ¤ì¼€ì¤„ë§ ì‹œì‘ ì‹œì ** | ë…¸ë“œê°€ ìƒì„±ë  ë•Œë§ˆë‹¤ ìˆœì°¨ ìŠ¤ì¼€ì¤„ë§ | ëª¨ë“  ë…¸ë“œ Ready â†’ ë™ì‹œ ìŠ¤ì¼€ì¤„ë§ |

**ì‹¤íŒ¨ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§:**

SetuëŠ” ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì‹¤íŒ¨ ì‹œ ì§€ëŠ¥ì ìœ¼ë¡œ ëŒ€ì‘í•©ë‹ˆë‹¤:

```
ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤:
1. NodeClaim 4ê°œ ì¤‘ 3ê°œë§Œ ì„±ê³µ (1ê°œëŠ” Spot ìš©ëŸ‰ ë¶€ì¡±)
2. Setuê°€ ì‹¤íŒ¨ ê°ì§€ â†’ ëª¨ë“  NodeClaim ì‚­ì œ (ë¡¤ë°±)
3. ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„:
   - 1íšŒ ì¬ì‹œë„: 5ì´ˆ í›„
   - 2íšŒ ì¬ì‹œë„: 10ì´ˆ í›„
   - 3íšŒ ì¬ì‹œë„: 20ì´ˆ í›„
   - 4íšŒ ì¬ì‹œë„: 40ì´ˆ í›„
   - 5íšŒ ì¬ì‹œë„: 80ì´ˆ í›„ (ìµœì¢…)
4. 5íšŒ ì‹¤íŒ¨ ì‹œ AdmissionCheck ì˜êµ¬ ì‹¤íŒ¨ â†’ Kueueê°€ Workload ê±°ë¶€
```

**Kueue í†µí•© ì•„í‚¤í…ì²˜:**

SetuëŠ” Kueueì˜ AdmissionCheck CRDë¥¼ ì‚¬ìš©í•˜ì—¬ ì›Œí¬ë¡œë“œ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ì— í†µí•©ë©ë‹ˆë‹¤.

```yaml
# 1. AdmissionCheck ì •ì˜: Setu ì»¨íŠ¸ë¡¤ëŸ¬ ì§€ì •
apiVersion: kueue.x-k8s.io/v1beta1
kind: AdmissionCheck
metadata:
  name: karpenter-provision
spec:
  controllerName: setu.io/karpenter-provision
---
# 2. ClusterQueue: AdmissionCheck ì ìš©
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: ml-training-queue
spec:
  namespaceSelector: {}
  resourceGroups:
  - coveredResources: ["cpu", "memory", "nvidia.com/gpu"]
    flavors:
    - name: gpu-flavor
      resources:
      - name: nvidia.com/gpu
        nominalQuota: 32  # ì´ 32 GPUê¹Œì§€ í—ˆìš©
  # Setu AdmissionCheck ì—°ê²°
  admissionChecks:
  - karpenter-provision
---
# 3. LocalQueue: ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ í
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: training-jobs
  namespace: ml-team
spec:
  clusterQueue: ml-training-queue
---
# 4. Job: Kueue ë¼ë²¨ ì¶”ê°€
apiVersion: batch/v1
kind: Job
metadata:
  name: distributed-training
  namespace: ml-team
  labels:
    kueue.x-k8s.io/queue-name: training-jobs  # LocalQueue ì§€ì •
spec:
  parallelism: 4
  completions: 4
  template:
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: g5.2xlarge
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      containers:
      - name: trainer
        image: ml/pytorch-distributed:v2.0
        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: "7"
            memory: 28Gi
      restartPolicy: OnFailure
```

**ì‹¤í–‰ íë¦„ ìƒì„¸:**

```
1. Job ì œì¶œ â†’ Kueue Workload ìƒì„± (Pending ìƒíƒœ)
2. Kueueê°€ ë¦¬ì†ŒìŠ¤ ì¿¼í„° í™•ì¸ (ClusterQueue: 32 GPU ì¤‘ 4 GPU ì‚¬ìš© ê°€ëŠ¥)
3. Kueueê°€ AdmissionCheck ì‹¤í–‰ â†’ Setu ì»¨íŠ¸ë¡¤ëŸ¬ í˜¸ì¶œ
4. Setuê°€ NodeClaim 4ê°œ ìƒì„± (g5.2xlarge, ê° 1 GPU)
5. Karpenterê°€ EC2 ì¸ìŠ¤í„´ìŠ¤ 4ëŒ€ í”„ë¡œë¹„ì €ë‹
6. ëª¨ë“  ë…¸ë“œ Ready í™•ì¸ (kubelet ë“±ë¡ + GPU ë””ë°”ì´ìŠ¤ í”ŒëŸ¬ê·¸ì¸ í™œì„±í™”)
7. Setuê°€ AdmissionCheck ìŠ¹ì¸ â†’ Kueueê°€ Workload Activeë¡œ ì „í™˜
8. Kueueê°€ Jobì˜ suspend: false ì„¤ì • â†’ Pod 4ê°œ ìŠ¤ì¼€ì¤„ë§ ì‹œì‘
9. Schedulerê°€ Podë¥¼ ìƒˆë¡œ ìƒì„±ëœ GPU ë…¸ë“œì— ì¦‰ì‹œ ë°°ì¹˜ (Pending ì‹œê°„ ì œë¡œ)
```

**ë¹„êµ: ê¸°ì¡´ Karpenter vs Setu + Kueue:**

| ë‹¨ê³„ | ê¸°ì¡´ Karpenter | Setu + Kueue |
|------|---------------|-------------|
| **Job ì œì¶œ** | ì¦‰ì‹œ Pod ìƒì„± (Pending) | Kueueê°€ Workloadë¡œ ê´€ë¦¬ (ìŠ¹ì¸ ëŒ€ê¸°) |
| **ë…¸ë“œ í”„ë¡œë¹„ì €ë‹** | Pod Pending ê°ì§€ í›„ ë°˜ì‘ | AdmissionCheckì—ì„œ ì‚¬ì „ í”„ë¡œë¹„ì €ë‹ |
| **ë¶€ë¶„ ì‹¤íŒ¨ ì²˜ë¦¬** | ì¼ë¶€ Podë§Œ Running, ë‚˜ë¨¸ì§€ ë¬´í•œ ëŒ€ê¸° | ì „ì²´ ë¡¤ë°± + ì¬ì‹œë„ (All-or-Nothing) |
| **ìŠ¤ì¼€ì¤„ë§ ì‹œì‘** | ë…¸ë“œ ìƒì„± ì‹œë§ˆë‹¤ ìˆœì°¨ | ëª¨ë“  ë…¸ë“œ Ready í›„ ë™ì‹œ |
| **ì†Œìš” ì‹œê°„** | 2-3ë¶„ (ìˆœì°¨ í”„ë¡œì„¸ìŠ¤) | 1-2ë¶„ (ë³‘ë ¬ + ì‚¬ì „ ì¤€ë¹„) |

**ê¶Œì¥ ì‚¬ìš© ì‚¬ë¡€:**

| ì›Œí¬ë¡œë“œ ìœ í˜• | Setu í•„ìš” ì—¬ë¶€ | ì´ìœ  |
|-------------|--------------|------|
| **ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµ** (16+ GPU) | âœ… í•„ìˆ˜ | Gang Scheduling ë³´ì¥, ë¶€ë¶„ í• ë‹¹ ë°©ì§€ |
| **ì†Œê·œëª¨ í•™ìŠµ** (1-4 GPU) | âš ï¸ ì„ íƒ | ì˜¤ë²„í—¤ë“œ ëŒ€ë¹„ ì´ë“ ì œí•œì  |
| **ë‹¨ì¼ GPU ì¶”ë¡ ** | âŒ ë¶ˆí•„ìš” | ê¸°ì¡´ Karpenterë¡œ ì¶©ë¶„ |
| **ë°°ì¹˜ ì²˜ë¦¬** (CPU ì›Œí¬ë¡œë“œ) | âš ï¸ ì„ íƒ | ë¹„ìš© íš¨ìœ¨ì„± ëª©ì ì´ë¼ë©´ ìœ ìš© |

:::info Setu ì„¤ì¹˜ ë° ì„¤ì •
SetuëŠ” Karpenter v0.32+ ë° Kueue v0.6+ë¥¼ ìš”êµ¬í•©ë‹ˆë‹¤. Helm ì°¨íŠ¸ë¥¼ í†µí•´ ì„¤ì¹˜ ê°€ëŠ¥í•˜ë©°, ìƒì„¸ ê°€ì´ë“œëŠ” [Setu GitHub ì €ì¥ì†Œ](https://github.com/sanjeevrg89/Setu)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
:::

:::warning í”„ë¡œë•ì…˜ ì‚¬ìš© ì‹œ ê³ ë ¤ì‚¬í•­
SetuëŠ” ì»¤ë®¤ë‹ˆí‹° í”„ë¡œì íŠ¸ë¡œ, í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë‹¤ìŒì„ ê²€ì¦í•´ì•¼ í•©ë‹ˆë‹¤:
- Karpenter/Kueue ë²„ì „ í˜¸í™˜ì„±
- NodeClaim ìƒì„± ì‹¤íŒ¨ ì‹œ ì•ŒëŒ ì„¤ì •
- ClusterQueue ì¿¼í„° ëª¨ë‹ˆí„°ë§ (ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ ë°©ì§€)
:::

**ì¶”ë¡  ì›Œí¬ë¡œë“œ ìŠ¤ì¼€ì¤„ë§ ì˜ˆì‹œ:**

```yaml
# ê³ ê°€ìš©ì„± ì¶”ë¡  ì„œë¹„ìŠ¤: On-Demand GPU ë…¸ë“œ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference
spec:
  replicas: 4
  selector:
    matchLabels:
      app: ml-inference
  template:
    metadata:
      labels:
        app: ml-inference
    spec:
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      nodeSelector:
        karpenter.sh/capacity-type: on-demand  # On-Demandë§Œ ì‚¬ìš©
      affinity:
        # Hard Anti-Affinity: ê° ë…¸ë“œì— ìµœëŒ€ 1ê°œ replica
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: ml-inference
            topologyKey: kubernetes.io/hostname
        # AZ ë¶„ì‚°
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: ml-inference
            topologyKey: topology.kubernetes.io/zone
      priorityClassName: high-priority
      containers:
      - name: inference
        image: ml/triton-inference:v2.0
        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: "3"
            memory: 12Gi
          limits:
            nvidia.com/gpu: 1
            cpu: "3"
            memory: 12Gi
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 8001
          name: grpc
        livenessProbe:
          httpGet:
            path: /v2/health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /v2/health/ready
            port: 8000
          initialDelaySeconds: 15
          periodSeconds: 5
---
# PDB: ìµœì†Œ 2ê°œ replica ìœ ì§€
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ml-inference-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: ml-inference
```

**Inferentia/Graviton ì¶”ë¡  ìµœì í™”:**

AWS InferentiaëŠ” ì¶”ë¡  ì „ìš© ê°€ì†ê¸°ë¡œ, GPU ëŒ€ë¹„ ìµœëŒ€ 70% ë¹„ìš© ì ˆê°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```yaml
# Inferentia ë…¸ë“œ ìŠ¤ì¼€ì¤„ë§
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inferentia-inference
spec:
  replicas: 6
  selector:
    matchLabels:
      app: inferentia-inference
  template:
    metadata:
      labels:
        app: inferentia-inference
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: inf2.xlarge  # AWS Inferentia2
      tolerations:
      - key: aws.amazon.com/neuron
        operator: Exists
        effect: NoSchedule
      containers:
      - name: inference
        image: ml/neuron-inference:v1.0
        resources:
          requests:
            aws.amazon.com/neuron: 1  # Inferentia ì½”ì–´ 1ê°œ
            cpu: "3"
            memory: 8Gi
          limits:
            aws.amazon.com/neuron: 1
        env:
        - name: NEURON_RT_NUM_CORES
          value: "1"
```

**ë¹„ìš© ìµœì í™” ì „ëµ ìš”ì•½:**

| ì›Œí¬ë¡œë“œ | ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | Spot ì‚¬ìš© | ê¶Œì¥ ì „ëµ |
|---------|-------------|----------|----------|
| **ëŒ€ê·œëª¨ í•™ìŠµ** | g5.12xlarge, p4d.24xlarge | âœ… ê°€ëŠ¥ | Spot + Checkpointing + Spot Interruption Handler |
| **ì†Œê·œëª¨ í•™ìŠµ** | g5.2xlarge, g5.4xlarge | âœ… ê°€ëŠ¥ | Spot 70% + On-Demand 30% í˜¼í•© |
| **ê³ ì„±ëŠ¥ ì¶”ë¡ ** | g5.xlarge, g5.2xlarge | âŒ ë¹„ê¶Œì¥ | On-Demand + Savings Plans |
| **ê²½ëŸ‰ ì¶”ë¡ ** | inf2.xlarge, c7g.xlarge | âŒ ë¹„ê¶Œì¥ | On-Demand ë˜ëŠ” Reserved Instances |
| **ë°°ì¹˜ ì¶”ë¡ ** | g5.xlarge | âœ… ê°€ëŠ¥ | Spot + ì¬ì‹œë„ ë¡œì§ |

### 9.2 ìŠ¤ì¼€ì¤„ë§ ì˜ì‚¬ê²°ì • í”Œë¡œìš°ì°¨íŠ¸

```mermaid
flowchart TB
    START[ìƒˆ ì›Œí¬ë¡œë“œ ë°°í¬]
    Q1{ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬?<br/>ë§¤ì¶œ/ë³´ì•ˆ ì˜í–¥}

    Q2{GPU/íŠ¹ìˆ˜ HW<br/>í•„ìš”?}
    Q3{ì¬ì‹œì‘<br/>í—ˆìš© ê°€ëŠ¥?}
    Q4{ì—¬ëŸ¬ AZì—<br/>ë¶„ì‚° í•„ìš”?}
    Q5{ë™ì¼ ë…¸ë“œì—<br/>ì—¬ëŸ¬ replica<br/>í—ˆìš© ê°€ëŠ¥?}
    Q6{íŠ¹ì • ë…¸ë“œ íƒ€ì…<br/>í•„ìš”?}

    A1[PriorityClass:<br/>business-critical]
    A2[PriorityClass:<br/>high-priority]
    A3[PriorityClass:<br/>standard-priority]
    A4[PriorityClass:<br/>low-priority]

    B1[Node Affinity:<br/>GPU ë…¸ë“œ ì§€ì •]
    B2[Taints/Tolerations:<br/>ì „ìš© ë…¸ë“œ ê²©ë¦¬]
    B3[Spot ë…¸ë“œ í—ˆìš©]

    C1[Topology Spread:<br/>maxSkew: 1, AZ ë¶„ì‚°]
    C2[Topology Spread:<br/>maxSkew: 2, Soft]

    D1[Pod Anti-Affinity:<br/>Hard, hostname]
    D2[Pod Anti-Affinity:<br/>Soft, hostname]

    E1[PDB:<br/>minAvailable: 2]
    E2[PDB:<br/>minAvailable: 67%]
    E3[PDB:<br/>maxUnavailable: 1]

    F1[Node Selector:<br/>íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…]
    F2[Node Affinity:<br/>ì¸ìŠ¤í„´ìŠ¤ íŒ¨ë°€ë¦¬ ì„ í˜¸]

    FINAL[ë°°í¬ ì„¤ì • ì™„ë£Œ]

    START --> Q1
    Q1 -->|ì˜ˆ| A1
    Q1 -->|ì¤‘ìš”| A2
    Q1 -->|ë³´í†µ| A3
    Q1 -->|ë°°ì¹˜ ì‘ì—…| A4

    A1 --> Q2
    A2 --> Q2
    A3 --> Q2
    A4 --> Q3

    Q2 -->|ì˜ˆ| B1
    Q2 -->|ì•„ë‹ˆì˜¤| Q6

    Q3 -->|ì˜ˆ| B3
    Q3 -->|ì•„ë‹ˆì˜¤| Q6

    B1 --> B2
    B2 --> Q4
    B3 --> FINAL

    Q6 -->|ì˜ˆ| F1
    Q6 -->|ì„ í˜¸| F2
    Q6 -->|ì•„ë‹ˆì˜¤| Q4
    F1 --> Q4
    F2 --> Q4

    Q4 -->|ì˜ˆ| C1
    Q4 -->|ì„ í˜¸| C2
    Q4 -->|ì•„ë‹ˆì˜¤| Q5

    C1 --> E1
    C2 --> Q5

    Q5 -->|ì•„ë‹ˆì˜¤| D1
    Q5 -->|ì˜ˆ| D2
    Q5 -->|ë¬´ê´€| E2

    D1 --> E1
    D2 --> E2
    E1 --> FINAL
    E2 --> FINAL
    E3 --> FINAL

    style START fill:#4286f4,stroke:#2a6acf,color:#fff
    style A1 fill:#ff4444,stroke:#cc3636,color:#fff
    style A2 fill:#ff9900,stroke:#cc7a00,color:#fff
    style FINAL fill:#34a853,stroke:#2a8642,color:#fff
```

**ì˜ì‚¬ê²°ì • ê°€ì´ë“œ:**

1. **ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ë„ í‰ê°€** â†’ PriorityClass ê²°ì •
2. **í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­** â†’ Node Affinity, Taints/Tolerations
3. **ë¹„ìš© ìµœì í™”** â†’ Spot ë…¸ë“œ í—ˆìš© ì—¬ë¶€
4. **ê³ ê°€ìš©ì„± ìš”êµ¬ì‚¬í•­** â†’ Topology Spread, Anti-Affinity
5. **ì—…ê·¸ë ˆì´ë“œ ì•ˆì „ì„±** â†’ PDB ì„¤ì •

---

## 10. 2025-2026 AWS í˜ì‹ ê³¼ ìŠ¤ì¼€ì¤„ë§ ì „ëµ

AWS re:Invent 2025ì—ì„œ ë°œí‘œëœ ì£¼ìš” í˜ì‹ ë“¤ì€ EKS ìŠ¤ì¼€ì¤„ë§ ì „ëµì— í° ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ì„¹ì…˜ì—ì„œëŠ” Provisioned Control Plane, EKS Auto Mode, Karpenter + ARC í†µí•©, Container Network Observability ë“± ìµœì‹  ê¸°ëŠ¥ì´ Pod ìŠ¤ì¼€ì¤„ë§ê³¼ ê°€ìš©ì„±ì— ì–´ë–»ê²Œ ì ìš©ë˜ëŠ”ì§€ ë‹¤ë£¹ë‹ˆë‹¤.

### 10.1 Provisioned Control Plane ìŠ¤ì¼€ì¤„ë§ ì„±ëŠ¥

**ê°œìš”:**

Provisioned Control Planeì€ XL, 2XL, 4XL ë“± ì‚¬ì „ ì •ì˜ëœ í‹°ì–´ë¡œ ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ìš©ëŸ‰ì„ í”„ë¡œë¹„ì €ë‹í•˜ì—¬ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê³ ì„±ëŠ¥ Kubernetes ìš´ì˜ì„ ì œê³µí•©ë‹ˆë‹¤.

**í‹°ì–´ë³„ ì„±ëŠ¥ íŠ¹ì„±:**

| í‹°ì–´ | API ë™ì‹œì„± | Pod ìŠ¤ì¼€ì¤„ë§ ì†ë„ | í´ëŸ¬ìŠ¤í„° ê·œëª¨ | ì‚¬ìš© ì‚¬ë¡€ |
|------|-----------|-----------------|------------|----------|
| **Standard** | ë™ì  ìŠ¤ì¼€ì¼ë§ | ì¼ë°˜ | ~1,000 ë…¸ë“œ | ì¼ë°˜ ì›Œí¬ë¡œë“œ |
| **XL** | ë†’ìŒ | ë¹ ë¦„ | ~2,000 ë…¸ë“œ | ëŒ€ê·œëª¨ ë°°í¬ |
| **2XL** | ë§¤ìš° ë†’ìŒ | ë§¤ìš° ë¹ ë¦„ | ~4,000 ë…¸ë“œ | AI/ML í•™ìŠµ, HPC |
| **4XL** | ê·¹ëŒ€í™” | ê·¹ëŒ€í™” | ~8,000 ë…¸ë“œ | ì´ˆëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„° |

**ìŠ¤ì¼€ì¤„ë§ ì„±ëŠ¥ í–¥ìƒ:**

Provisioned Control Planeì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ìŠ¤ì¼€ì¤„ë§ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤:

1. **API ì„œë²„ ë™ì‹œ ì²˜ë¦¬ ëŠ¥ë ¥**: ë” ë§ì€ ìŠ¤ì¼€ì¤„ë§ ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬
2. **etcd ìš©ëŸ‰ í™•ì¥**: ë” ë§ì€ ë…¸ë“œ ë° Pod ë©”íƒ€ë°ì´í„° ì €ì¥
3. **ìŠ¤ì¼€ì¤„ëŸ¬ ì²˜ë¦¬ëŸ‰ ì¦ê°€**: ì´ˆë‹¹ ë” ë§ì€ Pod ë°”ì¸ë”© ì²˜ë¦¬
4. **ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì§€ì—°**: íŠ¸ë˜í”½ ë²„ìŠ¤íŠ¸ ì‹œì—ë„ ì¼ê´€ëœ ìŠ¤ì¼€ì¤„ë§ ì§€ì—° ë³´ì¥

**ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„° ìŠ¤ì¼€ì¤„ë§ ì „ëµ:**

```yaml
# ì˜ˆì‹œ: Provisioned Control Plane XL í‹°ì–´ì—ì„œ ëŒ€ê·œëª¨ Deployment ë°°í¬
apiVersion: apps/v1
kind: Deployment
metadata:
  name: large-scale-app
spec:
  replicas: 1000  # 1000ê°œ replica ë™ì‹œ ë°°í¬
  selector:
    matchLabels:
      app: large-scale-app
  template:
    metadata:
      labels:
        app: large-scale-app
    spec:
      # Topology Spread: 1000ê°œ Podë¥¼ ê· ë“± ë¶„ì‚°
      topologySpreadConstraints:
      - maxSkew: 10  # ëŒ€ê·œëª¨ ë°°í¬ì—ì„œëŠ” maxSkewë¥¼ ë†’ì—¬ ìœ ì—°ì„± í™•ë³´
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: large-scale-app
      - maxSkew: 50
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: large-scale-app
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: large-scale-app
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: app:v1.0
        resources:
          requests:
            cpu: "500m"
            memory: 1Gi
```

**AI/ML í•™ìŠµ ì›Œí¬ë¡œë“œ ìµœì í™” (ìˆ˜ì²œ GPU Pod):**

Provisioned Control Planeì€ AI/ML í•™ìŠµ ì›Œí¬ë¡œë“œì—ì„œ ìˆ˜ì²œ ê°œì˜ GPU Podë¥¼ ë™ì‹œì— ìŠ¤ì¼€ì¤„ë§í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant User
    participant APIServer as API Server<br/>(Provisioned XL)
    participant Scheduler as Kube-Scheduler<br/>(Enhanced)
    participant Karpenter
    participant EC2 as EC2 Auto Scaling

    User->>APIServer: Job ìƒì„± (1000 GPU Pod)
    APIServer->>Scheduler: 1000 Pod ìŠ¤ì¼€ì¤„ë§ ìš”ì²­

    Note over Scheduler: ë³‘ë ¬ ìŠ¤ì¼€ì¤„ë§<br/>(ì´ˆë‹¹ 100+ Pod)

    Scheduler->>Karpenter: ë¶€ì¡±í•œ GPU ë…¸ë“œ ìš”ì²­
    Karpenter->>EC2: 250 GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹

    Note over EC2: ë…¸ë“œ ë³‘ë ¬ ìƒì„±<br/>(5-10ë¶„)

    EC2-->>Karpenter: ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ
    Karpenter-->>Scheduler: ë…¸ë“œ ë“±ë¡

    Scheduler->>APIServer: Pod ë°”ì¸ë”© (250 batch)
    APIServer->>Scheduler: ë‹¤ìŒ ë°°ì¹˜ ìŠ¤ì¼€ì¤„ë§

    Note over Scheduler,APIServer: 4ë²ˆ ë°˜ë³µ<br/>(1000 Pod ì™„ë£Œ)

    APIServer-->>User: Job ì‹¤í–‰ ì‹œì‘
```

**ì‚¬ìš© ì‚¬ë¡€ë³„ ê¶Œì¥ í‹°ì–´:**

| ì‚¬ìš© ì‚¬ë¡€ | ê¶Œì¥ í‹°ì–´ | ì´ìœ  |
|----------|----------|------|
| **ì¼ë°˜ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜** | Standard | ë™ì  ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì¶©ë¶„ |
| **ëŒ€ê·œëª¨ ë°°ì¹˜ ì‘ì—… (500+ Pod)** | XL | ë¹ ë¥¸ ë™ì‹œ ìŠ¤ì¼€ì¤„ë§ í•„ìš” |
| **ë¶„ì‚° ML í•™ìŠµ (1000+ GPU Pod)** | 2XL | ì´ˆê³ ì† ìŠ¤ì¼€ì¤„ë§ + ë†’ì€ API ë™ì‹œì„± |
| **HPC í´ëŸ¬ìŠ¤í„° (ìˆ˜ì²œ ë…¸ë“œ)** | 4XL | ìµœëŒ€ ìŠ¤ì¼€ì¼ + ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥ |
| **ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬ ì„œë¹„ìŠ¤** | XL ì´ìƒ | íŠ¸ë˜í”½ ë²„ìŠ¤íŠ¸ ì‹œì—ë„ ì¼ê´€ëœ ì§€ì—° |

:::tip Provisioned Control Plane ì„ íƒ ê¸°ì¤€
- **ë…¸ë“œ ìˆ˜ > 1,000**: XL ì´ìƒ ê³ ë ¤
- **ë¹ˆë²ˆí•œ ëŒ€ê·œëª¨ ë°°í¬ (500+ Pod)**: XL ì´ìƒ
- **GPU ì›Œí¬ë¡œë“œ (100+ GPU)**: 2XL ì´ìƒ
- **ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥ ìš”êµ¬**: ëª¨ë“  ê·œëª¨ì—ì„œ Provisioned ê³ ë ¤
:::

### 10.2 EKS Auto Mode ìë™ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹

**ê°œìš”:**

EKS Auto ModeëŠ” ì»´í“¨íŒ…, ìŠ¤í† ë¦¬ì§€, ë„¤íŠ¸ì›Œí‚¹ì˜ í”„ë¡œë¹„ì €ë‹ë¶€í„° ì§€ì†ì  ìœ ì§€ë³´ìˆ˜ê¹Œì§€ ì™„ì „ ìë™í™”í•˜ì—¬ Kubernetes ìš´ì˜ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.

**Auto Modeê°€ ìŠ¤ì¼€ì¤„ë§ì— ë¯¸ì¹˜ëŠ” ì˜í–¥:**

| ê¸°ëŠ¥ | ê¸°ì¡´ ë°©ì‹ (ìˆ˜ë™) | Auto Mode |
|------|---------------|----------|
| **ë…¸ë“œ ì„ íƒ** | NodeSelector, Node Affinity ëª…ì‹œ | ìë™ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ |
| **ë™ì  ìŠ¤ì¼€ì¼ë§** | Cluster Autoscaler ë˜ëŠ” Karpenter ì„¤ì • | ìë™ ìŠ¤ì¼€ì¼ë§ (ì„¤ì • ë¶ˆí•„ìš”) |
| **ë¹„ìš© ìµœì í™”** | Spot, Graviton ìˆ˜ë™ ì„¤ì • | ìë™ Spot + Graviton í™œìš© |
| **AZ ë°°ì¹˜** | Topology Spread ìˆ˜ë™ ì„¤ì • | ìë™ Multi-AZ ë¶„ì‚° |
| **ë…¸ë“œ ì—…ê·¸ë ˆì´ë“œ** | ìˆ˜ë™ AMI ì—…ë°ì´íŠ¸ | ìë™ OS íŒ¨ì¹­ |

**ìˆ˜ë™ NodeSelector/Affinity vs Auto Mode ë¹„êµ:**

```yaml
# ê¸°ì¡´ ë°©ì‹: ìˆ˜ë™ NodeSelector + Karpenter NodePool
---
# Karpenter NodePool ìƒì„±
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general-pool
spec:
  template:
    spec:
      requirements:
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["c6i.xlarge", "c6i.2xlarge", "c6a.xlarge"]
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]
---
# Deployment: NodeSelectorë¡œ ë…¸ë“œ ì§€ì •
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 10
  template:
    spec:
      nodeSelector:
        karpenter.sh/nodepool: general-pool
      containers:
      - name: api
        image: api:v1.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
```

```yaml
# Auto Mode ë°©ì‹: ìµœì†Œí•œì˜ ì„¤ì •
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 10
  template:
    spec:
      # NodeSelector, Affinity ë¶ˆí•„ìš” - Auto Modeê°€ ìë™ ì„ íƒ
      containers:
      - name: api
        image: api:v1.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
      # Auto Modeê°€ ìë™ìœ¼ë¡œ:
      # - ì í•©í•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ (c6i, c6a, c7i ë“±)
      # - Spot vs On-Demand ìµœì  ì¡°í•©
      # - Multi-AZ ë¶„ì‚°
      # - Graviton (ARM) ê°€ëŠ¥ ì‹œ í™œìš©
```

**Auto Mode í™˜ê²½ì—ì„œ ì—¬ì „íˆ í•„ìš”í•œ ìŠ¤ì¼€ì¤„ë§ ì„¤ì •:**

Auto ModeëŠ” ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ì„ ìë™í™”í•˜ì§€ë§Œ, ë‹¤ìŒ ìŠ¤ì¼€ì¤„ë§ ì„¤ì •ì€ **ì—¬ì „íˆ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤**:

| ì„¤ì • | Auto Mode ìë™í™” ì—¬ë¶€ | ì„¤ëª… |
|------|---------------------|------|
| **Resource Requests/Limits** | âŒ í•„ìˆ˜ ì„¤ì • | ì›Œí¬ë¡œë“œ ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ ëª…ì‹œ í•„ìš” |
| **Topology Spread** | âš ï¸ ê¸°ë³¸ ì œê³µ + ì„¸ë°€í•œ ì œì–´ ì‹œ ì„¤ì • | Auto Modeê°€ ê¸°ë³¸ ë¶„ì‚° ì œê³µ, ì„¸ë°€í•œ ì œì–´ í•„ìš” ì‹œ ëª…ì‹œ |
| **Pod Anti-Affinity** | âŒ í•„ìˆ˜ ì„¤ì • | ê°™ì€ ì•± replica ë¶„ì‚°ì€ ëª…ì‹œ í•„ìš” |
| **PDB** | âŒ í•„ìˆ˜ ì„¤ì • | ìµœì†Œ ê°€ìš©ì„± ë³´ì¥ì€ ì•± ë‹´ë‹¹ |
| **PriorityClass** | âŒ í•„ìˆ˜ ì„¤ì • | ìš°ì„ ìˆœìœ„ëŠ” ì•± ë‹´ë‹¹ |
| **Taints/Tolerations** | âš ï¸ íŠ¹ìˆ˜ ë…¸ë“œë§Œ | GPU ë“± íŠ¹ìˆ˜ ì›Œí¬ë¡œë“œëŠ” ëª…ì‹œ í•„ìš” |

**Auto Mode í™˜ê²½ì˜ ê¶Œì¥ ìŠ¤ì¼€ì¤„ë§ íŒ¨í„´:**

```yaml
# Auto Modeì—ì„œ ê¶Œì¥ë˜ëŠ” ìµœì†Œí•œì˜ ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
apiVersion: apps/v1
kind: Deployment
metadata:
  name: production-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: production-app
  template:
    metadata:
      labels:
        app: production-app
    spec:
      # 1. Resource Requests (í•„ìˆ˜)
      containers:
      - name: app
        image: app:v1.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
          limits:
            cpu: "2"
            memory: 4Gi

      # 2. Topology Spread (ì„¸ë°€í•œ AZ ë¶„ì‚° ì œì–´)
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: production-app
        minDomains: 3

      # 3. Pod Anti-Affinity (ë…¸ë“œ ë¶„ì‚°)
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: production-app
              topologyKey: kubernetes.io/hostname

      # 4. PriorityClass (ìš°ì„ ìˆœìœ„)
      priorityClassName: high-priority
---
# 5. PDB (ê°€ìš©ì„± ë³´ì¥)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: production-app-pdb
spec:
  minAvailable: 4
  selector:
    matchLabels:
      app: production-app
```

**Auto Mode + PDB + Karpenter ìƒí˜¸ì‘ìš©:**

Auto ModeëŠ” ë‚´ë¶€ì ìœ¼ë¡œ Karpenterì™€ ìœ ì‚¬í•œ ìë™ ìŠ¤ì¼€ì¼ë§ì„ ì œê³µí•˜ë©°, PDBë¥¼ ì¡´ì¤‘í•©ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph "Auto Mode í™˜ê²½"
        POD[ìƒˆ Pod ìƒì„± ìš”ì²­]
        AUTOMODE[EKS Auto Mode]
        SCHEDULE[Kubernetes Scheduler]
        NODE[ë…¸ë“œ í”„ë¡œë¹„ì €ë‹]
        PDB[PDB í™•ì¸]
    end

    POD --> SCHEDULE
    SCHEDULE -->|ì í•©í•œ ë…¸ë“œ ì—†ìŒ| AUTOMODE
    AUTOMODE -->|ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìë™ ì„ íƒ| NODE
    NODE -->|ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ| SCHEDULE
    SCHEDULE -->|Pod ë°°ì¹˜| DONE[ì‹¤í–‰]

    subgraph "ë…¸ë“œ í†µí•© (Consolidation)"
        UNDERUTIL[ì €í™œìš© ë…¸ë“œ ê°ì§€]
        EVICT[Pod Eviction ì‹œë„]
        UNDERUTIL --> PDB
        PDB -->|minAvailable í™•ì¸| EVICT
        EVICT -->|PDB ì¡´ì¤‘| REBALANCE[ì¬ë°°ì¹˜]
    end

    style AUTOMODE fill:#4286f4,stroke:#2a6acf,color:#fff
    style PDB fill:#ff9900,stroke:#cc7a00,color:#fff
    style DONE fill:#34a853,stroke:#2a8642,color:#fff
```

### 10.3 ARC + Karpenter í†µí•© AZ ëŒ€í”¼

**ê°œìš”:**

AWS Application Recovery Controller(ARC)ì™€ Karpenterì˜ í†µí•©ì€ AZ ì¥ì•  ì‹œ ìë™ Zonal Shiftë¥¼ í†µí•´ ì›Œí¬ë¡œë“œë¥¼ ê±´ê°•í•œ AZë¡œ ëŒ€í”¼ì‹œí‚µë‹ˆë‹¤.

**AZ ì¥ì•  ìë™ ë³µêµ¬ íŒ¨í„´:**

```mermaid
sequenceDiagram
    participant AZ1 as AZ us-east-1a<br/>(ì¥ì• )
    participant ARC as AWS ARC<br/>(Zonal Shift)
    participant Karpenter
    participant AZ2 as AZ us-east-1b<br/>(ì •ìƒ)
    participant AZ3 as AZ us-east-1c<br/>(ì •ìƒ)
    participant PDB as PodDisruptionBudget
    participant LB as Load Balancer

    Note over AZ1: Gray Failure ë°œìƒ<br/>(ë†’ì€ ì§€ì—°, íŒ¨í‚· ì†ì‹¤)

    AZ1->>ARC: CloudWatch ë©”íŠ¸ë¦­ ì´ìƒ íƒì§€
    ARC->>ARC: Zonal Shift ì‹œì‘<br/>(us-east-1a íŠ¸ë˜í”½ ì°¨ë‹¨)
    ARC->>LB: us-east-1a íŠ¸ë˜í”½ ì œê±°

    ARC->>Karpenter: AZ-1a Pod ëŒ€í”¼ ìš”ì²­
    Karpenter->>PDB: minAvailable í™•ì¸
    PDB-->>Karpenter: ì•ˆì „í•œ Eviction í—ˆìš©

    Karpenter->>AZ2: ì‹ ê·œ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
    Karpenter->>AZ3: ì‹ ê·œ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹

    AZ2-->>Karpenter: ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ
    AZ3-->>Karpenter: ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ

    Karpenter->>AZ1: AZ-1a Pod Eviction
    Note over AZ1: ê¸°ì¡´ Pod ì¢…ë£Œ

    Karpenter->>AZ2: Pod ì¬ìŠ¤ì¼€ì¤„ë§
    Karpenter->>AZ3: Pod ì¬ìŠ¤ì¼€ì¤„ë§

    Note over AZ2,AZ3: ì„œë¹„ìŠ¤ ë³µêµ¬ ì™„ë£Œ<br/>(2-3ë¶„ ì†Œìš”)

    AZ2->>LB: ìƒˆ Pod Ready
    AZ3->>LB: ìƒˆ Pod Ready
    LB-->>ARC: ì •ìƒ ìƒíƒœ í™•ì¸
```

**ARC + Karpenter í†µí•© ì„¤ì • ì˜ˆì‹œ:**

```yaml
# Karpenter NodePool: AZ ëŒ€í”¼ ì§€ì›
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: arc-enabled-pool
spec:
  template:
    spec:
      requirements:
      - key: topology.kubernetes.io/zone
        operator: In
        values:
        - us-east-1a
        - us-east-1b
        - us-east-1c
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand"]  # AZ ëŒ€í”¼ ì‹œ On-Demand ê¶Œì¥
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m
    budgets:
    - nodes: "30%"  # AZ ëŒ€í”¼ ì‹œ ë¹ ë¥¸ ì¬ë°°ì¹˜ë¥¼ ìœ„í•œ ì—¬ìœ 
---
# ì• í”Œë¦¬ì¼€ì´ì…˜: Topology Spread + PDB
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resilient-app
spec:
  replicas: 9  # 3 AZ x 3 replica
  selector:
    matchLabels:
      app: resilient-app
  template:
    metadata:
      labels:
        app: resilient-app
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: resilient-app
        minDomains: 3  # ë°˜ë“œì‹œ 3 AZì— ë¶„ì‚°
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: resilient-app
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: app:v1.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
---
# PDB: AZ ëŒ€í”¼ ì¤‘ì—ë„ 6ê°œ ìœ ì§€ (9ê°œ ì¤‘ 3ê°œ Evict í—ˆìš©)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: resilient-app-pdb
spec:
  minAvailable: 6
  selector:
    matchLabels:
      app: resilient-app
```

**Istio ì„œë¹„ìŠ¤ ë©”ì‹œ í†µí•© End-to-end ë³µêµ¬:**

Istioì™€ ARCë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ AZ ì¥ì•  ì‹œ íŠ¸ë˜í”½ ë¼ìš°íŒ…ê³¼ Pod ì¬ë°°ì¹˜ë¥¼ ì¡°ìœ¨í•˜ì—¬ End-to-end ë³µêµ¬ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.

```yaml
# Istio DestinationRule: AZë³„ Subset
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: resilient-app-dr
spec:
  host: resilient-app.default.svc.cluster.local
  trafficPolicy:
    loadBalancer:
      localityLbSetting:
        enabled: true
        failover:
        - from: us-east-1a
          to: us-east-1b
        - from: us-east-1b
          to: us-east-1c
        - from: us-east-1c
          to: us-east-1a
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  subsets:
  - name: az-1a
    labels:
      topology.kubernetes.io/zone: us-east-1a
  - name: az-1b
    labels:
      topology.kubernetes.io/zone: us-east-1b
  - name: az-1c
    labels:
      topology.kubernetes.io/zone: us-east-1c
---
# Istio VirtualService: ì •ìƒ AZë¡œë§Œ íŠ¸ë˜í”½
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: resilient-app-vs
spec:
  hosts:
  - resilient-app.default.svc.cluster.local
  http:
  - route:
    - destination:
        host: resilient-app.default.svc.cluster.local
        subset: az-1b
      weight: 50
    - destination:
        host: resilient-app.default.svc.cluster.local
        subset: az-1c
      weight: 50
    # ARC Zonal Shift ì‹œ az-1aëŠ” ìë™ ì œê±°ë¨
```

**Gray Failure ê°ì§€ íŒ¨í„´:**

Gray FailureëŠ” ì™„ì „í•œ ì¥ì• ëŠ” ì•„ë‹ˆì§€ë§Œ ì„±ëŠ¥ ì €í•˜ë¡œ ì„œë¹„ìŠ¤ í’ˆì§ˆì´ ë–¨ì–´ì§€ëŠ” ìƒí™©ì…ë‹ˆë‹¤. ARCëŠ” CloudWatch ë©”íŠ¸ë¦­ ê¸°ë°˜ìœ¼ë¡œ Gray Failureë¥¼ ê°ì§€í•©ë‹ˆë‹¤.

```yaml
# CloudWatch Alarm: Gray Failure ê°ì§€
apiVersion: v1
kind: ConfigMap
metadata:
  name: gray-failure-detection
data:
  alarm.json: |
    {
      "AlarmName": "EKS-AZ-1a-HighLatency",
      "MetricName": "TargetResponseTime",
      "Namespace": "AWS/ApplicationELB",
      "Statistic": "Average",
      "Period": 60,
      "EvaluationPeriods": 3,
      "Threshold": 1.0,
      "ComparisonOperator": "GreaterThanThreshold",
      "Dimensions": [
        {
          "Name": "AvailabilityZone",
          "Value": "us-east-1a"
        }
      ],
      "TreatMissingData": "notBreaching"
    }
```

**AZ ëŒ€í”¼ ì „ëµ ìš”ì•½:**

| ì‹œë‚˜ë¦¬ì˜¤ | PDB ì„¤ì • | Topology Spread | Karpenter ì„¤ì • | ë³µêµ¬ ì‹œê°„ |
|---------|---------|----------------|---------------|----------|
| **ì™„ì „ AZ ì¥ì• ** | `minAvailable: 6` (9ê°œ ì¤‘) | `minDomains: 3` | On-Demand ìš°ì„  | 2-3ë¶„ |
| **Gray Failure** | `minAvailable: 6` (9ê°œ ì¤‘) | `minDomains: 2` í—ˆìš© | Spot ê°€ëŠ¥ | 3-5ë¶„ |
| **ê³„íšëœ ìœ ì§€ë³´ìˆ˜** | `maxUnavailable: 3` | `minDomains: 2` í—ˆìš© | Spot + On-Demand | 5-10ë¶„ |

### 10.4 Container Network Observabilityì™€ ìŠ¤ì¼€ì¤„ë§

**ê°œìš”:**

Container Network ObservabilityëŠ” ì„¸ë¶„í™”ëœ ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ì„ ì œê³µí•˜ì—¬ Pod ë°°ì¹˜ì™€ ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³ , ìŠ¤ì¼€ì¤„ë§ ì „ëµì„ ìµœì í™”í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

**Pod ë°°ì¹˜ì™€ ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ìƒê´€ê´€ê³„:**

| Pod ë°°ì¹˜ íŒ¨í„´ | ë„¤íŠ¸ì›Œí¬ ì§€ì—° | Cross-AZ íŠ¸ë˜í”½ ë¹„ìš© | ì‚¬ìš© ì‚¬ë¡€ |
|-------------|-------------|-------------------|----------|
| **Same Node** | ~0.1ms | $0 | Cache ì„œë²„ + ì• í”Œë¦¬ì¼€ì´ì…˜ |
| **Same AZ** | ~0.5ms | $0 | ë¹ˆë²ˆí•œ í†µì‹ í•˜ëŠ” ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ |
| **Cross-AZ** | ~2-5ms | $0.01/GB | ê³ ê°€ìš©ì„± í•„ìš” ì„œë¹„ìŠ¤ |
| **Cross-Region** | ~50-100ms | $0.02/GB | ì§€ì—­ë³„ ë¶„ì‚° ì„œë¹„ìŠ¤ |

**Cross-AZ íŠ¸ë˜í”½ ë¹„ìš©ì„ ê³ ë ¤í•œ ìŠ¤ì¼€ì¤„ë§:**

```yaml
# ì˜ˆì‹œ: API Gateway + Backend Service ê°™ì€ AZ ë°°ì¹˜
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
spec:
  replicas: 6
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
        network-locality: same-az  # ë„¤íŠ¸ì›Œí¬ ê´€ì°°ì„± ë¼ë²¨
    spec:
      # Topology Spread: AZ ê· ë“± ë¶„ì‚°
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: api-gateway
      containers:
      - name: gateway
        image: api-gateway:v1.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
---
# Backend Service: API Gatewayì™€ ê°™ì€ AZ ì„ í˜¸
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-service
spec:
  replicas: 6
  selector:
    matchLabels:
      app: backend-service
  template:
    metadata:
      labels:
        app: backend-service
        network-locality: same-az
    spec:
      affinity:
        # Pod Affinity: API Gatewayì™€ ê°™ì€ AZ ì„ í˜¸ (Cross-AZ ë¹„ìš© ì ˆê°)
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - api-gateway
              topologyKey: topology.kubernetes.io/zone
      containers:
      - name: backend
        image: backend-service:v1.0
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
```

**ë„¤íŠ¸ì›Œí¬ ê´€ì°°ì„± ê¸°ë°˜ Topology Spread ìµœì í™”:**

Container Network Observability ë©”íŠ¸ë¦­ì„ ë¶„ì„í•˜ì—¬ ìŠ¤ì¼€ì¤„ë§ ì „ëµì„ ì¡°ì •í•©ë‹ˆë‹¤.

```yaml
# CloudWatch Container Insights ë©”íŠ¸ë¦­ ì¿¼ë¦¬ ì˜ˆì‹œ
apiVersion: v1
kind: ConfigMap
metadata:
  name: network-metrics-query
data:
  query.json: |
    {
      "MetricName": "pod_network_rx_bytes",
      "Namespace": "ContainerInsights",
      "Dimensions": [
        {"Name": "PodName", "Value": "api-gateway-*"},
        {"Name": "Namespace", "Value": "default"}
      ],
      "Period": 300,
      "Stat": "Sum"
    }
```

**ë„¤íŠ¸ì›Œí¬ ê´€ì°°ì„± ê¸°ë°˜ ìµœì í™” íŒ¨í„´:**

1. **ë†’ì€ Cross-AZ íŠ¸ë˜í”½ ê°ì§€** â†’ Pod Affinityë¡œ ê°™ì€ AZ ë°°ì¹˜
2. **íŠ¹ì • AZ ë„¤íŠ¸ì›Œí¬ í˜¼ì¡ ê°ì§€** â†’ Topology Spreadë¡œ ë‹¤ë¥¸ AZ ë¶„ì‚°
3. **Pod ê°„ í†µì‹  íŒ¨í„´ ë¶„ì„** â†’ Service Mesh(Istio)ë¡œ íŠ¸ë˜í”½ ìµœì í™”
4. **ë„¤íŠ¸ì›Œí¬ ì§€ì—° ê¸‰ì¦ ê°ì§€** â†’ ARC Zonal Shiftë¡œ ì¥ì•  AZ ëŒ€í”¼

```mermaid
flowchart TB
    subgraph "Container Network Observability"
        METRICS[ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘]
        ANALYZE[íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„]
        ALERT[ì´ìƒ íƒì§€ ì•Œë¦¼]
    end

    subgraph "ìŠ¤ì¼€ì¤„ë§ ìµœì í™”"
        DECISION{ìµœì í™” ìœ í˜•}
        AFFINITY[Pod Affinity ì¡°ì •]
        SPREAD[Topology Spread ì¡°ì •]
        SHIFT[AZ Shift]
    end

    METRICS --> ANALYZE
    ANALYZE --> ALERT
    ALERT --> DECISION

    DECISION -->|ë†’ì€ Cross-AZ íŠ¸ë˜í”½| AFFINITY
    DECISION -->|íŠ¹ì • AZ í˜¼ì¡| SPREAD
    DECISION -->|AZ ì¥ì• | SHIFT

    style METRICS fill:#4286f4,stroke:#2a6acf,color:#fff
    style ALERT fill:#ff9900,stroke:#cc7a00,color:#fff
    style DECISION fill:#fbbc04,stroke:#c99603,color:#000
```

**ì‹¤ì „ ì˜ˆì‹œ: ML ì¶”ë¡  ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí¬ ìµœì í™”:**

```yaml
# ML ì¶”ë¡  ì„œë¹„ìŠ¤: ë‚®ì€ ì§€ì—° + ë¹„ìš© ìµœì í™”
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference-optimized
spec:
  replicas: 9
  selector:
    matchLabels:
      app: ml-inference
  template:
    metadata:
      labels:
        app: ml-inference
    spec:
      # 1. Topology Spread: AZ ê· ë“± ë¶„ì‚° (ê³ ê°€ìš©ì„±)
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: ml-inference
        minDomains: 3

      # 2. Pod Affinity: API Gatewayì™€ ê°™ì€ AZ (ë‚®ì€ ì§€ì—°)
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - api-gateway
              topologyKey: topology.kubernetes.io/zone

      containers:
      - name: inference
        image: ml-inference:v1.0
        resources:
          requests:
            cpu: "2"
            memory: 8Gi
```

**ë„¤íŠ¸ì›Œí¬ ê´€ì°°ì„± ê¸°ë°˜ ë¹„ìš© ì ˆê° íš¨ê³¼:**

| ìµœì í™” ì „ | ìµœì í™” í›„ | ì ˆê° íš¨ê³¼ |
|----------|----------|----------|
| Cross-AZ íŠ¸ë˜í”½: 1TB/ì›” | Cross-AZ íŠ¸ë˜í”½: 0.2TB/ì›” | $8/ì›” ì ˆê° |
| í‰ê·  ì§€ì—°: 3ms | í‰ê·  ì§€ì—°: 0.5ms | 6ë°° ì„±ëŠ¥ í–¥ìƒ |
| Pod Affinity ë¯¸ì‚¬ìš© | Pod Affinity ìµœì í™” | ìš´ì˜ íš¨ìœ¨ ì¦ê°€ |

---

### 10.5 Node Readiness Controller â€” ìŠ¤ì¼€ì¤„ë§ ì•ˆì „ì„± ê°•í™”

**ê°œìš”:**

Node Readiness Controller(NRC)ëŠ” Kubernetes 1.32ì—ì„œ Alphaë¡œ ë„ì…ëœ ê¸°ëŠ¥ìœ¼ë¡œ, ë…¸ë“œê°€ `Ready` ìƒíƒœë¼ë„ ì‹¤ì œë¡œ Podë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•  ìˆ˜ ì—†ëŠ” ìƒí™©ì„ ë°©ì§€í•©ë‹ˆë‹¤. CNI í”ŒëŸ¬ê·¸ì¸, CSI ë“œë¼ì´ë²„, GPU ë“œë¼ì´ë²„ ë“± ì¸í”„ë¼ êµ¬ì„± ìš”ì†Œê°€ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ Pod ìŠ¤ì¼€ì¤„ë§ì„ ì°¨ë‹¨í•¨ìœ¼ë¡œì¨ ìŠ¤ì¼€ì¤„ë§ ì•ˆì „ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

**ìŠ¤ì¼€ì¤„ë§ ê´€ì ì—ì„œì˜ ë¬¸ì œ:**

ê¸°ì¡´ Kubernetes ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ë…¸ë“œì˜ `Ready` ìƒíƒœë§Œ í™•ì¸í•˜ì—¬ Podë¥¼ ë°°ì¹˜í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì—ì„œ Pod ë°°ì¹˜ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

| ì‹œë‚˜ë¦¬ì˜¤ | ë…¸ë“œ ìƒíƒœ | ì‹¤ì œ ìƒí™© | ê²°ê³¼ |
|---------|---------|----------|------|
| **CNI í”ŒëŸ¬ê·¸ì¸ ë¯¸ì¤€ë¹„** | `Ready` | Calico/Cilium Pod ì‹œì‘ ì¤‘ | Pod ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨ |
| **CSI ë“œë¼ì´ë²„ ë¯¸ì¤€ë¹„** | `Ready` | EBS CSI Driver ì´ˆê¸°í™” ì¤‘ | PVC ë§ˆìš´íŠ¸ ì‹¤íŒ¨ |
| **GPU ë“œë¼ì´ë²„ ë¯¸ì¤€ë¹„** | `Ready` | NVIDIA Device Plugin ë¡œë”© ì¤‘ | GPU ì›Œí¬ë¡œë“œ ì‹œì‘ ì‹¤íŒ¨ |
| **ì´ë¯¸ì§€ í”„ë¦¬í’€ ì§„í–‰ ì¤‘** | `Ready` | ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€(10GB) ë‹¤ìš´ë¡œë“œ ì¤‘ | Pod ì‹œì‘ ì§€ì—° (5ë¶„ ì´ìƒ) |

**Node Readiness Controllerì˜ ë™ì‘ ì›ë¦¬:**

NRCëŠ” `NodeReadinessRule` CRD(`readiness.node.x-k8s.io/v1alpha1`)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤:

1. **ì¡°ê±´ ê¸°ë°˜ Taint ê´€ë¦¬**: íŠ¹ì • Node Conditionì´ ì¶©ì¡±ë  ë•Œê¹Œì§€ taint ì ìš©
2. **ìŠ¤ì¼€ì¤„ëŸ¬ ì°¨ë‹¨**: Taintê°€ ì ìš©ëœ ë…¸ë“œì—ëŠ” Pod ìŠ¤ì¼€ì¤„ë§ ë¶ˆê°€
3. **ìë™ Taint ì œê±°**: ì¡°ê±´ ì¶©ì¡± ì‹œ taint ìë™ ì œê±° â†’ Pod ìŠ¤ì¼€ì¤„ë§ í—ˆìš©

```mermaid
sequenceDiagram
    participant Karpenter
    participant Node
    participant InfraAgent as ì¸í”„ë¼ ì—ì´ì „íŠ¸<br/>(CNI/CSI/GPU)
    participant NRC as Node Readiness<br/>Controller
    participant Scheduler as Kube Scheduler
    participant Pod

    Karpenter->>Node: ìƒˆ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
    NRC->>Node: Taint ì ìš©<br/>(NoSchedule)

    Note over Node: ë…¸ë“œëŠ” Ready<br/>í•˜ì§€ë§Œ ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨

    Node->>InfraAgent: ì¸í”„ë¼ ì´ˆê¸°í™” ì‹œì‘
    InfraAgent->>InfraAgent: CNI/CSI/GPU ì¤€ë¹„

    InfraAgent->>Node: Condition ì—…ë°ì´íŠ¸<br/>(NetworkReady=True)

    Node->>NRC: Condition ë³€ê²½ ì´ë²¤íŠ¸
    NRC->>NRC: Rule í™•ì¸<br/>(ì¡°ê±´ ì¶©ì¡±?)

    alt ì¡°ê±´ ì¶©ì¡±
        NRC->>Node: Taint ì œê±°
        Note over Node: ìŠ¤ì¼€ì¤„ë§ ê°€ëŠ¥ ìƒíƒœ
        Scheduler->>Node: Pod ë°°ì¹˜ ì‹œì‘
        Node->>Pod: ì»¨í…Œì´ë„ˆ ì‹œì‘
    else ì¡°ê±´ ë¯¸ì¶©ì¡±
        NRC->>Node: Taint ìœ ì§€
        Note over Scheduler: Pod Pending ìƒíƒœ ìœ ì§€
    end
```

**ë‘ ê°€ì§€ Enforcement ëª¨ë“œ:**

NRCëŠ” ë‘ ê°€ì§€ ëª¨ë“œë¡œ ë™ì‘í•˜ë©°, ê° ëª¨ë“œëŠ” ìŠ¤ì¼€ì¤„ë§ ì•ˆì „ì„±ì— ë‹¤ë¥¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤:

| ëª¨ë“œ | ë™ì‘ ë°©ì‹ | ìŠ¤ì¼€ì¤„ë§ ì˜í–¥ | ì‚¬ìš© ì‚¬ë¡€ |
|------|---------|-------------|----------|
| **bootstrap-only** | ë…¸ë“œ ì´ˆê¸°í™” ì‹œì—ë§Œ taint ì ìš©<br/>â†’ í•œë²ˆ ì¤€ë¹„ë˜ë©´ í•´ì œ í›„ ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨ | ì´ˆê¸° ìŠ¤ì¼€ì¤„ë§ ì•ˆì „ì„± ë³´ì¥<br/>ëŸ°íƒ€ì„ ì¥ì• ëŠ” ë¯¸íƒì§€ | CNI í”ŒëŸ¬ê·¸ì¸, ì´ë¯¸ì§€ í”„ë¦¬í’€<br/>(í•œë²ˆë§Œ í™•ì¸í•˜ë©´ ì¶©ë¶„) |
| **continuous** | ì§€ì†ì  ëª¨ë‹ˆí„°ë§<br/>â†’ ë“œë¼ì´ë²„ í¬ë˜ì‹œ ì‹œ ì¦‰ì‹œ re-taint | ëŸ°íƒ€ì„ ì¥ì•  ì‹œì—ë„<br/>ìƒˆ Pod ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨ | GPU ë“œë¼ì´ë²„, CSI ë“œë¼ì´ë²„<br/>(ëŸ°íƒ€ì„ ì¥ì•  ê°€ëŠ¥) |

**ì‹¤ì „ ì˜ˆì‹œ 1: CNI í”ŒëŸ¬ê·¸ì¸ ì¤€ë¹„ í™•ì¸ (Bootstrap-only)**

```yaml
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: network-readiness-rule
spec:
  # CNI í”ŒëŸ¬ê·¸ì¸ì´ NetworkReady Conditionì„ Trueë¡œ ë³´ê³ í•  ë•Œê¹Œì§€ ëŒ€ê¸°
  conditions:
    - type: "cniplugin.example.net/NetworkReady"
      requiredStatus: "True"

  # ì¤€ë¹„ë  ë•Œê¹Œì§€ ì´ taint ì ìš©
  taint:
    key: "readiness.k8s.io/network-unavailable"
    effect: "NoSchedule"
    value: "pending"

  # Bootstrap-only: í•œë²ˆ ì¤€ë¹„ë˜ë©´ ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨
  enforcementMode: "bootstrap-only"

  # Worker ë…¸ë“œì—ë§Œ ì ìš©
  nodeSelector:
    matchLabels:
      node.kubernetes.io/role: worker
```

**ì‹¤ì „ ì˜ˆì‹œ 2: GPU ë“œë¼ì´ë²„ ì§€ì† ëª¨ë‹ˆí„°ë§ (Continuous)**

```yaml
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-driver-readiness-rule
spec:
  # NVIDIA Device Pluginì´ GPUReady Conditionì„ Trueë¡œ ë³´ê³ í•  ë•Œê¹Œì§€ ëŒ€ê¸°
  conditions:
    - type: "nvidia.com/gpu.present"
      requiredStatus: "True"
    - type: "nvidia.com/gpu.driver.ready"
      requiredStatus: "True"

  # GPU ì¤€ë¹„ë  ë•Œê¹Œì§€ ì´ taint ì ìš©
  taint:
    key: "readiness.k8s.io/gpu-unavailable"
    effect: "NoSchedule"
    value: "pending"

  # Continuous: GPU ë“œë¼ì´ë²„ í¬ë˜ì‹œ ì‹œ re-taintë¡œ ìƒˆ Pod ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨
  enforcementMode: "continuous"

  # GPU ë…¸ë“œ ê·¸ë£¹ì—ë§Œ ì ìš©
  nodeSelector:
    matchLabels:
      node.kubernetes.io/instance-type: "p4d.24xlarge"
```

**Pod Scheduling Readiness(schedulingGates)ì™€ì˜ ë¹„êµ:**

KubernetesëŠ” Pod ìˆ˜ì¤€ê³¼ ë…¸ë“œ ìˆ˜ì¤€ ì–‘ìª½ì—ì„œ ìŠ¤ì¼€ì¤„ë§ ì•ˆì „ì„±ì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

| ë¹„êµ í•­ëª© | `schedulingGates` (Pod ìˆ˜ì¤€) | `NodeReadinessRule` (ë…¸ë“œ ìˆ˜ì¤€) |
|----------|------------------------------|--------------------------------|
| **ì œì–´ ëŒ€ìƒ** | íŠ¹ì • Podì˜ ìŠ¤ì¼€ì¤„ë§ | íŠ¹ì • ë…¸ë“œì˜ ëª¨ë“  Pod ìŠ¤ì¼€ì¤„ë§ |
| **ì‚¬ìš© ì‚¬ë¡€** | ì™¸ë¶€ ì¡°ê±´ ì¶©ì¡±ê¹Œì§€ Pod ë³´ë¥˜<br/>(ì˜ˆ: ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ëŒ€ê¸°) | ì¸í”„ë¼ ì¤€ë¹„ê¹Œì§€ ë…¸ë“œ ì°¨ë‹¨<br/>(ì˜ˆ: CNI/GPU ë“œë¼ì´ë²„ ë¡œë”©) |
| **ì¡°ê±´ ìœ„ì¹˜** | Pod Specì— ëª…ì‹œ | Node Conditionìœ¼ë¡œ ë³´ê³  |
| **ì œê±° ë°©ë²•** | ì™¸ë¶€ ì»¨íŠ¸ë¡¤ëŸ¬ê°€ gate ì œê±° | NRCê°€ ìë™ìœ¼ë¡œ taint ì œê±° |
| **ì˜í–¥ ë²”ìœ„** | ë‹¨ì¼ Pod | ë…¸ë“œì˜ ëª¨ë“  ì‹ ê·œ Pod |

**ì¡°í•© íŒ¨í„´:**

```yaml
# Pod ìˆ˜ì¤€ + ë…¸ë“œ ìˆ˜ì¤€ ìŠ¤ì¼€ì¤„ë§ ì•ˆì „ì„± ì¡°í•©
apiVersion: v1
kind: Pod
metadata:
  name: ml-training-job
spec:
  # Pod ìˆ˜ì¤€: ë°ì´í„°ì…‹ ì¤€ë¹„ê¹Œì§€ ìŠ¤ì¼€ì¤„ë§ ë³´ë¥˜
  schedulingGates:
    - name: "example.com/dataset-ready"

  # ë…¸ë“œ ìˆ˜ì¤€: GPU ë“œë¼ì´ë²„ ì¤€ë¹„ëœ ë…¸ë“œì—ë§Œ ë°°ì¹˜ (NodeReadinessRuleì´ taint ê´€ë¦¬)
  tolerations:
    - key: "readiness.k8s.io/gpu-unavailable"
      operator: "DoesNotExist"  # Taintê°€ ì—†ëŠ” ë…¸ë“œ(=GPU ì¤€ë¹„ëœ ë…¸ë“œ)ë§Œ í—ˆìš©

  containers:
    - name: trainer
      image: ml-trainer:v1.0
      resources:
        limits:
          nvidia.com/gpu: 8
```

**Karpenter + NRC ì—°ë™ íŒ¨í„´:**

Karpenterë¡œ ë™ì  ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ì„ ì‚¬ìš©í•˜ëŠ” í™˜ê²½ì—ì„œ NRCëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì›Œí¬í”Œë¡œìš°ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```mermaid
flowchart TB
    subgraph "1. ë…¸ë“œ í”„ë¡œë¹„ì €ë‹"
        PENDING[Pending Pod ê°ì§€]
        KARP[Karpenter:<br/>ìƒˆ ë…¸ë“œ ìƒì„±]
        NODE_UP[ë…¸ë“œ Ready ìƒíƒœ]
    end

    subgraph "2. NRC Taint ì ìš©"
        NRC_DETECT[NRC: ìƒˆ ë…¸ë“œ ê°ì§€]
        TAINT_APPLY[Taint ì ìš©<br/>NoSchedule]
        SCHED_BLOCK[ìŠ¤ì¼€ì¤„ëŸ¬:<br/>ë°°ì¹˜ ì°¨ë‹¨]
    end

    subgraph "3. ì¸í”„ë¼ ì¤€ë¹„"
        CNI_INIT[CNI í”ŒëŸ¬ê·¸ì¸ ì´ˆê¸°í™”]
        CSI_INIT[CSI ë“œë¼ì´ë²„ ì´ˆê¸°í™”]
        GPU_INIT[GPU ë“œë¼ì´ë²„ ë¡œë”©]
        COND_UPDATE[Node Condition ì—…ë°ì´íŠ¸]
    end

    subgraph "4. Taint ì œê±° & ìŠ¤ì¼€ì¤„ë§"
        NRC_CHECK[NRC: Condition í™•ì¸]
        TAINT_REMOVE[Taint ì œê±°]
        POD_SCHED[Pod ìŠ¤ì¼€ì¤„ë§ ì‹œì‘]
    end

    PENDING --> KARP
    KARP --> NODE_UP
    NODE_UP --> NRC_DETECT
    NRC_DETECT --> TAINT_APPLY
    TAINT_APPLY --> SCHED_BLOCK

    SCHED_BLOCK -.ëŒ€ê¸°.-> CNI_INIT
    CNI_INIT --> CSI_INIT
    CSI_INIT --> GPU_INIT
    GPU_INIT --> COND_UPDATE

    COND_UPDATE --> NRC_CHECK
    NRC_CHECK --> TAINT_REMOVE
    TAINT_REMOVE --> POD_SCHED

    style PENDING fill:#ff9900,stroke:#cc7a00,color:#fff
    style TAINT_APPLY fill:#ea4335,stroke:#c53929,color:#fff
    style SCHED_BLOCK fill:#fbbc04,stroke:#c99603,color:#000
    style TAINT_REMOVE fill:#34a853,stroke:#2a8642,color:#fff
    style POD_SCHED fill:#4286f4,stroke:#2a6acf,color:#fff
```

**GPU ë…¸ë“œ ê·¸ë£¹ ì‹¤ì „ ì˜ˆì‹œ:**

AI/ML ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ GPU ë…¸ë“œ ê·¸ë£¹ì—ì„œ NRCë¥¼ ì‚¬ìš©í•˜ë©´ NVIDIA ë“œë¼ì´ë²„ ë¡œë”©ì´ ì™„ë£Œë  ë•Œê¹Œì§€ AI ì›Œí¬ë¡œë“œ ìŠ¤ì¼€ì¤„ë§ì„ ì§€ì—°ì‹œì¼œ ë°°ì¹˜ ì‹¤íŒ¨ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
# Karpenter NodePool: GPU ë…¸ë“œ ê·¸ë£¹
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-pool
spec:
  template:
    spec:
      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["p4d.24xlarge", "p5.48xlarge"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]
      nodeClassRef:
        name: gpu-nodeclass
---
# NodeReadinessRule: GPU ë“œë¼ì´ë²„ ì¤€ë¹„ í™•ì¸
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-readiness-rule
spec:
  conditions:
    - type: "nvidia.com/gpu.driver.ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/gpu-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "continuous"
  nodeSelector:
    matchLabels:
      karpenter.sh/nodepool: gpu-pool
---
# AI ì›Œí¬ë¡œë“œ: Tolerationìœ¼ë¡œ ì¤€ë¹„ëœ GPU ë…¸ë“œì—ë§Œ ë°°ì¹˜
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-training
spec:
  template:
    spec:
      # GPU ì¤€ë¹„ëœ ë…¸ë“œì—ë§Œ ë°°ì¹˜
      tolerations:
        - key: "readiness.k8s.io/gpu-unavailable"
          operator: "DoesNotExist"

      containers:
        - name: trainer
          image: ml-trainer:v1.0
          resources:
            limits:
              nvidia.com/gpu: 8

      restartPolicy: OnFailure
```

:::tip ìŠ¤ì¼€ì¤„ë§ ì•ˆì „ì„± ìµœì í™” ê¶Œì¥ì‚¬í•­
- **CNI í”ŒëŸ¬ê·¸ì¸**: `bootstrap-only` ëª¨ë“œë¡œ ì´ˆê¸° ë„¤íŠ¸ì›Œí¬ ì¤€ë¹„ í™•ì¸
- **GPU ë“œë¼ì´ë²„**: `continuous` ëª¨ë“œë¡œ ëŸ°íƒ€ì„ ì¥ì•  ì‹œì—ë„ ìƒˆ Pod ë°°ì¹˜ ì°¨ë‹¨
- **CSI ë“œë¼ì´ë²„**: `continuous` ëª¨ë“œë¡œ ìŠ¤í† ë¦¬ì§€ ë“œë¼ì´ë²„ í¬ë˜ì‹œ ëŒ€ì‘
- **ì´ë¯¸ì§€ í”„ë¦¬í’€**: `bootstrap-only` ëª¨ë“œë¡œ ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
- **Karpenter ì—°ë™**: NodePoolë³„ NodeReadinessRule ì„¤ì •ìœ¼ë¡œ ì›Œí¬ë¡œë“œë³„ ë§ì¶¤ ì¤€ë¹„ ì¡°ê±´
:::

:::warning Alpha ê¸°ëŠ¥ ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­
Node Readiness ControllerëŠ” Kubernetes 1.32ì—ì„œ Alpha ê¸°ëŠ¥ì…ë‹ˆë‹¤:

1. **Feature Gate í™œì„±í™” í•„ìš”**: `--feature-gates=NodeReadiness=true` (kube-apiserver, kube-controller-manager)
2. **API ë³€ê²½ ê°€ëŠ¥ì„±**: Beta/GA ì „í™˜ ì‹œ `NodeReadinessRule` CRD ìŠ¤í‚¤ë§ˆ ë³€ê²½ ê°€ëŠ¥
3. **í”„ë¡œë•ì…˜ í™˜ê²½**: ì² ì €í•œ í…ŒìŠ¤íŠ¸ í›„ ë„ì… ê¶Œì¥
4. **ëŒ€ì²´ ë°©ë²•**: Alpha ê¸°ëŠ¥ ì‚¬ìš©ì´ ë¶€ë‹´ìŠ¤ëŸ½ë‹¤ë©´ ê¸°ì¡´ Node Taint ìˆ˜ë™ ê´€ë¦¬ ë˜ëŠ” Init Container íŒ¨í„´ í™œìš©
:::

**ì°¸ê³  ìë£Œ:**

- [Kubernetes Blog: Introducing Node Readiness Controller](https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/)
- [Node Readiness Controller GitHub](https://github.com/kubernetes-sigs/node-readiness-controller)

---

## 11. ì¢…í•© ì²´í¬ë¦¬ìŠ¤íŠ¸ & ì°¸ê³  ìë£Œ

### 11.1 ì¢…í•© ì²´í¬ë¦¬ìŠ¤íŠ¸

í”„ë¡œë•ì…˜ ë°°í¬ ì „ ì•„ë˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ìŠ¤ì¼€ì¤„ë§ ì„¤ì •ì„ ê²€ì¦í•˜ì„¸ìš”.

#### ê¸°ë³¸ ìŠ¤ì¼€ì¤„ë§ (ëª¨ë“  ì›Œí¬ë¡œë“œ)

| í•­ëª© | ì„¤ëª… | í™•ì¸ |
|------|------|------|
| **Resource Requests ì„¤ì •** | ëª¨ë“  ì»¨í…Œì´ë„ˆì— CPU, Memory requests ëª…ì‹œ | [ ] |
| **PriorityClass ì§€ì •** | ì›Œí¬ë¡œë“œ ì¤‘ìš”ë„ì— ë§ëŠ” PriorityClass í• ë‹¹ | [ ] |
| **Liveness/Readiness Probe** | í—¬ìŠ¤ ì²´í¬ ì„¤ì •ìœ¼ë¡œ Pod ì•ˆì •ì„± ë³´ì¥ | [ ] |
| **Graceful Shutdown** | preStop Hook + terminationGracePeriodSeconds | [ ] |
| **Image Pull Policy** | í”„ë¡œë•ì…˜: `IfNotPresent` ë˜ëŠ” `Always` | [ ] |

#### ê³ ê°€ìš©ì„± (Critical ì›Œí¬ë¡œë“œ)

| í•­ëª© | ì„¤ëª… | í™•ì¸ |
|------|------|------|
| **Replica ìˆ˜ â‰¥ 3** | ì¥ì•  ë„ë©”ì¸ ê²©ë¦¬ë¥¼ ìœ„í•œ ìµœì†Œ replica | [ ] |
| **Topology Spread Constraints** | AZ ê°„ ê· ë“± ë¶„ì‚° (maxSkew: 1) | [ ] |
| **Pod Anti-Affinity** | ë…¸ë“œ ë¶„ì‚° (Soft ë˜ëŠ” Hard) | [ ] |
| **PDB ì„¤ì •** | minAvailable ë˜ëŠ” maxUnavailable ëª…ì‹œ | [ ] |
| **PDB ê²€ì¦** | `minAvailable < replicas` í™•ì¸ | [ ] |
| **Multi-AZ ë°°í¬ í™•ì¸** | `kubectl get pods -o wide`ë¡œ AZ ë¶„ì‚° ê²€ì¦ | [ ] |

#### ë¦¬ì†ŒìŠ¤ ìµœì í™”

| í•­ëª© | ì„¤ëª… | í™•ì¸ |
|------|------|------|
| **Spot ë…¸ë“œ í™œìš©** | ì¬ì‹œì‘ ê°€ëŠ¥í•œ ì›Œí¬ë¡œë“œì— Spot ë…¸ë“œ í—ˆìš© | [ ] |
| **Node Affinity ìµœì í™”** | ì›Œí¬ë¡œë“œì— ë§ëŠ” ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ | [ ] |
| **Taints/Tolerations** | GPU, ê³ ì„±ëŠ¥ ë…¸ë“œ ë“± ì „ìš© ë…¸ë“œ ê²©ë¦¬ | [ ] |
| **Descheduler ì„¤ì •** | ë…¸ë“œ ë¶ˆê· í˜• í•´ì†Œ (optional) | [ ] |
| **Karpenter í†µí•©** | Disruption budget ì„¤ì • | [ ] |

#### íŠ¹ìˆ˜ ì›Œí¬ë¡œë“œ

| í•­ëª© | ì„¤ëª… | í™•ì¸ |
|------|------|------|
| **GPU ì›Œí¬ë¡œë“œ** | GPU Taint Tolerate + GPU ë¦¬ì†ŒìŠ¤ ìš”ì²­ | [ ] |
| **StatefulSet** | WaitForFirstConsumer StorageClass ì‚¬ìš© | [ ] |
| **DaemonSet** | ëª¨ë“  Taint Tolerate ì„¤ì • | [ ] |
| **ë°°ì¹˜ ì‘ì—…** | PriorityClass: low-priority, preemptionPolicy: Never | [ ] |

### Pod ìŠ¤ì¼€ì¤„ë§ ê²€ì¦ ëª…ë ¹ì–´

```bash
# 1. Pod ë°°ì¹˜ í™•ì¸ (AZ, ë…¸ë“œ ë¶„ì‚°)
kubectl get pods -n <namespace> -o wide

# 2. Pod ìŠ¤ì¼€ì¤„ë§ ì´ë²¤íŠ¸ í™•ì¸ (Pending ì›ì¸ íŒŒì•…)
kubectl describe pod <pod-name> -n <namespace>

# 3. PDB ìƒíƒœ í™•ì¸
kubectl get pdb -A
kubectl describe pdb <pdb-name> -n <namespace>

# 4. PriorityClass ëª©ë¡
kubectl get priorityclass

# 5. ë…¸ë“œ Taint í™•ì¸
kubectl describe node <node-name> | grep Taints

# 6. ë…¸ë“œë³„ Pod ë¶„í¬ í™•ì¸
kubectl get pods -A -o wide | awk '{print $8}' | sort | uniq -c

# 7. AZë³„ Pod ë¶„í¬ í™•ì¸
kubectl get pods -A -o json | \
  jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name) \(.spec.nodeName)"' | \
  while read ns pod node; do
    az=$(kubectl get node $node -o jsonpath='{.metadata.labels.topology\.kubernetes\.io/zone}')
    echo "$ns $pod $node $az"
  done | column -t

# 8. Pending Pod ì›ì¸ ë¶„ì„
kubectl get events --sort-by='.lastTimestamp' -A | grep -i warning

# 9. Descheduler ë¡œê·¸ í™•ì¸ (ì„¤ì¹˜ëœ ê²½ìš°)
kubectl logs -n kube-system -l app=descheduler --tail=100
```

### 11.2 ê´€ë ¨ ë¬¸ì„œ

**ë‚´ë¶€ ë¬¸ì„œ:**
- [EKS ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜ ê°€ì´ë“œ](/docs/operations-observability/eks-resiliency-guide) â€” Multi-AZ ì „ëµ, Topology Spread, Cell Architecture
- [Karpenterë¥¼ í™œìš©í•œ ì´ˆê³ ì† ì˜¤í† ìŠ¤ì¼€ì¼ë§](/docs/infrastructure-optimization/karpenter-autoscaling) â€” Karpenter NodePool ì‹¬ì¸µ ì„¤ì •
- [EKS ë¦¬ì†ŒìŠ¤ ìµœì í™” ê°€ì´ë“œ](/docs/infrastructure-optimization/eks-resource-optimization) â€” ë¦¬ì†ŒìŠ¤ Requests/Limits ìµœì í™”
- [EKS Pod í—¬ìŠ¤ì²´í¬ & ë¼ì´í”„ì‚¬ì´í´](/docs/operations-observability/eks-pod-health-lifecycle) â€” Probe, Lifecycle Hooks

### 11.3 ì™¸ë¶€ ì°¸ì¡°

**ê³µì‹ Kubernetes ë¬¸ì„œ:**
- [Kubernetes Scheduling Framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
- [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)
- [Pod Priority and Preemption](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/)
- [Taints and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)
- [Pod Topology Spread Constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)
- [PodDisruptionBudget](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)

**Descheduler:**
- [Descheduler GitHub](https://github.com/kubernetes-sigs/descheduler)
- [Descheduler Strategies](https://github.com/kubernetes-sigs/descheduler#policy-and-strategies)

**AWS EKS ê³µì‹ ë¬¸ì„œ:**
- [EKS Best Practices â€” Reliability](https://docs.aws.amazon.com/eks/latest/best-practices/reliability.html)
- [Karpenter Scheduling](https://karpenter.sh/docs/concepts/scheduling/)
- [EKS Node Taints](https://docs.aws.amazon.com/eks/latest/userguide/node-taints-managed-node-groups.html)

**AWS re:Invent 2025 ê´€ë ¨ ìë£Œ:**
- [Amazon EKS introduces Provisioned Control Plane](https://aws.amazon.com/blogs/containers/amazon-eks-introduces-provisioned-control-plane/) â€” XL/2XL/4XL í‹°ì–´ë³„ ìŠ¤ì¼€ì¤„ë§ ì„±ëŠ¥
- [Getting started with Amazon EKS Auto Mode](https://aws.amazon.com/blogs/containers/getting-started-with-amazon-eks-auto-mode) â€” ìë™ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
- [Enhance Kubernetes high availability with ARC and Karpenter](https://aws.amazon.com/blogs/containers/enhance-kubernetes-high-availability-with-amazon-application-recovery-controller-and-karpenter-integration/) â€” AZ ìë™ ëŒ€í”¼ íŒ¨í„´
- [Monitor network performance across EKS clusters](https://aws.amazon.com/blogs/aws/monitor-network-performance-and-traffic-across-your-eks-clusters-with-container-network-observability/) â€” Container Network Observability
- [Proactive EKS monitoring with CloudWatch Operator](https://aws.amazon.com/blogs/containers/proactive-amazon-eks-monitoring-with-amazon-cloudwatch-operator-and-aws-control-plane-metrics/) â€” Control Plane ë©”íŠ¸ë¦­

**Red Hat OpenShift ë¬¸ì„œ:**
- [Controlling Pod Placement with Taints and Tolerations](https://docs.openshift.com/container-platform/4.18/nodes/scheduling/nodes-scheduler-taints-tolerations.html) â€” Taints/Tolerations ìš´ì˜
- [Placing Pods on Specific Nodes with Pod Affinity](https://docs.openshift.com/container-platform/4.18/nodes/scheduling/nodes-scheduler-pod-affinity.html) â€” Pod Affinity/Anti-Affinity êµ¬ì„±
- [Evicting Pods Using the Descheduler](https://docs.openshift.com/container-platform/4.18/nodes/scheduling/nodes-descheduler.html) â€” Descheduler ì „ëµ ë° ì„¤ì •
- [Managing Pods](https://docs.openshift.com/container-platform/4.18/nodes/pods/nodes-pods-configuring.html) â€” Pod ê´€ë¦¬ ë° ìŠ¤ì¼€ì¤„ë§ ê¸°ë³¸

**ì»¤ë®¤ë‹ˆí‹°:**
- [CNCF Scheduler SIG](https://github.com/kubernetes/community/tree/master/sig-scheduling)
- [Kubernetes Scheduling Deep Dive (KubeCon)](https://www.youtube.com/results?search_query=kubecon+scheduling)
- [AWS re:Invent 2025 â€” Amazon EKS Sessions](https://aws.amazon.com/blogs/containers/guide-to-amazon-eks-and-kubernetes-sessions-at-aws-reinvent-2025/)
