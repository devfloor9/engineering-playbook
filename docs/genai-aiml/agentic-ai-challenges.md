---
title: "Agentic AI Platform ê¸°ìˆ ì  ë„ì „ê³¼ì œì™€ Kubernetes"
sidebar_label: "ê¸°ìˆ ì  ë„ì „ê³¼ì œ"
description: "Agentic AI Platform êµ¬ì¶• ì‹œ ì§ë©´í•˜ëŠ” 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œì™€ Kubernetes/EKS ê¸°ë°˜ í•´ê²° ë°©ì•ˆ"
tags: [eks, kubernetes, genai, agentic-ai, gpu, infrastructure, challenges]
category: "genai-aiml"
date: 2025-02-05
authors: [devfloor9]
sidebar_position: 3
---

# Agentic AI Platform ê¸°ìˆ ì  ë„ì „ê³¼ì œì™€ Kubernetes

> ğŸ“… **ì‘ì„±ì¼**: 2025-02-05 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 16ë¶„


Agentic AI Platformì„ êµ¬ì¶•í•˜ê³  ìš´ì˜í•˜ëŠ” ê³¼ì •ì—ì„œ í”Œë«í¼ ì—”ì§€ë‹ˆì–´ì™€ ì•„í‚¤í…íŠ¸ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì  ë„ì „ê³¼ì œì— ì§ë©´í•©ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œë¥¼ ë¶„ì„í•˜ê³ , Kubernetesì™€ Amazon EKSê°€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ì§€ ì„¤ëª…í•©ë‹ˆë‹¤.

## ê°œìš”

Frontier Model(ìµœì‹  ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì„ í™œìš©í•œ Agentic AI ì‹œìŠ¤í…œì€ ê¸°ì¡´ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ëŠ” ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì¸í”„ë¼ ìš”êµ¬ì‚¬í•­ì„ ê°€ì§‘ë‹ˆë‹¤. GPU ë¦¬ì†ŒìŠ¤ì˜ íš¨ìœ¨ì  í™œìš©, ë™ì  íŠ¸ë˜í”½ ê´€ë¦¬, ì„¸ë°€í•œ ë¹„ìš© ì¶”ì , ê·¸ë¦¬ê³  ì§€ì†ì ì¸ ëª¨ë¸ ê°œì„ ì„ ìœ„í•œ ìë™í™” íŒŒì´í”„ë¼ì¸ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œ"
        C1["ğŸ–¥ï¸ GPU ëª¨ë‹ˆí„°ë§ ë°<br/>ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§"]
        C2["ğŸ”€ Agentic AI ìš”ì²­<br/>ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§"]
        C3["ğŸ“Š í† í°/ì„¸ì…˜ ìˆ˜ì¤€<br/>ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© ì»¨íŠ¸ë¡¤"]
        C4["ğŸ”§ FM íŒŒì¸íŠœë‹ê³¼<br/>ìë™í™” íŒŒì´í”„ë¼ì¸"]
    end
    
    subgraph "Kubernetes ê¸°ë°˜ í•´ê²° ë°©ì•ˆ"
        S1["NVIDIA GPU Operator<br/>DCGM Exporter"]
        S2["Gateway API<br/>Kgateway / KEDA"]
        S3["LangFuse / LangSmith<br/>OpenTelemetry"]
        S4["Kubeflow / NeMo<br/>MLflow"]
    end
    
    C1 --> S1
    C2 --> S2
    C3 --> S3
    C4 --> S4
    
    style C1 fill:#ff6b6b
    style C2 fill:#4ecdc4
    style C3 fill:#45b7d1
    style C4 fill:#96ceb4
```

:::info ëŒ€ìƒ ë…ì
ì´ ë¬¸ì„œëŠ” Agentic AI Platform ë„ì…ì„ ê²€í† í•˜ëŠ” **ê¸°ìˆ  ì˜ì‚¬ê²°ì •ì**ì™€ **ì†”ë£¨ì…˜ ì•„í‚¤í…íŠ¸**ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤. í”Œë«í¼ ì•„í‚¤í…ì²˜ ì„ íƒì˜ ê·¼ê±°ë¥¼ ì´í•´í•˜ê³  EKS ë„ì…ì„ ì •ë‹¹í™”í•˜ëŠ” ë° í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
:::

---

## 4ê°€ì§€ í•µì‹¬ ê¸°ìˆ ì  ë„ì „ê³¼ì œ

### 1. GPU ëª¨ë‹ˆí„°ë§ ë° ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§

#### ë„ì „ê³¼ì œ

Agentic AI ì›Œí¬ë¡œë“œëŠ” GPU ë¦¬ì†ŒìŠ¤ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. ë³µìˆ˜ì˜ GPU í´ëŸ¬ìŠ¤í„°ë¥¼ ìš´ì˜í•  ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ì–´ë ¤ì›€ì— ì§ë©´í•©ë‹ˆë‹¤:

**ë³µìˆ˜ GPU í´ëŸ¬ìŠ¤í„° í™˜ê²½ì—ì„œì˜ ë¦¬ì†ŒìŠ¤ ê°€ì‹œì„± í™•ë³´ì˜ ì–´ë ¤ì›€**
- ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ì— ë¶„ì‚°ëœ GPU ë¦¬ì†ŒìŠ¤ì˜ í†µí•© ëª¨ë‹ˆí„°ë§ í•„ìš”
- ì‹¤ì‹œê°„ GPU í• ë‹¹ í˜„í™© íŒŒì•…ì˜ ë³µì¡ì„±
- í´ëŸ¬ìŠ¤í„° ê°„ ë¦¬ì†ŒìŠ¤ ë¶ˆê· í˜• ê°ì§€ ì–´ë ¤ì›€

**GPU í•˜ë“œì›¨ì–´ ë ˆë²¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì˜ ë³µì¡ì„±**
- GPU ì‚¬ìš©ë¥ , ë©”ëª¨ë¦¬, ì˜¨ë„, ì „ë ¥ ì†Œë¹„ ë“± ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í•„ìš”
- NVIDIA ë“œë¼ì´ë²„ ë° CUDA ë²„ì „ í˜¸í™˜ì„± ê´€ë¦¬
- ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™” í•„ìš”

**ì´ê¸°ì¢… GPU í˜¼í•© í™˜ê²½ì—ì„œì˜ ì›Œí¬ë¡œë“œ ë°°ì¹˜ ìµœì í™”**
- A100, H100, H200 ë“± ë‹¤ì–‘í•œ GPU ì„¸ëŒ€ í˜¼í•© ìš´ì˜
- ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë§ëŠ” ìµœì  GPU ì„ íƒ ë¡œì§ í•„ìš”
- GPU ë©”ëª¨ë¦¬ ìš©ëŸ‰ì— ë”°ë¥¸ ëª¨ë¸ ë°°ì¹˜ ì „ëµ ìˆ˜ë¦½

```mermaid
graph LR
    subgraph "GPU í´ëŸ¬ìŠ¤í„° í™˜ê²½"
        subgraph "Cluster A"
            A100_1["A100 x 8"]
            A100_2["A100 x 8"]
        end
        subgraph "Cluster B"
            H100_1["H100 x 8"]
            H100_2["H100 x 8"]
        end
        subgraph "Cluster C"
            H200_1["H200 x 8"]
        end
    end
    
    subgraph "ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ"
        DCGM["DCGM Exporter"]
        PROM["Prometheus"]
        GRAF["Grafana"]
    end
    
    A100_1 --> DCGM
    A100_2 --> DCGM
    H100_1 --> DCGM
    H100_2 --> DCGM
    H200_1 --> DCGM
    DCGM --> PROM
    PROM --> GRAF
```

#### Kubernetes ê¸°ë°˜ í•´ê²° ë°©ì•ˆ

KubernetesëŠ” **Device Plugin**, **NVIDIA GPU Operator**, **DCGM Exporter**ë¥¼ í†µí•´ ì´ëŸ¬í•œ ë„ì „ê³¼ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | ì£¼ìš” ê¸°ëŠ¥ |
|---------|------|----------|
| **Device Plugin** | GPU ë¦¬ì†ŒìŠ¤ ì¶”ìƒí™” | Podì— GPU í• ë‹¹, ë¦¬ì†ŒìŠ¤ ìš”ì²­/ì œí•œ ê´€ë¦¬ |
| **NVIDIA GPU Operator** | GPU ìŠ¤íƒ ìë™í™” | ë“œë¼ì´ë²„, ëŸ°íƒ€ì„, ëª¨ë‹ˆí„°ë§ ìë™ ì„¤ì¹˜ |
| **DCGM Exporter** | ë©”íŠ¸ë¦­ ìˆ˜ì§‘ | GPU ì‚¬ìš©ë¥ , ë©”ëª¨ë¦¬, ì˜¨ë„ ë“± Prometheus ë©”íŠ¸ë¦­ ë…¸ì¶œ |

```yaml
# NVIDIA GPU Operatorë¥¼ í†µí•œ GPU ìŠ¤íƒ ìë™í™”
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: cluster-policy
spec:
  operator:
    defaultRuntime: containerd
  driver:
    enabled: true
    version: "535.104.05"
  toolkit:
    enabled: true
  devicePlugin:
    enabled: true
  dcgmExporter:
    enabled: true
    config:
      name: dcgm-exporter-config
  migManager:
    enabled: true
```

:::tip GPU ë¦¬ì†ŒìŠ¤ ìš”ì²­ ì˜ˆì‹œ
Podì—ì„œ GPUë¥¼ ìš”ì²­í•  ë•ŒëŠ” `nvidia.com/gpu` ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:
```yaml
resources:
  limits:
    nvidia.com/gpu: 4
  requests:
    nvidia.com/gpu: 4
```
:::

---

### 2. Agentic AI ìš”ì²­ ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§

#### ë„ì „ê³¼ì œ

Agentic AI ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ FM(Foundation Model)ì„ ë™ì‹œì— ì„œë¹™í•˜ë©°, íŠ¸ë˜í”½ íŒ¨í„´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ëŒ€ì‘í•´ì•¼ í•©ë‹ˆë‹¤:

**ë‹¤ì¤‘ FM ëª¨ë¸ ì„œë¹™ í™˜ê²½ì—ì„œì˜ ì§€ëŠ¥í˜• íŠ¸ë˜í”½ ë¶„ë°° í•„ìš”ì„±**
- GPT-4, Claude, Llama ë“± ì—¬ëŸ¬ ëª¨ë¸ì„ ë™ì‹œì— ìš´ì˜
- ìš”ì²­ íŠ¹ì„±ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ ë¡œì§ í•„ìš”
- ëª¨ë¸ë³„ ê°€ìš©ì„± ë° ì‘ë‹µ í’ˆì§ˆ ê¸°ë°˜ ë¼ìš°íŒ…

**ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„, í† í° ì²˜ë¦¬ëŸ‰ ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…ì˜ ë³µì¡ì„±**
- ì‹¤ì‹œê°„ ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë°˜ ë¼ìš°íŒ… ê²°ì •
- í† í°/ì´ˆ(TPS) ê¸°ë°˜ ë¡œë“œ ë°¸ëŸ°ì‹±
- ì‘ë‹µ ì§€ì—° ì‹œê°„ ê¸°ë°˜ ìë™ í´ë°±

**íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œ ì‹¤ì‹œê°„ ìŠ¤ì¼€ì¼ë§ê³¼ ë¦¬ì†ŒìŠ¤ ì¬ë°°ì¹˜ì˜ ì–´ë ¤ì›€**
- ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ íŠ¸ë˜í”½ ìŠ¤íŒŒì´í¬ ëŒ€ì‘
- GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì‹œê°„ ìµœì†Œí™”
- ëª¨ë¸ ê°„ ë¦¬ì†ŒìŠ¤ ë™ì  ì¬ë°°ì¹˜

```mermaid
graph TB
    subgraph "Client Requests"
        REQ1["Chat Request"]
        REQ2["Code Generation"]
        REQ3["RAG Query"]
    end
    
    subgraph "Gateway Layer"
        GW["Kgateway<br/>Inference Gateway"]
        ROUTE["Dynamic Router"]
    end
    
    subgraph "Model Serving"
        M1["vLLM - GPT-4"]
        M2["vLLM - Claude"]
        M3["TGI - Llama"]
        M4["vLLM - Mixtral"]
    end
    
    subgraph "Auto Scaling"
        KEDA["KEDA"]
        HPA["HPA"]
    end
    
    REQ1 --> GW
    REQ2 --> GW
    REQ3 --> GW
    GW --> ROUTE
    ROUTE --> M1
    ROUTE --> M2
    ROUTE --> M3
    ROUTE --> M4
    M1 --> KEDA
    M2 --> KEDA
    M3 --> HPA
    M4 --> HPA
    
    style GW fill:#4286f4
    style KEDA fill:#ff6b6b
```

#### Kubernetes ê¸°ë°˜ í•´ê²° ë°©ì•ˆ

**Gateway API**, **Kgateway**, **KEDA**ë¥¼ í™œìš©í•˜ì—¬ ì§€ëŠ¥í˜• íŠ¸ë˜í”½ ê´€ë¦¬ì™€ ìë™ ìŠ¤ì¼€ì¼ë§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | ì£¼ìš” ê¸°ëŠ¥ |
|---------|------|----------|
| **Gateway API** | í‘œì¤€ íŠ¸ë˜í”½ ê´€ë¦¬ | HTTPRoute, ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¼ìš°íŒ…, í—¤ë” ê¸°ë°˜ ë¼ìš°íŒ… |
| **Kgateway** | AI íŠ¹í™” ê²Œì´íŠ¸ì›¨ì´ | ëª¨ë¸ë³„ ë¼ìš°íŒ…, í† í° ê¸°ë°˜ ë¡œë“œ ë°¸ëŸ°ì‹± |
| **KEDA** | ì´ë²¤íŠ¸ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ | í ê¸¸ì´, ë©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§ |

```yaml
# Kgateway HTTPRoute ì„¤ì • ì˜ˆì‹œ
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: ai-model-routing
  namespace: ai-inference
spec:
  parentRefs:
    - name: ai-gateway
      namespace: ai-gateway
  rules:
    # ëª¨ë¸ Aë¡œ 80%, Canaryë¡œ 20% íŠ¸ë˜í”½ ë¶„ë°°
    - matches:
        - path:
            type: PathPrefix
            value: /v1/chat/completions
          headers:
            - name: x-model-id
              value: "gpt-4"
      backendRefs:
        - name: vllm-gpt4
          port: 8000
          weight: 80
        - name: vllm-gpt4-canary
          port: 8000
          weight: 20
    # Claude ëª¨ë¸ ë¼ìš°íŒ…
    - matches:
        - path:
            type: PathPrefix
            value: /v1/chat/completions
          headers:
            - name: x-model-id
              value: "claude-3"
      backendRefs:
        - name: vllm-claude
          port: 8000
```

```yaml
# KEDA ScaledObject ì„¤ì • ì˜ˆì‹œ
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: vllm-scaler
  namespace: ai-inference
spec:
  scaleTargetRef:
    name: vllm-deployment
  minReplicaCount: 1
  maxReplicaCount: 10
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus:9090
        metricName: vllm_pending_requests
        threshold: "100"
        query: sum(vllm_pending_requests{namespace="ai-inference"})
```

:::warning ìŠ¤ì¼€ì¼ë§ ì£¼ì˜ì‚¬í•­
GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ì€ ì¼ë°˜ CPU ë…¸ë“œë³´ë‹¤ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. Karpenterì˜ `consolidationPolicy`ë¥¼ ì ì ˆíˆ ì„¤ì •í•˜ì—¬ ë¶ˆí•„ìš”í•œ ìŠ¤ì¼€ì¼ ë‹¤ìš´ì„ ë°©ì§€í•˜ì„¸ìš”.
:::

---

### 3. í† í°/ì„¸ì…˜ ìˆ˜ì¤€ ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© ì»¨íŠ¸ë¡¤

#### ë„ì „ê³¼ì œ

LLM ê¸°ë°˜ ì‹œìŠ¤í…œì—ì„œëŠ” í† í° ë‹¨ìœ„ì˜ ì„¸ë°€í•œ ëª¨ë‹ˆí„°ë§ê³¼ ë¹„ìš© ê´€ë¦¬ê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤:

**LLM í˜¸ì¶œë³„ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ë° ë¹„ìš© ì‚°ì •ì˜ ë³µì¡ì„±**
- ì…ë ¥/ì¶œë ¥ í† í° ìˆ˜ ì •í™•í•œ ì¸¡ì •
- ëª¨ë¸ë³„ í† í° ë‹¨ê°€ ì ìš©
- ì‹¤ì‹œê°„ ë¹„ìš© ëˆ„ì  ê³„ì‚°

**í”„ë¡¬í”„íŠ¸ í’ˆì§ˆê³¼ ì‘ë‹µ í’ˆì§ˆì˜ ìƒê´€ê´€ê³„ ë¶„ì„ í•„ìš”ì„±**
- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ë³„ ì„±ëŠ¥ ë¹„êµ
- ì‘ë‹µ í’ˆì§ˆ ë©”íŠ¸ë¦­ ì •ì˜ ë° ì¸¡ì •
- A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ í”„ë¡¬í”„íŠ¸ ìµœì í™”

**ë©€í‹° í…Œë„ŒíŠ¸ í™˜ê²½ì—ì„œì˜ ì‚¬ìš©ëŸ‰ í• ë‹¹ ë° ì²­êµ¬ ë©”ì»¤ë‹ˆì¦˜**
- íŒ€/í”„ë¡œì íŠ¸ë³„ ì‚¬ìš©ëŸ‰ ë¶„ë¦¬
- í• ë‹¹ëŸ‰(Quota) ê´€ë¦¬ ë° ì œí•œ
- ìƒì„¸ ì²­êµ¬ ë¦¬í¬íŠ¸ ìƒì„±

```mermaid
graph TB
    subgraph "AI Application"
        APP["Agent Application"]
        SDK["LangFuse SDK"]
    end
    
    subgraph "Observability Stack"
        LF["LangFuse"]
        LS["LangSmith"]
        OTEL["OpenTelemetry<br/>Collector"]
    end
    
    subgraph "Metrics & Analysis"
        PROM["Prometheus"]
        GRAF["Grafana"]
        COST["Cost Dashboard"]
    end
    
    subgraph "Tracked Metrics"
        TOK["Token Usage"]
        LAT["Latency"]
        QUAL["Quality Score"]
        ERR["Error Rate"]
    end
    
    APP --> SDK
    SDK --> LF
    SDK --> LS
    LF --> OTEL
    LS --> OTEL
    OTEL --> PROM
    PROM --> GRAF
    PROM --> COST
    
    LF --> TOK
    LF --> LAT
    LF --> QUAL
    LF --> ERR
    
    style LF fill:#45b7d1
    style COST fill:#96ceb4
```

#### Kubernetes ê¸°ë°˜ í•´ê²° ë°©ì•ˆ

**LangFuse**, **LangSmith**, **OpenTelemetry**ë¥¼ í†µí•©í•˜ì—¬ ì¢…í•©ì ì¸ ê´€ì¸¡ì„± ìŠ¤íƒì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | ì£¼ìš” ê¸°ëŠ¥ |
|---------|------|----------|
| **LangFuse** | LLM ê´€ì¸¡ì„± í”Œë«í¼ | íŠ¸ë ˆì´ìŠ¤, í† í° ì¶”ì , ë¹„ìš© ë¶„ì„, í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ |
| **LangSmith** | LangChain ëª¨ë‹ˆí„°ë§ | ì—ì´ì „íŠ¸ ë””ë²„ê¹…, ì›Œí¬í”Œë¡œìš° ì¶”ì  |
| **OpenTelemetry** | í‘œì¤€ ê´€ì¸¡ì„± | ë¶„ì‚° íŠ¸ë ˆì´ì‹±, ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ë¡œê·¸ í†µí•© |

```yaml
# LangFuse Kubernetes ë°°í¬ ì˜ˆì‹œ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langfuse
  namespace: observability
spec:
  replicas: 2
  selector:
    matchLabels:
      app: langfuse
  template:
    metadata:
      labels:
        app: langfuse
    spec:
      containers:
        - name: langfuse
          image: langfuse/langfuse:latest
          ports:
            - containerPort: 3000
          env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: database-url
            - name: NEXTAUTH_SECRET
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: nextauth-secret
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
```

```python
# LangFuseë¥¼ í†µí•œ í† í° ì¶”ì  ì˜ˆì‹œ
from langfuse import Langfuse
from langfuse.decorators import observe

langfuse = Langfuse()

@observe(as_type="generation")
def call_llm(prompt: str, model: str = "gpt-4"):
    """LLM í˜¸ì¶œ ë° ìë™ í† í° ì¶”ì """
    response = openai.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# ë©€í‹° í…Œë„ŒíŠ¸ ë¹„ìš© ì¶”ì 
@observe()
def process_request(tenant_id: str, request: dict):
    """í…Œë„ŒíŠ¸ë³„ ìš”ì²­ ì²˜ë¦¬ ë° ë¹„ìš© ì¶”ì """
    langfuse.trace(
        name="tenant-request",
        metadata={"tenant_id": tenant_id},
        tags=[f"tenant:{tenant_id}"]
    )
    return call_llm(request["prompt"])
```

:::tip ë¹„ìš© ìµœì í™” íŒ
LangFuseì˜ ëŒ€ì‹œë³´ë“œë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ë³„, í…Œë„ŒíŠ¸ë³„ ë¹„ìš©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³ , ë¹„ìš© ì„ê³„ê°’ ì•Œë¦¼ì„ ì„¤ì •í•˜ì„¸ìš”.
:::

---

### 4. FM íŒŒì¸íŠœë‹ê³¼ ìë™í™” íŒŒì´í”„ë¼ì¸

#### ë„ì „ê³¼ì œ

Foundation Modelì„ íŠ¹ì • ë„ë©”ì¸ì— ë§ê²Œ íŒŒì¸íŠœë‹í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ê²ƒì€ ë³µì¡í•œ ê³¼ì •ì…ë‹ˆë‹¤:

**ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµ í™˜ê²½ êµ¬ì„±ì˜ ë³µì¡ì„±**
- ë©€í‹° ë…¸ë“œ, ë©€í‹° GPU í•™ìŠµ í™˜ê²½ ì„¤ì •
- ë°ì´í„° ë³‘ë ¬í™”, ëª¨ë¸ ë³‘ë ¬í™”, í…ì„œ ë³‘ë ¬í™” ì „ëµ
- í•™ìŠµ ì¤‘ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ë° ì¥ì•  ë³µêµ¬

**í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ í•™ìŠµ, í‰ê°€, ë°°í¬ì˜ End-to-End ìë™í™” í•„ìš”ì„±**
- ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìë™í™”
- í•™ìŠµ ì‹¤í—˜ ì¶”ì  ë° ë¹„êµ
- ëª¨ë¸ í‰ê°€ ìë™í™” ë° í’ˆì§ˆ ê²Œì´íŠ¸

**ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ë° A/B í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•ì˜ ì–´ë ¤ì›€**
- ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë²„ì „ ê´€ë¦¬
- ì ì§„ì  ë¡¤ì•„ì›ƒ ì „ëµ
- ì„±ëŠ¥ ë¹„êµ ë° ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜

```mermaid
graph LR
    subgraph "Data Pipeline"
        DATA["Training Data"]
        PREP["Data Preprocessing"]
    end
    
    subgraph "Training Pipeline"
        NEMO["NeMo Framework"]
        DIST["Distributed Training"]
        CKPT["Checkpointing"]
    end
    
    subgraph "Evaluation & Registry"
        EVAL["Model Evaluation"]
        MLFLOW["MLflow Registry"]
        VER["Version Control"]
    end
    
    subgraph "Deployment"
        SERVE["Model Serving"]
        AB["A/B Testing"]
        PROD["Production"]
    end
    
    DATA --> PREP
    PREP --> NEMO
    NEMO --> DIST
    DIST --> CKPT
    CKPT --> EVAL
    EVAL --> MLFLOW
    MLFLOW --> VER
    VER --> SERVE
    SERVE --> AB
    AB --> PROD
    
    style NEMO fill:#76b900
    style MLFLOW fill:#0194e2
```

#### Kubernetes ê¸°ë°˜ í•´ê²° ë°©ì•ˆ

**Kubeflow**, **NeMo**, **MLflow**ë¥¼ í™œìš©í•˜ì—¬ End-to-End MLOps íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | ì£¼ìš” ê¸°ëŠ¥ |
|---------|------|----------|
| **Kubeflow** | ML íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ | ì›Œí¬í”Œë¡œìš° ì •ì˜, ì‹¤í—˜ ì¶”ì , í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ |
| **NeMo** | LLM í•™ìŠµ í”„ë ˆì„ì›Œí¬ | ë¶„ì‚° í•™ìŠµ, PEFT, TensorRT-LLM ë³€í™˜ |
| **MLflow** | ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ | ëª¨ë¸ ë²„ì „ ê´€ë¦¬, ì‹¤í—˜ ì¶”ì , ë°°í¬ ê´€ë¦¬ |

```yaml
# NeMo ë¶„ì‚° í•™ìŠµ Job ì˜ˆì‹œ
apiVersion: batch/v1
kind: Job
metadata:
  name: nemo-finetune-llama
  namespace: ai-training
spec:
  parallelism: 4
  completions: 4
  template:
    spec:
      containers:
        - name: nemo
          image: nvcr.io/nvidia/nemo:24.01
          command:
            - python
            - -m
            - torch.distributed.launch
            - --nproc_per_node=8
            - --nnodes=4
            - /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_finetuning.py
          args:
            - model.data.train_ds.file_path=/data/train.jsonl
            - model.data.validation_ds.file_path=/data/val.jsonl
            - trainer.devices=8
            - trainer.num_nodes=4
            - trainer.max_epochs=3
          resources:
            limits:
              nvidia.com/gpu: 8
          volumeMounts:
            - name: training-data
              mountPath: /data
            - name: checkpoints
              mountPath: /checkpoints
      nodeSelector:
        node.kubernetes.io/instance-type: p5.48xlarge
      restartPolicy: OnFailure
```

:::info Kubeflow Pipeline ì˜ˆì‹œ
Kubeflowë¥¼ ì‚¬ìš©í•˜ë©´ ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ ë°°í¬ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì„ ì–¸ì ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
```python
@dsl.pipeline(name="llm-finetune-pipeline")
def finetune_pipeline(model_name: str, dataset_path: str):
    preprocess = preprocess_op(dataset_path)
    train = train_op(preprocess.output, model_name)
    evaluate = evaluate_op(train.output)
    deploy = deploy_op(evaluate.output).after(evaluate)
```
:::

---

## Kubernetesê°€ í•„ìˆ˜ì¸ ì´ìœ 

Agentic AI Platform êµ¬ì¶•ì— Kubernetesê°€ í•„ìˆ˜ì ì¸ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### ì„ ì–¸ì  ì¸í”„ë¼ ê´€ë¦¬ë¥¼ í†µí•œ ì¬í˜„ ê°€ëŠ¥í•œ í™˜ê²½ êµ¬ì„±

```yaml
# ì¸í”„ë¼ë¥¼ ì½”ë“œë¡œ ì •ì˜í•˜ì—¬ ì¬í˜„ ê°€ëŠ¥í•œ í™˜ê²½ êµ¬ì„±
apiVersion: v1
kind: Namespace
metadata:
  name: ai-platform
  labels:
    environment: production
    team: ml-platform
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-server
  namespace: ai-platform
spec:
  replicas: 3
  # ... ì„ ì–¸ì  ì„¤ì •
```

- **GitOps ì›Œí¬í”Œë¡œìš°**: ì¸í”„ë¼ ë³€ê²½ ì‚¬í•­ì„ Gitìœ¼ë¡œ ê´€ë¦¬
- **í™˜ê²½ ì¼ê´€ì„±**: ê°œë°œ, ìŠ¤í…Œì´ì§•, í”„ë¡œë•ì…˜ í™˜ê²½ ë™ì¼í•˜ê²Œ ìœ ì§€
- **ë¡¤ë°± ìš©ì´ì„±**: ì´ì „ ë²„ì „ìœ¼ë¡œ ì‰½ê²Œ ë³µì› ê°€ëŠ¥

### ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ì™€ì˜ ë„¤ì´í‹°ë¸Œ í†µí•©

KubernetesëŠ” AI/ML ìƒíƒœê³„ì˜ í•µì‹¬ ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬ë“¤ê³¼ ë„¤ì´í‹°ë¸Œí•˜ê²Œ í†µí•©ë©ë‹ˆë‹¤:

| ì¹´í…Œê³ ë¦¬ | ë„êµ¬ | Kubernetes í†µí•© |
|---------|------|----------------|
| **GPU ê´€ë¦¬** | NVIDIA GPU Operator | Operator íŒ¨í„´ìœ¼ë¡œ ìë™í™” |
| **ë…¸ë“œ ìŠ¤ì¼€ì¼ë§** | Karpenter | NodePool CRDë¡œ ì„ ì–¸ì  ê´€ë¦¬ |
| **ì´ë²¤íŠ¸ ìŠ¤ì¼€ì¼ë§** | KEDA | ScaledObject CRDë¡œ ìë™ ìŠ¤ì¼€ì¼ë§ |
| **ML íŒŒì´í”„ë¼ì¸** | Kubeflow | Kubernetes ë„¤ì´í‹°ë¸Œ ML í”Œë«í¼ |
| **ëª¨ë¸ ì„œë¹™** | KServe | InferenceService CRDë¡œ ëª¨ë¸ ë°°í¬ |
| **ì„œë¹„ìŠ¤ ë©”ì‹œ** | Istio | íŠ¸ë˜í”½ ê´€ë¦¬ ë° ê´€ì¸¡ì„± |

### ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ í†µí•œ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± ê·¹ëŒ€í™”

```mermaid
graph TB
    subgraph "ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±"
        BIN["Bin Packing<br/>ìµœì  ë¦¬ì†ŒìŠ¤ ë°°ì¹˜"]
        SHARE["Resource Sharing<br/>GPU ì‹œë¶„í• /MIG"]
        SCALE["Auto Scaling<br/>ìˆ˜ìš” ê¸°ë°˜ í™•ì¥"]
    end
    
    subgraph "ë¹„ìš© ìµœì í™”"
        SPOT["Spot Instances<br/>ë¹„ìš© ì ˆê°"]
        CONSOL["Consolidation<br/>ìœ íœ´ ë…¸ë“œ ì œê±°"]
        RIGHT["Right-sizing<br/>ì ì • ë¦¬ì†ŒìŠ¤ í• ë‹¹"]
    end
    
    BIN --> SPOT
    SHARE --> CONSOL
    SCALE --> RIGHT
```

- **Bin Packing**: ë…¸ë“œ ë¦¬ì†ŒìŠ¤ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ë„ë¡ Pod ë°°ì¹˜
- **GPU ê³µìœ **: MIG, Time-slicingì„ í†µí•œ GPU ë¦¬ì†ŒìŠ¤ ê³µìœ 
- **ìë™ ìŠ¤ì¼€ì¼ë§**: ìˆ˜ìš”ì— ë”°ë¥¸ ë™ì  ë¦¬ì†ŒìŠ¤ ì¡°ì •

### Custom Resource Definition(CRD)ì„ í†µí•œ AI ì›Œí¬ë¡œë“œ ì¶”ìƒí™”

CRDë¥¼ í†µí•´ AI ì›Œí¬ë¡œë“œë¥¼ Kubernetes ë„¤ì´í‹°ë¸Œ ë¦¬ì†ŒìŠ¤ë¡œ ì¶”ìƒí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
# AI Agentë¥¼ Kubernetes ë¦¬ì†ŒìŠ¤ë¡œ ì •ì˜
apiVersion: kagent.dev/v1alpha1
kind: Agent
metadata:
  name: customer-support-agent
spec:
  model:
    provider: openai
    name: gpt-4-turbo
  tools:
    - name: search-knowledge-base
      type: retrieval
  memory:
    type: redis
  scaling:
    minReplicas: 2
    maxReplicas: 10
```

- **ë„ë©”ì¸ íŠ¹í™” ì¶”ìƒí™”**: AI ì›Œí¬ë¡œë“œì— ë§ëŠ” ë¦¬ì†ŒìŠ¤ ì •ì˜
- **ì„ ì–¸ì  ê´€ë¦¬**: kubectlë¡œ AI ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
- **Operator íŒ¨í„´**: ë³µì¡í•œ ìš´ì˜ ë¡œì§ ìë™í™”

---

## Amazon EKSì˜ ì¥ì 

Amazon EKSëŠ” Kubernetesì˜ ëª¨ë“  ì¥ì ì„ ì œê³µí•˜ë©´ì„œ, ì¶”ê°€ì ì¸ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤:

### AWS ê´€ë¦¬í˜• ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ìœ¼ë¡œ ìš´ì˜ ë¶€ë‹´ ìµœì†Œí™”

```mermaid
graph TB
    subgraph "AWS ê´€ë¦¬ ì˜ì—­"
        CP["Control Plane<br/>etcd, API Server, Scheduler"]
        UP["ìë™ ì—…ê·¸ë ˆì´ë“œ"]
        HA["ê³ ê°€ìš©ì„± (Multi-AZ)"]
        SEC["ë³´ì•ˆ íŒ¨ì¹˜"]
    end
    
    subgraph "ì‚¬ìš©ì ê´€ë¦¬ ì˜ì—­"
        WL["AI Workloads"]
        APP["Applications"]
        DATA["Data"]
    end
    
    CP --> WL
    UP --> WL
    HA --> WL
    SEC --> WL
    
    style CP fill:#ff9900
    style UP fill:#ff9900
    style HA fill:#ff9900
    style SEC fill:#ff9900
```

- **99.95% SLA**: ê³ ê°€ìš©ì„± ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ë³´ì¥
- **ìë™ ì—…ê·¸ë ˆì´ë“œ**: Kubernetes ë²„ì „ ì—…ê·¸ë ˆì´ë“œ ìë™í™”
- **ë³´ì•ˆ íŒ¨ì¹˜**: ë³´ì•ˆ ì·¨ì•½ì  ìë™ íŒ¨ì¹˜

### Karpenterë¥¼ í†µí•œ GPU ë…¸ë“œ ìë™ í”„ë¡œë¹„ì €ë‹ ë° ë¹„ìš© ìµœì í™”

```yaml
# Karpenter NodePool for GPU Workloads
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-nodepool
spec:
  template:
    spec:
      requirements:
        - key: "node.kubernetes.io/instance-type"
          operator: In
          values: 
            - "p4d.24xlarge"   # A100 x 8
            - "p5.48xlarge"    # H100 x 8
            - "g5.48xlarge"    # A10G x 8
        - key: "karpenter.sh/capacity-type"
          operator: In
          values: ["on-demand", "spot"]
        - key: "kubernetes.io/arch"
          operator: In
          values: ["amd64"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodeclass
  limits:
    nvidia.com/gpu: 100
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
```

- **Just-in-Time í”„ë¡œë¹„ì €ë‹**: ì›Œí¬ë¡œë“œ ìš”êµ¬ì— ë”°ë¼ ì¦‰ì‹œ ë…¸ë“œ ìƒì„±
- **Spot ì¸ìŠ¤í„´ìŠ¤ í™œìš©**: ìµœëŒ€ 90% ë¹„ìš© ì ˆê°
- **ìë™ í†µí•©(Consolidation)**: ìœ íœ´ ë…¸ë“œ ìë™ ì œê±°

### EKS Auto Modeë¥¼ í†µí•œ ì¸í”„ë¼ ìë™í™”

EKS Auto ModeëŠ” ë…¸ë“œ ê´€ë¦¬, ìŠ¤ì¼€ì¼ë§, ì—…ê·¸ë ˆì´ë“œë¥¼ ì™„ì „íˆ ìë™í™”í•©ë‹ˆë‹¤:

| ê¸°ëŠ¥ | ì„¤ëª… |
|------|------|
| **ìë™ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹** | ì›Œí¬ë¡œë“œ ìš”êµ¬ì— ë”°ë¼ ìµœì ì˜ ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ |
| **ìë™ ìŠ¤ì¼€ì¼ë§** | ìˆ˜ìš” ë³€í™”ì— ë”°ë¥¸ ë…¸ë“œ ìˆ˜ ìë™ ì¡°ì • |
| **ìë™ ì—…ê·¸ë ˆì´ë“œ** | ë…¸ë“œ AMI ë° Kubernetes ë²„ì „ ìë™ ì—…ê·¸ë ˆì´ë“œ |
| **ìë™ ë³µêµ¬** | ë¹„ì •ìƒ ë…¸ë“œ ìë™ ê°ì§€ ë° êµì²´ |

:::tip EKS Auto Mode í™œì„±í™”
EKS Auto Modeë¥¼ ì‚¬ìš©í•˜ë©´ Karpenter, CoreDNS, kube-proxy ë“±ì˜ ê´€ë¦¬ê°€ ìë™í™”ë˜ì–´ ìš´ì˜ ë¶€ë‹´ì´ í¬ê²Œ ì¤„ì–´ë“­ë‹ˆë‹¤.
:::

### AWS ì„œë¹„ìŠ¤ì™€ì˜ ë„¤ì´í‹°ë¸Œ í†µí•©

```mermaid
graph LR
    subgraph "EKS Cluster"
        POD["AI Workloads"]
    end
    
    subgraph "AWS Services"
        S3["S3<br/>Model Storage"]
        FSX["FSx for Lustre<br/>High-Performance Storage"]
        CW["CloudWatch<br/>Monitoring"]
        SM["SageMaker<br/>Training Jobs"]
        BR["Bedrock<br/>Foundation Models"]
    end
    
    POD --> S3
    POD --> FSX
    POD --> CW
    POD --> SM
    POD --> BR
    
    style S3 fill:#569a31
    style FSX fill:#569a31
    style CW fill:#569a31
    style SM fill:#569a31
    style BR fill:#569a31
```

| AWS ì„œë¹„ìŠ¤ | ìš©ë„ | EKS í†µí•© ë°©ë²• |
|-----------|------|--------------|
| **Amazon S3** | ëª¨ë¸ ì•„í‹°íŒ©íŠ¸, í•™ìŠµ ë°ì´í„° ì €ì¥ | CSI Driver, IRSA |
| **FSx for Lustre** | ê³ ì„±ëŠ¥ í•™ìŠµ ë°ì´í„° ìŠ¤í† ë¦¬ì§€ | CSI Driver |
| **Amazon CloudWatch** | ë©”íŠ¸ë¦­, ë¡œê·¸, ì•Œë¦¼ | Container Insights |
| **Amazon SageMaker** | í•™ìŠµ ì‘ì—…, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | SageMaker Operators |
| **Amazon Bedrock** | ê´€ë¦¬í˜• Foundation Model | API í†µí•© |

### í”Œë«í¼ ì—”ì§€ë‹ˆì–´ê°€ Agentic AI íŠœë‹ì— ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” í™˜ê²½ ì œê³µ

EKSë¥¼ ì‚¬ìš©í•˜ë©´ í”Œë«í¼ ì—”ì§€ë‹ˆì–´ê°€ ì¸í”„ë¼ ê´€ë¦¬ ëŒ€ì‹  **Agentic AI ìµœì í™”**ì— ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

| ê¸°ì¡´ (Self-managed K8s) | EKS ì‚¬ìš© ì‹œ |
|------------------------|------------|
| ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ê´€ë¦¬ | âœ… AWS ê´€ë¦¬ |
| etcd ë°±ì—…/ë³µêµ¬ | âœ… AWS ê´€ë¦¬ |
| Kubernetes ì—…ê·¸ë ˆì´ë“œ | âœ… ìë™í™” ê°€ëŠ¥ |
| ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ | âœ… Karpenter/Auto Mode |
| ë³´ì•ˆ íŒ¨ì¹˜ | âœ… ìë™í™” |
| **AI ëª¨ë¸ íŠœë‹** | ğŸ¯ ì§‘ì¤‘ ê°€ëŠ¥ |
| **í”„ë¡¬í”„íŠ¸ ìµœì í™”** | ğŸ¯ ì§‘ì¤‘ ê°€ëŠ¥ |
| **ë¹„ìš© ìµœì í™”** | ğŸ¯ ì§‘ì¤‘ ê°€ëŠ¥ |

---

## ê²°ë¡ 

Agentic AI Platform êµ¬ì¶•ì€ GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬, ë™ì  ë¼ìš°íŒ…, ë¹„ìš© ì»¨íŠ¸ë¡¤, ìë™í™” íŒŒì´í”„ë¼ì¸ì´ë¼ëŠ” 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œë¥¼ ìˆ˜ë°˜í•©ë‹ˆë‹¤. KubernetesëŠ” ì´ëŸ¬í•œ ë„ì „ê³¼ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ **ì„ ì–¸ì  ì¸í”„ë¼ ê´€ë¦¬**, **ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ í†µí•©**, **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±**, **CRD ê¸°ë°˜ ì¶”ìƒí™”**ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

Amazon EKSëŠ” Kubernetesì˜ ëª¨ë“  ì¥ì ì— ë”í•´ **ê´€ë¦¬í˜• ì»¨íŠ¸ë¡¤ í”Œë ˆì¸**, **Karpenter í†µí•©**, **EKS Auto Mode**, **AWS ì„œë¹„ìŠ¤ ë„¤ì´í‹°ë¸Œ í†µí•©**ì„ ì œê³µí•˜ì—¬, í”Œë«í¼ ì—”ì§€ë‹ˆì–´ê°€ ì¸í”„ë¼ ê´€ë¦¬ ëŒ€ì‹  Agentic AI ìµœì í™”ì— ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.

:::info ë‹¤ìŒ ë‹¨ê³„
ì´ ë¬¸ì„œì—ì„œ ì†Œê°œí•œ ê° ë„ì „ê³¼ì œì— ëŒ€í•œ ìƒì„¸í•œ êµ¬í˜„ ê°€ì´ë“œëŠ” ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ì¡°í•˜ì„¸ìš”:
- [GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬](./gpu-resource-management.md) - GPU í´ëŸ¬ìŠ¤í„° ë™ì  ë¦¬ì†ŒìŠ¤ í• ë‹¹
- [Inference Gateway](./inference-gateway-routing.md) - Kgateway ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…
- [Agent ëª¨ë‹ˆí„°ë§](./agent-monitoring.md) - LangFuse, LangSmith í†µí•©
- [NeMo í”„ë ˆì„ì›Œí¬](./nemo-framework.md) - FM íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸
:::

---

## ì°¸ê³  ìë£Œ

- [NVIDIA GPU Operator Documentation](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html)
- [Kubernetes Gateway API](https://gateway-api.sigs.k8s.io/)
- [KEDA - Kubernetes Event-driven Autoscaling](https://keda.sh/)
- [LangFuse Documentation](https://langfuse.com/docs)
- [NVIDIA NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)
- [Amazon EKS Best Practices Guide](https://aws.github.io/aws-eks-best-practices/)
- [Karpenter Documentation](https://karpenter.sh/docs/)