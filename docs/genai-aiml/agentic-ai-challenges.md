---
title: "Agentic AI Platform ê¸°ìˆ ì  ë„ì „ê³¼ì œì™€ Karpenter ê¸°ë°˜ í•´ê²° ë°©ì•ˆ"
sidebar_label: "ê¸°ìˆ ì  ë„ì „ê³¼ì œ"
description: "Agentic AI Platform êµ¬ì¶• ì‹œ ì§ë©´í•˜ëŠ” 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œì™€ Karpenterë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ EKS ê¸°ë°˜ í•´ê²° ë°©ì•ˆ"
tags: [eks, kubernetes, genai, agentic-ai, gpu, infrastructure, challenges, karpenter]
category: "genai-aiml"
date: 2025-02-05
authors: [devfloor9]
sidebar_position: 3
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

> ğŸ“… **ì‘ì„±ì¼**: 2025-02-05 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 25ë¶„

Agentic AI Platformì„ êµ¬ì¶•í•˜ê³  ìš´ì˜í•˜ëŠ” ê³¼ì •ì—ì„œ í”Œë«í¼ ì—”ì§€ë‹ˆì–´ì™€ ì•„í‚¤í…íŠ¸ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì  ë„ì „ê³¼ì œì— ì§ë©´í•©ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œë¥¼ ë¶„ì„í•˜ê³ , **Karpenterë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ EKS ê¸°ë°˜ í•´ê²° ë°©ì•ˆ**ì„ ì œì‹œí•©ë‹ˆë‹¤.

## ê°œìš”

Frontier Model(ìµœì‹  ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì„ í™œìš©í•œ Agentic AI ì‹œìŠ¤í…œì€ ê¸°ì¡´ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ëŠ” ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì¸í”„ë¼ ìš”êµ¬ì‚¬í•­ì„ ê°€ì§‘ë‹ˆë‹¤. íŠ¹íˆ **GPU ë¦¬ì†ŒìŠ¤ì˜ ë™ì  í”„ë¡œë¹„ì €ë‹ê³¼ ë¹„ìš© ìµœì í™”**ê°€ í•µì‹¬ ê³¼ì œì´ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Karpenter**ê°€ ê°€ì¥ íš¨ê³¼ì ì¸ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œ"
        C1["ğŸ–¥ï¸ GPU ëª¨ë‹ˆí„°ë§ ë°<br/>ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§"]
        C2["ğŸ”€ Agentic AI ìš”ì²­<br/>ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§"]
        C3["ğŸ“Š í† í°/ì„¸ì…˜ ìˆ˜ì¤€<br/>ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© ì»¨íŠ¸ë¡¤"]
        C4["ğŸ”§ FM íŒŒì¸íŠœë‹ê³¼<br/>ìë™í™” íŒŒì´í”„ë¼ì¸"]
    end

    subgraph "Karpenter ì¤‘ì‹¬ í•´ê²° ë°©ì•ˆ"
        S1["â­ Karpenter<br/>Just-in-Time GPU í”„ë¡œë¹„ì €ë‹"]
        S2["Gateway API + KEDA<br/>ë™ì  ìŠ¤ì¼€ì¼ë§ ì—°ë™"]
        S3["LangFuse + OpenTelemetry<br/>ë¹„ìš© ì¶”ì "]
        S4["Kubeflow + NeMo<br/>í•™ìŠµ íŒŒì´í”„ë¼ì¸"]
    end

    C1 --> S1
    C2 --> S1
    C2 --> S2
    C3 --> S3
    C4 --> S4

    style C1 fill:#ff6b6b
    style C2 fill:#4ecdc4
    style C3 fill:#45b7d1
    style C4 fill:#96ceb4
    style S1 fill:#ffd93d
```

:::info ëŒ€ìƒ ë…ì
ì´ ë¬¸ì„œëŠ” Agentic AI Platform ë„ì…ì„ ê²€í† í•˜ëŠ” **ê¸°ìˆ  ì˜ì‚¬ê²°ì •ì**ì™€ **ì†”ë£¨ì…˜ ì•„í‚¤í…íŠ¸**ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤. Karpenterë¥¼ í™œìš©í•œ GPU ë¦¬ì†ŒìŠ¤ ìµœì í™” ì „ëµê³¼ EKS ë„ì…ì˜ ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
:::

## Karpenter: Agentic AI ì¸í”„ë¼ì˜ í•µì‹¬

KarpenterëŠ” Agentic AI Platformì˜ ëª¨ë“  ë„ì „ê³¼ì œë¥¼ í•´ê²°í•˜ëŠ” **í•µì‹¬ ì»´í¬ë„ŒíŠ¸**ì…ë‹ˆë‹¤. ê¸°ì¡´ Cluster Autoscalerì™€ ë‹¬ë¦¬ KarpenterëŠ” ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­ì„ ì§ì ‘ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë…¸ë“œë¥¼ ì¦‰ì‹œ í”„ë¡œë¹„ì €ë‹í•©ë‹ˆë‹¤.

### Karpenterê°€ ì œê³µí•˜ëŠ” í•µì‹¬ ê°€ì¹˜

| ê¸°ëŠ¥ | ì„¤ëª… | Agentic AI ì ìš© |
| --- | --- | --- |
| Just-in-Time í”„ë¡œë¹„ì €ë‹ | ì›Œí¬ë¡œë“œ ìš”êµ¬ì— ë”°ë¼ ì¦‰ì‹œ ë…¸ë“œ ìƒì„± | GPU ë…¸ë“œ ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™” |
| Spot ì¸ìŠ¤í„´ìŠ¤ ì§€ì› | ìµœëŒ€ 90% ë¹„ìš© ì ˆê° | ì¶”ë¡  ì›Œí¬ë¡œë“œ ë¹„ìš© ìµœì í™” |
| Consolidation | ìœ íœ´ ë…¸ë“œ ìë™ ì •ë¦¬ | GPU ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± ê·¹ëŒ€í™” |
| ë‹¤ì–‘í•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | ì›Œí¬ë¡œë“œì— ìµœì í™”ëœ ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ | ëª¨ë¸ í¬ê¸°ë³„ ìµœì  GPU ë§¤ì¹­ |

```mermaid
flowchart LR
    subgraph "ê¸°ì¡´ ë°©ì‹ (Cluster Autoscaler)"
        CA1[Pod Pending] --> CA2[Node Group í™•ì¸]
        CA2 --> CA3[ASG ìŠ¤ì¼€ì¼ ì•„ì›ƒ]
        CA3 --> CA4[ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ]
        CA4 --> CA5[Pod ìŠ¤ì¼€ì¤„ë§]
    end

    subgraph "Karpenter ë°©ì‹"
        K1[Pod Pending] --> K2[ì›Œí¬ë¡œë“œ ë¶„ì„]
        K2 --> K3[ìµœì  ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ]
        K3 --> K4[ì¦‰ì‹œ í”„ë¡œë¹„ì €ë‹]
    end

    style K2 fill:#ffd93d
    style K3 fill:#ffd93d
    style K4 fill:#ffd93d
```

:::tip Karpenter vs Cluster Autoscaler
KarpenterëŠ” Node Group ì—†ì´ ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­ì„ ì§ì ‘ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. GPU ì›Œí¬ë¡œë“œì˜ ê²½ìš° í”„ë¡œë¹„ì €ë‹ ì‹œê°„ì´ **50% ì´ìƒ ë‹¨ì¶•**ë©ë‹ˆë‹¤.
:::

## 4ê°€ì§€ í•µì‹¬ ê¸°ìˆ ì  ë„ì „ê³¼ì œ

### ë„ì „ê³¼ì œ 1: GPU ëª¨ë‹ˆí„°ë§ ë° ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§

Agentic AI ì›Œí¬ë¡œë“œëŠ” GPU ë¦¬ì†ŒìŠ¤ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. ë³µìˆ˜ì˜ GPU í´ëŸ¬ìŠ¤í„°ë¥¼ ìš´ì˜í•  ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ì–´ë ¤ì›€ì— ì§ë©´í•©ë‹ˆë‹¤.

#### ê¸°ìˆ ì  ë¬¸ì œì  ìƒì„¸ ë¶„ì„

**1. ë©€í‹° í´ëŸ¬ìŠ¤í„° GPU ê°€ì‹œì„± ë¶€ì¬**

ëŒ€ê·œëª¨ AI í”Œë«í¼ì—ì„œëŠ” ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ì— ë¶„ì‚°ëœ GPU ë¦¬ì†ŒìŠ¤ë¥¼ í†µí•©ì ìœ¼ë¡œ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "ê°€ì‹œì„± ë¬¸ì œ"
        Q1["í´ëŸ¬ìŠ¤í„° Aì˜ GPU ì‚¬ìš©ë¥ ì€?"]
        Q2["ì „ì²´ ìœ íœ´ GPUëŠ” ëª‡ ê°œ?"]
        Q3["ì–´ëŠ í´ëŸ¬ìŠ¤í„°ì— ì›Œí¬ë¡œë“œ ë°°ì¹˜?"]
    end

    subgraph "ë¶„ì‚°ëœ GPU í´ëŸ¬ìŠ¤í„°"
        subgraph "Cluster A (US-East)"
            A1["A100 x 16<br/>ì‚¬ìš©ë¥ : ???"]
        end
        subgraph "Cluster B (US-West)"
            B1["H100 x 8<br/>ì‚¬ìš©ë¥ : ???"]
        end
        subgraph "Cluster C (EU)"
            C1["A100 x 24<br/>ì‚¬ìš©ë¥ : ???"]
        end
    end

    Q1 -.-> A1
    Q2 -.-> A1 & B1 & C1
    Q3 -.-> A1 & B1 & C1

    style Q1 fill:#ff6b6b
    style Q2 fill:#ff6b6b
    style Q3 fill:#ff6b6b
```

| ë¬¸ì œ ì˜ì—­ | êµ¬ì²´ì  ì–´ë ¤ì›€ | ì˜í–¥ |
| --- | --- | --- |
| ë©”íŠ¸ë¦­ ìˆ˜ì§‘ | í´ëŸ¬ìŠ¤í„°ë³„ ë‹¤ë¥¸ ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ | í†µí•© ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ì–´ë ¤ì›€ |
| ì‹¤ì‹œê°„ í˜„í™© | GPU í• ë‹¹ ìƒíƒœ íŒŒì•… ì§€ì—° | ë¦¬ì†ŒìŠ¤ ë‚­ë¹„, ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨ |
| ìš©ëŸ‰ ê³„íš | ì „ì²´ GPU ì¸ë²¤í† ë¦¬ íŒŒì•… ë¶ˆê°€ | ê³¼ì‰/ë¶€ì¡± í”„ë¡œë¹„ì €ë‹ |

**2. GPU ì„¸ëŒ€ë³„ ì›Œí¬ë¡œë“œ ë§¤ì¹­ ë³µì¡ì„±**

A100, H100, H200 ë“± ë‹¤ì–‘í•œ GPU ì„¸ëŒ€ê°€ í˜¼í•© ìš´ì˜ë  ë•Œ, ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë§ëŠ” ìµœì ì˜ GPUë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤:

| GPU ì„¸ëŒ€ | ë©”ëª¨ë¦¬ | FP16 ì„±ëŠ¥ | ì í•© ì›Œí¬ë¡œë“œ | ì‹œê°„ë‹¹ ë¹„ìš© |
| --- | --- | --- | --- | --- |
| A10G | 24GB | 125 TFLOPS | ì†Œê·œëª¨ ì¶”ë¡  (7B ì´í•˜) | ~$1.0 |
| A100 40GB | 40GB | 312 TFLOPS | ì¤‘ê·œëª¨ ì¶”ë¡ /í•™ìŠµ | ~$4.1 |
| A100 80GB | 80GB | 312 TFLOPS | ëŒ€ê·œëª¨ ëª¨ë¸ | ~$5.1 |
| H100 80GB | 80GB | 989 TFLOPS | ì´ˆëŒ€ê·œëª¨ í•™ìŠµ/ì¶”ë¡  | ~$12.3 |
| H200 | 141GB | 989 TFLOPS | ìµœëŒ€ ê·œëª¨ ëª¨ë¸ | ~$15.0+ |

**3. GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì˜ ê¸°ìˆ ì  í•œê³„**

- DCGM Exporterì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì£¼ê¸°ì™€ ì •í™•ë„
- MIG(Multi-Instance GPU) í™˜ê²½ì—ì„œì˜ ë©”íŠ¸ë¦­ ë¶„ë¦¬
- ì»¨í…Œì´ë„ˆ ë ˆë²¨ GPU ì‚¬ìš©ëŸ‰ ì¶”ì ì˜ ì–´ë ¤ì›€

```mermaid
graph LR
    subgraph "GPU í´ëŸ¬ìŠ¤í„° í™˜ê²½"
        subgraph "Cluster A"
            A100_1["A100 x 8"]
            A100_2["A100 x 8"]
        end
        subgraph "Cluster B"
            H100_1["H100 x 8"]
            H100_2["H100 x 8"]
        end
        subgraph "Cluster C"
            H200_1["H200 x 8"]
        end
    end

    subgraph "Karpenter + ëª¨ë‹ˆí„°ë§"
        KARP["Karpenter<br/>NodePool"]
        DCGM["DCGM Exporter"]
        PROM["Prometheus"]
    end

    A100_1 --> DCGM
    H100_1 --> DCGM
    H200_1 --> DCGM
    DCGM --> PROM
    PROM --> KARP

    style KARP fill:#ffd93d
```

#### Karpenter ê¸°ë°˜ í•´ê²° ë°©ì•ˆ (ê¶Œì¥)

**Karpenter NodePool**ì„ í™œìš©í•˜ë©´ GPU ì›Œí¬ë¡œë“œì— ìµœì í™”ëœ ë…¸ë“œë¥¼ ìë™ìœ¼ë¡œ í”„ë¡œë¹„ì €ë‹í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<Tabs>
<TabItem value="nodepool" label="GPU NodePool ì„¤ì •" default>

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-inference
        workload: genai
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge    # 8x A100 40GB
            - p5.48xlarge     # 8x H100 80GB
            - g5.48xlarge     # 8x A10G 24GB
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 100
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
  weight: 100
```

</TabItem>
<TabItem value="nodeclass" label="EC2NodeClass ì„¤ì •">

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu-nodeclass
spec:
  role: KarpenterNodeRole-${CLUSTER_NAME}
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 500Gi
        volumeType: gp3
        iops: 10000
        throughput: 500
        encrypted: true
  instanceStorePolicy: RAID0
  userData: |
    #!/bin/bash
    nvidia-smi -pm 1
    modprobe efa
```

</TabItem>
</Tabs>

#### Karpenterì˜ GPU ì›Œí¬ë¡œë“œ ìµœì í™” ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì„¤ëª… | íš¨ê³¼ |
| --- | --- | --- |
| ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìë™ ì„ íƒ | ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” GPU ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ | ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ë°©ì§€ |
| Spot ì¸ìŠ¤í„´ìŠ¤ í´ë°± | Spot ë¶ˆê°€ ì‹œ On-Demandë¡œ ìë™ ì „í™˜ | ê°€ìš©ì„± ë³´ì¥ |
| Consolidation | ìœ íœ´ GPU ë…¸ë“œ ìë™ ì •ë¦¬ | ë¹„ìš© 30% ì ˆê° |
| ë¹ ë¥¸ í”„ë¡œë¹„ì €ë‹ | Node Group ì—†ì´ ì§ì ‘ EC2 API í˜¸ì¶œ | í”„ë¡œë¹„ì €ë‹ ì‹œê°„ 50% ë‹¨ì¶• |

#### ë³´ì¡° ì†”ë£¨ì…˜: NVIDIA GPU Operator

Karpenterì™€ í•¨ê»˜ NVIDIA GPU Operatorë¥¼ ì‚¬ìš©í•˜ì—¬ GPU ë“œë¼ì´ë²„ ë° ëª¨ë‹ˆí„°ë§ ìŠ¤íƒì„ ìë™í™”í•©ë‹ˆë‹¤.

```yaml
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: cluster-policy
spec:
  operator:
    defaultRuntime: containerd
  driver:
    enabled: true
    version: "535.104.05"
  toolkit:
    enabled: true
  devicePlugin:
    enabled: true
  dcgmExporter:
    enabled: true
  migManager:
    enabled: true
```

### ë„ì „ê³¼ì œ 2: Agentic AI ìš”ì²­ ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§

Agentic AI ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ FM(Foundation Model)ì„ ë™ì‹œì— ì„œë¹™í•˜ë©°, íŠ¸ë˜í”½ íŒ¨í„´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ëŒ€ì‘í•´ì•¼ í•©ë‹ˆë‹¤.

#### ê¸°ìˆ ì  ë¬¸ì œì  ìƒì„¸ ë¶„ì„

**1. ë©€í‹° ëª¨ë¸ ì„œë¹™ì˜ ë³µì¡ì„±**

Agentic AI ì‹œìŠ¤í…œì€ ë‹¨ì¼ ëª¨ë¸ì´ ì•„ë‹Œ ì—¬ëŸ¬ ëª¨ë¸ì„ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "Agent ìš”ì²­ ì²˜ë¦¬ íë¦„"
        REQ["ì‚¬ìš©ì ìš”ì²­"]
        ROUTER["ìš”ì²­ ë¼ìš°í„°"]
        
        subgraph "ëª¨ë¸ ì„ íƒ ë¡œì§"
            M1["GPT-4<br/>ë³µì¡í•œ ì¶”ë¡ "]
            M2["Claude-3<br/>ê¸´ ì»¨í…ìŠ¤íŠ¸"]
            M3["Llama-70B<br/>ë¹„ìš© íš¨ìœ¨"]
            M4["Embedding<br/>ë²¡í„° ê²€ìƒ‰"]
        end
        
        RESP["ì‘ë‹µ ì¡°í•©"]
    end

    REQ --> ROUTER
    ROUTER --> M1
    ROUTER --> M2
    ROUTER --> M3
    ROUTER --> M4
    M1 & M2 & M3 & M4 --> RESP

    style ROUTER fill:#4ecdc4
```

| ë¼ìš°íŒ… ê¸°ì¤€ | ì„¤ëª… | êµ¬í˜„ ë³µì¡ë„ |
| --- | --- | --- |
| ìš”ì²­ ìœ í˜• | ì½”ë“œ ìƒì„±, ëŒ€í™”, ìš”ì•½ ë“± | ì¤‘ê°„ |
| ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ | í† í° ìˆ˜ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ | ë‚®ìŒ |
| ë¹„ìš© ì œì•½ | ì˜ˆì‚° ë‚´ ìµœì  ëª¨ë¸ ì„ íƒ | ë†’ìŒ |
| ì§€ì—° ì‹œê°„ ìš”êµ¬ | SLA ê¸°ë°˜ ëª¨ë¸ ì„ íƒ | ë†’ìŒ |
| ëª¨ë¸ ê°€ìš©ì„± | ì¥ì•  ì‹œ í´ë°± ëª¨ë¸ ì„ íƒ | ì¤‘ê°„ |

**2. ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ íŠ¸ë˜í”½ íŒ¨í„´**

Agentic AI ì›Œí¬ë¡œë“œëŠ” ê¸°ì¡´ ì›¹ ì„œë¹„ìŠ¤ì™€ ë‹¤ë¥¸ íŠ¸ë˜í”½ íŠ¹ì„±ì„ ë³´ì…ë‹ˆë‹¤:

```mermaid
graph LR
    subgraph "íŠ¸ë˜í”½ íŠ¹ì„± ë¹„êµ"
        subgraph "ì¼ë°˜ ì›¹ ì„œë¹„ìŠ¤"
            W1["ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´"]
            W2["ì§§ì€ ìš”ì²­ ì‹œê°„"]
            W3["ê· ì¼í•œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©"]
        end
        
        subgraph "Agentic AI ì„œë¹„ìŠ¤"
            A1["ë²„ìŠ¤íŠ¸ íŠ¸ë˜í”½"]
            A2["ê¸´ ìš”ì²­ ì‹œê°„ (ìˆ˜ì´ˆ~ìˆ˜ë¶„)"]
            A3["ìš”ì²­ë³„ ë¦¬ì†ŒìŠ¤ í¸ì°¨ í¼"]
        end
    end

    style A1 fill:#ff6b6b
    style A2 fill:#ff6b6b
    style A3 fill:#ff6b6b
```

**3. GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì§€ì—°**

íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œ GPU ë…¸ë“œ í™•ë³´ê¹Œì§€ì˜ ì‹œê°„ì´ ì„œë¹„ìŠ¤ í’ˆì§ˆì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤:

| ë‹¨ê³„ | ê¸°ì¡´ ë°©ì‹ (Cluster Autoscaler) | Karpenter |
| --- | --- | --- |
| Pending Pod ê°ì§€ | 30-60ì´ˆ | ì¦‰ì‹œ |
| ìŠ¤ì¼€ì¼ë§ ê²°ì • | Node Group ê¸°ë°˜ | ì›Œí¬ë¡œë“œ ì§ì ‘ ë¶„ì„ |
| ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ | ê³ ì •ëœ íƒ€ì… | ìµœì  íƒ€ì… ìë™ ì„ íƒ |
| í”„ë¡œë¹„ì €ë‹ | ASG ê²½ìœ  (2-5ë¶„) | ì§ì ‘ EC2 API (1-3ë¶„) |
| **ì´ ì†Œìš” ì‹œê°„** | **5-10ë¶„** | **2-4ë¶„** |

**4. ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì‹œ ì„œë¹„ìŠ¤ ì˜í–¥**

GPU ë…¸ë“œ ì¶•ì†Œ ì‹œ ì§„í–‰ ì¤‘ì¸ ìš”ì²­ ì²˜ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤:

- LLM ì¶”ë¡ ì€ ìˆ˜ì´ˆ~ìˆ˜ë¶„ ì†Œìš”
- ê°‘ì‘ìŠ¤ëŸ¬ìš´ ë…¸ë“œ ì¢…ë£Œ ì‹œ ìš”ì²­ ì‹¤íŒ¨
- Graceful shutdown êµ¬í˜„ í•„ìš”

```mermaid
graph TB
    subgraph "Client Requests"
        REQ1["Chat Request"]
        REQ2["Code Generation"]
        REQ3["RAG Query"]
    end

    subgraph "Gateway Layer"
        GW["Kgateway<br/>Inference Gateway"]
        ROUTE["Dynamic Router"]
    end

    subgraph "Karpenter ê´€ë¦¬ ë…¸ë“œ"
        subgraph "Model Serving"
            M1["vLLM - GPT-4"]
            M2["vLLM - Claude"]
            M3["TGI - Llama"]
        end
        KARP["Karpenter<br/>Auto Provisioning"]
    end

    REQ1 --> GW
    REQ2 --> GW
    REQ3 --> GW
    GW --> ROUTE
    ROUTE --> M1
    ROUTE --> M2
    ROUTE --> M3
    M1 & M2 & M3 -.-> KARP

    style KARP fill:#ffd93d
    style GW fill:#4286f4
```

#### Karpenter + KEDA ì—°ë™ í•´ê²° ë°©ì•ˆ (ê¶Œì¥)

Karpenterì™€ KEDAë¥¼ ì—°ë™í•˜ë©´ **ì›Œí¬ë¡œë“œ ìŠ¤ì¼€ì¼ë§ê³¼ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ì´ ìë™ìœ¼ë¡œ ì—°ê³„**ë©ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì íŠ¸ë˜í”½
    participant KEDA as KEDA Controller
    participant HPA as HPA
    participant Karpenter as Karpenter
    participant AWS as AWS EC2

    User->>KEDA: íŠ¸ë˜í”½ ê¸‰ì¦ ê°ì§€
    KEDA->>HPA: Pod ìŠ¤ì¼€ì¼ ì•„ì›ƒ íŠ¸ë¦¬ê±°
    HPA->>Karpenter: Pending Pod ê°ì§€
    Karpenter->>AWS: ìµœì  GPU ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œë¹„ì €ë‹
    AWS-->>Karpenter: p4d.24xlarge ì¤€ë¹„ ì™„ë£Œ
    Karpenter-->>HPA: ìƒˆ ë…¸ë“œì— Pod ìŠ¤ì¼€ì¤„ë§
    HPA-->>User: ì‘ë‹µ ì§€ì—° ì‹œê°„ ì •ìƒí™”
```

<Tabs>
<TabItem value="keda" label="KEDA ScaledObject" default>

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: vllm-gpu-scaler
  namespace: ai-inference
spec:
  scaleTargetRef:
    name: vllm-deployment
  minReplicaCount: 2
  maxReplicaCount: 20
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.observability:9090
        metricName: vllm_pending_requests
        threshold: "50"
        query: |
          sum(vllm_pending_requests{namespace="ai-inference"})
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.observability:9090
        metricName: gpu_utilization
        threshold: "70"
        query: |
          avg(DCGM_FI_DEV_GPU_UTIL{namespace="ai-inference"})
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
        scaleDown:
          stabilizationWindowSeconds: 300
```

</TabItem>
<TabItem value="httproute" label="Gateway API HTTPRoute">

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: ai-model-routing
  namespace: ai-inference
spec:
  parentRefs:
    - name: ai-gateway
      namespace: ai-gateway
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /v1/chat/completions
          headers:
            - name: x-model-id
              value: "gpt-4"
      backendRefs:
        - name: vllm-gpt4
          port: 8000
          weight: 80
        - name: vllm-gpt4-canary
          port: 8000
          weight: 20
    - matches:
        - path:
            type: PathPrefix
            value: /v1/chat/completions
          headers:
            - name: x-model-id
              value: "claude-3"
      backendRefs:
        - name: vllm-claude
          port: 8000
```

</TabItem>
</Tabs>

#### Karpenter Disruption ì •ì±…ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´

íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œì—ë„ ì„œë¹„ìŠ¤ ì•ˆì •ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ Karpenter ì„¤ì •ì…ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-stable
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
    budgets:
      # ë™ì‹œì— ì¤‘ë‹¨ ê°€ëŠ¥í•œ ë…¸ë“œ ìˆ˜ ì œí•œ
      - nodes: "20%"
      # ì—…ë¬´ ì‹œê°„ì—ëŠ” ì¤‘ë‹¨ ë°©ì§€
      - nodes: "0"
        schedule: "0 9 * * 1-5"
        duration: 10h
```

:::warning ìŠ¤ì¼€ì¼ë§ ì£¼ì˜ì‚¬í•­
GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ì€ ì¼ë°˜ CPU ë…¸ë“œë³´ë‹¤ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. Karpenterì˜ `consolidationPolicy`ë¥¼ ì ì ˆíˆ ì„¤ì •í•˜ì—¬ ë¶ˆí•„ìš”í•œ ìŠ¤ì¼€ì¼ ë‹¤ìš´ì„ ë°©ì§€í•˜ì„¸ìš”.
:::

### ë„ì „ê³¼ì œ 3: í† í°/ì„¸ì…˜ ìˆ˜ì¤€ ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© ì»¨íŠ¸ë¡¤

LLM ê¸°ë°˜ ì‹œìŠ¤í…œì—ì„œëŠ” í† í° ë‹¨ìœ„ì˜ ì„¸ë°€í•œ ëª¨ë‹ˆí„°ë§ê³¼ ë¹„ìš© ê´€ë¦¬ê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤. íŠ¹íˆ GPU ì¸í”„ë¼ ë¹„ìš©ì´ ì „ì²´ ìš´ì˜ ë¹„ìš©ì˜ 70-80%ë¥¼ ì°¨ì§€í•˜ë¯€ë¡œ, **ì¸í”„ë¼ ë ˆë²¨ì˜ ë¹„ìš© ìµœì í™”**ê°€ í•µì‹¬ì…ë‹ˆë‹¤.

#### ê¸°ìˆ ì  ë¬¸ì œì  ìƒì„¸ ë¶„ì„

**1. í† í° ë ˆë²¨ ë¹„ìš© ì¶”ì ì˜ ë³µì¡ì„±**

LLM ì„œë¹„ìŠ¤ì˜ ë¹„ìš© êµ¬ì¡°ëŠ” ë‹¤ì¸µì ì…ë‹ˆë‹¤:

```
ì´ ë¹„ìš© = GPU ì¸í”„ë¼ ë¹„ìš© + API í˜¸ì¶œ ë¹„ìš© + ìŠ¤í† ë¦¬ì§€ ë¹„ìš© + ë„¤íŠ¸ì›Œí¬ ë¹„ìš©
```

| ë¹„ìš© ìš”ì†Œ | ì¸¡ì • ë‚œì´ë„ | ë¹„ì¤‘ | ë¬¸ì œì  |
| --- | --- | --- | --- |
| GPU ì¸í”„ë¼ | ì¤‘ê°„ | 70-80% | ìœ íœ´ ì‹œê°„ ë¹„ìš© ë°œìƒ, ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ë³„ ë‹¨ê°€ ì°¨ì´ |
| í† í° ì‚¬ìš©ëŸ‰ | ë†’ìŒ | 10-15% | ì…ë ¥/ì¶œë ¥ í† í° ë¹„ìœ¨ ì˜ˆì¸¡ ì–´ë ¤ì›€ |
| ìŠ¤í† ë¦¬ì§€ | ë‚®ìŒ | 5-10% | ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ í¬ê¸° ì¦ê°€ |
| ë„¤íŠ¸ì›Œí¬ | ë‚®ìŒ | 3-5% | Cross-AZ íŠ¸ë˜í”½ ë¹„ìš© |

**2. GPU ìœ íœ´ ë¹„ìš© ë¬¸ì œ**

```mermaid
graph LR
    subgraph "ì¼ë°˜ì ì¸ GPU ì‚¬ìš© íŒ¨í„´"
        direction TB
        T1["09:00-12:00<br/>ì‚¬ìš©ë¥  80%"]
        T2["12:00-14:00<br/>ì‚¬ìš©ë¥  30%"]
        T3["14:00-18:00<br/>ì‚¬ìš©ë¥  70%"]
        T4["18:00-09:00<br/>ì‚¬ìš©ë¥  10%"]
    end

    subgraph "ë¹„ìš© ë‚­ë¹„ ì˜ì—­"
        W1["ì ì‹¬ ì‹œê°„<br/>ìœ íœ´ GPU ë¹„ìš©"]
        W2["ì•¼ê°„/ì£¼ë§<br/>ìœ íœ´ GPU ë¹„ìš©"]
    end

    T2 --> W1
    T4 --> W2

    style W1 fill:#ff6b6b
    style W2 fill:#ff6b6b
```

**3. ë©€í‹° í…Œë„ŒíŠ¸ ë¹„ìš© ë¶„ë¦¬ì˜ ì–´ë ¤ì›€**

- íŒ€/í”„ë¡œì íŠ¸ë³„ GPU ì‚¬ìš©ëŸ‰ ì •í™•í•œ ì¸¡ì • í•„ìš”
- ê³µìœ  GPU ë…¸ë“œì—ì„œì˜ ë¹„ìš© í• ë‹¹ ë¡œì§ ë³µì¡
- ì‹¤ì‹œê°„ í• ë‹¹ëŸ‰(Quota) ê´€ë¦¬ ë° ì´ˆê³¼ ë°©ì§€

**4. ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ë¹„ìš© ê¸‰ì¦**

- íŠ¸ë˜í”½ ìŠ¤íŒŒì´í¬ ì‹œ ìë™ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì¸í•œ ë¹„ìš© ê¸‰ì¦
- Spot ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì‹œ On-Demand í´ë°±ìœ¼ë¡œ ë¹„ìš© ì¦ê°€
- ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ ì¼ì‹œì  ë¦¬ì†ŒìŠ¤ ì¤‘ë³µ ì‚¬ìš©

```mermaid
graph TB
    subgraph "AI Application"
        APP["Agent Application"]
        SDK["LangFuse SDK"]
    end

    subgraph "Observability Stack"
        LF["LangFuse"]
        OTEL["OpenTelemetry<br/>Collector"]
    end

    subgraph "Metrics & Cost"
        PROM["Prometheus"]
        GRAF["Grafana"]
        COST["Cost Dashboard"]
    end

    subgraph "Karpenter ë¹„ìš© ìµœì í™”"
        KARP["Karpenter"]
        SPOT["Spot ì¸ìŠ¤í„´ìŠ¤"]
        CONSOL["Consolidation"]
        BUDGET["Budget ì •ì±…"]
    end

    APP --> SDK
    SDK --> LF
    LF --> OTEL
    OTEL --> PROM
    PROM --> GRAF
    PROM --> COST
    KARP --> SPOT
    KARP --> CONSOL
    KARP --> BUDGET
    SPOT --> COST
    CONSOL --> COST

    style LF fill:#45b7d1
    style KARP fill:#ffd93d
```

#### Karpenter ê¸°ë°˜ ë¹„ìš© ìµœì í™” ì „ëµ (ê¶Œì¥)

KarpenterëŠ” GPU ì¸í”„ë¼ ë¹„ìš© ìµœì í™”ì˜ **í•µì‹¬ ë ˆë²„**ì…ë‹ˆë‹¤. ë‹¤ìŒ 4ê°€ì§€ ì „ëµì„ ì¡°í•©í•˜ì—¬ ìµœëŒ€ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì „ëµ 1: Spot ì¸ìŠ¤í„´ìŠ¤ ìš°ì„  í™œìš©**

Karpenterì˜ Spot ì¸ìŠ¤í„´ìŠ¤ ì§€ì›ì„ í™œìš©í•˜ë©´ GPU ë¹„ìš©ì„ **ìµœëŒ€ 90%ê¹Œì§€ ì ˆê°**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-spot-inference
spec:
  template:
    metadata:
      labels:
        cost-tier: spot
        workload: inference
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.12xlarge
            - g5.24xlarge
            - g5.48xlarge
            - p4d.24xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-spot-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: karpenter.sh/capacity-type
          value: "spot"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 32
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 30s
  weight: 50  # On-Demandë³´ë‹¤ ìš°ì„  ì„ íƒ
```

**ì „ëµ 2: ì‹œê°„ëŒ€ë³„ ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ë¹„ìš© ê´€ë¦¬**

ì—…ë¬´ ì‹œê°„ê³¼ ë¹„ì—…ë¬´ ì‹œê°„ì— ë”°ë¥¸ ì°¨ë³„í™”ëœ ë¦¬ì†ŒìŠ¤ ì •ì±…ì„ ì ìš©í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-scheduled-pool
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.12xlarge
            - g5.24xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodeclass
  limits:
    nvidia.com/gpu: 16
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
    budgets:
      # ì—…ë¬´ ì‹œê°„: ì•ˆì •ì„± ìš°ì„  (ë…¸ë“œ ì¤‘ë‹¨ ìµœì†Œí™”)
      - nodes: "10%"
        schedule: "0 9 * * 1-5"
        duration: 9h
      # ë¹„ì—…ë¬´ ì‹œê°„: ë¹„ìš© ìš°ì„  (ì ê·¹ì  í†µí•©)
      - nodes: "50%"
        schedule: "0 18 * * 1-5"
        duration: 15h
      # ì£¼ë§: ìµœì†Œ ë¦¬ì†ŒìŠ¤ ìœ ì§€
      - nodes: "80%"
        schedule: "0 0 * * 0,6"
        duration: 24h
```

**ì „ëµ 3: Consolidationì„ í†µí•œ ìœ íœ´ ë¦¬ì†ŒìŠ¤ ì œê±°**

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-consolidation-pool
spec:
  disruption:
    # ë…¸ë“œê°€ ë¹„ì–´ìˆê±°ë‚˜ í™œìš©ë„ê°€ ë‚®ì„ ë•Œ í†µí•©
    consolidationPolicy: WhenEmptyOrUnderutilized
    # ë¹ ë¥¸ í†µí•©ìœ¼ë¡œ ë¹„ìš© ì ˆê° (30ì´ˆ ëŒ€ê¸° í›„ í†µí•©)
    consolidateAfter: 30s
```

**ì „ëµ 4: ì›Œí¬ë¡œë“œë³„ ì¸ìŠ¤í„´ìŠ¤ ìµœì í™”**

```yaml
# ì†Œê·œëª¨ ëª¨ë¸ìš© (7B ì´í•˜) - ë¹„ìš© íš¨ìœ¨ì 
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-small-models
spec:
  template:
    spec:
      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.xlarge      # 1x A10G - $1.01/hr
            - g5.2xlarge     # 1x A10G - $1.21/hr
  weight: 100  # ìµœìš°ì„  ì„ íƒ

---
# ëŒ€ê·œëª¨ ëª¨ë¸ìš© (70B+) - ì„±ëŠ¥ ìš°ì„ 
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-large-models
spec:
  template:
    spec:
      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge   # 8x A100 - $32.77/hr
            - p5.48xlarge    # 8x H100 - $98.32/hr
  weight: 10   # í•„ìš”ì‹œì—ë§Œ ì„ íƒ
```

#### ë¹„ìš© ìµœì í™” ì „ëµ ë¹„êµ

| ì „ëµ | êµ¬í˜„ ë°©ë²• | ì˜ˆìƒ ì ˆê°ë¥  | ì ìš© ì›Œí¬ë¡œë“œ | ìœ„í—˜ë„ |
| --- | --- | --- | --- | --- |
| Spot ì¸ìŠ¤í„´ìŠ¤ | Karpenter NodePool | 60-90% | ì¶”ë¡ , ë°°ì¹˜ ì²˜ë¦¬ | ì¤‘ê°„ (ì¤‘ë‹¨ ê°€ëŠ¥) |
| Consolidation | Karpenter disruption | 20-30% | ëª¨ë“  ì›Œí¬ë¡œë“œ | ë‚®ìŒ |
| Right-sizing | Karpenter ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ | 15-25% | ëª¨ë“  ì›Œí¬ë¡œë“œ | ë‚®ìŒ |
| ìŠ¤ì¼€ì¤„ ê¸°ë°˜ | Karpenter budgets | 30-40% | ë¹„ì—…ë¬´ ì‹œê°„ | ë‚®ìŒ |
| ë³µí•© ì ìš© | ìœ„ ì „ëµ ì¡°í•© | 50-70% | ì „ì²´ | ì¤‘ê°„ |

#### ë³´ì¡° ì†”ë£¨ì…˜: LangFuse ê¸°ë°˜ í† í° ì¶”ì 

ì¸í”„ë¼ ë¹„ìš©ê³¼ í•¨ê»˜ í† í° ë ˆë²¨ ë¹„ìš©ë„ ì¶”ì í•´ì•¼ ì™„ì „í•œ ë¹„ìš© ê°€ì‹œì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langfuse
  namespace: observability
spec:
  replicas: 2
  selector:
    matchLabels:
      app: langfuse
  template:
    metadata:
      labels:
        app: langfuse
    spec:
      containers:
        - name: langfuse
          image: langfuse/langfuse:latest
          ports:
            - containerPort: 3000
          env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: database-url
            - name: NEXTAUTH_SECRET
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: nextauth-secret
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
```

#### ë¹„ìš© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„±

```yaml
# Prometheus ë¹„ìš© ê´€ë ¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê·œì¹™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: gpu-cost-rules
  namespace: monitoring
spec:
  groups:
    - name: gpu-cost
      rules:
        - record: gpu:hourly_cost:sum
          expr: |
            sum(
              karpenter_nodes_total_pod_requests{resource_type="nvidia.com/gpu"} 
              * on(instance_type) group_left() 
              aws_ec2_instance_hourly_cost
            )
        - alert: HighGPUCostAlert
          expr: gpu:hourly_cost:sum > 100
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "ì‹œê°„ë‹¹ GPU ë¹„ìš©ì´ $100ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤"
```

:::tip ë¹„ìš© ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸
1. **Spot ì¸ìŠ¤í„´ìŠ¤ ë¹„ìœ¨**: ì¶”ë¡  ì›Œí¬ë¡œë“œì˜ 70% ì´ìƒì„ Spotìœ¼ë¡œ ìš´ì˜
2. **Consolidation í™œì„±í™”**: 30ì´ˆ ì´ë‚´ ìœ íœ´ ë…¸ë“œ ì •ë¦¬
3. **ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ì •ì±…**: ë¹„ì—…ë¬´ ì‹œê°„ ë¦¬ì†ŒìŠ¤ 50% ì´ìƒ ì¶•ì†Œ
4. **Right-sizing**: ëª¨ë¸ í¬ê¸°ì— ë§ëŠ” ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìë™ ì„ íƒ
:::

:::warning ë¹„ìš© ìµœì í™” ì£¼ì˜ì‚¬í•­
- Spot ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì‹œ ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”ë¥¼ ìœ„í•œ graceful shutdown êµ¬í˜„ í•„ìˆ˜
- ê³¼ë„í•œ Consolidationì€ ìŠ¤ì¼€ì¼ ì•„ì›ƒ ì§€ì—°ì„ ìœ ë°œí•  ìˆ˜ ìˆìŒ
- ë¹„ìš© ì ˆê°ê³¼ SLA ì¤€ìˆ˜ ì‚¬ì´ì˜ ê· í˜•ì  ì„¤ì • í•„ìš”
:::

### ë„ì „ê³¼ì œ 4: FM íŒŒì¸íŠœë‹ê³¼ ìë™í™” íŒŒì´í”„ë¼ì¸

Foundation Modelì„ íŠ¹ì • ë„ë©”ì¸ì— ë§ê²Œ íŒŒì¸íŠœë‹í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ê²ƒì€ ë³µì¡í•œ ê³¼ì •ì…ë‹ˆë‹¤. íŠ¹íˆ **ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµ í™˜ê²½ì—ì„œì˜ GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**ê°€ í•µì‹¬ ê³¼ì œì…ë‹ˆë‹¤.

#### ê¸°ìˆ ì  ë¬¸ì œì  ìƒì„¸ ë¶„ì„

**1. ë¶„ì‚° í•™ìŠµ í™˜ê²½ì˜ ë³µì¡ì„±**

ëŒ€ê·œëª¨ LLM íŒŒì¸íŠœë‹ì€ ë‹¨ì¼ GPUë¡œëŠ” ë¶ˆê°€ëŠ¥í•˜ë©°, ë©€í‹° ë…¸ë“œ ë¶„ì‚° í•™ìŠµì´ í•„ìˆ˜ì…ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "ë¶„ì‚° í•™ìŠµ í† í´ë¡œì§€"
        direction LR
        subgraph "Node 1"
            N1G1["GPU 0-3"]
            N1G2["GPU 4-7"]
        end
        subgraph "Node 2"
            N2G1["GPU 0-3"]
            N2G2["GPU 4-7"]
        end
        subgraph "Node 3"
            N3G1["GPU 0-3"]
            N3G2["GPU 4-7"]
        end
        subgraph "Node 4"
            N4G1["GPU 0-3"]
            N4G2["GPU 4-7"]
        end
    end

    subgraph "í†µì‹  íŒ¨í„´"
        NCCL["NCCL All-Reduce"]
        EFA["EFA ë„¤íŠ¸ì›Œí¬"]
    end

    N1G1 <--> NCCL
    N2G1 <--> NCCL
    N3G1 <--> NCCL
    N4G1 <--> NCCL
    NCCL <--> EFA

    style EFA fill:#ff9900
```

| ë³‘ë ¬í™” ì „ëµ | ì„¤ëª… | ì ìš© ì‹œë‚˜ë¦¬ì˜¤ | ë³µì¡ë„ |
| --- | --- | --- | --- |
| Data Parallelism | ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ ê° GPUì—ì„œ ë™ì¼ ëª¨ë¸ í•™ìŠµ | ì‘ì€ ëª¨ë¸, ëŒ€ìš©ëŸ‰ ë°ì´í„° | ë‚®ìŒ |
| Tensor Parallelism | ëª¨ë¸ì˜ í…ì„œë¥¼ GPU ê°„ ë¶„í•  | ë‹¨ì¼ ë ˆì´ì–´ê°€ GPU ë©”ëª¨ë¦¬ ì´ˆê³¼ ì‹œ | ë†’ìŒ |
| Pipeline Parallelism | ëª¨ë¸ ë ˆì´ì–´ë¥¼ GPU ê°„ ë¶„í•  | ë§¤ìš° ê¹Šì€ ëª¨ë¸ | ì¤‘ê°„ |
| FSDP | ëª¨ë¸ íŒŒë¼ë¯¸í„°, ê·¸ë˜ë””ì–¸íŠ¸, ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¶„í•  | ëŒ€ê·œëª¨ ëª¨ë¸ íš¨ìœ¨ì  í•™ìŠµ | ì¤‘ê°„ |

**2. GPU ë¦¬ì†ŒìŠ¤ í”„ë¡œë¹„ì €ë‹ ì§€ì—°**

í•™ìŠµ ì‘ì—…ì€ ì¼ë°˜ì ìœ¼ë¡œ **ë°°ì¹˜ í˜•íƒœ**ë¡œ ì‹¤í–‰ë˜ë©°, ë¦¬ì†ŒìŠ¤ í™•ë³´ ì‹œê°„ì´ ì „ì²´ íŒŒì´í”„ë¼ì¸ íš¨ìœ¨ì„±ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤:

```mermaid
sequenceDiagram
    participant User as ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸
    participant Pipeline as ML Pipeline
    participant Scheduler as K8s Scheduler
    participant Karpenter as Karpenter
    participant AWS as AWS EC2

    User->>Pipeline: í•™ìŠµ Job ì œì¶œ
    Pipeline->>Scheduler: Pod ìƒì„± ìš”ì²­ (32 GPU)
    
    Note over Scheduler: ê¸°ì¡´ ë°©ì‹: Node Group ëŒ€ê¸°
    Scheduler->>Karpenter: Pending Pod ê°ì§€
    
    Note over Karpenter: ì›Œí¬ë¡œë“œ ë¶„ì„
    Karpenter->>Karpenter: ìµœì  ì¸ìŠ¤í„´ìŠ¤ ê³„ì‚°<br/>(4x p4d.24xlarge)
    
    Karpenter->>AWS: ë³‘ë ¬ ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œë¹„ì €ë‹
    AWS-->>Karpenter: ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ (2-3ë¶„)
    
    Karpenter-->>Scheduler: ë…¸ë“œ ë“±ë¡
    Scheduler-->>Pipeline: Pod ìŠ¤ì¼€ì¤„ë§ ì™„ë£Œ
    Pipeline-->>User: í•™ìŠµ ì‹œì‘
```

**3. í•™ìŠµ ì¤‘ ì¥ì•  ë³µêµ¬ì˜ ì–´ë ¤ì›€**

- ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë³µêµ¬ ì „ëµ í•„ìš”
- ë…¸ë“œ ì¥ì•  ì‹œ ì „ì²´ í•™ìŠµ ì¬ì‹œì‘ ë°©ì§€
- Spot ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© ì‹œ ì¤‘ë‹¨ ì²˜ë¦¬

**4. ë¦¬ì†ŒìŠ¤ í™œìš© íš¨ìœ¨ì„±**

- í•™ìŠµ ì™„ë£Œ í›„ GPU ë…¸ë“œ ìœ íœ´ ìƒíƒœ ì§€ì†
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„
- ì‹¤í—˜ê³¼ í”„ë¡œë•ì…˜ í•™ìŠµ ê°„ ë¦¬ì†ŒìŠ¤ ê²½í•©

```mermaid
graph LR
    subgraph "Data Pipeline"
        DATA["Training Data"]
        PREP["Data Preprocessing"]
    end

    subgraph "Karpenter ê´€ë¦¬ í•™ìŠµ í´ëŸ¬ìŠ¤í„°"
        KARP["Karpenter<br/>Training NodePool"]
        NEMO["NeMo Framework"]
        DIST["Distributed Training"]
    end

    subgraph "Model Registry"
        CKPT["Checkpoint Storage"]
        MLFLOW["MLflow Registry"]
    end

    subgraph "Deployment"
        SERVE["Model Serving"]
        CANARY["Canary Deployment"]
    end

    DATA --> PREP
    PREP --> NEMO
    KARP --> NEMO
    NEMO --> DIST
    DIST --> CKPT
    CKPT --> MLFLOW
    MLFLOW --> SERVE
    SERVE --> CANARY

    style KARP fill:#ffd93d
    style NEMO fill:#76b900
```

#### Karpenter ê¸°ë°˜ í•™ìŠµ ì¸í”„ë¼ êµ¬ì„± (ê¶Œì¥)

**ì „ëµ 1: í•™ìŠµ ì „ìš© NodePool ë¶„ë¦¬**

í•™ìŠµ ì›Œí¬ë¡œë“œëŠ” ì¶”ë¡ ê³¼ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§€ë¯€ë¡œ ë³„ë„ì˜ NodePoolë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-training-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-training
        workload: ml-training
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]  # í•™ìŠµì€ On-Demand ê¶Œì¥ (ì•ˆì •ì„±)
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p5.48xlarge     # 8x H100 80GB - ëŒ€ê·œëª¨ í•™ìŠµ
            - p4d.24xlarge    # 8x A100 40GB - ì¤‘ê·œëª¨ í•™ìŠµ
            - p4de.24xlarge   # 8x A100 80GB - ë©”ëª¨ë¦¬ ì§‘ì•½ì  í•™ìŠµ
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-training-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: workload-type
          value: "training"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 64
  disruption:
    # í•™ìŠµ ì¤‘ì—ëŠ” ë…¸ë“œ ì¤‘ë‹¨ ë°©ì§€
    consolidationPolicy: WhenEmpty
    consolidateAfter: 1h  # í•™ìŠµ ì™„ë£Œ í›„ 1ì‹œê°„ ëŒ€ê¸°
    budgets:
      # í•™ìŠµ ì¤‘ì—ëŠ” ë…¸ë“œ ì¤‘ë‹¨ ì™„ì „ ë°©ì§€
      - nodes: "0"
```

**ì „ëµ 2: EFA ë„¤íŠ¸ì›Œí¬ ìµœì í™” NodeClass**

ë¶„ì‚° í•™ìŠµì˜ ì„±ëŠ¥ì€ GPU ê°„ í†µì‹  ì†ë„ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. EFA(Elastic Fabric Adapter)ë¥¼ í™œìš©í•˜ì—¬ ìµœëŒ€ ì„±ëŠ¥ì„ í™•ë³´í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu-training-nodeclass
spec:
  role: KarpenterNodeRole-${CLUSTER_NAME}
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
        network-type: efa-enabled  # EFA ì§€ì› ì„œë¸Œë„·
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 1000Gi  # ëŒ€ìš©ëŸ‰ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        volumeType: gp3
        iops: 16000
        throughput: 1000
        encrypted: true
        deleteOnTermination: true
  instanceStorePolicy: RAID0  # NVMe ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ í™œìš©
  userData: |
    #!/bin/bash
    set -e
    
    # NVIDIA ë“œë¼ì´ë²„ ì„¤ì •
    nvidia-smi -pm 1
    nvidia-smi -ac 1593,1410  # H100 ìµœì  í´ëŸ­ ì„¤ì •
    
    # EFA ë“œë¼ì´ë²„ ë¡œë“œ
    modprobe efa
    
    # NCCL í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    echo 'export NCCL_DEBUG=INFO' >> /etc/profile.d/nccl.sh
    echo 'export NCCL_SOCKET_IFNAME=eth0' >> /etc/profile.d/nccl.sh
    echo 'export FI_EFA_USE_DEVICE_RDMA=1' >> /etc/profile.d/nccl.sh
    echo 'export FI_PROVIDER=efa' >> /etc/profile.d/nccl.sh
    
    # ëŒ€ìš©ëŸ‰ í˜ì´ì§€ ì„¤ì • (í•™ìŠµ ì„±ëŠ¥ í–¥ìƒ)
    echo 'vm.nr_hugepages=5120' >> /etc/sysctl.conf
    sysctl -p
  tags:
    Environment: production
    Workload: ml-training
    CostCenter: ml-platform
```

**ì „ëµ 3: ì‹¤í—˜ìš© Spot ê¸°ë°˜ NodePool**

í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ë‚˜ ì‹¤í—˜ì  í•™ìŠµì—ëŠ” Spot ì¸ìŠ¤í„´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ë¹„ìš©ì„ ì ˆê°í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-experiment-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-experiment
        workload: ml-experiment
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge
            - g5.48xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-experiment-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: workload-type
          value: "experiment"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 32
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 10m  # ì‹¤í—˜ ì™„ë£Œ í›„ ë¹ ë¥¸ ì •ë¦¬
  weight: 30  # í”„ë¡œë•ì…˜ í•™ìŠµë³´ë‹¤ ë‚®ì€ ìš°ì„ ìˆœìœ„
```

#### NeMo ë¶„ì‚° í•™ìŠµ Job ì˜ˆì œ

Karpenterê°€ í”„ë¡œë¹„ì €ë‹í•œ ë…¸ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” NeMo ë¶„ì‚° í•™ìŠµ Jobì…ë‹ˆë‹¤.

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: nemo-finetune-llama-70b
  namespace: ai-training
spec:
  parallelism: 4  # 4ê°œ ë…¸ë“œ ë³‘ë ¬ ì‹¤í–‰
  completions: 4
  completionMode: Indexed
  template:
    metadata:
      labels:
        app: nemo-training
        model: llama-70b
    spec:
      restartPolicy: OnFailure
      containers:
        - name: nemo
          image: nvcr.io/nvidia/nemo:24.01
          command:
            - /bin/bash
            - -c
            - |
              # ë¶„ì‚° í•™ìŠµ í™˜ê²½ ì„¤ì •
              export MASTER_ADDR=$(hostname -i)
              export MASTER_PORT=29500
              export WORLD_SIZE=32  # 4 nodes x 8 GPUs
              export RANK=$JOB_COMPLETION_INDEX
              
              python -m torch.distributed.launch \
                --nproc_per_node=8 \
                --nnodes=4 \
                --node_rank=$RANK \
                --master_addr=$MASTER_ADDR \
                --master_port=$MASTER_PORT \
                /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_finetuning.py \
                --config-path=/config \
                --config-name=llama_70b_finetune
          args:
            - model.data.train_ds.file_path=/data/train.jsonl
            - model.data.validation_ds.file_path=/data/val.jsonl
            - trainer.devices=8
            - trainer.num_nodes=4
            - trainer.max_epochs=3
            - trainer.precision=bf16-mixed
            - model.tensor_model_parallel_size=4
            - model.pipeline_model_parallel_size=2
            - exp_manager.checkpoint_callback_params.save_top_k=3
          resources:
            requests:
              nvidia.com/gpu: 8
              memory: "900Gi"
              cpu: "90"
            limits:
              nvidia.com/gpu: 8
              memory: "1100Gi"
              cpu: "96"
          volumeMounts:
            - name: training-data
              mountPath: /data
            - name: checkpoints
              mountPath: /checkpoints
            - name: config
              mountPath: /config
            - name: shm
              mountPath: /dev/shm
      nodeSelector:
        node-type: gpu-training
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - key: workload-type
          operator: Equal
          value: "training"
          effect: NoSchedule
      volumes:
        - name: training-data
          persistentVolumeClaim:
            claimName: training-data-pvc
        - name: checkpoints
          persistentVolumeClaim:
            claimName: checkpoints-pvc
        - name: config
          configMap:
            name: nemo-training-config
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 256Gi  # ëŒ€ìš©ëŸ‰ ê³µìœ  ë©”ëª¨ë¦¬
```

#### í•™ìŠµ íŒŒì´í”„ë¼ì¸ ìë™í™”

Kubeflow Pipelinesì™€ Karpenterë¥¼ ì—°ë™í•˜ì—¬ End-to-End í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ìë™í™”í•©ë‹ˆë‹¤.

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: llm-finetune-pipeline
  namespace: ai-training
spec:
  entrypoint: finetune-pipeline
  templates:
    - name: finetune-pipeline
      dag:
        tasks:
          - name: data-preparation
            template: prepare-data
          - name: training
            template: distributed-training
            dependencies: [data-preparation]
          - name: evaluation
            template: evaluate-model
            dependencies: [training]
          - name: deployment
            template: deploy-model
            dependencies: [evaluation]

    - name: distributed-training
      resource:
        action: create
        manifest: |
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: nemo-finetune-{{workflow.uid}}
          spec:
            # ... (ìœ„ì˜ Job ìŠ¤í™)
      # Karpenterê°€ ìë™ìœ¼ë¡œ í•„ìš”í•œ GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
```

#### í•™ìŠµ ì¸í”„ë¼ ë¹„ìš© ìµœì í™” ì „ëµ

| ì „ëµ | ì ìš© ëŒ€ìƒ | ì˜ˆìƒ ì ˆê°ë¥  | êµ¬í˜„ ë°©ë²• |
| --- | --- | --- | --- |
| Spot ì‹¤í—˜ í´ëŸ¬ìŠ¤í„° | í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | 60-80% | ë³„ë„ NodePool |
| ìë™ ë…¸ë“œ ì •ë¦¬ | í•™ìŠµ ì™„ë£Œ í›„ | 20-30% | Consolidation |
| ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì¬ì‹œì‘ | Spot ì¤‘ë‹¨ ëŒ€ì‘ | 10-20% | NeMo ì²´í¬í¬ì¸íŠ¸ |
| ì‹œê°„ëŒ€ë³„ ìŠ¤ì¼€ì¤„ë§ | ë¹„ì—…ë¬´ ì‹œê°„ í•™ìŠµ | 15-25% | CronJob + Karpenter |

:::tip í•™ìŠµ ì¸í”„ë¼ ëª¨ë²” ì‚¬ë¡€
1. **í”„ë¡œë•ì…˜ í•™ìŠµ**: On-Demand ì¸ìŠ¤í„´ìŠ¤ë¡œ ì•ˆì •ì„± í™•ë³´
2. **ì‹¤í—˜/íŠœë‹**: Spot ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¹„ìš© ì ˆê°
3. **ì²´í¬í¬ì¸íŠ¸**: FSx for Lustreì— ì£¼ê¸°ì  ì €ì¥
4. **ëª¨ë‹ˆí„°ë§**: TensorBoard + Prometheusë¡œ í•™ìŠµ ì§„í–‰ ì¶”ì 
:::

:::warning ë¶„ì‚° í•™ìŠµ ì£¼ì˜ì‚¬í•­
- EFA ë„¤íŠ¸ì›Œí¬ê°€ ì§€ì›ë˜ëŠ” ì„œë¸Œë„·ì—ì„œë§Œ ìµœì  ì„±ëŠ¥ ë°œíœ˜
- NCCL í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì´ ì„±ëŠ¥ì— í° ì˜í–¥
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°ì™€ ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ê°„ ê· í˜• í•„ìš”
:::

## Amazon EKSì™€ Karpenterì˜ ì‹œë„ˆì§€

Amazon EKSëŠ” Karpenterì™€ í•¨ê»˜ ì‚¬ìš©í•  ë•Œ ìµœëŒ€ì˜ íš¨ê³¼ë¥¼ ë°œíœ˜í•©ë‹ˆë‹¤.

### EKS + Karpenter ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "AWS ê´€ë¦¬ ì˜ì—­"
        CP["EKS Control Plane<br/>etcd, API Server, Scheduler"]
        UP["ìë™ ì—…ê·¸ë ˆì´ë“œ"]
        HA["ê³ ê°€ìš©ì„± (Multi-AZ)"]
    end

    subgraph "Karpenter ê´€ë¦¬ ì˜ì—­"
        KARP["Karpenter Controller"]
        NP1["GPU Inference NodePool"]
        NP2["GPU Training NodePool"]
        NP3["Spot NodePool"]
    end

    subgraph "AI Workloads"
        INF["ì¶”ë¡  ì„œë¹„ìŠ¤"]
        TRAIN["í•™ìŠµ ì‘ì—…"]
        BATCH["ë°°ì¹˜ ì²˜ë¦¬"]
    end

    CP --> KARP
    KARP --> NP1
    KARP --> NP2
    KARP --> NP3
    NP1 --> INF
    NP2 --> TRAIN
    NP3 --> BATCH

    style CP fill:#ff9900
    style KARP fill:#ffd93d
```

### EKS Auto Modeì™€ Karpenter

EKS Auto Modeë¥¼ ì‚¬ìš©í•˜ë©´ Karpenterê°€ ìë™ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìš´ì˜ ë¶€ë‹´ì´ í¬ê²Œ ì¤„ì–´ë“­ë‹ˆë‹¤.

| ê¸°ëŠ¥ | EKS Standard + Karpenter | EKS Auto Mode |
| --- | --- | --- |
| Karpenter ì„¤ì¹˜ | ìˆ˜ë™ ì„¤ì¹˜ í•„ìš” | ìë™ êµ¬ì„± |
| NodePool ê´€ë¦¬ | ì§ì ‘ ì •ì˜ | ê¸°ë³¸ ì œê³µ + ì»¤ìŠ¤í…€ |
| ì—…ê·¸ë ˆì´ë“œ | ìˆ˜ë™ ê´€ë¦¬ | ìë™ ì—…ê·¸ë ˆì´ë“œ |
| ëª¨ë‹ˆí„°ë§ | ë³„ë„ êµ¬ì„± | í†µí•© ì œê³µ |

### AWS ì„œë¹„ìŠ¤ í†µí•©

| AWS ì„œë¹„ìŠ¤ | ìš©ë„ | Karpenter ì—°ë™ |
| --- | --- | --- |
| Amazon S3 | ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ | CSI Driver, IRSA |
| FSx for Lustre | ê³ ì„±ëŠ¥ í•™ìŠµ ë°ì´í„° | CSI Driver |
| CloudWatch | ë©”íŠ¸ë¦­, ë¡œê·¸ | Container Insights |
| EC2 Spot | ë¹„ìš© ìµœì í™” | Karpenter capacity-type |

## Karpenter ë„ì… íš¨ê³¼ ìš”ì•½

### ì •ëŸ‰ì  íš¨ê³¼

| ì§€í‘œ | ê¸°ì¡´ ë°©ì‹ | Karpenter ë„ì… í›„ | ê°œì„ ìœ¨ |
| --- | --- | --- | --- |
| GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì‹œê°„ | 5-10ë¶„ | 2-3ë¶„ | 50-70% ë‹¨ì¶• |
| GPU ë¦¬ì†ŒìŠ¤ í™œìš©ë¥  | 40-50% | 70-80% | 40-60% í–¥ìƒ |
| ì›”ê°„ GPU ë¹„ìš© | ê¸°ì¤€ | Spot í™œìš© ì‹œ | 60-90% ì ˆê° |
| ìœ íœ´ ë…¸ë“œ ë¹„ìš© | ë°œìƒ | Consolidation | 20-30% ì ˆê° |

### ì •ì„±ì  íš¨ê³¼

- **ìš´ì˜ ë³µì¡ì„± ê°ì†Œ**: Node Group ê´€ë¦¬ ë¶ˆí•„ìš”
- **ìë™í™” ìˆ˜ì¤€ í–¥ìƒ**: ì›Œí¬ë¡œë“œ ê¸°ë°˜ ìë™ í”„ë¡œë¹„ì €ë‹
- **ë¹„ìš© ê°€ì‹œì„± ê°œì„ **: ì›Œí¬ë¡œë“œë³„ ë¹„ìš© ì¶”ì  ìš©ì´
- **í™•ì¥ì„± í™•ë³´**: íŠ¸ë˜í”½ ê¸‰ì¦ì— ì¦‰ê° ëŒ€ì‘

## ê²°ë¡ 

Agentic AI Platform êµ¬ì¶•ì˜ 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œëŠ” **Karpenterë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ EKS ê¸°ë°˜ ì•„í‚¤í…ì²˜**ë¡œ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë„ì „ê³¼ì œë³„ Karpenter í•´ê²° ë°©ì•ˆ ìš”ì•½

| ë„ì „ê³¼ì œ | í•µì‹¬ ë¬¸ì œ | Karpenter í•´ê²° ë°©ì•ˆ | ê¸°ëŒ€ íš¨ê³¼ |
| --- | --- | --- | --- |
| GPU ëª¨ë‹ˆí„°ë§ | ë©€í‹° í´ëŸ¬ìŠ¤í„° ê°€ì‹œì„± ë¶€ì¬ | NodePool ê¸°ë°˜ í†µí•© ê´€ë¦¬ | ë¦¬ì†ŒìŠ¤ í™œìš©ë¥  40% í–¥ìƒ |
| ë™ì  ë¼ìš°íŒ…/ìŠ¤ì¼€ì¼ë§ | íŠ¸ë˜í”½ ê¸‰ì¦ ëŒ€ì‘ ì§€ì—° | Just-in-Time í”„ë¡œë¹„ì €ë‹ | í”„ë¡œë¹„ì €ë‹ ì‹œê°„ 50% ë‹¨ì¶• |
| ë¹„ìš© ì»¨íŠ¸ë¡¤ | GPU ìœ íœ´ ë¹„ìš© | Spot + Consolidation | ë¹„ìš© 50-70% ì ˆê° |
| FM íŒŒì¸íŠœë‹ | ë¶„ì‚° í•™ìŠµ ì¸í”„ë¼ ë³µì¡ì„± | í•™ìŠµ ì „ìš© NodePool | í•™ìŠµ íš¨ìœ¨ì„± 30% í–¥ìƒ |

### í•µì‹¬ ê¶Œì¥ì‚¬í•­

1. **Karpenter ìš°ì„  ë„ì…**: GPU ë…¸ë“œ ê´€ë¦¬ì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¡œ Karpenter í™œìš©
2. **ì›Œí¬ë¡œë“œë³„ NodePool ë¶„ë¦¬**: ì¶”ë¡ /í•™ìŠµ/ì‹¤í—˜ ì›Œí¬ë¡œë“œë³„ ìµœì í™”ëœ NodePool êµ¬ì„±
3. **Spot ì¸ìŠ¤í„´ìŠ¤ ì ê·¹ í™œìš©**: ì¶”ë¡  ì›Œí¬ë¡œë“œì— Spot ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¹„ìš© ìµœì í™”
4. **KEDA ì—°ë™**: Karpenterì™€ KEDAë¥¼ ì—°ë™í•˜ì—¬ End-to-End ìë™ ìŠ¤ì¼€ì¼ë§ êµ¬í˜„
5. **Consolidation í™œì„±í™”**: ìœ íœ´ ë¦¬ì†ŒìŠ¤ ìë™ ì •ë¦¬ë¡œ ë¹„ìš© íš¨ìœ¨ì„± ê·¹ëŒ€í™”
6. **ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ì •ì±…**: ì—…ë¬´/ë¹„ì—…ë¬´ ì‹œê°„ì— ë”°ë¥¸ ì°¨ë³„í™”ëœ ë¦¬ì†ŒìŠ¤ ì •ì±… ì ìš©

### êµ¬í˜„ ë¡œë“œë§µ

```mermaid
gantt
    title Karpenter ê¸°ë°˜ Agentic AI ì¸í”„ë¼ êµ¬ì¶• ë¡œë“œë§µ
    dateFormat  YYYY-MM-DD
    section Phase 1: ê¸°ë°˜ êµ¬ì¶•
    Karpenter ì„¤ì¹˜ ë° ê¸°ë³¸ NodePool    :a1, 2025-01-01, 2w
    DCGM Exporter ì—°ë™                 :a2, after a1, 1w
    section Phase 2: ì¶”ë¡  ìµœì í™”
    ì¶”ë¡ ìš© NodePool êµ¬ì„±               :b1, after a2, 2w
    Spot ì¸ìŠ¤í„´ìŠ¤ í†µí•©                 :b2, after b1, 1w
    KEDA ì—°ë™                          :b3, after b2, 2w
    section Phase 3: í•™ìŠµ ì¸í”„ë¼
    í•™ìŠµìš© NodePool êµ¬ì„±               :c1, after b3, 2w
    EFA ë„¤íŠ¸ì›Œí¬ ìµœì í™”                :c2, after c1, 1w
    section Phase 4: ë¹„ìš© ìµœì í™”
    Consolidation ì •ì±… íŠœë‹            :d1, after c2, 2w
    ë¹„ìš© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ             :d2, after d1, 1w
```

:::info ë‹¤ìŒ ë‹¨ê³„
ì´ ë¬¸ì„œì—ì„œ ì†Œê°œí•œ ê° ë„ì „ê³¼ì œì— ëŒ€í•œ ìƒì„¸í•œ êµ¬í˜„ ê°€ì´ë“œëŠ” ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ì¡°í•˜ì„¸ìš”:

- [GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬](./gpu-resource-management.md) - Karpenter ê¸°ë°˜ GPU í´ëŸ¬ìŠ¤í„° ë™ì  ë¦¬ì†ŒìŠ¤ í• ë‹¹
- [Inference Gateway](./inference-gateway-routing.md) - Kgateway ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…
- [Agent ëª¨ë‹ˆí„°ë§](./agent-monitoring.md) - LangFuse, LangSmith í†µí•©
- [NeMo í”„ë ˆì„ì›Œí¬](./nemo-framework.md) - FM íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸

:::

## ì°¸ê³  ìë£Œ

- [Karpenter ê³µì‹ ë¬¸ì„œ](https://karpenter.sh/docs/)
- [Amazon EKS Best Practices Guide](https://aws.github.io/aws-eks-best-practices/)
- [NVIDIA GPU Operator Documentation](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html)
- [KEDA - Kubernetes Event-driven Autoscaling](https://keda.sh/)
- [LangFuse Documentation](https://langfuse.com/docs)
- [NVIDIA NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)
