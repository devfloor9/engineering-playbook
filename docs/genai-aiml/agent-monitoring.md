---
title: "AI Agent ëª¨ë‹ˆí„°ë§ (LangFuse & LangSmith)"
sidebar_label: "Agent ëª¨ë‹ˆí„°ë§"
description: "LangFuse, LangSmithë¥¼ í™œìš©í•œ Agentic AI ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë‹ˆí„°ë§ ë° ì¶”ì "
tags: [eks, langfuse, langsmith, monitoring, observability, tracing, opentelemetry]
category: "genai-aiml"
date: 2025-02-05
authors: [devfloor9]
sidebar_position: 8
---

# AI Agent ëª¨ë‹ˆí„°ë§ (LangFuse & LangSmith)

ì´ ë¬¸ì„œì—ì„œëŠ” LangFuseì™€ LangSmithë¥¼ í™œìš©í•˜ì—¬ Agentic AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ê³¼ ë™ì‘ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì í•˜ê³  ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. Kubernetes í™˜ê²½ì—ì„œì˜ ë°°í¬ë¶€í„° Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±ê¹Œì§€ ì‹¤ë¬´ì— í•„ìš”í•œ ì „ì²´ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ê°œìš”

Agentic AI ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ë³µì¡í•œ ì¶”ë¡  ì²´ì¸ê³¼ ë‹¤ì–‘í•œ ë„êµ¬ í˜¸ì¶œì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì—, ì „í†µì ì¸ APM(Application Performance Monitoring) ë„êµ¬ë§Œìœ¼ë¡œëŠ” ì¶©ë¶„í•œ ê°€ì‹œì„±ì„ í™•ë³´í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. LLM íŠ¹í™” ê´€ì¸¡ì„± ë„êµ¬ì¸ LangFuseì™€ LangSmithëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- **íŠ¸ë ˆì´ìŠ¤ ì¶”ì **: LLM í˜¸ì¶œ, ë„êµ¬ ì‹¤í–‰, ì—ì´ì „íŠ¸ ì¶”ë¡  ê³¼ì •ì˜ ì „ì²´ íë¦„ ì¶”ì 
- **í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„**: ì…ë ¥/ì¶œë ¥ í† í° ìˆ˜ ë° ë¹„ìš© ê³„ì‚°
- **í’ˆì§ˆ í‰ê°€**: ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜í™” ë° í”¼ë“œë°± ìˆ˜ì§‘
- **ë””ë²„ê¹…**: í”„ë¡¬í”„íŠ¸ ë° ì‘ë‹µ ë‚´ìš© ê²€í† ë¥¼ í†µí•œ ë¬¸ì œ ì§„ë‹¨

:::info ëŒ€ìƒ ë…ì
ì´ ë¬¸ì„œëŠ” í”Œë«í¼ ìš´ì˜ì, MLOps ì—”ì§€ë‹ˆì–´, AI ê°œë°œìë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤. Kubernetesì™€ Pythonì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤.
:::

## LangFuse vs LangSmith ë¹„êµ

| íŠ¹ì„± | LangFuse | LangSmith |
| ---- | -------- | --------- |
| **ë¼ì´ì„ ìŠ¤** | ì˜¤í”ˆì†ŒìŠ¤ (MIT) | ìƒìš© (ë¬´ë£Œ í‹°ì–´ ì œê³µ) |
| **ë°°í¬ ë°©ì‹** | Self-hosted / Cloud | Cloud only |
| **ë°ì´í„° ì£¼ê¶Œ** | ì™„ì „í•œ ì œì–´ | LangChain ì„œë²„ |
| **í†µí•©** | ë‹¤ì–‘í•œ í”„ë ˆì„ì›Œí¬ | LangChain ìµœì í™” |
| **ë¹„ìš©** | ì¸í”„ë¼ ë¹„ìš©ë§Œ | ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê³¼ê¸ˆ |
| **í™•ì¥ì„±** | Kubernetes ë„¤ì´í‹°ë¸Œ | ê´€ë¦¬í˜• |

:::tip ì„ íƒ ê°€ì´ë“œ
- **LangFuse**: ë°ì´í„° ì£¼ê¶Œì´ ì¤‘ìš”í•˜ê±°ë‚˜, ë¹„ìš© ìµœì í™”ê°€ í•„ìš”í•œ ê²½ìš°
- **LangSmith**: LangChain ê¸°ë°˜ ê°œë°œì´ ì£¼ë ¥ì´ê³ , ë¹ ë¥¸ ì‹œì‘ì´ í•„ìš”í•œ ê²½ìš°
:::


## LangFuse Kubernetes ë°°í¬

### ì•„í‚¤í…ì²˜ ê°œìš”

LangFuseëŠ” ë‹¤ìŒ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph LangFuseStack["LangFuse Stack"]
        WEB["LangFuse Web<br/>(Next.js)"]
        API["LangFuse API<br/>(tRPC)"]
        WORKER["Background Worker<br/>(Queue Processing)"]
    end
    
    subgraph Storage["Storage Layer"]
        PG["PostgreSQL<br/>(Metadata)"]
        REDIS["Redis<br/>(Cache/Queue)"]
        S3["S3<br/>(Blob Storage)"]
    end
    
    subgraph Clients["AI Applications"]
        AGENT1["Agent 1"]
        AGENT2["Agent 2"]
        AGENTN["Agent N"]
    end
    
    AGENT1 --> API
    AGENT2 --> API
    AGENTN --> API
    WEB --> API
    API --> PG
    API --> REDIS
    WORKER --> PG
    WORKER --> REDIS
    WORKER --> S3
    
    style LangFuseStack fill:#e8f5e9
    style Storage fill:#e3f2fd
    style Clients fill:#fff3e0
```

### PostgreSQL ë°°í¬

LangFuseì˜ ë©”íƒ€ë°ì´í„° ì €ì¥ì„ ìœ„í•œ PostgreSQLì„ ë°°í¬í•©ë‹ˆë‹¤.

```yaml
# langfuse-postgres.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: observability
  labels:
    app.kubernetes.io/part-of: langfuse
---
apiVersion: v1
kind: Secret
metadata:
  name: langfuse-postgres-secret
  namespace: observability
type: Opaque
stringData:
  POSTGRES_USER: langfuse
  POSTGRES_PASSWORD: "your-secure-password-here"  # í”„ë¡œë•ì…˜ì—ì„œëŠ” Secrets Manager ì‚¬ìš©
  POSTGRES_DB: langfuse
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: langfuse-postgres-pvc
  namespace: observability
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: gp3
  resources:
    requests:
      storage: 100Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: langfuse-postgres
  namespace: observability
spec:
  serviceName: langfuse-postgres
  replicas: 1
  selector:
    matchLabels:
      app: langfuse-postgres
  template:
    metadata:
      labels:
        app: langfuse-postgres
    spec:
      containers:
        - name: postgres
          image: postgres:15-alpine
          ports:
            - containerPort: 5432
          envFrom:
            - secretRef:
                name: langfuse-postgres-secret
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
          livenessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - langfuse
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - langfuse
            initialDelaySeconds: 5
            periodSeconds: 5
      volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: langfuse-postgres-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: langfuse-postgres
  namespace: observability
spec:
  selector:
    app: langfuse-postgres
  ports:
    - port: 5432
      targetPort: 5432
  clusterIP: None
```


### LangFuse Deployment

LangFuse ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°°í¬í•©ë‹ˆë‹¤.

```yaml
# langfuse-deployment.yaml
apiVersion: v1
kind: Secret
metadata:
  name: langfuse-secret
  namespace: observability
type: Opaque
stringData:
  # í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜
  DATABASE_URL: "postgresql://langfuse:your-secure-password-here@langfuse-postgres:5432/langfuse"
  NEXTAUTH_SECRET: "your-nextauth-secret-32-chars-min"  # openssl rand -base64 32
  SALT: "your-salt-value-here"  # openssl rand -base64 32
  ENCRYPTION_KEY: "0000000000000000000000000000000000000000000000000000000000000000"  # 64 hex chars
  
  # ì„ íƒì  í™˜ê²½ ë³€ìˆ˜
  NEXTAUTH_URL: "https://langfuse.your-domain.com"
  LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: "true"
  
  # S3 ì„¤ì • (ì„ íƒì )
  S3_ENDPOINT: "https://s3.ap-northeast-2.amazonaws.com"
  S3_ACCESS_KEY_ID: "your-access-key"
  S3_SECRET_ACCESS_KEY: "your-secret-key"
  S3_BUCKET_NAME: "langfuse-traces"
  S3_REGION: "ap-northeast-2"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langfuse
  namespace: observability
  labels:
    app: langfuse
spec:
  replicas: 2
  selector:
    matchLabels:
      app: langfuse
  template:
    metadata:
      labels:
        app: langfuse
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "3000"
        prometheus.io/path: "/api/public/metrics"
    spec:
      containers:
        - name: langfuse
          image: langfuse/langfuse:2
          ports:
            - containerPort: 3000
              name: http
          envFrom:
            - secretRef:
                name: langfuse-secret
          env:
            - name: NODE_ENV
              value: "production"
            - name: PORT
              value: "3000"
            - name: HOSTNAME
              value: "0.0.0.0"
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /api/public/health
              port: 3000
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /api/public/health
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: langfuse
                topologyKey: kubernetes.io/hostname
---
apiVersion: v1
kind: Service
metadata:
  name: langfuse
  namespace: observability
spec:
  selector:
    app: langfuse
  ports:
    - port: 80
      targetPort: 3000
      name: http
  type: ClusterIP
```


### Ingress ì„¤ì •

ì™¸ë¶€ ì ‘ê·¼ì„ ìœ„í•œ Ingressë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

```yaml
# langfuse-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: langfuse-ingress
  namespace: observability
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:XXXXXXXXXXXX:certificate/xxx
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
    alb.ingress.kubernetes.io/ssl-redirect: "443"
    alb.ingress.kubernetes.io/healthcheck-path: /api/public/health
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: "15"
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: "5"
    alb.ingress.kubernetes.io/healthy-threshold-count: "2"
    alb.ingress.kubernetes.io/unhealthy-threshold-count: "2"
spec:
  ingressClassName: alb
  rules:
    - host: langfuse.your-domain.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: langfuse
                port:
                  number: 80
```

### HPA ì„¤ì •

íŠ¸ë˜í”½ì— ë”°ë¥¸ ìë™ ìŠ¤ì¼€ì¼ë§ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

```yaml
# langfuse-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langfuse-hpa
  namespace: observability
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langfuse
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 4
          periodSeconds: 15
      selectPolicy: Max
```

:::warning í”„ë¡œë•ì…˜ ë°°í¬ ì‹œ ì£¼ì˜ì‚¬í•­
- `NEXTAUTH_SECRET`, `SALT`, `ENCRYPTION_KEY`ëŠ” ë°˜ë“œì‹œ ì•ˆì „í•œ ëœë¤ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”
- í”„ë¡œë•ì…˜ì—ì„œëŠ” AWS Secrets Manager ë˜ëŠ” HashiCorp Vaultë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œí¬ë¦¿ì„ ê´€ë¦¬í•˜ì„¸ìš”
- PostgreSQLì€ RDSë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤ (ê³ ê°€ìš©ì„±, ìë™ ë°±ì—…)
:::


## LangSmith í†µí•©

LangSmithëŠ” LangChainì—ì„œ ì œê³µí•˜ëŠ” ê´€ë¦¬í˜• ê´€ì¸¡ì„± í”Œë«í¼ì…ë‹ˆë‹¤. Self-hosted ì˜µì…˜ì´ ì—†ì§€ë§Œ, LangChain ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ì˜ í†µí•©ì´ ë§¤ìš° ê°„í¸í•©ë‹ˆë‹¤.

### í™˜ê²½ ì„¤ì •

LangSmithë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
# langsmith-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: langsmith-config
  namespace: ai-agents
type: Opaque
stringData:
  LANGCHAIN_TRACING_V2: "true"
  LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
  LANGCHAIN_API_KEY: "ls__your-api-key-here"
  LANGCHAIN_PROJECT: "agentic-ai-production"
```

### LangChain ì—ì´ì „íŠ¸ ì—°ë™

LangSmithì™€ LangChain ì—ì´ì „íŠ¸ë¥¼ ì—°ë™í•˜ëŠ” Python ì½”ë“œ ì˜ˆì œì…ë‹ˆë‹¤.

```python
# agent_with_langsmith.py
import os
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import tool
from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (Kubernetes Secretì—ì„œ ì£¼ì…)
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
# LANGCHAIN_API_KEY=ls__xxx
# LANGCHAIN_PROJECT=agentic-ai-production

# ì»¤ìŠ¤í…€ ë„êµ¬ ì •ì˜
@tool
def search_knowledge_base(query: str) -> str:
    """ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # Milvus ê²€ìƒ‰ ë¡œì§
    return f"ê²€ìƒ‰ ê²°ê³¼: {query}ì— ëŒ€í•œ ì •ë³´..."

@tool
def create_support_ticket(title: str, description: str, priority: str = "medium") -> str:
    """ê³ ê° ì§€ì› í‹°ì¼“ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # í‹°ì¼“ ìƒì„± ë¡œì§
    return f"í‹°ì¼“ ìƒì„± ì™„ë£Œ: {title} (ìš°ì„ ìˆœìœ„: {priority})"

# ì—ì´ì „íŠ¸ ì„¤ì •
llm = ChatOpenAI(
    model="gpt-4-turbo",
    temperature=0.7,
    max_tokens=4096,
)

prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ê³ ê° ì§€ì› ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
    í•­ìƒ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ì¸ì •í•˜ì„¸ìš”.
    í•„ìš”í•œ ê²½ìš° ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê±°ë‚˜ í‹°ì¼“ì„ ìƒì„±í•˜ì„¸ìš”."""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

tools = [search_knowledge_base, create_support_ticket]
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    max_iterations=10,
    return_intermediate_steps=True,
)

# íŠ¸ë ˆì´ìŠ¤ ê°€ëŠ¥í•œ í•¨ìˆ˜ë¡œ ë˜í•‘
@traceable(
    name="customer_support_agent",
    run_type="chain",
    tags=["production", "customer-support"],
)
def run_agent(user_input: str, chat_history: list = None, metadata: dict = None):
    """ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  LangSmithì— íŠ¸ë ˆì´ìŠ¤ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
    if chat_history is None:
        chat_history = []
    
    # í˜„ì¬ ì‹¤í–‰ íŠ¸ë¦¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
    run_tree = get_current_run_tree()
    if run_tree and metadata:
        run_tree.extra["metadata"] = metadata
    
    result = agent_executor.invoke({
        "input": user_input,
        "chat_history": chat_history,
    })
    
    return result

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    response = run_agent(
        user_input="ì£¼ë¬¸ #12345ì˜ ë°°ì†¡ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
        metadata={
            "user_id": "user_123",
            "session_id": "session_456",
            "tenant_id": "tenant_abc",
        }
    )
    print(response)
```


### LangFuse Python í†µí•©

LangFuseë¥¼ Python ì• í”Œë¦¬ì¼€ì´ì…˜ì— í†µí•©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```python
# agent_with_langfuse.py
import os
from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context
from langfuse.openai import openai  # OpenAI ë˜í¼
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.callbacks import LangfuseCallbackHandler

# LangFuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
langfuse = Langfuse(
    public_key=os.environ.get("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.environ.get("LANGFUSE_SECRET_KEY"),
    host=os.environ.get("LANGFUSE_HOST", "https://langfuse.your-domain.com"),
)

# LangChain ì½œë°± í•¸ë“¤ëŸ¬
langfuse_handler = LangfuseCallbackHandler(
    public_key=os.environ.get("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.environ.get("LANGFUSE_SECRET_KEY"),
    host=os.environ.get("LANGFUSE_HOST"),
)

# ì—ì´ì „íŠ¸ ì„¤ì •
llm = ChatOpenAI(
    model="gpt-4-turbo",
    temperature=0.7,
    callbacks=[langfuse_handler],
)

@observe(name="customer_support_agent")
def run_agent_with_langfuse(
    user_input: str,
    user_id: str = None,
    session_id: str = None,
    tenant_id: str = None,
):
    """LangFuse íŠ¸ë ˆì´ì‹±ì´ ì ìš©ëœ ì—ì´ì „íŠ¸ ì‹¤í–‰"""
    
    # íŠ¸ë ˆì´ìŠ¤ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
    langfuse_context.update_current_trace(
        user_id=user_id,
        session_id=session_id,
        metadata={
            "tenant_id": tenant_id,
            "environment": os.environ.get("ENVIRONMENT", "production"),
        },
        tags=["customer-support", "production"],
    )
    
    # ì—ì´ì „íŠ¸ ì‹¤í–‰
    result = agent_executor.invoke(
        {"input": user_input, "chat_history": []},
        config={"callbacks": [langfuse_handler]},
    )
    
    # ì¶œë ¥ í† í° ë° ë¹„ìš© ê¸°ë¡
    langfuse_context.update_current_observation(
        output=result["output"],
        metadata={
            "intermediate_steps": len(result.get("intermediate_steps", [])),
        },
    )
    
    return result

@observe(name="vector_search", as_type="span")
def search_with_tracing(query: str, collection: str, top_k: int = 5):
    """ë²¡í„° ê²€ìƒ‰ì„ íŠ¸ë ˆì´ì‹±ê³¼ í•¨ê»˜ ìˆ˜í–‰"""
    from pymilvus import Collection
    
    langfuse_context.update_current_observation(
        input={"query": query, "collection": collection, "top_k": top_k},
    )
    
    # Milvus ê²€ìƒ‰ ìˆ˜í–‰
    collection = Collection(collection)
    results = collection.search(
        data=[get_embedding(query)],
        anns_field="embedding",
        param={"metric_type": "COSINE", "params": {"ef": 64}},
        limit=top_k,
        output_fields=["content", "metadata"],
    )
    
    langfuse_context.update_current_observation(
        output={"num_results": len(results[0])},
    )
    
    return results

# ì ìˆ˜ ë° í”¼ë“œë°± ê¸°ë¡
def record_feedback(trace_id: str, score: float, comment: str = None):
    """ì‚¬ìš©ì í”¼ë“œë°±ì„ LangFuseì— ê¸°ë¡"""
    langfuse.score(
        trace_id=trace_id,
        name="user_feedback",
        value=score,
        comment=comment,
    )

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    response = run_agent_with_langfuse(
        user_input="ì œí’ˆ ë°˜í’ˆ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
        user_id="user_123",
        session_id="session_456",
        tenant_id="tenant_abc",
    )
    
    # í”¼ë“œë°± ê¸°ë¡ (ì˜ˆ: ì‚¬ìš©ìê°€ ì‘ë‹µì— ë§Œì¡±)
    trace_id = langfuse_context.get_current_trace_id()
    record_feedback(trace_id, score=1.0, comment="ì •í™•í•œ ë‹µë³€")
    
    # í”ŒëŸ¬ì‹œí•˜ì—¬ ëª¨ë“  ì´ë²¤íŠ¸ ì „ì†¡
    langfuse.flush()
```


## í•µì‹¬ ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

Agentic AI ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì¶”ì í•´ì•¼ í•  í•µì‹¬ ë©”íŠ¸ë¦­ì„ ì •ì˜í•©ë‹ˆë‹¤.

### ë©”íŠ¸ë¦­ ì¹´í…Œê³ ë¦¬

```mermaid
graph TB
    subgraph Metrics["í•µì‹¬ ë©”íŠ¸ë¦­"]
        subgraph Latency["â±ï¸ Latency"]
            E2E["End-to-End ì§€ì—°"]
            LLM_LAT["LLM ì¶”ë¡  ì‹œê°„"]
            TOOL_LAT["ë„êµ¬ ì‹¤í–‰ ì‹œê°„"]
            RETRIEVAL_LAT["ê²€ìƒ‰ ì§€ì—°"]
        end
        
        subgraph Tokens["ğŸ”¢ Token Usage"]
            INPUT_TOK["ì…ë ¥ í† í°"]
            OUTPUT_TOK["ì¶œë ¥ í† í°"]
            TOTAL_TOK["ì´ í† í°"]
            COST["ë¹„ìš©"]
        end
        
        subgraph Errors["âŒ Error Rate"]
            LLM_ERR["LLM ì˜¤ë¥˜"]
            TOOL_ERR["ë„êµ¬ ì˜¤ë¥˜"]
            TIMEOUT["íƒ€ì„ì•„ì›ƒ"]
            RATE_LIMIT["Rate Limit"]
        end
        
        subgraph Traces["ğŸ” Trace Analysis"]
            CHAIN_LEN["ì²´ì¸ ê¸¸ì´"]
            TOOL_CALLS["ë„êµ¬ í˜¸ì¶œ ìˆ˜"]
            ITERATIONS["ë°˜ë³µ íšŸìˆ˜"]
            SUCCESS["ì„±ê³µë¥ "]
        end
    end
    
    style Latency fill:#e3f2fd
    style Tokens fill:#e8f5e9
    style Errors fill:#ffcdd2
    style Traces fill:#fff3e0
```

### Latency ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ëª©í‘œê°’ | ì•Œë¦¼ ì„ê³„ê°’ |
| ------ | ---- | ------ | ----------- |
| `agent_request_duration_seconds` | ì „ì²´ ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ | P95 < 5s | P99 > 10s |
| `llm_inference_duration_seconds` | LLM ì¶”ë¡  ì‹œê°„ | P95 < 3s | P99 > 8s |
| `tool_execution_duration_seconds` | ë„êµ¬ ì‹¤í–‰ ì‹œê°„ | P95 < 1s | P99 > 3s |
| `vector_search_duration_seconds` | ë²¡í„° ê²€ìƒ‰ ì‹œê°„ | P95 < 200ms | P99 > 500ms |

### Token Usage ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ëª¨ë‹ˆí„°ë§ ëª©ì  |
| ------ | ---- | ------------- |
| `llm_input_tokens_total` | ì…ë ¥ í† í° ì´í•© | í”„ë¡¬í”„íŠ¸ ìµœì í™” |
| `llm_output_tokens_total` | ì¶œë ¥ í† í° ì´í•© | ì‘ë‹µ ê¸¸ì´ ë¶„ì„ |
| `llm_total_tokens_total` | ì „ì²´ í† í° ì´í•© | ë¹„ìš© ì¶”ì  |
| `llm_cost_dollars_total` | ì¶”ì • ë¹„ìš© (USD) | ì˜ˆì‚° ê´€ë¦¬ |

### Error Rate ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ì•Œë¦¼ ì„ê³„ê°’ |
| ------ | ---- | ----------- |
| `agent_errors_total` | ì—ì´ì „íŠ¸ ì˜¤ë¥˜ ì´í•© | ì˜¤ë¥˜ìœ¨ > 5% |
| `llm_rate_limit_errors_total` | Rate Limit ì˜¤ë¥˜ | ë¶„ë‹¹ 10íšŒ ì´ìƒ |
| `tool_execution_errors_total` | ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜ | ì˜¤ë¥˜ìœ¨ > 10% |
| `agent_timeout_total` | íƒ€ì„ì•„ì›ƒ ë°œìƒ | ë¶„ë‹¹ 5íšŒ ì´ìƒ |

### Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„¤ì •

```yaml
# prometheus-scrape-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-agent-scrape
  namespace: observability
data:
  agent-scrape.yaml: |
    scrape_configs:
      - job_name: 'langfuse'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - observability
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            regex: langfuse
            action: keep
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            regex: "3000"
            action: keep
        metrics_path: /api/public/metrics
        
      - job_name: 'ai-agents'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - ai-agents
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            regex: "true"
            action: keep
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
```


### Python ë©”íŠ¸ë¦­ ìµìŠ¤í¬í„°

ì—ì´ì „íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ Prometheus ë©”íŠ¸ë¦­ì„ ë…¸ì¶œí•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

```python
# metrics_exporter.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# ë©”íŠ¸ë¦­ ì •ì˜
AGENT_REQUEST_DURATION = Histogram(
    'agent_request_duration_seconds',
    'Agent request duration in seconds',
    ['agent_name', 'model', 'tenant_id'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

LLM_INFERENCE_DURATION = Histogram(
    'llm_inference_duration_seconds',
    'LLM inference duration in seconds',
    ['model', 'provider'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

LLM_TOKENS = Counter(
    'llm_tokens_total',
    'Total LLM tokens used',
    ['model', 'token_type', 'tenant_id']  # token_type: input, output
)

LLM_COST = Counter(
    'llm_cost_dollars_total',
    'Total LLM cost in USD',
    ['model', 'tenant_id']
)

AGENT_ERRORS = Counter(
    'agent_errors_total',
    'Total agent errors',
    ['agent_name', 'error_type', 'tenant_id']
)

TOOL_EXECUTION_DURATION = Histogram(
    'tool_execution_duration_seconds',
    'Tool execution duration in seconds',
    ['tool_name', 'agent_name'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]
)

ACTIVE_SESSIONS = Gauge(
    'agent_active_sessions',
    'Number of active agent sessions',
    ['agent_name', 'tenant_id']
)

# ëª¨ë¸ë³„ ë¹„ìš© (USD per 1K tokens)
MODEL_COSTS = {
    "gpt-4-turbo": {"input": 0.01, "output": 0.03},
    "gpt-4": {"input": 0.03, "output": 0.06},
    "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
    "claude-3-opus": {"input": 0.015, "output": 0.075},
    "claude-3-sonnet": {"input": 0.003, "output": 0.015},
    "claude-3-haiku": {"input": 0.00025, "output": 0.00125},
}

def record_llm_usage(
    model: str,
    input_tokens: int,
    output_tokens: int,
    tenant_id: str,
    duration: float,
):
    """LLM ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­ ê¸°ë¡"""
    # í† í° ìˆ˜ ê¸°ë¡
    LLM_TOKENS.labels(model=model, token_type="input", tenant_id=tenant_id).inc(input_tokens)
    LLM_TOKENS.labels(model=model, token_type="output", tenant_id=tenant_id).inc(output_tokens)
    
    # ë¹„ìš© ê³„ì‚° ë° ê¸°ë¡
    if model in MODEL_COSTS:
        cost = (
            (input_tokens / 1000) * MODEL_COSTS[model]["input"] +
            (output_tokens / 1000) * MODEL_COSTS[model]["output"]
        )
        LLM_COST.labels(model=model, tenant_id=tenant_id).inc(cost)
    
    # ì¶”ë¡  ì‹œê°„ ê¸°ë¡
    LLM_INFERENCE_DURATION.labels(model=model, provider="openai").observe(duration)

def record_agent_request(
    agent_name: str,
    model: str,
    tenant_id: str,
    duration: float,
    success: bool,
    error_type: str = None,
):
    """ì—ì´ì „íŠ¸ ìš”ì²­ ë©”íŠ¸ë¦­ ê¸°ë¡"""
    AGENT_REQUEST_DURATION.labels(
        agent_name=agent_name,
        model=model,
        tenant_id=tenant_id
    ).observe(duration)
    
    if not success and error_type:
        AGENT_ERRORS.labels(
            agent_name=agent_name,
            error_type=error_type,
            tenant_id=tenant_id
        ).inc()

# ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘
def start_metrics_server(port: int = 8000):
    """Prometheus ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘"""
    start_http_server(port)
    print(f"Metrics server started on port {port}")
```


## Grafana ëŒ€ì‹œë³´ë“œ

### ëŒ€ì‹œë³´ë“œ ê°œìš”

AI Agent ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ Grafana ëŒ€ì‹œë³´ë“œë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph Dashboard["Grafana Dashboard"]
        subgraph Row1["Overview"]
            TOTAL_REQ["ì´ ìš”ì²­ ìˆ˜"]
            SUCCESS_RATE["ì„±ê³µë¥ "]
            AVG_LATENCY["í‰ê·  ì§€ì—°"]
            ACTIVE_USERS["í™œì„± ì‚¬ìš©ì"]
        end
        
        subgraph Row2["Performance"]
            LATENCY_CHART["ì§€ì—° ì‹œê°„ ë¶„í¬"]
            THROUGHPUT["ì²˜ë¦¬ëŸ‰ ì¶”ì´"]
            ERROR_RATE["ì˜¤ë¥˜ìœ¨ ì¶”ì´"]
        end
        
        subgraph Row3["Cost & Usage"]
            TOKEN_USAGE["í† í° ì‚¬ìš©ëŸ‰"]
            COST_BY_MODEL["ëª¨ë¸ë³„ ë¹„ìš©"]
            COST_BY_TENANT["í…Œë„ŒíŠ¸ë³„ ë¹„ìš©"]
        end
        
        subgraph Row4["Traces"]
            TRACE_TABLE["ìµœê·¼ íŠ¸ë ˆì´ìŠ¤"]
            SLOW_TRACES["ëŠë¦° ìš”ì²­"]
            ERROR_TRACES["ì˜¤ë¥˜ íŠ¸ë ˆì´ìŠ¤"]
        end
    end
    
    style Row1 fill:#e3f2fd
    style Row2 fill:#e8f5e9
    style Row3 fill:#fff3e0
    style Row4 fill:#fce4ec
```

### ëŒ€ì‹œë³´ë“œ JSON ì„¤ì •

```json
{
  "dashboard": {
    "id": null,
    "uid": "ai-agent-monitoring",
    "title": "AI Agent Monitoring",
    "tags": ["ai", "agent", "langfuse", "llm"],
    "timezone": "browser",
    "schemaVersion": 38,
    "version": 1,
    "refresh": "30s",
    "panels": [
      {
        "id": 1,
        "title": "Total Requests",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "sum(increase(agent_request_duration_seconds_count[24h]))",
            "legendFormat": "Total"
          }
        ],
        "options": {
          "colorMode": "value",
          "graphMode": "area",
          "justifyMode": "auto"
        },
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"color": "green", "value": null}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "Success Rate",
        "type": "gauge",
        "gridPos": {"h": 4, "w": 6, "x": 6, "y": 0},
        "targets": [
          {
            "expr": "1 - (sum(rate(agent_errors_total[5m])) / sum(rate(agent_request_duration_seconds_count[5m])))",
            "legendFormat": "Success Rate"
          }
        ],
        "options": {
          "showThresholdLabels": false,
          "showThresholdMarkers": true
        },
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "min": 0,
            "max": 1,
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"color": "red", "value": null},
                {"color": "yellow", "value": 0.9},
                {"color": "green", "value": 0.95}
              ]
            }
          }
        }
      },
      {
        "id": 3,
        "title": "P95 Latency",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 12, "y": 0},
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(agent_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P95"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 5},
                {"color": "red", "value": 10}
              ]
            }
          }
        }
      },
      {
        "id": 4,
        "title": "Active Sessions",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 18, "y": 0},
        "targets": [
          {
            "expr": "sum(agent_active_sessions)",
            "legendFormat": "Active"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"color": "green", "value": null}
              ]
            }
          }
        }
      },
      {
        "id": 5,
        "title": "Request Latency Distribution",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 4},
        "targets": [
          {
            "expr": "histogram_quantile(0.50, sum(rate(agent_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P50"
          },
          {
            "expr": "histogram_quantile(0.95, sum(rate(agent_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, sum(rate(agent_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P99"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "custom": {
              "drawStyle": "line",
              "lineInterpolation": "smooth",
              "fillOpacity": 10
            }
          }
        }
      },
      {
        "id": 6,
        "title": "Error Rate by Type",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 4},
        "targets": [
          {
            "expr": "sum(rate(agent_errors_total[5m])) by (error_type)",
            "legendFormat": "{{error_type}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "custom": {
              "drawStyle": "bars",
              "fillOpacity": 80
            }
          }
        }
      },
      {
        "id": 7,
        "title": "Token Usage by Model",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 12},
        "targets": [
          {
            "expr": "sum(rate(llm_tokens_total[1h])) by (model)",
            "legendFormat": "{{model}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "custom": {
              "drawStyle": "line",
              "fillOpacity": 20,
              "stacking": {"mode": "normal"}
            }
          }
        }
      },
      {
        "id": 8,
        "title": "Cost by Tenant (Daily)",
        "type": "piechart",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 12},
        "targets": [
          {
            "expr": "sum(increase(llm_cost_dollars_total[24h])) by (tenant_id)",
            "legendFormat": "{{tenant_id}}"
          }
        ],
        "options": {
          "legend": {
            "displayMode": "table",
            "placement": "right",
            "values": ["value", "percent"]
          },
          "pieType": "donut"
        },
        "fieldConfig": {
          "defaults": {
            "unit": "currencyUSD"
          }
        }
      }
    ]
  }
}
```


### ì•Œë¦¼ ì„¤ì •

Grafana ì•Œë¦¼ ê·œì¹™ì„ ì„¤ì •í•˜ì—¬ ì´ìƒ ìƒí™©ì„ ê°ì§€í•©ë‹ˆë‹¤.

```yaml
# grafana-alerts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-alert-rules
  namespace: observability
data:
  ai-agent-alerts.yaml: |
    apiVersion: 1
    groups:
      - orgId: 1
        name: AI Agent Alerts
        folder: AI Monitoring
        interval: 1m
        rules:
          - uid: agent-high-latency
            title: Agent High Latency
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: histogram_quantile(0.99, sum(rate(agent_request_duration_seconds_bucket[5m])) by (le, agent_name))
                  intervalMs: 1000
                  maxDataPoints: 43200
              - refId: B
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [10]
                        type: gt
                      operator:
                        type: and
                      query:
                        params: [A]
                      reducer:
                        type: last
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            noDataState: NoData
            execErrState: Error
            for: 5m
            annotations:
              summary: "Agent {{ $labels.agent_name }} P99 latency is above 10s"
              description: "Current P99 latency: {{ $values.A }}s"
            labels:
              severity: warning
              
          - uid: agent-high-error-rate
            title: Agent High Error Rate
            condition: C
            data:
              - refId: A
                datasourceUid: prometheus
                model:
                  expr: |
                    sum(rate(agent_errors_total[5m])) by (agent_name) / 
                    sum(rate(agent_request_duration_seconds_count[5m])) by (agent_name)
              - refId: B
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [0.05]
                        type: gt
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            for: 5m
            annotations:
              summary: "Agent {{ $labels.agent_name }} error rate is above 5%"
              description: "Current error rate: {{ printf \"%.2f\" $values.A }}%"
            labels:
              severity: critical
              
          - uid: llm-rate-limit
            title: LLM Rate Limit Errors
            condition: C
            data:
              - refId: A
                datasourceUid: prometheus
                model:
                  expr: sum(increase(llm_rate_limit_errors_total[5m])) by (model)
              - refId: B
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [10]
                        type: gt
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            for: 2m
            annotations:
              summary: "LLM {{ $labels.model }} rate limit errors detected"
              description: "{{ $values.A }} rate limit errors in last 5 minutes"
            labels:
              severity: warning
              
          - uid: cost-budget-alert
            title: Daily Cost Budget Exceeded
            condition: C
            data:
              - refId: A
                datasourceUid: prometheus
                model:
                  expr: sum(increase(llm_cost_dollars_total[24h])) by (tenant_id)
              - refId: B
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [100]  # $100 daily budget
                        type: gt
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            for: 0s
            annotations:
              summary: "Tenant {{ $labels.tenant_id }} exceeded daily cost budget"
              description: "Current daily cost: ${{ printf \"%.2f\" $values.A }}"
            labels:
              severity: warning
```


## ë¹„ìš© ì¶”ì 

### ëª¨ë¸ë³„ ë¹„ìš© ë¶„ì„

LLM ì‚¬ìš© ë¹„ìš©ì„ ëª¨ë¸ë³„ë¡œ ì¶”ì í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.

```python
# cost_tracker.py
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json

@dataclass
class ModelPricing:
    """ëª¨ë¸ë³„ ê°€ê²© ì •ë³´ (USD per 1K tokens)"""
    input_price: float
    output_price: float
    
# 2024ë…„ ê¸°ì¤€ ëª¨ë¸ ê°€ê²©
MODEL_PRICING: Dict[str, ModelPricing] = {
    # OpenAI
    "gpt-4-turbo": ModelPricing(0.01, 0.03),
    "gpt-4-turbo-preview": ModelPricing(0.01, 0.03),
    "gpt-4": ModelPricing(0.03, 0.06),
    "gpt-4-32k": ModelPricing(0.06, 0.12),
    "gpt-3.5-turbo": ModelPricing(0.0005, 0.0015),
    "gpt-3.5-turbo-16k": ModelPricing(0.003, 0.004),
    
    # Anthropic
    "claude-3-opus": ModelPricing(0.015, 0.075),
    "claude-3-sonnet": ModelPricing(0.003, 0.015),
    "claude-3-haiku": ModelPricing(0.00025, 0.00125),
    "claude-2.1": ModelPricing(0.008, 0.024),
    
    # Amazon Bedrock (Claude)
    "anthropic.claude-3-opus-20240229-v1:0": ModelPricing(0.015, 0.075),
    "anthropic.claude-3-sonnet-20240229-v1:0": ModelPricing(0.003, 0.015),
    "anthropic.claude-3-haiku-20240307-v1:0": ModelPricing(0.00025, 0.00125),
    
    # Self-hosted (ì˜ˆ: vLLM) - ì¸í”„ë¼ ë¹„ìš© ê¸°ë°˜ ì¶”ì •
    "llama-3-70b": ModelPricing(0.001, 0.001),
    "mixtral-8x7b": ModelPricing(0.0005, 0.0005),
}

@dataclass
class UsageRecord:
    """ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
    timestamp: datetime
    model: str
    input_tokens: int
    output_tokens: int
    tenant_id: str
    agent_name: str
    trace_id: str
    
    @property
    def total_tokens(self) -> int:
        return self.input_tokens + self.output_tokens
    
    @property
    def cost(self) -> float:
        if self.model not in MODEL_PRICING:
            return 0.0
        pricing = MODEL_PRICING[self.model]
        return (
            (self.input_tokens / 1000) * pricing.input_price +
            (self.output_tokens / 1000) * pricing.output_price
        )

class CostTracker:
    """ë¹„ìš© ì¶”ì ê¸°"""
    
    def __init__(self, langfuse_client=None):
        self.langfuse = langfuse_client
        self.records: List[UsageRecord] = []
    
    def record_usage(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int,
        tenant_id: str,
        agent_name: str,
        trace_id: str,
    ):
        """ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
        record = UsageRecord(
            timestamp=datetime.utcnow(),
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            tenant_id=tenant_id,
            agent_name=agent_name,
            trace_id=trace_id,
        )
        self.records.append(record)
        
        # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        from metrics_exporter import record_llm_usage
        record_llm_usage(
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            tenant_id=tenant_id,
            duration=0,  # ë³„ë„ ì¸¡ì • í•„ìš”
        )
        
        return record
    
    def get_cost_by_tenant(
        self,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> Dict[str, float]:
        """í…Œë„ŒíŠ¸ë³„ ë¹„ìš© ì§‘ê³„"""
        if start_time is None:
            start_time = datetime.utcnow() - timedelta(days=30)
        if end_time is None:
            end_time = datetime.utcnow()
        
        costs: Dict[str, float] = {}
        for record in self.records:
            if start_time <= record.timestamp <= end_time:
                if record.tenant_id not in costs:
                    costs[record.tenant_id] = 0.0
                costs[record.tenant_id] += record.cost
        
        return costs
    
    def get_cost_by_model(
        self,
        tenant_id: Optional[str] = None,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> Dict[str, Dict[str, float]]:
        """ëª¨ë¸ë³„ ë¹„ìš© ë° í† í° ì‚¬ìš©ëŸ‰ ì§‘ê³„"""
        if start_time is None:
            start_time = datetime.utcnow() - timedelta(days=30)
        if end_time is None:
            end_time = datetime.utcnow()
        
        result: Dict[str, Dict[str, float]] = {}
        for record in self.records:
            if start_time <= record.timestamp <= end_time:
                if tenant_id and record.tenant_id != tenant_id:
                    continue
                    
                if record.model not in result:
                    result[record.model] = {
                        "input_tokens": 0,
                        "output_tokens": 0,
                        "total_tokens": 0,
                        "cost": 0.0,
                        "requests": 0,
                    }
                
                result[record.model]["input_tokens"] += record.input_tokens
                result[record.model]["output_tokens"] += record.output_tokens
                result[record.model]["total_tokens"] += record.total_tokens
                result[record.model]["cost"] += record.cost
                result[record.model]["requests"] += 1
        
        return result
    
    def generate_cost_report(
        self,
        tenant_id: Optional[str] = None,
        period_days: int = 30,
    ) -> str:
        """ë¹„ìš© ë¦¬í¬íŠ¸ ìƒì„±"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=period_days)
        
        model_costs = self.get_cost_by_model(tenant_id, start_time, end_time)
        tenant_costs = self.get_cost_by_tenant(start_time, end_time)
        
        report = {
            "period": {
                "start": start_time.isoformat(),
                "end": end_time.isoformat(),
                "days": period_days,
            },
            "summary": {
                "total_cost": sum(tenant_costs.values()),
                "total_requests": sum(m["requests"] for m in model_costs.values()),
                "total_tokens": sum(m["total_tokens"] for m in model_costs.values()),
            },
            "by_model": model_costs,
            "by_tenant": tenant_costs,
        }
        
        return json.dumps(report, indent=2, default=str)
```


### í…Œë„ŒíŠ¸ë³„ ë¹„ìš© í• ë‹¹

ë©€í‹° í…Œë„ŒíŠ¸ í™˜ê²½ì—ì„œ ë¹„ìš©ì„ ê³µì •í•˜ê²Œ í• ë‹¹í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```yaml
# cost-allocation-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-allocation-config
  namespace: observability
data:
  allocation-rules.yaml: |
    # í…Œë„ŒíŠ¸ë³„ ì˜ˆì‚° ì„¤ì •
    tenants:
      tenant-a:
        monthly_budget_usd: 5000
        alert_threshold_percent: 80
        models_allowed:
          - gpt-4-turbo
          - gpt-3.5-turbo
          - claude-3-sonnet
        rate_limits:
          requests_per_minute: 100
          tokens_per_day: 1000000
          
      tenant-b:
        monthly_budget_usd: 2000
        alert_threshold_percent: 80
        models_allowed:
          - gpt-3.5-turbo
          - claude-3-haiku
        rate_limits:
          requests_per_minute: 50
          tokens_per_day: 500000
          
      tenant-c:
        monthly_budget_usd: 10000
        alert_threshold_percent: 90
        models_allowed:
          - gpt-4-turbo
          - gpt-4
          - claude-3-opus
          - claude-3-sonnet
        rate_limits:
          requests_per_minute: 200
          tokens_per_day: 5000000
    
    # ê³µìœ  ì¸í”„ë¼ ë¹„ìš© ë¶„ë°°
    infrastructure_cost_allocation:
      method: proportional  # proportional, equal, fixed
      base_monthly_cost_usd: 1000  # GPU ì¸í”„ë¼ ê¸°ë³¸ ë¹„ìš©
      
    # ì•Œë¦¼ ì„¤ì •
    alerts:
      budget_warning:
        threshold_percent: 80
        channels: [slack, email]
      budget_exceeded:
        threshold_percent: 100
        channels: [slack, email, pagerduty]
      rate_limit_warning:
        threshold_percent: 90
        channels: [slack]
```

### ë¹„ìš© ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬

Grafanaì—ì„œ ë¹„ìš© ë¶„ì„ì„ ìœ„í•œ PromQL ì¿¼ë¦¬ ì˜ˆì‹œì…ë‹ˆë‹¤.

```promql
# ì¼ë³„ ì´ ë¹„ìš©
sum(increase(llm_cost_dollars_total[24h]))

# í…Œë„ŒíŠ¸ë³„ ì¼ë³„ ë¹„ìš©
sum(increase(llm_cost_dollars_total[24h])) by (tenant_id)

# ëª¨ë¸ë³„ ë¹„ìš© ë¹„ìœ¨
sum(increase(llm_cost_dollars_total[24h])) by (model) 
/ ignoring(model) group_left 
sum(increase(llm_cost_dollars_total[24h]))

# ì‹œê°„ë‹¹ ë¹„ìš© ì¶”ì´
sum(rate(llm_cost_dollars_total[1h])) * 3600

# ì˜ˆì‚° ëŒ€ë¹„ ì‚¬ìš©ë¥  (ì›”ê°„)
sum(increase(llm_cost_dollars_total[30d])) by (tenant_id) 
/ on(tenant_id) group_left 
tenant_monthly_budget_usd

# í† í°ë‹¹ í‰ê·  ë¹„ìš©
sum(rate(llm_cost_dollars_total[1h])) 
/ sum(rate(llm_tokens_total[1h]))

# ìš”ì²­ë‹¹ í‰ê·  ë¹„ìš©
sum(increase(llm_cost_dollars_total[24h])) 
/ sum(increase(agent_request_duration_seconds_count[24h]))
```

:::tip ë¹„ìš© ìµœì í™” íŒ
1. **ëª¨ë¸ ì„ íƒ ìµœì í™”**: ê°„ë‹¨í•œ ì‘ì—…ì—ëŠ” ì €ë ´í•œ ëª¨ë¸(GPT-3.5, Claude Haiku) ì‚¬ìš©
2. **í”„ë¡¬í”„íŠ¸ ìµœì í™”**: ë¶ˆí•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì œê±°ë¡œ ì…ë ¥ í† í° ì ˆê°
3. **ìºì‹± í™œìš©**: ë°˜ë³µì ì¸ ì¿¼ë¦¬ì— ëŒ€í•œ ì‘ë‹µ ìºì‹±
4. **ë°°ì¹˜ ì²˜ë¦¬**: ê°€ëŠ¥í•œ ê²½ìš° ìš”ì²­ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
:::


## OpenTelemetry í†µí•©

LangFuseì™€ ê¸°ì¡´ ê´€ì¸¡ì„± ìŠ¤íƒì„ OpenTelemetryë¡œ í†µí•©í•©ë‹ˆë‹¤.

### OpenTelemetry Collector ì„¤ì •

```yaml
# otel-collector-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: observability
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      
      prometheus:
        config:
          scrape_configs:
            - job_name: 'langfuse'
              static_configs:
                - targets: ['langfuse:3000']
              metrics_path: /api/public/metrics
    
    processors:
      batch:
        timeout: 10s
        send_batch_size: 1000
      
      memory_limiter:
        check_interval: 1s
        limit_mib: 1000
        spike_limit_mib: 200
      
      attributes:
        actions:
          - key: environment
            value: production
            action: upsert
          - key: service.namespace
            value: ai-platform
            action: upsert
    
    exporters:
      prometheus:
        endpoint: "0.0.0.0:8889"
        namespace: ai_agent
        
      otlp/jaeger:
        endpoint: jaeger-collector.observability:4317
        tls:
          insecure: true
      
      awsxray:
        region: ap-northeast-2
        
      logging:
        loglevel: info
    
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      pprof:
        endpoint: 0.0.0.0:1777
    
    service:
      extensions: [health_check, pprof]
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch, attributes]
          exporters: [otlp/jaeger, awsxray, logging]
        
        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, batch]
          exporters: [prometheus]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: observability
spec:
  replicas: 2
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:0.92.0
          args:
            - --config=/etc/otel/config.yaml
          ports:
            - containerPort: 4317  # OTLP gRPC
            - containerPort: 4318  # OTLP HTTP
            - containerPort: 8889  # Prometheus exporter
            - containerPort: 13133 # Health check
          volumeMounts:
            - name: config
              mountPath: /etc/otel
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "500m"
      volumes:
        - name: config
          configMap:
            name: otel-collector-config
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
spec:
  selector:
    app: otel-collector
  ports:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
    - name: otlp-http
      port: 4318
      targetPort: 4318
    - name: prometheus
      port: 8889
      targetPort: 8889
```

### Python OpenTelemetry í†µí•©

```python
# otel_integration.py
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.sdk.resources import Resource
from opentelemetry.semconv.resource import ResourceAttributes
import os

def setup_opentelemetry():
    """OpenTelemetry ì´ˆê¸°í™”"""
    resource = Resource.create({
        ResourceAttributes.SERVICE_NAME: "ai-agent",
        ResourceAttributes.SERVICE_VERSION: "1.0.0",
        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: os.environ.get("ENVIRONMENT", "production"),
    })
    
    provider = TracerProvider(resource=resource)
    
    # OTLP Exporter ì„¤ì •
    otlp_exporter = OTLPSpanExporter(
        endpoint=os.environ.get("OTEL_EXPORTER_OTLP_ENDPOINT", "otel-collector:4317"),
        insecure=True,
    )
    
    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
    trace.set_tracer_provider(provider)
    
    # HTTP ìš”ì²­ ìë™ ê³„ì¸¡
    RequestsInstrumentor().instrument()
    
    return trace.get_tracer(__name__)

# ì‚¬ìš© ì˜ˆì‹œ
tracer = setup_opentelemetry()

@tracer.start_as_current_span("agent_execution")
def execute_agent(user_input: str, tenant_id: str):
    """OpenTelemetry íŠ¸ë ˆì´ì‹±ì´ ì ìš©ëœ ì—ì´ì „íŠ¸ ì‹¤í–‰"""
    current_span = trace.get_current_span()
    current_span.set_attribute("tenant_id", tenant_id)
    current_span.set_attribute("input_length", len(user_input))
    
    # ì—ì´ì „íŠ¸ ë¡œì§ ì‹¤í–‰
    with tracer.start_as_current_span("llm_inference") as llm_span:
        llm_span.set_attribute("model", "gpt-4-turbo")
        # LLM í˜¸ì¶œ
        response = call_llm(user_input)
        llm_span.set_attribute("output_tokens", response.usage.completion_tokens)
    
    return response
```


## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²° ë°©ë²•

#### LangFuse ì—°ê²° ì˜¤ë¥˜

```bash
# ì¦ìƒ: LangFuseì— íŠ¸ë ˆì´ìŠ¤ê°€ ê¸°ë¡ë˜ì§€ ì•ŠìŒ

# 1. LangFuse ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
kubectl get pods -n observability -l app=langfuse

# 2. LangFuse ë¡œê·¸ í™•ì¸
kubectl logs -n observability -l app=langfuse --tail=100

# 3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl -v http://langfuse.observability.svc/api/public/health

# 4. í™˜ê²½ ë³€ìˆ˜ í™•ì¸
kubectl exec -n ai-agents <pod-name> -- env | grep LANGFUSE
```

#### ë†’ì€ ì§€ì—° ì‹œê°„ ë””ë²„ê¹…

```python
# trace_analyzer.py
from langfuse import Langfuse
from datetime import datetime, timedelta

langfuse = Langfuse()

def analyze_slow_traces(threshold_seconds: float = 10.0, hours: int = 24):
    """ëŠë¦° íŠ¸ë ˆì´ìŠ¤ ë¶„ì„"""
    traces = langfuse.get_traces(
        limit=100,
        order_by="latency",
        order="desc",
    )
    
    slow_traces = []
    for trace in traces.data:
        if trace.latency and trace.latency > threshold_seconds:
            # ê° ìŠ¤íŒ¬ë³„ ì§€ì—° ì‹œê°„ ë¶„ì„
            observations = langfuse.get_observations(trace_id=trace.id)
            
            breakdown = []
            for obs in observations.data:
                if obs.end_time and obs.start_time:
                    duration = (obs.end_time - obs.start_time).total_seconds()
                    breakdown.append({
                        "name": obs.name,
                        "type": obs.type,
                        "duration": duration,
                    })
            
            slow_traces.append({
                "trace_id": trace.id,
                "total_latency": trace.latency,
                "breakdown": sorted(breakdown, key=lambda x: x["duration"], reverse=True),
            })
    
    return slow_traces

# ë¶„ì„ ì‹¤í–‰
slow = analyze_slow_traces(threshold_seconds=10.0)
for trace in slow[:5]:
    print(f"\nTrace: {trace['trace_id']}")
    print(f"Total Latency: {trace['total_latency']:.2f}s")
    print("Breakdown:")
    for item in trace['breakdown'][:5]:
        print(f"  - {item['name']}: {item['duration']:.2f}s ({item['type']})")
```

#### ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì§„ë‹¨

```yaml
# memory-debug-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: memory-debug
  namespace: observability
spec:
  containers:
    - name: debug
      image: python:3.11-slim
      command: ["sleep", "infinity"]
      resources:
        requests:
          memory: "256Mi"
        limits:
          memory: "512Mi"
      env:
        - name: LANGFUSE_HOST
          value: "http://langfuse.observability.svc"
```

```python
# memory_profiler.py
import tracemalloc
import gc
from langfuse import Langfuse

def profile_langfuse_memory():
    """LangFuse í´ë¼ì´ì–¸íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§"""
    tracemalloc.start()
    
    langfuse = Langfuse()
    
    # ì—¬ëŸ¬ íŠ¸ë ˆì´ìŠ¤ ìƒì„±
    for i in range(100):
        trace = langfuse.trace(name=f"test-trace-{i}")
        trace.generation(
            name="test-generation",
            input="test input",
            output="test output",
        )
    
    langfuse.flush()
    
    # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics('lineno')
    
    print("Top 10 memory allocations:")
    for stat in top_stats[:10]:
        print(stat)
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    gc.collect()
    
    tracemalloc.stop()
```

### ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | í™•ì¸ ì‚¬í•­ | ê¶Œì¥ ì„¤ì • |
| ---- | --------- | --------- |
| **ë°°ì¹˜ í¬ê¸°** | íŠ¸ë ˆì´ìŠ¤ ë°°ì¹˜ ì „ì†¡ í¬ê¸° | 100-500 |
| **í”ŒëŸ¬ì‹œ ê°„ê²©** | ìë™ í”ŒëŸ¬ì‹œ ì£¼ê¸° | 5-10ì´ˆ |
| **ì—°ê²° í’€** | HTTP ì—°ê²° ì¬ì‚¬ìš© | í™œì„±í™” |
| **ë¹„ë™ê¸° ì „ì†¡** | ë°±ê·¸ë¼ìš´ë“œ ì „ì†¡ | í™œì„±í™” |
| **ìƒ˜í”Œë§** | ê³ íŠ¸ë˜í”½ ì‹œ ìƒ˜í”Œë§ | 10-50% |

:::danger ì£¼ì˜ì‚¬í•­
- í”„ë¡œë•ì…˜ì—ì„œ ëª¨ë“  íŠ¸ë ˆì´ìŠ¤ë¥¼ ì €ì¥í•˜ë©´ ìŠ¤í† ë¦¬ì§€ ë¹„ìš©ì´ ê¸‰ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë¯¼ê°í•œ ë°ì´í„°(PII)ê°€ íŠ¸ë ˆì´ìŠ¤ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ ë§ˆìŠ¤í‚¹ì„ ì ìš©í•˜ì„¸ìš”
- LangFuse ë°ì´í„°ë² ì´ìŠ¤ì˜ ì •ê¸°ì ì¸ ë°±ì—…ê³¼ ë³´ì¡´ ì •ì±…ì„ ì„¤ì •í•˜ì„¸ìš”
:::


## ê²°ë¡ 

AI Agent ëª¨ë‹ˆí„°ë§ì€ Agentic AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì•ˆì •ì ì¸ ìš´ì˜ê³¼ ì§€ì†ì ì¸ ê°œì„ ì„ ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œ ë‹¤ë£¬ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ë©´:

### í•µì‹¬ ìš”ì•½

1. **LangFuse ë°°í¬**: Kubernetes í™˜ê²½ì—ì„œ Self-hosted LangFuseë¥¼ ë°°í¬í•˜ì—¬ ë°ì´í„° ì£¼ê¶Œì„ í™•ë³´í•˜ê³  ë¹„ìš©ì„ ìµœì í™”
2. **LangSmith í†µí•©**: LangChain ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ê°„í¸í•˜ê²Œ íŠ¸ë ˆì´ì‹± í™œì„±í™”
3. **í•µì‹¬ ë©”íŠ¸ë¦­**: Latency, Token Usage, Error Rate, Trace ë¶„ì„ì„ í†µí•œ ì¢…í•©ì ì¸ ëª¨ë‹ˆí„°ë§
4. **Grafana ëŒ€ì‹œë³´ë“œ**: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ì„ í†µí•œ proactive ìš´ì˜
5. **ë¹„ìš© ì¶”ì **: ëª¨ë¸ë³„, í…Œë„ŒíŠ¸ë³„ ë¹„ìš© ë¶„ì„ìœ¼ë¡œ ì˜ˆì‚° ê´€ë¦¬ ë° ìµœì í™”

### ëª¨ë‹ˆí„°ë§ ì„±ìˆ™ë„ ëª¨ë¸

| ë‹¨ê³„ | ìˆ˜ì¤€ | êµ¬í˜„ ë‚´ìš© |
| ---- | ---- | --------- |
| **Level 1** | ê¸°ë³¸ | ë¡œê·¸ ìˆ˜ì§‘, ê¸°ë³¸ ë©”íŠ¸ë¦­ |
| **Level 2** | í‘œì¤€ | LangFuse/LangSmith íŠ¸ë ˆì´ì‹±, Grafana ëŒ€ì‹œë³´ë“œ |
| **Level 3** | ê³ ê¸‰ | ë¹„ìš© ì¶”ì , í’ˆì§ˆ í‰ê°€, ìë™ ì•Œë¦¼ |
| **Level 4** | ìµœì í™” | A/B í…ŒìŠ¤íŠ¸, ìë™ íŠœë‹, ì˜ˆì¸¡ ë¶„ì„ |

:::tip ë‹¤ìŒ ë‹¨ê³„
- [Agentic AI Platform ì•„í‚¤í…ì²˜](./agentic-platform-architecture.md) - ì „ì²´ í”Œë«í¼ ì„¤ê³„
- [Kagent Kubernetes Agent ê´€ë¦¬](./kagent-kubernetes-agents.md) - ì—ì´ì „íŠ¸ ë°°í¬ ë° ìš´ì˜
- [RAG í‰ê°€ í”„ë ˆì„ì›Œí¬](./ragas-evaluation.md) - Ragasë¥¼ í™œìš©í•œ í’ˆì§ˆ í‰ê°€
:::

## ì°¸ê³  ìë£Œ

- [LangFuse Documentation](https://langfuse.com/docs)
- [LangFuse GitHub Repository](https://github.com/langfuse/langfuse)
- [LangSmith Documentation](https://docs.smith.langchain.com/)
- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)
- [Prometheus Monitoring](https://prometheus.io/docs/)
- [Grafana Dashboards](https://grafana.com/docs/grafana/latest/dashboards/)
- [LangChain Callbacks](https://python.langchain.com/docs/modules/callbacks/)
