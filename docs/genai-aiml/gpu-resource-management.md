---
title: "GPU í´ëŸ¬ìŠ¤í„° ë™ì  ë¦¬ì†ŒìŠ¤ ê´€ë¦¬"
sidebar_label: "GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬"
description: "ë³µìˆ˜ GPU í´ëŸ¬ìŠ¤í„° í™˜ê²½ì—ì„œì˜ ë™ì  ë¦¬ì†ŒìŠ¤ í• ë‹¹ ë° Karpenter ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§"
tags: [eks, gpu, karpenter, autoscaling, resource-management, dcgm]
category: "genai-aiml"
date: 2025-02-05
authors: [devfloor9]
sidebar_position: 5
---

# GPU í´ëŸ¬ìŠ¤í„° ë™ì  ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

> ğŸ“… **ì‘ì„±ì¼**: 2025-02-05 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 9ë¶„


## ê°œìš”

ëŒ€ê·œëª¨ GenAI ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œëŠ” ë³µìˆ˜ì˜ GPU í´ëŸ¬ìŠ¤í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³ , íŠ¸ë˜í”½ ë³€í™”ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ë¥¼ ì¬í• ë‹¹í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” Amazon EKS í™˜ê²½ì—ì„œ Karpenterë¥¼ í™œìš©í•œ GPU ë…¸ë“œ ìë™ ìŠ¤ì¼€ì¼ë§ê³¼ DCGM(Data Center GPU Manager) ê¸°ë°˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ê·¸ë¦¬ê³  KEDAë¥¼ í†µí•œ ì›Œí¬ë¡œë“œ ìë™ ìŠ¤ì¼€ì¼ë§ ì „ëµì„ ë‹¤ë£¹ë‹ˆë‹¤.

### ì£¼ìš” ëª©í‘œ

- **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±**: GPU ë¦¬ì†ŒìŠ¤ì˜ ìœ íœ´ ì‹œê°„ ìµœì†Œí™”
- **ë¹„ìš© ìµœì í™”**: Spot ì¸ìŠ¤í„´ìŠ¤ í™œìš© ë° Consolidationì„ í†µí•œ ë¹„ìš© ì ˆê°
- **ìë™í™”ëœ ìŠ¤ì¼€ì¼ë§**: íŠ¸ë˜í”½ íŒ¨í„´ì— ë”°ë¥¸ ìë™ ë¦¬ì†ŒìŠ¤ ì¡°ì •
- **ì„œë¹„ìŠ¤ ì•ˆì •ì„±**: SLA ì¤€ìˆ˜ë¥¼ ìœ„í•œ ì ì ˆí•œ ë¦¬ì†ŒìŠ¤ í™•ë³´

---

## ë©€í‹° GPU í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì²˜

### ì „ì²´ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
flowchart TB
    subgraph "Traffic Layer"
        ALB[Application Load Balancer]
        IGW[Ingress Gateway]
    end

    subgraph "EKS Control Plane"
        API[Kubernetes API Server]
        KARP[Karpenter Controller]
        KEDA_OP[KEDA Operator]
    end

    subgraph "GPU Node Pool A - Model Serving"
        NPA1[p4d.24xlarge Node 1]
        NPA2[p4d.24xlarge Node 2]
        NPA3[p5.48xlarge Node 3]
        
        subgraph "Model A Pods"
            MA1[vLLM Pod A-1]
            MA2[vLLM Pod A-2]
            MA3[vLLM Pod A-3]
        end
    end

    subgraph "GPU Node Pool B - Model Serving"
        NPB1[g5.48xlarge Node 1]
        NPB2[g5.48xlarge Node 2]
        
        subgraph "Model B Pods"
            MB1[vLLM Pod B-1]
            MB2[vLLM Pod B-2]
        end
    end

    subgraph "Monitoring Stack"
        DCGM[DCGM Exporter]
        PROM[Prometheus]
        GRAF[Grafana]
    end

    ALB --> IGW
    IGW --> MA1 & MA2 & MA3
    IGW --> MB1 & MB2
    
    API --> KARP
    API --> KEDA_OP
    
    KARP --> NPA1 & NPA2 & NPA3
    KARP --> NPB1 & NPB2
    
    DCGM --> PROM
    PROM --> KEDA_OP
    PROM --> GRAF
```

### ë¦¬ì†ŒìŠ¤ ê³µìœ  ì•„í‚¤í…ì²˜

ë³µìˆ˜ ëª¨ë¸ ê°„ GPU ë¦¬ì†ŒìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³µìœ í•˜ê¸° ìœ„í•œ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph "Shared GPU Pool"
        direction TB
        GPU1[GPU 1-4<br/>Model A Primary]
        GPU2[GPU 5-8<br/>Model B Primary]
        GPU3[GPU 9-12<br/>Elastic Pool]
    end

    subgraph "Resource Allocator"
        RA[Dynamic Resource<br/>Allocator]
        METRICS[GPU Metrics<br/>Collector]
    end

    subgraph "Workloads"
        WA[Model A<br/>Workload]
        WB[Model B<br/>Workload]
    end

    METRICS --> RA
    RA --> GPU1 & GPU2 & GPU3
    WA --> GPU1
    WB --> GPU2
    WA -.->|Traffic Surge| GPU3
    WB -.->|Traffic Surge| GPU3
```

:::info ë¦¬ì†ŒìŠ¤ ê³µìœ  ì›ì¹™

- **Primary Pool**: ê° ëª¨ë¸ì— í• ë‹¹ëœ ê¸°ë³¸ GPU ë¦¬ì†ŒìŠ¤
- **Elastic Pool**: íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œ ë™ì ìœ¼ë¡œ í• ë‹¹ë˜ëŠ” ê³µìœ  ë¦¬ì†ŒìŠ¤
- **Priority-based Allocation**: ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ í• ë‹¹ìœ¼ë¡œ ì¤‘ìš” ì›Œí¬ë¡œë“œ ë³´í˜¸

:::

---

## ë™ì  ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì „ëµ

### íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œë‚˜ë¦¬ì˜¤

ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œë‚˜ë¦¬ì˜¤ì™€ ëŒ€ì‘ ì „ëµì…ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì íŠ¸ë˜í”½
    participant LB as Load Balancer
    participant ModelA as Model A Service
    participant ModelB as Model B Service
    participant KEDA as KEDA Controller
    participant Karpenter as Karpenter
    participant AWS as AWS EC2

    Note over User,AWS: ì •ìƒ ìƒíƒœ: Model A 40%, Model B 30% GPU ì‚¬ìš©ë¥ 
    
    User->>LB: íŠ¸ë˜í”½ ê¸‰ì¦ (Model A)
    LB->>ModelA: ìš”ì²­ ì „ë‹¬
    ModelA->>KEDA: GPU ì‚¬ìš©ë¥  85% ê°ì§€
    
    KEDA->>ModelA: HPA íŠ¸ë¦¬ê±° - Pod ìŠ¤ì¼€ì¼ ì•„ì›ƒ
    KEDA->>Karpenter: ì¶”ê°€ ë…¸ë“œ ìš”ì²­
    
    Karpenter->>AWS: p4d.24xlarge í”„ë¡œë¹„ì €ë‹
    AWS-->>Karpenter: ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ
    
    Note over ModelA,ModelB: Model B ë¦¬ì†ŒìŠ¤ ì¼ë¶€ë¥¼ Model Aë¡œ ì¬í• ë‹¹
    
    Karpenter->>ModelA: ìƒˆ ë…¸ë“œì— Pod ìŠ¤ì¼€ì¤„ë§
    ModelA-->>User: ì‘ë‹µ ì§€ì—° ì‹œê°„ ì •ìƒí™”
```

### ëª¨ë¸ ê°„ ë¦¬ì†ŒìŠ¤ ì¬í• ë‹¹ ì ˆì°¨

Model Aì— íŠ¸ë˜í”½ì´ ê¸‰ì¦í•  ë•Œ Model Bì˜ ìœ íœ´ ë¦¬ì†ŒìŠ¤ë¥¼ Model Aì— í• ë‹¹í•˜ëŠ” êµ¬ì²´ì ì¸ ì ˆì°¨ì…ë‹ˆë‹¤.

#### ë‹¨ê³„ 1: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„

```yaml
# DCGM Exporterê°€ ìˆ˜ì§‘í•˜ëŠ” ì£¼ìš” ë©”íŠ¸ë¦­
# - DCGM_FI_DEV_GPU_UTIL: GPU ì‚¬ìš©ë¥ 
# - DCGM_FI_DEV_MEM_COPY_UTIL: ë©”ëª¨ë¦¬ ë³µì‚¬ ì‚¬ìš©ë¥ 
# - DCGM_FI_DEV_FB_USED: í”„ë ˆì„ë²„í¼ ì‚¬ìš©ëŸ‰
```

#### ë‹¨ê³„ 2: ìŠ¤ì¼€ì¼ë§ ê²°ì •

| ì¡°ê±´ | ì•¡ì…˜ |
|------|------|
| Model A GPU ì‚¬ìš©ë¥  > 80% | Model A Pod ìŠ¤ì¼€ì¼ ì•„ì›ƒ íŠ¸ë¦¬ê±° |
| Model B GPU ì‚¬ìš©ë¥  < 30% | Model B Pod ìŠ¤ì¼€ì¼ ì¸ ê°€ëŠ¥ |
| Elastic Pool ê°€ìš© | Elastic Poolì—ì„œ ë¦¬ì†ŒìŠ¤ í• ë‹¹ |

#### ë‹¨ê³„ 3: ë¦¬ì†ŒìŠ¤ ì¬í• ë‹¹ ì‹¤í–‰

```bash
# Model Bì˜ replica ìˆ˜ ê°ì†Œ (ìœ íœ´ ë¦¬ì†ŒìŠ¤ í™•ë³´)
kubectl scale deployment model-b-serving --replicas=1 -n inference

# Model Aì˜ replica ìˆ˜ ì¦ê°€
kubectl scale deployment model-a-serving --replicas=5 -n inference

# ë˜ëŠ” KEDAê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
```

#### ë‹¨ê³„ 4: ë…¸ë“œ ë ˆë²¨ ìŠ¤ì¼€ì¼ë§

Karpenterê°€ ìë™ìœ¼ë¡œ ì¶”ê°€ ë…¸ë“œë¥¼ í”„ë¡œë¹„ì €ë‹í•˜ê±°ë‚˜ ìœ íœ´ ë…¸ë“œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

:::warning ì£¼ì˜ì‚¬í•­

ë¦¬ì†ŒìŠ¤ ì¬í• ë‹¹ ì‹œ Model Bì˜ ìµœì†Œ SLAë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•´ `minReplicas`ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì™„ì „í•œ ë¦¬ì†ŒìŠ¤ íšŒìˆ˜ëŠ” ì„œë¹„ìŠ¤ ì¤‘ë‹¨ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

:::

---

## Karpenter ê¸°ë°˜ ë…¸ë“œ ìŠ¤ì¼€ì¼ë§

### NodePool ì„¤ì •

GPU ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ Karpenter NodePool ì„¤ì • ì˜ˆì œì…ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-inference
        workload: genai
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge    # 8x A100 40GB
            - p5.48xlarge     # 8x H100 80GB
            - g5.48xlarge     # 8x A10G 24GB
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
  limits:
    cpu: 1000
    memory: 4000Gi
    nvidia.com/gpu: 64
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
  weight: 100
```

### EC2NodeClass ì„¤ì •

GPU ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìœ„í•œ EC2NodeClass ì„¤ì •ì…ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu-nodeclass
spec:
  role: KarpenterNodeRole-${CLUSTER_NAME}
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 500Gi
        volumeType: gp3
        iops: 10000
        throughput: 500
        encrypted: true
        deleteOnTermination: true
  instanceStorePolicy: RAID0
  userData: |
    #!/bin/bash
    # NVIDIA ë“œë¼ì´ë²„ ë° Container Toolkit ì„¤ì •
    nvidia-smi
    
    # GPU ë©”ëª¨ë¦¬ ëª¨ë“œ ì„¤ì • (Persistence Mode)
    nvidia-smi -pm 1
    
    # EFA ë“œë¼ì´ë²„ ë¡œë“œ (p4d, p5 ì¸ìŠ¤í„´ìŠ¤ìš©)
    modprobe efa
  tags:
    Environment: production
    Workload: genai-inference
```

### GPU ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë¹„êµ

| ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | GPU | GPU ë©”ëª¨ë¦¬ | vCPU | ë©”ëª¨ë¦¬ | ë„¤íŠ¸ì›Œí¬ | ìš©ë„ |
|--------------|-----|-----------|------|--------|---------|------|
| p4d.24xlarge | 8x A100 | 40GB x 8 | 96 | 1152 GiB | 400 Gbps EFA | ëŒ€ê·œëª¨ LLM ì¶”ë¡  |
| p5.48xlarge | 8x H100 | 80GB x 8 | 192 | 2048 GiB | 3200 Gbps EFA | ì´ˆëŒ€ê·œëª¨ ëª¨ë¸, í•™ìŠµ |
| g5.48xlarge | 8x A10G | 24GB x 8 | 192 | 768 GiB | 100 Gbps | ì¤‘ì†Œê·œëª¨ ëª¨ë¸ ì¶”ë¡  |

:::tip ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ ê°€ì´ë“œ

- **p5.48xlarge**: 70B+ íŒŒë¼ë¯¸í„° ëª¨ë¸, ìµœê³  ì„±ëŠ¥ ìš”êµ¬ ì‹œ
- **p4d.24xlarge**: 13B-70B íŒŒë¼ë¯¸í„° ëª¨ë¸, ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ ê· í˜•
- **g5.48xlarge**: 7B ì´í•˜ ëª¨ë¸, ë¹„ìš© íš¨ìœ¨ì ì¸ ì¶”ë¡ 

:::

---

## GPU ë©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§

### DCGM Exporter ì„¤ì •

NVIDIA DCGM Exporterë¥¼ í†µí•´ GPU ë©”íŠ¸ë¦­ì„ Prometheusë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: dcgm-exporter
  namespace: gpu-monitoring
  labels:
    app: dcgm-exporter
spec:
  selector:
    matchLabels:
      app: dcgm-exporter
  template:
    metadata:
      labels:
        app: dcgm-exporter
    spec:
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: dcgm-exporter
          image: nvcr.io/nvidia/k8s/dcgm-exporter:3.3.5-3.4.0-ubuntu22.04
          ports:
            - name: metrics
              containerPort: 9400
          env:
            - name: DCGM_EXPORTER_LISTEN
              value: ":9400"
            - name: DCGM_EXPORTER_KUBERNETES
              value: "true"
            - name: DCGM_EXPORTER_COLLECTORS
              value: "/etc/dcgm-exporter/dcp-metrics-included.csv"
          volumeMounts:
            - name: pod-resources
              mountPath: /var/lib/kubelet/pod-resources
              readOnly: true
          securityContext:
            runAsNonRoot: false
            runAsUser: 0
            capabilities:
              add: ["SYS_ADMIN"]
      volumes:
        - name: pod-resources
          hostPath:
            path: /var/lib/kubelet/pod-resources
```

### ì£¼ìš” GPU ë©”íŠ¸ë¦­

DCGM Exporterê°€ ìˆ˜ì§‘í•˜ëŠ” í•µì‹¬ ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤.

| ë©”íŠ¸ë¦­ ì´ë¦„ | ì„¤ëª… | ìŠ¤ì¼€ì¼ë§ í™œìš© |
|------------|------|--------------|
| `DCGM_FI_DEV_GPU_UTIL` | GPU ì½”ì–´ ì‚¬ìš©ë¥  (%) | HPA íŠ¸ë¦¬ê±° ê¸°ì¤€ |
| `DCGM_FI_DEV_MEM_COPY_UTIL` | ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì‚¬ìš©ë¥  (%) | ë©”ëª¨ë¦¬ ë³‘ëª© ê°ì§€ |
| `DCGM_FI_DEV_FB_USED` | í”„ë ˆì„ë²„í¼ ì‚¬ìš©ëŸ‰ (MB) | OOM ë°©ì§€ |
| `DCGM_FI_DEV_FB_FREE` | í”„ë ˆì„ë²„í¼ ì—¬ìœ ëŸ‰ (MB) | ìš©ëŸ‰ ê³„íš |
| `DCGM_FI_DEV_POWER_USAGE` | ì „ë ¥ ì‚¬ìš©ëŸ‰ (W) | ë¹„ìš© ëª¨ë‹ˆí„°ë§ |
| `DCGM_FI_DEV_SM_CLOCK` | SM í´ëŸ­ ì†ë„ (MHz) | ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ |
| `DCGM_FI_DEV_GPU_TEMP` | GPU ì˜¨ë„ (Â°C) | ì—´ ê´€ë¦¬ |

### Prometheus ServiceMonitor ì„¤ì •

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: dcgm-exporter
  namespace: gpu-monitoring
spec:
  selector:
    matchLabels:
      app: dcgm-exporter
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics
  namespaceSelector:
    matchNames:
      - gpu-monitoring
```

### KEDA ScaledObject ì„¤ì •

KEDAë¥¼ ì‚¬ìš©í•˜ì—¬ GPU ë©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: model-a-gpu-scaler
  namespace: inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: model-a-serving
  pollingInterval: 15
  cooldownPeriod: 60
  minReplicaCount: 2
  maxReplicaCount: 10
  fallback:
    failureThreshold: 3
    replicas: 3
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: Percent
              value: 25
              periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
            - type: Pods
              value: 4
              periodSeconds: 15
          selectPolicy: Max
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus-server.monitoring:9090
        metricName: gpu_utilization
        query: |
          avg(DCGM_FI_DEV_GPU_UTIL{pod=~"model-a-.*"})
        threshold: "70"
        activationThreshold: "50"
```

### ìë™ ìŠ¤ì¼€ì¼ë§ ì„ê³„ê°’ ì„¤ì •

ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë”°ë¥¸ ê¶Œì¥ ì„ê³„ê°’ì…ë‹ˆë‹¤.

| ì›Œí¬ë¡œë“œ ìœ í˜• | Scale Up ì„ê³„ê°’ | Scale Down ì„ê³„ê°’ | Cooldown |
|--------------|----------------|------------------|----------|
| ì‹¤ì‹œê°„ ì¶”ë¡  | GPU 70% | GPU 30% | 60ì´ˆ |
| ë°°ì¹˜ ì²˜ë¦¬ | GPU 85% | GPU 40% | 300ì´ˆ |
| ëŒ€í™”í˜• ì„œë¹„ìŠ¤ | GPU 60% | GPU 25% | 30ì´ˆ |

:::tip ì„ê³„ê°’ íŠœë‹ ê°€ì´ë“œ

1. **ì´ˆê¸° ì„¤ì •**: ë³´ìˆ˜ì ì¸ ê°’(Scale Up 80%, Scale Down 20%)ìœ¼ë¡œ ì‹œì‘
2. **ëª¨ë‹ˆí„°ë§**: 2-3ì¼ê°„ ì‹¤ì œ íŠ¸ë˜í”½ íŒ¨í„´ ê´€ì°°
3. **ì¡°ì •**: ì‘ë‹µ ì‹œê°„ SLAì™€ ë¹„ìš©ì„ ê³ ë ¤í•˜ì—¬ ì ì§„ì  ì¡°ì •
4. **ê²€ì¦**: ë¶€í•˜ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì„¤ì • ê²€ì¦

:::

### HPAì™€ KEDA ì—°ë™

ê¸°ë³¸ HPAì™€ KEDAë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì˜ ì„¤ì •ì…ë‹ˆë‹¤.

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: model-a-hpa
  namespace: inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: model-a-serving
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: External
      external:
        metric:
          name: gpu_utilization
          selector:
            matchLabels:
              scaledobject.keda.sh/name: model-a-gpu-scaler
        target:
          type: AverageValue
          averageValue: "70"
```

---

## ë¹„ìš© ìµœì í™” ì „ëµ

### Spot ì¸ìŠ¤í„´ìŠ¤ í™œìš©

GPU Spot ì¸ìŠ¤í„´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ë¹„ìš©ì„ ìµœëŒ€ 90%ê¹Œì§€ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-spot-pool
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.12xlarge
            - g5.24xlarge
            - g5.48xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-spot-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: karpenter.sh/capacity-type
          value: "spot"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 32
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 30s
  weight: 50
```

:::warning Spot ì¸ìŠ¤í„´ìŠ¤ ì£¼ì˜ì‚¬í•­

- **ì¤‘ë‹¨ ì²˜ë¦¬**: Spot ì¸ìŠ¤í„´ìŠ¤ëŠ” 2ë¶„ ì „ ì¤‘ë‹¨ ì•Œë¦¼ì„ ë°›ìŠµë‹ˆë‹¤. ì ì ˆí•œ graceful shutdown êµ¬í˜„ í•„ìš”
- **ì›Œí¬ë¡œë“œ ì í•©ì„±**: ìƒíƒœ ë¹„ì €ì¥(stateless) ì¶”ë¡  ì›Œí¬ë¡œë“œì— ì í•©
- **ê°€ìš©ì„±**: íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì˜ Spot ê°€ìš©ì„±ì´ ë‚®ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ì–‘í•œ íƒ€ì… ì§€ì • ê¶Œì¥

:::

### Spot ì¤‘ë‹¨ ì²˜ë¦¬

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-serving-spot
  namespace: inference
spec:
  template:
    spec:
      terminationGracePeriodSeconds: 120
      containers:
        - name: vllm
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    # ìƒˆ ìš”ì²­ ìˆ˜ì‹  ì¤‘ë‹¨
                    curl -X POST localhost:8000/drain
                    # ì§„í–‰ ì¤‘ì¸ ìš”ì²­ ì™„ë£Œ ëŒ€ê¸°
                    sleep 90
      tolerations:
        - key: karpenter.sh/capacity-type
          operator: Equal
          value: "spot"
          effect: NoSchedule
```

### Consolidation ì •ì±…

ìœ íœ´ ë…¸ë“œë¥¼ ìë™ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ë¹„ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-pool
spec:
  disruption:
    # ë…¸ë“œê°€ ë¹„ì–´ìˆê±°ë‚˜ í™œìš©ë„ê°€ ë‚®ì„ ë•Œ í†µí•©
    consolidationPolicy: WhenEmptyOrUnderutilized
    # í†µí•© ëŒ€ê¸° ì‹œê°„
    consolidateAfter: 30s
    # ì˜ˆì‚° ì„¤ì • - ë™ì‹œì— ì¤‘ë‹¨ ê°€ëŠ¥í•œ ë…¸ë“œ ìˆ˜ ì œí•œ
    budgets:
      - nodes: "20%"
      - nodes: "0"
        schedule: "0 9 * * 1-5"  # í‰ì¼ ì—…ë¬´ ì‹œê°„ì—ëŠ” ì¤‘ë‹¨ ë°©ì§€
        duration: 8h
```

### ë¹„ìš© ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | ì„¤ëª… | ì˜ˆìƒ ì ˆê° |
|------|------|----------|
| Spot ì¸ìŠ¤í„´ìŠ¤ í™œìš© | ë¹„í”„ë¡œë•ì…˜ ë° ë‚´ê²°í•¨ì„± ì›Œí¬ë¡œë“œ | 60-90% |
| Consolidation í™œì„±í™” | ìœ íœ´ ë…¸ë“œ ìë™ ì •ë¦¬ | 20-30% |
| Right-sizing | ì›Œí¬ë¡œë“œì— ë§ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ | 15-25% |
| ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ | ë¹„ì—…ë¬´ ì‹œê°„ ë¦¬ì†ŒìŠ¤ ì¶•ì†Œ | 30-40% |

:::tip ë¹„ìš© ëª¨ë‹ˆí„°ë§

Kubecost ë˜ëŠ” AWS Cost Explorerë¥¼ í™œìš©í•˜ì—¬ GPU ì›Œí¬ë¡œë“œë³„ ë¹„ìš©ì„ ì¶”ì í•˜ê³ , ì •ê¸°ì ìœ¼ë¡œ ìµœì í™” ê¸°íšŒë¥¼ ê²€í† í•˜ì„¸ìš”.

:::

---

## ìš´ì˜ ëª¨ë²” ì‚¬ë¡€

### GPU ë¦¬ì†ŒìŠ¤ ìš”ì²­ ì„¤ì •

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-a-serving
  namespace: inference
spec:
  template:
    spec:
      containers:
        - name: vllm
          resources:
            requests:
              nvidia.com/gpu: 1
              memory: "32Gi"
              cpu: "8"
            limits:
              nvidia.com/gpu: 1
              memory: "64Gi"
              cpu: "16"
```

### ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„±

Grafana ëŒ€ì‹œë³´ë“œì—ì„œ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•  í•µì‹¬ íŒ¨ë„:

1. **GPU ì‚¬ìš©ë¥  íŠ¸ë Œë“œ**: ì‹œê°„ë³„ GPU ì‚¬ìš©ë¥  ë³€í™”
2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì—¬ìœ  ê³µê°„
3. **Pod ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸**: HPA/KEDA ìŠ¤ì¼€ì¼ë§ ì´ë ¥
4. **ë…¸ë“œ í”„ë¡œë¹„ì €ë‹**: Karpenter ë…¸ë“œ ìƒì„±/ì‚­ì œ ì´ë²¤íŠ¸
5. **ë¹„ìš© ì¶”ì **: ì‹œê°„ë‹¹/ì¼ë³„ GPU ë¹„ìš©

### ì•Œë¦¼ ì„¤ì •

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: gpu-alerts
  namespace: monitoring
spec:
  groups:
    - name: gpu-alerts
      rules:
        - alert: HighGPUUtilization
          expr: avg(DCGM_FI_DEV_GPU_UTIL) > 90
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "GPU ì‚¬ìš©ë¥ ì´ 90%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤"
            
        - alert: GPUMemoryPressure
          expr: (DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_FREE) > 0.9
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜"
```

---

## ìš”ì•½

GPU í´ëŸ¬ìŠ¤í„°ì˜ ë™ì  ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ëŠ” GenAI ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ê³¼ ë¹„ìš© íš¨ìœ¨ì„±ì„ ê²°ì •í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.

### í•µì‹¬ í¬ì¸íŠ¸

1. **Karpenter í™œìš©**: GPU ë…¸ë“œì˜ ìë™ í”„ë¡œë¹„ì €ë‹ ë° ì •ë¦¬ë¡œ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
2. **DCGM ë©”íŠ¸ë¦­**: ì •í™•í•œ GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ë°ì´í„° ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ ê²°ì •
3. **KEDA ì—°ë™**: GPU ë©”íŠ¸ë¦­ ê¸°ë°˜ ì›Œí¬ë¡œë“œ ìë™ ìŠ¤ì¼€ì¼ë§
4. **Spot ì¸ìŠ¤í„´ìŠ¤**: ì ì ˆí•œ ì›Œí¬ë¡œë“œì— Spot í™œìš©ìœ¼ë¡œ ë¹„ìš© ì ˆê°
5. **Consolidation**: ìœ íœ´ ë¦¬ì†ŒìŠ¤ ìë™ ì •ë¦¬ë¡œ ë¹„ìš© ìµœì í™”

### ë‹¤ìŒ ë‹¨ê³„

- [Agentic AI í”Œë«í¼ ì•„í‚¤í…ì²˜](./agentic-platform-architecture.md) - ì „ì²´ í”Œë«í¼ êµ¬ì„±
- [Agentic AI ì¸í”„ë¼](./agentic-ai-challenges.md) - AI ì—ì´ì „íŠ¸ ìš´ì˜ ì „ëµ

---

## ì°¸ê³  ìë£Œ

- [Karpenter ê³µì‹ ë¬¸ì„œ](https://karpenter.sh/)
- [NVIDIA DCGM Exporter](https://github.com/NVIDIA/dcgm-exporter)
- [KEDA ê³µì‹ ë¬¸ì„œ](https://keda.sh/)
- [AWS GPU ì¸ìŠ¤í„´ìŠ¤ ê°€ì´ë“œ](https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing)
