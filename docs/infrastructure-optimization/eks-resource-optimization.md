---
title: "EKS Pod ë¦¬ì†ŒìŠ¤ ìµœì í™” ê°€ì´ë“œ"
sidebar_label: "5. Pod ë¦¬ì†ŒìŠ¤ ìµœì í™”"
description: "Kubernetes Podì˜ CPU/Memory ë¦¬ì†ŒìŠ¤ ì„¤ì •, QoS í´ë˜ìŠ¤, VPA/HPA ì˜¤í† ìŠ¤ì¼€ì¼ë§, ë¦¬ì†ŒìŠ¤ Right-Sizing ì „ëµ"
tags: [eks, kubernetes, resources, cpu, memory, qos, vpa, hpa, right-sizing, optimization]
category: "performance-networking"
last_update:
  date: 2026-02-12
  author: devfloor9
sidebar_position: 5
---

# EKS Pod ë¦¬ì†ŒìŠ¤ ìµœì í™” ê°€ì´ë“œ

> ğŸ“… **ì‘ì„±ì¼**: 2026-02-12 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 20ë¶„

> **ğŸ“Œ ê¸°ì¤€ í™˜ê²½**: EKS 1.30+, Kubernetes 1.30+, Metrics Server v0.7+

## ê°œìš”

Kubernetes í™˜ê²½ì—ì„œ Pod ë¦¬ì†ŒìŠ¤ ì„¤ì •ì€ í´ëŸ¬ìŠ¤í„° íš¨ìœ¨ì„±ê³¼ ë¹„ìš©ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. **ì»¨í…Œì´ë„ˆì˜ 50%ê°€ ìš”ì²­í•œ CPUì˜ 1/3ë§Œ ì‚¬ìš©**í•˜ë©°, ì´ëŠ” í‰ê·  40-60%ì˜ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤. ì´ ê°€ì´ë“œëŠ” Pod ë ˆë²¨ ë¦¬ì†ŒìŠ¤ ìµœì í™”ë¥¼ í†µí•´ í´ëŸ¬ìŠ¤í„° íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê³  ë¹„ìš©ì„ 30-50% ì ˆê°í•˜ëŠ” ì‹¤ì „ ì „ëµì„ ì œê³µí•©ë‹ˆë‹¤.

:::info ê´€ë ¨ ë¬¸ì„œì™€ì˜ ì°¨ì´ì 
- **[karpenter-autoscaling.md](/docs/infrastructure-optimization/karpenter-autoscaling)**: ë…¸ë“œ ë ˆë²¨ ì˜¤í† ìŠ¤ì¼€ì¼ë§ (ì´ ë¬¸ì„œëŠ” Pod ë ˆë²¨)
- **[cost-management.md](/docs/infrastructure-optimization/cost-management)**: ì „ì²´ ë¹„ìš© ì „ëµ (ì´ ë¬¸ì„œëŠ” ë¦¬ì†ŒìŠ¤ ì„¤ì •ì— ì§‘ì¤‘)
- **[eks-resiliency-guide.md](/docs/operations-observability/eks-resiliency-guide)**: ë¦¬ì†ŒìŠ¤ ì„¤ì •ì„ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ìœ¼ë¡œë§Œ ë‹¤ë£¸
:::

### í•µì‹¬ ë‚´ìš©

- **Requests vs Limits ì‹¬ì¸µ ì´í•´**: CPU throttlingê³¼ OOM Kill ë©”ì»¤ë‹ˆì¦˜
- **QoS í´ë˜ìŠ¤ ì „ëµ**: Guaranteed, Burstable, BestEffortì˜ ì‹¤ì „ í™œìš©
- **VPA ì™„ë²½ ê°€ì´ë“œ**: ìë™ ë¦¬ì†ŒìŠ¤ ì¡°ì •ê³¼ HPA ê³µì¡´ íŒ¨í„´
- **Right-Sizing ë°©ë²•ë¡ **: P95 ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ì‚°ì • ë° Goldilocks í™œìš©
- **ë¹„ìš© ì˜í–¥ ë¶„ì„**: ë¦¬ì†ŒìŠ¤ ìµœì í™”ì˜ ì‹¤ì œ ì ˆê° íš¨ê³¼

### í•™ìŠµ ëª©í‘œ

ì´ ê°€ì´ë“œë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- CPUì™€ Memory requests/limitsì˜ ì •í™•í•œ ë™ì‘ ì›ë¦¬ ì´í•´
- ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë§ëŠ” QoS í´ë˜ìŠ¤ ì„ íƒ
- VPAì™€ HPAë¥¼ ì•ˆì „í•˜ê²Œ ê³µì¡´ì‹œí‚¤ëŠ” êµ¬ì„±
- ì‹¤ì œ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ Right-Sizing ìˆ˜í–‰
- ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± 30% ì´ìƒ ê°œì„ 

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### í•„ìš”í•œ ë„êµ¬

| ë„êµ¬ | ë²„ì „ | ìš©ë„ |
|------|------|------|
| kubectl | 1.28+ | Kubernetes í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ |
| helm | 3.12+ | VPA, Goldilocks ì„¤ì¹˜ |
| metrics-server | 0.7+ | ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ |
| kubectl-top | ë‚´ì¥ | ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸ |

### í•„ìš”í•œ ê¶Œí•œ

```bash
# RBAC ê¶Œí•œ í™•ì¸
kubectl auth can-i get pods --all-namespaces
kubectl auth can-i get resourcequotas
kubectl auth can-i create verticalpodautoscaler
```

### ì„ í–‰ ì§€ì‹

- Kubernetes Pod, Deployment ê¸°ë³¸ ê°œë…
- YAML ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì‘ì„± ê²½í—˜
- Linux cgroups ê¸°ë³¸ ì´í•´ (ê¶Œì¥)
- Prometheus/Grafana ê¸°ë³¸ ì‚¬ìš©ë²• (ê¶Œì¥)

## Resource Requests & Limits ì‹¬ì¸µ ì´í•´

### 2.1 Requests vs Limitsì˜ ì •í™•í•œ ì˜ë¯¸

Resource requestsì™€ limitsëŠ” Kubernetes ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ì˜ í•µì‹¬ ê°œë…ì…ë‹ˆë‹¤.

**Requests (ìš”ì²­ëŸ‰)**
- **ì •ì˜**: ìŠ¤ì¼€ì¤„ëŸ¬ê°€ Pod ë°°ì¹˜ ì‹œ ë³´ì¥í•˜ëŠ” ìµœì†Œ ë¦¬ì†ŒìŠ¤
- **ì—­í• **: ë…¸ë“œ ì„ íƒ ê¸°ì¤€, QoS í´ë˜ìŠ¤ ê²°ì •
- **ë³´ì¥**: kubeletì´ ì´ ì–‘ì„ í•­ìƒ í™•ë³´

**Limits (ì œí•œëŸ‰)**
- **ì •ì˜**: kubeletì´ ê°•ì œí•˜ëŠ” ìµœëŒ€ ë¦¬ì†ŒìŠ¤
- **ì—­í• **: ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ ë°©ì§€, ë…¸ì´ì§€ ë„¤ì´ë²„(noisy neighbor) ì œí•œ
- **ê°•ì œ**: CPUëŠ” throttling, MemoryëŠ” OOM Kill

```mermaid
graph TB
    subgraph "ë¦¬ì†ŒìŠ¤ í• ë‹¹ íë¦„"
        A[Pod ìƒì„± ìš”ì²­] --> B{Scheduler}
        B -->|Requests í™•ì¸| C[ì ì ˆí•œ ë…¸ë“œ ì„ íƒ]
        C --> D[kubelet]
        D -->|cgroups ì„¤ì •| E[Container Runtime]

        subgraph "ì‹¤í–‰ ì¤‘ ì œì–´"
            E --> F{ì‹¤ì œ ì‚¬ìš©ëŸ‰}
            F -->|CPU > Limit| G[CPU Throttling]
            F -->|Memory > Limit| H[OOM Kill]
            F -->|ì •ìƒ ë²”ìœ„| I[ì •ìƒ ì‹¤í–‰]
        end
    end

    style G fill:#ff6b6b
    style H fill:#ff0000,color:#fff
    style I fill:#51cf66
```

**í•µì‹¬ ì°¨ì´ì **

| ì†ì„± | CPU | Memory |
|------|-----|--------|
| **Requests ì´ˆê³¼ ì‹œ** | ë‹¤ë¥¸ Podê°€ ì‚¬ìš© ì•ˆ í•˜ë©´ ì‚¬ìš© ê°€ëŠ¥ | ë‹¤ë¥¸ Podê°€ ì‚¬ìš© ì•ˆ í•˜ë©´ ì‚¬ìš© ê°€ëŠ¥ |
| **Limits ì´ˆê³¼ ì‹œ** | **Throttling** (í”„ë¡œì„¸ìŠ¤ ì†ë„ ì €í•˜) | **OOM Kill** (í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ) |
| **ì••ì¶• ê°€ëŠ¥ ì—¬ë¶€** | ì••ì¶• ê°€ëŠ¥ (Compressible) | ì••ì¶• ë¶ˆê°€ (Incompressible) |
| **ì´ˆê³¼ ì‚¬ìš© ìœ„í—˜** | ì„±ëŠ¥ ì €í•˜ | ì„œë¹„ìŠ¤ ì¤‘ë‹¨ |

### 2.2 CPU ë¦¬ì†ŒìŠ¤ ê¹Šì´ ì´í•´

#### CPU Millicore ë‹¨ìœ„

```yaml
# CPU í‘œê¸°ë²•
resources:
  requests:
    cpu: "500m"    # 500 millicore = 0.5 CPU core
    cpu: "1"       # 1000 millicore = 1 CPU core
    cpu: "2.5"     # 2500 millicore = 2.5 CPU cores
```

**1 CPU core = 1000 millicore**
- AWS vCPU, Azure vCore ëª¨ë‘ ë™ì¼
- í•˜ì´í¼ìŠ¤ë ˆë”© í™˜ê²½ì—ì„œë„ ë…¼ë¦¬ ì½”ì–´ ê¸°ì¤€

#### CFS Bandwidth Throttling

Linux CFS (Completely Fair Scheduler)ëŠ” CPU limitsë¥¼ ê°•ì œí•©ë‹ˆë‹¤:

```bash
# cgroups v2 ê¸°ì¤€
/sys/fs/cgroup/cpu.max
# ì˜ˆì‹œ: "100000 100000" = 100ms ì£¼ê¸°ë‹¹ 100ms ì‚¬ìš© ê°€ëŠ¥ (100% = 1 CPU)
# ì˜ˆì‹œ: "50000 100000" = 100ms ì£¼ê¸°ë‹¹ 50ms ì‚¬ìš© ê°€ëŠ¥ (50% = 0.5 CPU)
```

**Throttling ë©”ì»¤ë‹ˆì¦˜**

```
ì‹œê°„ ì£¼ê¸°: 100ms
CPU Limit: 500m (0.5 CPU)
â†’ 100ms ì¤‘ 50msë§Œ ì‚¬ìš© ê°€ëŠ¥

ì‹¤ì œ ë™ì‘:
[0-50ms] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (ì‹¤í–‰)
[50-100ms] ...................... (throttled)
[100-150ms] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (ì‹¤í–‰)
[150-200ms] ...................... (throttled)
```

:::warning CPU Limitsë¥¼ ì„¤ì •í•˜ì§€ ì•ŠëŠ” ì „ëµ
Google, Datadog ë“± ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„° ìš´ì˜ ì¡°ì§ì€ CPU limitsë¥¼ ì„¤ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:

**ì´ìœ :**
- CPUëŠ” ì••ì¶• ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ (ë‹¤ë¥¸ Podê°€ í•„ìš”í•˜ë©´ ìë™ ì¡°ì •)
- Throttlingìœ¼ë¡œ ì¸í•œ ë¶ˆí•„ìš”í•œ ì„±ëŠ¥ ì €í•˜ ë°©ì§€
- Requestsë§Œìœ¼ë¡œë„ ìŠ¤ì¼€ì¤„ë§ê³¼ QoS ì œì–´ ê°€ëŠ¥

**ëŒ€ì‹  ê¶Œì¥:**
- CPU requestsëŠ” P95 ì‚¬ìš©ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
- HPAë¡œ ë¶€í•˜ì— ë”°ë¥¸ ìˆ˜í‰ í™•ì¥
- Node-level ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ê°•í™”

**ì˜ˆì™¸ (Limits ì„¤ì • í•„ìš”):**
- ë°°ì¹˜ ì‘ì—… (CPU ë…ì  ë°©ì§€)
- ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì›Œí¬ë¡œë“œ
- ë©€í‹°í…Œë„ŒíŠ¸ í™˜ê²½
:::

#### CPU ë¦¬ì†ŒìŠ¤ ì„¤ì • ì˜ˆì‹œ

```yaml
# íŒ¨í„´ 1: Requestsë§Œ ì„¤ì • (ê¶Œì¥)
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    resources:
      requests:
        cpu: "250m"       # P95 ì‚¬ìš©ëŸ‰ ê¸°ì¤€
        memory: "128Mi"
      # limits ìƒëµ - CPU ì••ì¶• ê°€ëŠ¥ ë¦¬ì†ŒìŠ¤ í™œìš©

---
# íŒ¨í„´ 2: ë°°ì¹˜ ì‘ì—… (Limits ì„¤ì •)
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing
spec:
  template:
    spec:
      containers:
      - name: processor
        image: data-processor:v1
        resources:
          requests:
            cpu: "1000m"
          limits:
            cpu: "2000m"   # CPU ë…ì  ë°©ì§€
            memory: "4Gi"
      restartPolicy: OnFailure
```

### 2.3 Memory ë¦¬ì†ŒìŠ¤ ê¹Šì´ ì´í•´

#### Memory ë‹¨ìœ„

```yaml
# Memory í‘œê¸°ë²• (1024 ê¸°ë°˜ vs 1000 ê¸°ë°˜)
resources:
  requests:
    memory: "128Mi"    # 128 * 1024^2 bytes = 134,217,728 bytes
    memory: "128M"     # 128 * 1000^2 bytes = 128,000,000 bytes
    memory: "1Gi"      # 1 * 1024^3 bytes = 1,073,741,824 bytes
    memory: "1G"       # 1 * 1000^3 bytes = 1,000,000,000 bytes
```

**ê¶Œì¥**: **Mi, Gi ì‚¬ìš©** (1024 ê¸°ë°˜, Kubernetes í‘œì¤€)

#### OOM Kill ë©”ì»¤ë‹ˆì¦˜

Memory limits ì´ˆê³¼ ì‹œ Linux OOM Killerê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤:

```
ì‹¤ì œ ì‚¬ìš©ëŸ‰ > Memory Limit
â†’ cgroup memory.max ì´ˆê³¼
â†’ Kernel OOM Killer ë°œë™
â†’ í”„ë¡œì„¸ìŠ¤ SIGKILL
â†’ Pod ìƒíƒœ: OOMKilled
â†’ kubeletì´ Pod ì¬ì‹œì‘ (RestartPolicy ë”°ë¦„)
```

**OOM Score ê³„ì‚°**

```bash
# í”„ë¡œì„¸ìŠ¤ë³„ OOM Score í™•ì¸
cat /proc/<PID>/oom_score

# OOM Score ê³„ì‚° ìš”ì†Œ
# 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ë†’ì„ìˆ˜ë¡ ì ìˆ˜ ë†’ìŒ)
# 2. oom_score_adj ê°’ (QoS í´ë˜ìŠ¤ë³„ë¡œ ë‹¤ë¦„)
# 3. ë£¨íŠ¸ í”„ë¡œì„¸ìŠ¤ ë³´í˜¸ (-1000 = ì ˆëŒ€ Kill ì•ˆ í•¨)
```

:::danger Memory limitsëŠ” ë°˜ë“œì‹œ ì„¤ì •
MemoryëŠ” ì••ì¶• ë¶ˆê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ì´ë¯€ë¡œ **ë°˜ë“œì‹œ limits ì„¤ì • í•„ìš”**:

**ì´ìœ :**
- Memory ê³ ê°ˆ ì‹œ ì „ì²´ ë…¸ë“œ ë¶ˆì•ˆì •
- Kernel Panic ê°€ëŠ¥ì„±
- ë‹¤ë¥¸ Podì— ì˜í–¥ (ë…¸ë“œ Eviction)

**ê¶Œì¥ ì„¤ì •:**
- `requests = limits` (Guaranteed QoS)
- ë˜ëŠ” `limits = requests * 1.5` (Burstable QoS)
- JVM ì• í”Œë¦¬ì¼€ì´ì…˜: Heap í¬ê¸°ëŠ” limitsì˜ 75%ë¡œ ì„¤ì •
:::

#### Memory ë¦¬ì†ŒìŠ¤ ì„¤ì • ì˜ˆì‹œ

```yaml
# íŒ¨í„´ 1: Guaranteed QoS (ì•ˆì •ì„± ìµœìš°ì„ )
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: postgres
        image: postgres:16
        resources:
          requests:
            cpu: "2000m"
            memory: "4Gi"
          limits:
            cpu: "2000m"      # requestsì™€ ë™ì¼
            memory: "4Gi"     # requestsì™€ ë™ì¼ (Guaranteed)

---
# íŒ¨í„´ 2: JVM ì• í”Œë¦¬ì¼€ì´ì…˜
apiVersion: apps/v1
kind: Deployment
metadata:
  name: java-app
spec:
  template:
    spec:
      containers:
      - name: app
        image: java-app:v1
        env:
        - name: JAVA_OPTS
          value: "-Xmx3072m -Xms3072m"  # limitsì˜ 75% (4Gi * 0.75 = 3Gi)
        resources:
          requests:
            memory: "4Gi"
          limits:
            memory: "4Gi"

---
# íŒ¨í„´ 3: Node.js ì• í”Œë¦¬ì¼€ì´ì…˜
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-api
spec:
  template:
    spec:
      containers:
      - name: api
        image: nodejs-api:v2
        env:
        - name: NODE_OPTIONS
          value: "--max-old-space-size=896"  # limitsì˜ 70% (1280Mi * 0.7 = 896Mi)
        resources:
          requests:
            memory: "1280Mi"
          limits:
            memory: "1280Mi"
```

### 2.4 Ephemeral Storage

ì»¨í…Œì´ë„ˆ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ë„ ë¦¬ì†ŒìŠ¤ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-demo
spec:
  containers:
  - name: app
    image: busybox
    resources:
      requests:
        ephemeral-storage: "2Gi"    # ìµœì†Œ ë³´ì¥
      limits:
        ephemeral-storage: "4Gi"    # ìµœëŒ€ ì‚¬ìš©ëŸ‰
    volumeMounts:
    - name: cache
      mountPath: /cache
  volumes:
  - name: cache
    emptyDir:
      sizeLimit: "4Gi"
```

**Ephemeral Storage í¬í•¨ í•­ëª©:**
- ì»¨í…Œì´ë„ˆ ë ˆì´ì–´ ì“°ê¸°
- ë¡œê·¸ íŒŒì¼ (`/var/log`)
- emptyDir ë³¼ë¥¨
- ì„ì‹œ íŒŒì¼

**ë…¸ë“œ Eviction Threshold:**

```yaml
# kubelet ì„¤ì •
evictionHard:
  nodefs.available: "10%"      # ë…¸ë“œ ì „ì²´ ë””ìŠ¤í¬ 10% ë¯¸ë§Œ ì‹œ eviction
  nodefs.inodesFree: "5%"      # inode 5% ë¯¸ë§Œ ì‹œ eviction
  imagefs.available: "10%"     # ì´ë¯¸ì§€ íŒŒì¼ì‹œìŠ¤í…œ 10% ë¯¸ë§Œ ì‹œ eviction
```

### 2.5 EKS Auto Mode ë¦¬ì†ŒìŠ¤ ìµœì í™”

EKS Auto ModeëŠ” Kubernetes í´ëŸ¬ìŠ¤í„° ìš´ì˜ì˜ ë³µì¡ì„±ì„ ê·¹ì ìœ¼ë¡œ ì¤„ì´ëŠ” ì™„ì „ ê´€ë¦¬í˜• ì†”ë£¨ì…˜ì…ë‹ˆë‹¤. ì»´í“¨íŒ…, ìŠ¤í† ë¦¬ì§€, ë„¤íŠ¸ì›Œí‚¹ì˜ í”„ë¡œë¹„ì €ë‹ë¶€í„° ì§€ì†ì  ìœ ì§€ë³´ìˆ˜ê¹Œì§€ ìë™í™”í•˜ì—¬ ìš´ì˜íŒ€ì´ ì¸í”„ë¼ ê´€ë¦¬ ëŒ€ì‹  ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

#### 2.5.1 Auto Mode ê°œìš”

**í•µì‹¬ ê¸°ëŠ¥:**
- **ë‹¨ì¼ í´ë¦­ í™œì„±í™”**: í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ `--compute-config autoMode` í”Œë˜ê·¸ë§Œìœ¼ë¡œ í™œì„±í™”
- **ìë™ ì¸í”„ë¼ í”„ë¡œë¹„ì €ë‹**: Pod ìŠ¤ì¼€ì¤„ë§ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ìµœì  ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìë™ ì„ íƒ
- **ì§€ì†ì  ìœ ì§€ë³´ìˆ˜**: OS íŒ¨ì¹˜, ë³´ì•ˆ ì—…ë°ì´íŠ¸, ì½”ì–´ ì• ë“œì˜¨ ê´€ë¦¬ ìë™í™”
- **ë¹„ìš© ìµœì í™”**: Graviton í”„ë¡œì„¸ì„œì™€ Spot ì¸ìŠ¤í„´ìŠ¤ ìë™ í™œìš©
- **í†µí•© ë³´ì•ˆ**: AWS ë³´ì•ˆ ì„œë¹„ìŠ¤ ê¸°ë³¸ í†µí•©

```bash
# Auto Mode í´ëŸ¬ìŠ¤í„° ìƒì„±
aws eks create-cluster \
  --name my-auto-cluster \
  --compute-config autoMode=ENABLED \
  --kubernetes-network-config serviceIpv4Cidr=10.100.0.0/16 \
  --access-config bootstrapClusterCreatorAdminPermissions=true
```

:::info Auto Mode vs ìˆ˜ë™ ê´€ë¦¬
Auto ModeëŠ” ê¸°ì¡´ ìˆ˜ë™ ê´€ë¦¬ ë°©ì‹ì„ ì™„ì „íˆ ëŒ€ì²´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ìš´ì˜ ì˜¤ë²„í—¤ë“œë¥¼ ìµœì†Œí™”í•˜ë ¤ëŠ” íŒ€ì„ ìœ„í•œ **ë³´ì™„ì  ì„ íƒì§€**ì…ë‹ˆë‹¤. ì„¸ë°€í•œ ì œì–´ê°€ í•„ìš”í•œ ê²½ìš° ì—¬ì „íˆ ìˆ˜ë™ ê´€ë¦¬ ë°©ì‹ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

#### 2.5.2 Auto Mode vs ìˆ˜ë™ ê´€ë¦¬ ë¹„êµ

| í•­ëª© | ìˆ˜ë™ ê´€ë¦¬ | Auto Mode |
|------|----------|-----------|
| **ë…¸ë“œ í”„ë¡œë¹„ì €ë‹** | Managed Node Group, Self-managed, Karpenter ì§ì ‘ êµ¬ì„± | ìë™ í”„ë¡œë¹„ì €ë‹ (EC2 Managed Instances ê¸°ë°˜) |
| **ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ** | ìˆ˜ë™ ì„ íƒ ë° NodePool êµ¬ì„± | Pod ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ìë™ ì„ íƒ (Graviton ìš°ì„ ) |
| **VPA ì„¤ì •** | ìˆ˜ë™ ì„¤ì¹˜ ë° êµ¬ì„± í•„ìš” | í•„ìš” ì—†ìŒ (ìë™ ë¦¬ì†ŒìŠ¤ ìµœì í™”) |
| **HPA ì„¤ì •** | ìˆ˜ë™ ì„¤ì • ë° ë©”íŠ¸ë¦­ êµ¬ì„± | ìë™ êµ¬ì„± ê°€ëŠ¥ (ê°œë°œìëŠ” ì„ ì–¸ë§Œ) |
| **OS íŒ¨ì¹˜** | ìˆ˜ë™ ë˜ëŠ” ìë™í™” ìŠ¤í¬ë¦½íŠ¸ | ì™„ì „ ìë™ (ë¬´ì¤‘ë‹¨) |
| **ë³´ì•ˆ ì—…ë°ì´íŠ¸** | ìˆ˜ë™ ì ìš© | ìë™ ì ìš© |
| **ì½”ì–´ ì• ë“œì˜¨ ê´€ë¦¬** | ìˆ˜ë™ ì—…ê·¸ë ˆì´ë“œ (CoreDNS, kube-proxy, VPC CNI) | ìë™ ì—…ê·¸ë ˆì´ë“œ |
| **ë¹„ìš© ìµœì í™”** | Spot, Graviton ìˆ˜ë™ êµ¬ì„± | ìë™ í™œìš© (ìµœëŒ€ 90% ì ˆê°) |
| **Request/Limit ì„¤ì •** | ê°œë°œì ì±…ì„ (í•„ìˆ˜) | ê°œë°œì ì±…ì„ (ì—¬ì „íˆ í•„ìˆ˜) |
| **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±** | VPA Off ëª¨ë“œ + ìˆ˜ë™ ì ìš© | ìë™ Right-Sizing (ì§€ì†ì ) |
| **í•™ìŠµ ê³¡ì„ ** | ë†’ìŒ (Kubernetes, AWS ì „ë¬¸ ì§€ì‹ í•„ìš”) | ë‚®ìŒ (Kubernetes ê¸°ë³¸ë§Œ í•„ìš”) |
| **ìš´ì˜ ì˜¤ë²„í—¤ë“œ** | ë†’ìŒ | ìµœì†Œ |

:::warning Auto Modeì—ì„œë„ ê°œë°œì ì±…ì„
Auto ModeëŠ” ì¸í”„ë¼ë¥¼ ìë™í™”í•˜ì§€ë§Œ, **Pod-level requests/limits ì„¤ì •ì€ ì—¬ì „íˆ ê°œë°œìì˜ ì±…ì„**ì…ë‹ˆë‹¤. ì´ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì‹¤ì œ ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ì„ ê°€ì¥ ì˜ ì•„ëŠ” ì‚¬ëŒì´ ê°œë°œìì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
:::

#### 2.5.3 Graviton + Spot ì¡°í•© ìµœì í™”

Auto ModeëŠ” AWS Graviton í”„ë¡œì„¸ì„œì™€ Spot ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ì¡°í•©í•˜ì—¬ ë¹„ìš© íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

**Graviton í”„ë¡œì„¸ì„œì˜ ì¥ì :**
- **40% í–¥ìƒëœ ê°€ê²© ëŒ€ë¹„ ì„±ëŠ¥** (x86 ëŒ€ë¹„)
- ë²”ìš© ì›Œí¬ë¡œë“œ, ì›¹ ì„œë²„, ì»¨í…Œì´ë„ˆí™”ëœ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ì— ìµœì 
- Arm64 ì•„í‚¤í…ì²˜ ì§€ì› (ëŒ€ë¶€ë¶„ì˜ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ í˜¸í™˜)

**Spot ì¸ìŠ¤í„´ìŠ¤ ì ˆê°:**
- **ìµœëŒ€ 90% ë¹„ìš© ì ˆê°** (On-Demand ëŒ€ë¹„)
- Auto Modeê°€ ìë™ìœ¼ë¡œ Spot ê°€ìš©ì„± ëª¨ë‹ˆí„°ë§ ë° Fallback ì²˜ë¦¬
- ì¤‘ë‹¨ 2ë¶„ ì „ ì•Œë¦¼ìœ¼ë¡œ Graceful Termination ë³´ì¥

```mermaid
graph TB
    subgraph "Auto Mode ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ ë¡œì§"
        A[Pod ìŠ¤ì¼€ì¤„ë§ ìš”ì²­] --> B{ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ ë¶„ì„}
        B --> C[Graviton Spot ìš°ì„  ì‹œë„]
        C --> D{Spot ê°€ìš©ì„± í™•ì¸}
        D -->|ê°€ìš©| E[Graviton Spot ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œë¹„ì €ë‹]
        D -->|ë¶ˆê°€| F[Graviton On-Demand ì‹œë„]
        F --> G{On-Demand ê°€ìš©ì„±}
        G -->|ê°€ìš©| H[Graviton On-Demand í”„ë¡œë¹„ì €ë‹]
        G -->|ë¶ˆê°€| I[x86 Spot/On-Demand Fallback]

        E --> J[Pod ë°°ì¹˜ ì™„ë£Œ]
        H --> J
        I --> J
    end

    style E fill:#51cf66
    style H fill:#ffa94d
    style I fill:#ff6b6b
```

**NodePool YAML ì˜ˆì‹œ (ìˆ˜ë™ ê´€ë¦¬ í´ëŸ¬ìŠ¤í„° - Karpenter ê¸°ë°˜):**

```yaml
# Auto ModeëŠ” ì´ëŸ¬í•œ NodePoolì„ ìë™ ìƒì„±í•˜ì§€ë§Œ,
# ì°¸ê³ ë¥¼ ìœ„í•´ ìˆ˜ë™ ì„¤ì • ì‹œ Graviton + Spot íŒ¨í„´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: graviton-spot-pool
spec:
  template:
    spec:
      requirements:
      # Graviton ì¸ìŠ¤í„´ìŠ¤ ìš°ì„ 
      - key: kubernetes.io/arch
        operator: In
        values: ["arm64"]

      # Spot ìš°ì„ , Fallbackìœ¼ë¡œ On-Demand
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot", "on-demand"]

      # ë²”ìš© ì›Œí¬ë¡œë“œìš© ì¸ìŠ¤í„´ìŠ¤ íŒ¨ë°€ë¦¬
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["m7g.medium", "m7g.large", "m7g.xlarge", "m7g.2xlarge"]

      nodeClassRef:
        name: default

  # Spot ì¤‘ë‹¨ ì²˜ë¦¬
  disruption:
    consolidationPolicy: WhenUnderutilized
    expireAfter: 720h

  # ë¦¬ì†ŒìŠ¤ ì œí•œ
  limits:
    cpu: "1000"
    memory: "1000Gi"

---
# Fallback: x86 On-Demand (Spot ë¶ˆê°€ ì‹œ)
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: x86-ondemand-fallback
spec:
  weight: 10  # ë‚®ì€ ìš°ì„ ìˆœìœ„
  template:
    spec:
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]

      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand"]

      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["m6i.large", "m6i.xlarge", "m6i.2xlarge"]

      nodeClassRef:
        name: default
```

**Auto Modeì—ì„œì˜ ìë™ ì²˜ë¦¬:**

Auto ModeëŠ” ìœ„ì™€ ê°™ì€ NodePool êµ¬ì„±ì„ ìˆ˜ë™ìœ¼ë¡œ ì‘ì„±í•  í•„ìš” ì—†ì´, Podì˜ ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ê³¼ ì›Œí¬ë¡œë“œ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ìµœì  ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

```yaml
# Auto Mode í™˜ê²½ì—ì„œ ê°œë°œìê°€ ì‘ì„±í•˜ëŠ” Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: production
spec:
  replicas: 10
  template:
    spec:
      containers:
      - name: nginx
        image: nginx:1.25-arm64  # Gravitonìš© ì´ë¯¸ì§€
        resources:
          requests:
            cpu: "250m"
            memory: "512Mi"
          limits:
            memory: "1Gi"

      # Auto Modeê°€ ìë™ìœ¼ë¡œ:
      # 1. Graviton Spot ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ ì‹œë„
      # 2. Spot ë¶ˆê°€ ì‹œ Graviton On-Demandë¡œ Fallback
      # 3. ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìë™ ì„ íƒ (m7g.large ë“±)
      # 4. ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ë° Pod ë°°ì¹˜
```

:::tip Graviton ì´ë¯¸ì§€ ì¤€ë¹„
Graviton ì¸ìŠ¤í„´ìŠ¤ë¥¼ í™œìš©í•˜ë ¤ë©´ **arm64 ì•„í‚¤í…ì²˜ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€**ê°€ í•„ìš”í•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê³µì‹ ì´ë¯¸ì§€ëŠ” multi-archë¥¼ ì§€ì›í•˜ë¯€ë¡œ, ë™ì¼í•œ ì´ë¯¸ì§€ íƒœê·¸ë¡œ Gravitonê³¼ x86 ëª¨ë‘ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```bash
# multi-arch ì´ë¯¸ì§€ í™•ì¸
docker manifest inspect nginx:1.25 | jq '.manifests[].platform'

# ì¶œë ¥ ì˜ˆì‹œ:
# { "architecture": "amd64", "os": "linux" }
# { "architecture": "arm64", "os": "linux" }
```
:::

**ì‹¤ì œ ë¹„ìš© ì ˆê° ì˜ˆì‹œ:**

| ì‹œë‚˜ë¦¬ì˜¤ | ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | ì‹œê°„ë‹¹ ë¹„ìš© | ì›”ê°„ ë¹„ìš© (730ì‹œê°„) | ì ˆê°ë¥  |
|---------|-------------|-----------|-------------------|--------|
| x86 On-Demand | m6i.2xlarge | $0.384 | $280.32 | - |
| Graviton On-Demand | m7g.2xlarge | $0.3264 | $238.27 | 15% |
| Graviton Spot | m7g.2xlarge | $0.0979 | $71.47 | 75% |

10ê°œ ë…¸ë“œ ê¸°ì¤€:
- x86 On-Demand: $2,803/ì›”
- Graviton On-Demand: $2,383/ì›” (15% ì ˆê°)
- **Graviton Spot: $715/ì›” (75% ì ˆê°)** â­

**Graviton4 íŠ¹í™” ìµœì í™”:**

Graviton4 (R8g, M8g, C8g) ì¸ìŠ¤í„´ìŠ¤ëŠ” Graviton3 ëŒ€ë¹„ **30% í–¥ìƒëœ ì»´í“¨íŒ… ì„±ëŠ¥**ê³¼ **75% í–¥ìƒëœ ë©”ëª¨ë¦¬ ëŒ€ì—­í­**ì„ ì œê³µí•©ë‹ˆë‹¤.

| ì„¸ëŒ€ | ì¸ìŠ¤í„´ìŠ¤ íŒ¨ë°€ë¦¬ | ì„±ëŠ¥ ê°œì„  | ì£¼ìš” ì›Œí¬ë¡œë“œ |
|------|---------------|---------|-------------|
| Graviton3 | m7g, c7g, r7g | ê¸°ì¤€ | ë²”ìš© ì›¹/API, ì»¨í…Œì´ë„ˆ |
| **Graviton4** | **m8g, c8g, r8g** | **+30% ì»´í“¨íŒ…, +75% ë©”ëª¨ë¦¬** | **ê³ ì„±ëŠ¥ ë°ì´í„°ë² ì´ìŠ¤, ML ì¶”ë¡ , ì‹¤ì‹œê°„ ë¶„ì„** |

**ARM64 Multi-Arch ë¹Œë“œ íŒŒì´í”„ë¼ì¸:**

Graviton ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ë ¤ë©´ ARM64ì™€ AMD64ë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” multi-arch ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.

```dockerfile
# Multi-arch Dockerfile ì˜ˆì‹œ
FROM --platform=$BUILDPLATFORM golang:1.22-alpine AS builder
ARG TARGETOS TARGETARCH

WORKDIR /app
COPY . .

# íƒ€ê²Ÿ ì•„í‚¤í…ì²˜ì— ë§ê²Œ ë¹Œë“œ
RUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o app .

# ëŸ°íƒ€ì„ ì´ë¯¸ì§€
FROM alpine:3.19
COPY --from=builder /app/app /usr/local/bin/app
ENTRYPOINT ["/usr/local/bin/app"]
```

**GitHub Actions CI/CDì—ì„œ multi-arch ë¹Œë“œ:**

```yaml
# .github/workflows/build.yml
name: Build Multi-Arch Image
on:
  push:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push multi-arch
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64,linux/arm64  # ARM64 í¬í•¨
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/myapp:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/myapp:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
```

**Graviton3 â†’ Graviton4 ë§ˆì´ê·¸ë ˆì´ì…˜ ë²¤ì¹˜ë§ˆí¬ í¬ì¸íŠ¸:**

```yaml
# Graviton4 ìš°ì„  NodePool ì˜ˆì‹œ (Karpenter)
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: graviton4-spot-pool
spec:
  template:
    spec:
      requirements:
      # Graviton4 ìš°ì„ , Graviton3 Fallback
      - key: node.kubernetes.io/instance-type
        operator: In
        values:
          # Graviton4 (ìµœìš°ì„ )
          - "m8g.medium"
          - "m8g.large"
          - "m8g.xlarge"
          - "m8g.2xlarge"
          # Graviton3 (Fallback)
          - "m7g.medium"
          - "m7g.large"
          - "m7g.xlarge"
          - "m7g.2xlarge"

      - key: kubernetes.io/arch
        operator: In
        values: ["arm64"]

      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot", "on-demand"]

      nodeClassRef:
        name: default

  disruption:
    consolidationPolicy: WhenUnderutilized
    consolidateAfter: 30s

  limits:
    cpu: "1000"
    memory: "2000Gi"
```

**Graviton4 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì²´í¬í¬ì¸íŠ¸:**

ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œ ë‹¤ìŒ ë©”íŠ¸ë¦­ì„ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ì„±ëŠ¥ ê°œì„ ì„ ê²€ì¦í•©ë‹ˆë‹¤:

| ë©”íŠ¸ë¦­ | Graviton3 ê¸°ì¤€ | Graviton4 ëª©í‘œ | ì¸¡ì • ë°©ë²• |
|-------|--------------|--------------|---------|
| **P99 ì‘ë‹µ ì‹œê°„** | 100ms | 70ms (-30%) | Prometheus `http_request_duration_seconds` |
| **ì²˜ë¦¬ëŸ‰ (RPS)** | 1000 req/s | 1300 req/s (+30%) | Load testing (k6, Locust) |
| **ë©”ëª¨ë¦¬ ëŒ€ì—­í­** | 205 GB/s | 358 GB/s (+75%) | `sysbench memory` |
| **CPU ì‚¬ìš©ë¥ ** | 60% | 45% (-25%) | `node_cpu_seconds_total` |

```bash
# Graviton4 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
# 1. ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í…ŒìŠ¤íŠ¸
sysbench memory --memory-total-size=100G --memory-oper=write run

# 2. CPU ë²¤ì¹˜ë§ˆí¬
sysbench cpu --cpu-max-prime=20000 --threads=8 run

# 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ë¶€í•˜ í…ŒìŠ¤íŠ¸ (k6)
k6 run --vus 100 --duration 5m loadtest.js

# 4. Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
curl -s http://localhost:9090/api/v1/query?query=rate(http_request_duration_seconds_sum[5m]) | jq .
```

:::tip Graviton4 ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€**: ARM64 ì§€ì› í™•ì¸ (`docker manifest inspect`)
- [ ] **ì˜ì¡´ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬**: ARM64 í˜¸í™˜ì„± ê²€ì¦
- [ ] **CI/CD íŒŒì´í”„ë¼ì¸**: Multi-arch ë¹Œë“œ í™œì„±í™”
- [ ] **NodePool ìš°ì„ ìˆœìœ„**: Graviton4 â†’ Graviton3 â†’ x86 ìˆœì„œ ì„¤ì •
- [ ] **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**: P99 ë ˆì´í„´ì‹œ, ì²˜ë¦¬ëŸ‰, CPU ì‚¬ìš©ë¥  ì¸¡ì •
- [ ] **ë¹„ìš© ë¶„ì„**: Graviton3 ëŒ€ë¹„ ê°€ê²©/ì„±ëŠ¥ ë¹„ìœ¨ ê³„ì‚°
:::

#### 2.5.4 Auto Mode í™˜ê²½ì˜ ë¦¬ì†ŒìŠ¤ ì„¤ì • ê¶Œì¥ì‚¬í•­

Auto ModeëŠ” ë§ì€ ë¶€ë¶„ì„ ìë™í™”í•˜ì§€ë§Œ, ê°œë°œìëŠ” ì—¬ì „íˆ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ì„ ì •í™•íˆ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

**Auto Modeê°€ ìë™ ì²˜ë¦¬í•˜ëŠ” í•­ëª©:**

| í•­ëª© | ìˆ˜ë™ ê´€ë¦¬ | Auto Mode |
|------|----------|-----------|
| ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ | Karpenter, Managed Node Group ì„¤ì • | ìë™ |
| ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ | NodePoolì—ì„œ ìˆ˜ë™ ì§€ì • | Pod requests ê¸°ë°˜ ìë™ ì„ íƒ |
| Spot/On-Demand ì „í™˜ | ìˆ˜ë™ ë˜ëŠ” Karpenter ì„¤ì • | ìë™ Fallback |
| ë…¸ë“œ ìŠ¤ì¼€ì¼ë§ | HPA + Cluster Autoscaler/Karpenter | ìë™ |
| OS íŒ¨ì¹˜ | ìˆ˜ë™ ë˜ëŠ” ìë™í™” ìŠ¤í¬ë¦½íŠ¸ | ìë™ (ë¬´ì¤‘ë‹¨) |

**ê°œë°œìê°€ ì—¬ì „íˆ ì„¤ì •í•´ì•¼ í•˜ëŠ” í•­ëª©:**

| í•­ëª© | ì´ìœ  | ê¶Œì¥ ë°©ë²• |
|------|------|----------|
| **CPU Requests** | ìŠ¤ì¼€ì¤„ë§ ê²°ì • ê¸°ì¤€ | P95 ì‚¬ìš©ëŸ‰ + 20% |
| **Memory Requests** | ìŠ¤ì¼€ì¤„ë§ ë° OOM ë°©ì§€ | P95 ì‚¬ìš©ëŸ‰ + 20% |
| **Memory Limits** | OOM Kill ë°©ì§€ (í•„ìˆ˜) | Requests Ã— 1.5~2 |
| **CPU Limits** | ì¼ë°˜ ì›Œí¬ë¡œë“œëŠ” ë¯¸ì„¤ì • ê¶Œì¥ | ë°°ì¹˜ ì‘ì—…ë§Œ ì„¤ì • |
| **HPA ë©”íŠ¸ë¦­** | ìˆ˜í‰ í™•ì¥ ê¸°ì¤€ | CPU 70%, Custom Metrics |

**Auto Mode í™˜ê²½ì—ì„œì˜ VPA ì—­í•  ë³€í™”:**

```mermaid
graph TB
    subgraph "ìˆ˜ë™ ê´€ë¦¬ í´ëŸ¬ìŠ¤í„°"
        A1[VPA Recommender] --> A2[ê¶Œì¥ì‚¬í•­ ìƒì„±]
        A2 --> A3[VPA Updater]
        A3 --> A4[Pod ì¬ì‹œì‘ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ë³€ê²½]
    end

    subgraph "Auto Mode í´ëŸ¬ìŠ¤í„°"
        B1[ë‚´ì¥ Right-Sizing ì—”ì§„] --> B2[ì§€ì†ì  ì‚¬ìš©ëŸ‰ ë¶„ì„]
        B2 --> B3[ìë™ ë¦¬ì†ŒìŠ¤ ìµœì í™”]
        B3 --> B4[ê°œë°œì ê¶Œì¥ì‚¬í•­ ì œê³µ]
        B4 --> B5[ê°œë°œìê°€ Deployment ì—…ë°ì´íŠ¸]
    end

    style A4 fill:#ffa94d
    style B3 fill:#51cf66
```

**Auto Modeì—ì„œ VPAëŠ”:**
- ë³„ë„ ì„¤ì¹˜ ë¶ˆí•„ìš”
- ë‚´ì¥ Right-Sizing ì—”ì§„ì´ ì§€ì†ì ìœ¼ë¡œ ì›Œí¬ë¡œë“œ ë¶„ì„
- ê°œë°œìì—ê²Œ ê¶Œì¥ì‚¬í•­ ì œê³µ (ìë™ ì ìš© ëŒ€ì‹ )
- ê°œë°œìê°€ ê²€í†  í›„ Deployment ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì— ë°˜ì˜

**ê¶Œì¥ ì›Œí¬í”Œë¡œìš°:**

```bash
# 1. Auto Mode í´ëŸ¬ìŠ¤í„°ì— ë°°í¬
kubectl apply -f deployment.yaml

# 2. 7-14ì¼ í›„ Auto Mode ëŒ€ì‹œë³´ë“œì—ì„œ ê¶Œì¥ì‚¬í•­ í™•ì¸
# (AWS Console â†’ EKS â†’ Clusters â†’ <cluster-name> â†’ Insights)

# 3. ê¶Œì¥ì‚¬í•­ì„ Deploymentì— ë°˜ì˜
kubectl set resources deployment web-app \
  --requests=cpu=300m,memory=512Mi \
  --limits=memory=1Gi

# 4. GitOpsë¡œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
git add deployment.yaml
git commit -m "chore: apply Auto Mode resource recommendations"
git push
```

:::tip Auto Mode ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤
Auto ModeëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì— íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤:

- **ì‹ ê·œ í´ëŸ¬ìŠ¤í„°**: ê¸°ì¡´ ì¸í”„ë¼ ì—†ì´ ë¹ ë¥´ê²Œ ì‹œì‘
- **ìš´ì˜ ë¦¬ì†ŒìŠ¤ ë¶€ì¡±**: ì†Œê·œëª¨ íŒ€ì—ì„œ Kubernetes ì „ë¬¸ê°€ ì—†ì´ ìš´ì˜
- **ë¹„ìš© ìµœì í™” ìš°ì„ **: Graviton + Spot ìë™ í™œìš©ìœ¼ë¡œ ì¦‰ì‹œ ì ˆê°
- **í‘œì¤€í™”ëœ ì›Œí¬ë¡œë“œ**: ì¼ë°˜ì ì¸ ì›¹/API ì„œë²„, ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

**ìˆ˜ë™ ê´€ë¦¬ ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤:**
- **ì„¸ë°€í•œ ì œì–´ í•„ìš”**: íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…, AZ ë°°ì¹˜, ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
- **ê¸°ì¡´ Karpenter íˆ¬ì**: ê³ ë„í™”ëœ NodePool ì •ì±… ë³´ìœ 
- **ê·œì œ ìš”êµ¬ì‚¬í•­**: íŠ¹ì • í•˜ë“œì›¨ì–´, ë³´ì•ˆ ê·¸ë£¹ ê°•ì œ
:::

**Auto Mode + ìˆ˜ë™ Right-Sizing ë¹„êµ:**

| í•­ëª© | ìˆ˜ë™ Right-Sizing (VPA Off) | Auto Mode |
|------|---------------------------|-----------|
| ì´ˆê¸° ì„¤ì • ë³µì¡ë„ | ë†’ìŒ (VPA ì„¤ì¹˜, Prometheus êµ¬ì„±) | ë‚®ìŒ (í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ í”Œë˜ê·¸ë§Œ) |
| ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹œê°„ | 7-14ì¼ | 7-14ì¼ (ë™ì¼) |
| ê¶Œì¥ì‚¬í•­ ì •í™•ë„ | ë†’ìŒ (Prometheus ê¸°ë°˜) | ë†’ìŒ (ë‚´ì¥ ë¶„ì„ ì—”ì§„) |
| ì ìš© ë°©ì‹ | ìˆ˜ë™ (ê°œë°œìê°€ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìˆ˜ì •) | ìˆ˜ë™ (ê°œë°œìê°€ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìˆ˜ì •) |
| ì§€ì†ì  ëª¨ë‹ˆí„°ë§ | ìˆ˜ë™ (ì£¼ê¸°ì  VPA í™•ì¸) | ìë™ (ëŒ€ì‹œë³´ë“œ ì•Œë¦¼) |
| ì¸í”„ë¼ ìµœì í™” | ìˆ˜ë™ (Karpenter ì„¤ì •) | ìë™ (Graviton + Spot) |
| ì´ ìš´ì˜ ì˜¤ë²„í—¤ë“œ | ë†’ìŒ | ë‚®ìŒ |

**ê²°ë¡ :**

Auto ModeëŠ” **ë¦¬ì†ŒìŠ¤ ìµœì í™”ì˜ ë³µì¡ì„±ì„ ì œê±°**í•˜ì§€ë§Œ, **ë¦¬ì†ŒìŠ¤ ì„¤ì •ì˜ ì±…ì„ì€ ì œê±°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**. ê°œë°œìëŠ” ì—¬ì „íˆ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ requests/limitsë¥¼ ì„¤ì •í•´ì•¼ í•˜ë©°, Auto ModeëŠ” ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ì¸í”„ë¼ë¥¼ ìë™ìœ¼ë¡œ í”„ë¡œë¹„ì €ë‹í•©ë‹ˆë‹¤.

ì´ëŠ” **"ê°œë°œìëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ìš”êµ¬ì‚¬í•­ ì •ì˜, AWSëŠ” ì¸í”„ë¼ ê´€ë¦¬"**ë¼ëŠ” ëª…í™•í•œ ì±…ì„ ë¶„ë¦¬ë¥¼ í†µí•´, ì–‘ì¸¡ ëª¨ë‘ê°€ ìì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

## QoS (Quality of Service) í´ë˜ìŠ¤

### 3.1 ì„¸ ê°€ì§€ QoS í´ë˜ìŠ¤

KubernetesëŠ” ë¦¬ì†ŒìŠ¤ ì„¤ì •ì— ë”°ë¼ Podë¥¼ 3ê°€ì§€ QoS í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤:

#### Guaranteed (ìµœê³  ìš°ì„ ìˆœìœ„)

**ì¡°ê±´:**
- ëª¨ë“  ì»¨í…Œì´ë„ˆì— CPUì™€ Memory requestsì™€ limits ì„¤ì •
- **requests == limits** (ë™ì¼ ê°’)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: guaranteed-pod
  labels:
    qos: guaranteed
spec:
  containers:
  - name: app
    image: nginx:1.25
    resources:
      requests:
        cpu: "500m"
        memory: "256Mi"
      limits:
        cpu: "500m"        # requestsì™€ ë™ì¼
        memory: "256Mi"    # requestsì™€ ë™ì¼
  - name: sidecar
    image: fluentd:v1
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "100m"
        memory: "128Mi"
```

**íŠ¹ì§•:**
- oom_score_adj: **-997** (ê°€ì¥ ë‚®ìŒ, OOM Kill ìš°ì„ ìˆœìœ„ ìµœí•˜)
- ë…¸ë“œ ì••ë°• ì‹œì—ë„ ë§ˆì§€ë§‰ì— Eviction
- CPU ìŠ¤ì¼€ì¤„ë§ ìš°ì„ ìˆœìœ„ ë†’ìŒ

#### Burstable (ì¤‘ê°„ ìš°ì„ ìˆœìœ„)

**ì¡°ê±´:**
- ìµœì†Œ 1ê°œ ì»¨í…Œì´ë„ˆì— CPU ë˜ëŠ” Memory requests ì„¤ì •
- Guaranteed ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŒ

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: burstable-pod
  labels:
    qos: burstable
spec:
  containers:
  - name: app
    image: web-app:v1
    resources:
      requests:
        cpu: "250m"
        memory: "512Mi"
      limits:
        cpu: "1000m"       # requestsë³´ë‹¤ í¼ (Burstable)
        memory: "1Gi"      # requestsë³´ë‹¤ í¼

  - name: cache
    image: redis:7
    resources:
      requests:
        memory: "256Mi"    # CPU requests ì—†ìŒ (Burstable)
      limits:
        memory: "512Mi"
```

**íŠ¹ì§•:**
- oom_score_adj: **min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)**
- ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì¡°ì •
- ì—¬ìœ  ìˆì„ ë•Œ burst ê°€ëŠ¥

#### BestEffort (ìµœì € ìš°ì„ ìˆœìœ„)

**ì¡°ê±´:**
- ëª¨ë“  ì»¨í…Œì´ë„ˆì— requestsì™€ limits ë¯¸ì„¤ì •

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: besteffort-pod
  labels:
    qos: besteffort
spec:
  containers:
  - name: app
    image: test-app:latest
    # resources ì„¹ì…˜ ì—†ìŒ ë˜ëŠ” ë¹„ì–´ìˆìŒ
```

**íŠ¹ì§•:**
- oom_score_adj: **1000** (ê°€ì¥ ë†’ìŒ, OOM Kill ìµœìš°ì„ )
- ë…¸ë“œ ì••ë°• ì‹œ ê°€ì¥ ë¨¼ì € Eviction
- ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© ê¶Œì¥

### 3.2 QoSì™€ Eviction ìš°ì„ ìˆœìœ„

ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ì••ë°• ì‹œ kubeletì€ ë‹¤ìŒ ìˆœì„œë¡œ Podë¥¼ Evictioní•©ë‹ˆë‹¤:

```mermaid
graph TB
    A[ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ì••ë°•] --> B{Eviction ê²°ì •}

    B --> C[1ë‹¨ê³„: BestEffort Pod]
    C --> D{ë¦¬ì†ŒìŠ¤ í™•ë³´?}
    D -->|No| E[2ë‹¨ê³„: Burstable Pod<br/>requests ì´ˆê³¼ ì‚¬ìš© ì¤‘]
    D -->|Yes| Z[Eviction ì¤‘ë‹¨]

    E --> F{ë¦¬ì†ŒìŠ¤ í™•ë³´?}
    F -->|No| G[3ë‹¨ê³„: Burstable Pod<br/>requests ì´í•˜ ì‚¬ìš© ì¤‘]
    F -->|Yes| Z

    G --> H{ë¦¬ì†ŒìŠ¤ í™•ë³´?}
    H -->|No| I[4ë‹¨ê³„: Guaranteed Pod<br/>í•„ìˆ˜ ì‹œìŠ¤í…œ Podë§Œ ì œì™¸]
    H -->|Yes| Z

    I --> Z

    style C fill:#ff6b6b
    style E fill:#ffa94d
    style G fill:#ffd43b
    style I fill:#ff0000,color:#fff
    style Z fill:#51cf66
```

**Eviction ìˆœì„œ ìš”ì•½:**

| ìˆœìœ„ | QoS í´ë˜ìŠ¤ | ì¡°ê±´ | oom_score_adj |
|------|-----------|------|---------------|
| 1 (ìµœìš°ì„ ) | BestEffort | ëª¨ë“  Pod | 1000 |
| 2 | Burstable | requests ì´ˆê³¼ ì‚¬ìš© ì¤‘ | 2-999 (ì‚¬ìš©ëŸ‰ ë¹„ë¡€) |
| 3 | Burstable | requests ì´í•˜ ì‚¬ìš© ì¤‘ | 2-999 (ì‚¬ìš©ëŸ‰ ë¹„ë¡€) |
| 4 (ìµœí›„) | Guaranteed | ì‹œìŠ¤í…œ ì¤‘ìš” Pod ì œì™¸ | -997 |

**oom_score_adj í™•ì¸ ë°©ë²•:**

```bash
# Podì˜ ë©”ì¸ ì»¨í…Œì´ë„ˆ í”„ë¡œì„¸ìŠ¤ ì°¾ê¸°
kubectl get pod <pod-name> -o jsonpath='{.status.containerStatuses[0].containerID}'

# ë…¸ë“œì—ì„œ oom_score_adj í™•ì¸
docker inspect <container-id> | grep Pid
cat /proc/<pid>/oom_score_adj

# ì˜ˆì‹œ ì¶œë ¥
# BestEffort: 1000
# Burstable: 500 (ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ë³€ë™)
# Guaranteed: -997
```

### 3.3 ì‹¤ì „ QoS ì „ëµ

ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë§ëŠ” QoS í´ë˜ìŠ¤ ì„ íƒ ê°€ì´ë“œ:

| ì›Œí¬ë¡œë“œ ìœ í˜• | ê¶Œì¥ QoS | ì„¤ì • íŒ¨í„´ | ì´ìœ  |
|-------------|---------|----------|------|
| **í”„ë¡œë•ì…˜ API** | Guaranteed | requests = limits | ì•ˆì •ì„± ìµœìš°ì„ , Eviction ë°©ì§€ |
| **ë°ì´í„°ë² ì´ìŠ¤** | Guaranteed | requests = limits | ë©”ëª¨ë¦¬ ì••ë°• ì‹œì—ë„ ë³´í˜¸ |
| **ë°°ì¹˜ ì‘ì—…** | Burstable | limits > requests | ìœ íœ´ ì‹œ ë¦¬ì†ŒìŠ¤ í™œìš©, ë¹„ìš© íš¨ìœ¨ |
| **í ì›Œì»¤** | Burstable | limits > requests | ë¶€í•˜ ë³€ë™ ëŒ€ì‘ |
| **ê°œë°œ/í…ŒìŠ¤íŠ¸** | BestEffort | ì„¤ì • ì—†ìŒ | ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ (ìš´ì˜ í™˜ê²½ ê¸ˆì§€) |
| **ëª¨ë‹ˆí„°ë§ Agent** | Guaranteed | ë‚®ì€ ê°’ìœ¼ë¡œ ì„¤ì • | ì‹œìŠ¤í…œ ì•ˆì •ì„± |

**í”„ë¡œë•ì…˜ ê¶Œì¥ ì„¤ì •:**

```yaml
# íŒ¨í„´ 1: ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬ ì„œë¹„ìŠ¤ (Guaranteed)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-api
  namespace: production
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: payment-api
        tier: critical
    spec:
      containers:
      - name: api
        image: payment-api:v2.1
        resources:
          requests:
            cpu: "1000m"
            memory: "2Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
      priorityClassName: system-cluster-critical  # ì¶”ê°€ ë³´í˜¸

---
# íŒ¨í„´ 2: ì¼ë°˜ ì›¹ ì„œë¹„ìŠ¤ (Burstable)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
  namespace: production
spec:
  replicas: 10
  template:
    spec:
      containers:
      - name: frontend
        image: web-frontend:v1.5
        resources:
          requests:
            cpu: "200m"       # P50 ì‚¬ìš©ëŸ‰
            memory: "256Mi"
          limits:
            cpu: "500m"       # P95 ì‚¬ìš©ëŸ‰
            memory: "512Mi"   # OOM ë°©ì§€

---
# íŒ¨í„´ 3: ë°°ì¹˜ ì›Œì»¤ (Burstable)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-report
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: report-generator
            image: report-gen:v1
            resources:
              requests:
                cpu: "500m"
                memory: "1Gi"
              limits:
                cpu: "4000m"     # ì•¼ê°„ ì‹œê°„ëŒ€ ë¦¬ì†ŒìŠ¤ í™œìš©
                memory: "8Gi"
          restartPolicy: OnFailure
```

## VPA (Vertical Pod Autoscaler) ìƒì„¸ ê°€ì´ë“œ

### 4.1 VPA ì•„í‚¤í…ì²˜

VPAëŠ” 3ê°œì˜ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "VPA ì•„í‚¤í…ì²˜"
        subgraph "ë©”íŠ¸ë¦­ ìˆ˜ì§‘"
            MS[Metrics Server] -->|ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­| PROM[Prometheus]
            PROM -->|ì‹œê³„ì—´ ë°ì´í„°| REC[VPA Recommender]
        end

        subgraph "VPA ì»´í¬ë„ŒíŠ¸"
            REC -->|ê¶Œì¥ì‚¬í•­ ê³„ì‚°| VPA_OBJ[VPA CRD Object]
            VPA_OBJ -->|ëª¨ë“œ í™•ì¸| UPD[VPA Updater]
            VPA_OBJ -->|ìƒˆ Pod ê²€ì¦| ADM[VPA Admission Controller]

            UPD -->|Pod ì¬ì‹œì‘| POD[Running Pods]
            ADM -->|ë¦¬ì†ŒìŠ¤ ì£¼ì…| NEW_POD[New Pods]
        end

        subgraph "ì›Œí¬ë¡œë“œ"
            POD -->|ì‚¬ìš©ëŸ‰ ë³´ê³ | MS
            NEW_POD -->|ì‚¬ìš©ëŸ‰ ë³´ê³ | MS
        end
    end

    style REC fill:#4dabf7
    style UPD fill:#ffa94d
    style ADM fill:#51cf66
```

**ì»´í¬ë„ŒíŠ¸ ì—­í• :**

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | ë°ì´í„° ì†ŒìŠ¤ |
|---------|------|-----------|
| **Recommender** | ê³¼ê±° ì‚¬ìš©ëŸ‰ ë¶„ì„, ê¶Œì¥ì‚¬í•­ ê³„ì‚° | Metrics Server, Prometheus |
| **Updater** | Auto ëª¨ë“œì—ì„œ Pod ì¬ì‹œì‘ | VPA CRD ìƒíƒœ |
| **Admission Controller** | ìƒˆ Podì— ë¦¬ì†ŒìŠ¤ ìë™ ì£¼ì… | VPA CRD ê¶Œì¥ì‚¬í•­ |

#### 4.1.4 VPA Recommender ML ì•Œê³ ë¦¬ì¦˜ ìƒì„¸

VPA RecommenderëŠ” ë‹¨ìˆœí•œ í‰ê·  ê³„ì‚°ì´ ì•„ë‹Œ, ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ì˜ ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ì¶”ì²œê°’ì„ ì‚°ì¶œí•©ë‹ˆë‹¤.

##### ì§€ìˆ˜ ê°€ì¤‘ íˆìŠ¤í† ê·¸ë¨ (Exponentially-weighted Histogram)

VPA Recommenderì˜ í•µì‹¬ì€ ì‹œê°„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ê°€ ê°ì†Œí•˜ëŠ” íˆìŠ¤í† ê·¸ë¨ì…ë‹ˆë‹¤:

```
ìµœê·¼ ë°ì´í„° â†’ ë†’ì€ ê°€ì¤‘ì¹˜
ì˜¤ë˜ëœ ë°ì´í„° â†’ ë‚®ì€ ê°€ì¤‘ì¹˜ (ì§€ìˆ˜ì  ê°ì†Œ)
```

**ì•Œê³ ë¦¬ì¦˜ ë™ì‘:**

1. **ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì£¼ê¸°**: 1ë¶„ë§ˆë‹¤ Pod ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìˆ˜ì§‘
2. **íˆìŠ¤í† ê·¸ë¨ ì—…ë°ì´íŠ¸**: ê° ì¸¡ì •ê°’ì„ íˆìŠ¤í† ê·¸ë¨ ë²„í‚·ì— ëˆ„ì 
3. **ê°€ì¤‘ì¹˜ ì ìš©**: ì˜¤ë˜ëœ ë°ì´í„°ëŠ” `e^(-t/decay_half_life)` ê°€ì¤‘ì¹˜ë¡œ ê°ì†Œ
4. **ì¶”ì²œê°’ ê³„ì‚°**: íˆìŠ¤í† ê·¸ë¨ì—ì„œ ë°±ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ì¶”ì²œ

```mermaid
graph TB
    subgraph "VPA Recommender ì•Œê³ ë¦¬ì¦˜"
        A[Metrics Server] -->|1ë¶„ë§ˆë‹¤| B[ë©”íŠ¸ë¦­ ìˆ˜ì§‘]
        B --> C[íˆìŠ¤í† ê·¸ë¨ ë²„í‚· ì—…ë°ì´íŠ¸]
        C --> D[ì§€ìˆ˜ ê°€ì¤‘ì¹˜ ì ìš©]
        D --> E[ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°]

        E --> F[Lower Bound<br/>P5]
        E --> G[Target<br/>P95]
        E --> H[Upper Bound<br/>P99]
        E --> I[Uncapped Target<br/>ì œì•½ ì—†ëŠ” P95]

        F --> J[VPA CRD ì—…ë°ì´íŠ¸]
        G --> J
        H --> J
        I --> J
    end

    style G fill:#51cf66
    style J fill:#4dabf7
```

##### 4ê°€ì§€ ì¶”ì²œê°’ ê³„ì‚° ë°©ë²•

| ì¶”ì²œê°’ | ê³„ì‚° ë°©ë²• | ì˜ë¯¸ |
|--------|----------|------|
| **Lower Bound** | P5 (5ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜) | ìµœì†Œ í•„ìš” ë¦¬ì†ŒìŠ¤ - 95% ì‹œê°„ ë™ì•ˆ ì¶©ë¶„ |
| **Target** | P95 (95ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜) | **ê¶Œì¥ ì„¤ì •ê°’** - 5% í”¼í¬ ë¶€í•˜ ëŒ€ì‘ |
| **Upper Bound** | P99 (99ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜) | ìµœëŒ€ ê´€ì°° ì‚¬ìš©ëŸ‰ - Limits ì„¤ì • ì°¸ê³  |
| **Uncapped Target** | maxAllowed ì œì•½ ì—†ì´ ê³„ì‚°í•œ P95 | ì‹¤ì œ í•„ìš”ëŸ‰ í™•ì¸ìš© |

**ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚° ì˜ˆì‹œ:**

```python
# ê°€ìƒì˜ CPU ì‚¬ìš©ëŸ‰ íˆìŠ¤í† ê·¸ë¨ (1ì¼ = 1440ë¶„)
cpu_samples = [100m, 150m, 200m, 250m, 300m, 350m, 400m, 450m, 500m, ...]

# ì§€ìˆ˜ ê°€ì¤‘ì¹˜ ì ìš© (decay_half_life = 24ì‹œê°„)
weighted_samples = [
    (100m, weight=1.0),    # ìµœê·¼ (1ì‹œê°„ ì „)
    (150m, weight=0.97),   # 2ì‹œê°„ ì „
    (200m, weight=0.92),   # 5ì‹œê°„ ì „
    (250m, weight=0.71),   # 12ì‹œê°„ ì „
    (300m, weight=0.50),   # 24ì‹œê°„ ì „ (ë°˜ê°ê¸°)
    (350m, weight=0.25),   # 48ì‹œê°„ ì „
    ...
]

# ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
P5  = 150m  # Lower Bound
P95 = 450m  # Target â­
P99 = 500m  # Upper Bound
```

##### Confidence Multiplier: ì‹ ë¢°ë„ ê¸°ë°˜ ì¡°ì •

ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ì´ ì§§ì„ìˆ˜ë¡ ì•ˆì „í•˜ê²Œ ë†’ì€ ê°’ì„ ì¶”ì²œí•©ë‹ˆë‹¤:

```
Confidence Multiplier = f(ë°ì´í„°_ìˆ˜ì§‘_ê¸°ê°„)

0-24ì‹œê°„:  multiplier = 1.5  (50% ì•ˆì „ ë§ˆì§„)
1-3ì¼:     multiplier = 1.3  (30% ì•ˆì „ ë§ˆì§„)
3-7ì¼:     multiplier = 1.1  (10% ì•ˆì „ ë§ˆì§„)
7ì¼ ì´ìƒ:  multiplier = 1.0  (ì‹ ë¢°ë„ ì¶©ë¶„)
```

**ì‹¤ì œ ì ìš© ì˜ˆì‹œ:**

```yaml
# ë°ì´í„° ìˆ˜ì§‘ 2ì¼ì°¨
ì›ë³¸ P95: 450m
Confidence Multiplier: 1.3
ìµœì¢… Target: 450m Ã— 1.3 = 585m â‰ˆ 600m

# ë°ì´í„° ìˆ˜ì§‘ 10ì¼ì°¨
ì›ë³¸ P95: 450m
Confidence Multiplier: 1.0
ìµœì¢… Target: 450m Ã— 1.0 = 450m
```

:::info ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ì˜ ì¤‘ìš”ì„±
VPAê°€ ì •í™•í•œ ì¶”ì²œì„ ì œê³µí•˜ë ¤ë©´ **ìµœì†Œ 7ì¼, ê¶Œì¥ 14ì¼**ì˜ ë°ì´í„° ìˆ˜ì§‘ì´ í•„ìš”í•©ë‹ˆë‹¤. ì£¼ê°„ íŒ¨í„´(í‰ì¼ vs ì£¼ë§)ì„ í¬ì°©í•˜ë ¤ë©´ ìµœì†Œ 2ì£¼ ì´ìƒì˜ ê´€ì°°ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.
:::

##### Memory ì¶”ì²œ: OOM ì´ë²¤íŠ¸ ê¸°ë°˜ Bump-Up

MemoryëŠ” CPUì™€ ë‹¤ë¥´ê²Œ OOM Kill ì´ë²¤íŠ¸ë¥¼ íŠ¹ë³„íˆ ê³ ë ¤í•©ë‹ˆë‹¤:

**OOM ì´ë²¤íŠ¸ ê°ì§€ ì‹œ:**

```
í˜„ì¬ Memory Target: 500Mi
OOM Kill ë°œìƒ ì‹œì  ë©”ëª¨ë¦¬: 600Mi
â†’ ìƒˆë¡œìš´ Target: 600Mi Ã— 1.2 = 720Mi (20% ì•ˆì „ ë§ˆì§„ ì¶”ê°€)
```

**OOM Bump-Up ë¡œì§:**

```python
if oom_kill_detected:
    oom_memory = get_memory_at_oom_time()
    new_target = max(
        current_target,
        oom_memory * 1.2  # 20% ì•ˆì „ ë§ˆì§„
    )

    # ê¸‰ê²©í•œ ë³€ê²½ ë°©ì§€ (ìµœëŒ€ 2ë°°)
    new_target = min(new_target, current_target * 2)
```

:::warning OOM Killì€ ì¦‰ì‹œ ë°˜ì˜
CPU throttlingê³¼ ë‹¬ë¦¬, OOM Kill ì´ë²¤íŠ¸ëŠ” **ì¦‰ì‹œ Memory Targetì„ ìƒí–¥ ì¡°ì •**í•©ë‹ˆë‹¤. ì´ëŠ” ì„œë¹„ìŠ¤ ì¤‘ë‹¨ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì•ˆì „ ì¥ì¹˜ì…ë‹ˆë‹¤.
:::

##### CPU ì¶”ì²œ: P95/P99 ì‚¬ìš©ëŸ‰ ê¸°ë°˜

CPUëŠ” ì••ì¶• ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ì´ë¯€ë¡œ ë³´ìˆ˜ì ìœ¼ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤:

```
CPU Target = P95 ì‚¬ìš©ëŸ‰
CPU Upper Bound = P99 ì‚¬ìš©ëŸ‰

Throttling ë°œìƒ ì‹œ:
â†’ ì¶”ì²œê°’ì€ ë³€ê²½í•˜ì§€ ì•ŠìŒ (HPAë¡œ í•´ê²° ê¶Œì¥)
```

**CPU Throttling ê°ì§€ ì‹œ:**

```python
if cpu_throttling_detected:
    throttled_percentage = get_throttled_time_percentage()

    if throttled_percentage > 10:
        # VPA ìì²´ ì¶”ì²œê°’ì€ ìœ ì§€
        # ëŒ€ì‹  ë‹¤ìŒì„ ì œì•ˆ:
        # 1. HPA ì¶”ê°€ë¡œ ìˆ˜í‰ í™•ì¥
        # 2. CPU limits ì œê±° (Google, Datadog íŒ¨í„´)
        # 3. ë˜ëŠ” Targetì„ P99ë¡œ ìƒí–¥ (ìˆ˜ë™ ì¡°ì •)
        pass
```

:::tip CPU Throttling vs HPA
VPAëŠ” CPU throttlingì„ ê°ì§€í•˜ë©´ ì¶”ì²œê°’ì„ í¬ê²Œ ì˜¬ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹  **HPAë¡œ ìˆ˜í‰ í™•ì¥**í•˜ëŠ” ê²ƒì´ Kubernetes ëª¨ë²” ì‚¬ë¡€ì…ë‹ˆë‹¤.
:::

##### VPAì™€ Prometheus ë°ì´í„° ì†ŒìŠ¤ í†µí•©

VPA RecommenderëŠ” Metrics Serverë§Œìœ¼ë¡œë„ ë™ì‘í•˜ì§€ë§Œ, Prometheusì™€ í†µí•©í•˜ë©´ ë”ìš± ì •êµí•œ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:

**Prometheus ë©”íŠ¸ë¦­ í™œìš©:**

```yaml
# VPA Recommenderì— Prometheus ì—°ë™ ì„¤ì •
apiVersion: v1
kind: ConfigMap
metadata:
  name: vpa-recommender-config
  namespace: vpa-system
data:
  recommender-config.yaml: |
    # Prometheus ë©”íŠ¸ë¦­ ì†ŒìŠ¤ í™œì„±í™”
    metrics-provider: prometheus
    prometheus-url: http://prometheus-server.monitoring.svc:9090

    # íˆìŠ¤í† ê·¸ë¨ ì„¤ì •
    histogram-decay-half-life: 24h
    histogram-bucket-size-growth: 1.05

    # CPU ì¶”ì²œ ì„¤ì •
    cpu-histogram-decay-half-life: 24h
    memory-histogram-decay-half-life: 48h  # MemoryëŠ” ë” ê¸´ ê´€ì°°

    # OOM ì´ë²¤íŠ¸ ì²˜ë¦¬
    oom-min-bump-up: 1.2  # ìµœì†Œ 20% ì¦ê°€
    oom-bump-up-ratio: 0.5  # 50% ì•ˆì „ ë§ˆì§„
```

**Prometheus Custom Metrics API ì—°ë™:**

```bash
# Custom Metrics API ì–´ëŒ‘í„° ë°°í¬ (Prometheus Adapter)
helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace monitoring \
  --set prometheus.url=http://prometheus-server.monitoring.svc \
  --set rules.default=true

# VPAê°€ Custom Metrics API ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
kubectl edit deploy vpa-recommender -n vpa-system

# í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€:
# - PROMETHEUS_ADDRESS=http://prometheus-server.monitoring.svc:9090
# - USE_CUSTOM_METRICS=true
```

**ì—°ë™ í™•ì¸:**

```bash
# VPA Recommenderê°€ Prometheus ë©”íŠ¸ë¦­ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸
kubectl logs -n vpa-system deploy/vpa-recommender | grep prometheus

# ì¶œë ¥ ì˜ˆì‹œ:
# I0212 10:15:30.123456  1 metrics_client.go:45] Using Prometheus metrics provider
# I0212 10:15:31.234567  1 prometheus_client.go:78] Connected to Prometheus at http://prometheus-server.monitoring.svc:9090
```

##### VPA ì¶”ì²œ í’ˆì§ˆ ê²€ì¦ ë°©ë²•

ì¶”ì²œê°’ì´ ì‹¤ì œë¡œ ì ì ˆí•œì§€ ê²€ì¦í•˜ëŠ” PromQL ì¿¼ë¦¬:

**1. CPU ì¶”ì²œê°’ vs ì‹¤ì œ ì‚¬ìš©ëŸ‰ ë¹„êµ:**

```promql
# VPA Target vs ì‹¤ì œ P95 ì‚¬ìš©ëŸ‰ ë¹„êµ
(
  kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="cpu"}
  -
  quantile_over_time(0.95,
    container_cpu_usage_seconds_total{pod=~"web-app-.*"}[7d]
  ) * 1000
) /
kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="cpu"} * 100

# ì¶œë ¥: ì¶”ì²œê°’ê³¼ ì‹¤ì œ P95 ì°¨ì´ (%)
# 10-20% ë²”ìœ„: ì ì ˆ âœ…
# >30%: ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ âš ï¸
# <0%: ê³¼ì†Œ í”„ë¡œë¹„ì €ë‹ (ì¦‰ì‹œ ì¡°ì • í•„ìš”) ğŸš¨
```

**2. Memory ì¶”ì²œê°’ ê²€ì¦:**

```promql
# VPA Target vs ì‹¤ì œ P99 ì‚¬ìš©ëŸ‰
(
  kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="memory"}
  -
  quantile_over_time(0.99,
    container_memory_working_set_bytes{pod=~"web-app-.*"}[7d]
  )
) /
kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="memory"} * 100

# 20-30% ì—¬ìœ : ì´ìƒì  âœ…
# <10% ì—¬ìœ : OOM ìœ„í—˜ ğŸš¨
```

**3. OOM Kill ë¹ˆë„ ëª¨ë‹ˆí„°ë§:**

```promql
# ìµœê·¼ 7ì¼ OOM Kill ì´ë²¤íŠ¸ ìˆ˜
increase(
  kube_pod_container_status_terminated_reason{reason="OOMKilled"}[7d]
)

# 0ê±´: VPA ì¶”ì²œ ì •í™• âœ…
# 1-2ê±´: ìˆ˜ìš© ê°€ëŠ¥ (í”¼í¬ ë¶€í•˜)
# >3ê±´: VPA Target ìˆ˜ë™ ìƒí–¥ í•„ìš” ğŸš¨
```

**4. CPU Throttling ë¹„ìœ¨:**

```promql
# CPU Throttling ì‹œê°„ ë¹„ìœ¨ (%)
rate(container_cpu_cfs_throttled_seconds_total{pod=~"web-app-.*"}[5m])
/
rate(container_cpu_cfs_periods_total{pod=~"web-app-.*"}[5m]) * 100

# <5%: ì •ìƒ âœ…
# 5-10%: ëª¨ë‹ˆí„°ë§ í•„ìš” âš ï¸
# >10%: HPA ì¶”ê°€ ë˜ëŠ” CPU limits ì œê±° ê³ ë ¤ ğŸš¨
```

**Grafana ëŒ€ì‹œë³´ë“œ ì˜ˆì‹œ:**

```yaml
# VPA ì¶”ì²œ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
apiVersion: v1
kind: ConfigMap
metadata:
  name: vpa-quality-dashboard
  namespace: monitoring
data:
  dashboard.json: |
    {
      "panels": [
        {
          "title": "CPU: VPA Target vs P95 ì‹¤ì œ ì‚¬ìš©ëŸ‰",
          "targets": [
            {
              "expr": "kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource=\"cpu\"}",
              "legendFormat": "VPA Target"
            },
            {
              "expr": "quantile_over_time(0.95, container_cpu_usage_seconds_total[7d]) * 1000",
              "legendFormat": "ì‹¤ì œ P95"
            }
          ]
        },
        {
          "title": "Memory: VPA Target vs P99 ì‹¤ì œ ì‚¬ìš©ëŸ‰",
          "targets": [
            {
              "expr": "kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource=\"memory\"}",
              "legendFormat": "VPA Target"
            },
            {
              "expr": "quantile_over_time(0.99, container_memory_working_set_bytes[7d])",
              "legendFormat": "ì‹¤ì œ P99"
            }
          ]
        },
        {
          "title": "OOM Kill ì´ë²¤íŠ¸ (7ì¼)",
          "targets": [
            {
              "expr": "increase(kube_pod_container_status_terminated_reason{reason=\"OOMKilled\"}[7d])"
            }
          ]
        }
      ]
    }
```

:::tip VPA ì¶”ì²œì˜ í•œê³„
VPAëŠ” ê³¼ê±° ë°ì´í„° ê¸°ë°˜ ì¶”ì²œì´ë¯€ë¡œ ë‹¤ìŒ ìƒí™©ì—ì„œëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤:
- **ê°‘ì‘ìŠ¤ëŸ¬ìš´ íŠ¸ë˜í”½ íŒ¨í„´ ë³€í™”**: ê³¼ê±°ì— ì—†ë˜ í”¼í¬ ë¶€í•˜
- **ê³„ì ˆì„± ì›Œí¬ë¡œë“œ**: ì›”ë§ ë°°ì¹˜, ì—°ë§ ê²°ì‚° ë“±
- **ì´ˆê¸° ë¶€íŠ¸ìŠ¤íŠ¸ë©**: ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

ì´ëŸ¬í•œ ê²½ìš° **ìˆ˜ë™ ì¡°ì •** ë˜ëŠ” **HPAì™€ì˜ ì¡°í•©**ì´ í•„ìš”í•©ë‹ˆë‹¤.
:::

### 4.2 VPA ì„¤ì¹˜ ë° êµ¬ì„±

#### Helmì„ í†µí•œ ì„¤ì¹˜

```bash
# 1. Metrics Server ì„¤ì¹˜ (ì‚¬ì „ ìš”êµ¬ì‚¬í•­)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# 2. Metrics Server í™•ì¸
kubectl get deployment metrics-server -n kube-system
kubectl top nodes

# 3. VPA Helm ë ˆí¬ì§€í† ë¦¬ ì¶”ê°€
helm repo add fairwinds-stable https://charts.fairwinds.com/stable
helm repo update

# 4. VPA ì„¤ì¹˜
helm install vpa fairwinds-stable/vpa \
  --namespace vpa-system \
  --create-namespace \
  --set recommender.enabled=true \
  --set updater.enabled=true \
  --set admissionController.enabled=true

# 5. ì„¤ì¹˜ í™•ì¸
kubectl get pods -n vpa-system
# ì˜ˆìƒ ì¶œë ¥:
# NAME                                      READY   STATUS    RESTARTS   AGE
# vpa-admission-controller-xxx              1/1     Running   0          1m
# vpa-recommender-xxx                       1/1     Running   0          1m
# vpa-updater-xxx                           1/1     Running   0          1m
```

#### ìˆ˜ë™ ì„¤ì¹˜ (ê³µì‹ ë°©ë²•)

```bash
# VPA ê³µì‹ ë ˆí¬ì§€í† ë¦¬ í´ë¡ 
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler

# VPA ì„¤ì¹˜
./hack/vpa-up.sh

# ì„¤ì¹˜ í™•ì¸
kubectl get crd | grep verticalpodautoscaler
```

### 4.3 VPA ëª¨ë“œ

VPAëŠ” 3ê°€ì§€ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤:

#### Off ëª¨ë“œ (ê¶Œì¥ì‚¬í•­ë§Œ ì œê³µ)

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"    # ê¶Œì¥ì‚¬í•­ë§Œ í‘œì‹œ, ìë™ ì ìš© ì•ˆ í•¨
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- VPAë¥¼ ì²˜ìŒ ë„ì…í•  ë•Œ
- í”„ë¡œë•ì…˜ ì›Œí¬ë¡œë“œ ë¶„ì„
- ìˆ˜ë™ ê²€í†  í›„ ì ìš© ì›í•  ë•Œ

**ê¶Œì¥ì‚¬í•­ í™•ì¸:**

```bash
# VPA ìƒíƒœ í™•ì¸
kubectl describe vpa web-app-vpa -n production

# ì¶œë ¥ ì˜ˆì‹œ:
# Recommendation:
#   Container Recommendations:
#     Container Name: web-app
#     Lower Bound:
#       Cpu:     150m
#       Memory:  200Mi
#     Target:          # â† ì´ ê°’ ì‚¬ìš© ê¶Œì¥
#       Cpu:     250m
#       Memory:  300Mi
#     Uncapped Target:
#       Cpu:     350m
#       Memory:  400Mi
#     Upper Bound:
#       Cpu:     500m
#       Memory:  600Mi
```

#### Initial ëª¨ë“œ (Pod ìƒì„± ì‹œì—ë§Œ ì ìš©)

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: batch-worker-vpa
  namespace: batch
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: batch-worker
  updatePolicy:
    updateMode: "Initial"    # Pod ìƒì„± ì‹œì—ë§Œ ë¦¬ì†ŒìŠ¤ ì„¤ì •
  resourcePolicy:
    containerPolicies:
    - containerName: worker
      minAllowed:
        cpu: "100m"
        memory: "128Mi"
      maxAllowed:
        cpu: "4000m"
        memory: "16Gi"
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- CronJob, Job ì›Œí¬ë¡œë“œ
- ì¬ì‹œì‘ì´ í—ˆìš©ë˜ì§€ ì•ŠëŠ” StatefulSet
- ìˆ˜ë™ ìŠ¤ì¼€ì¼ë§ì„ ì›í•˜ëŠ” ê²½ìš°

**ë™ì‘ ë°©ì‹:**
1. ìƒˆ Pod ìƒì„± ìš”ì²­
2. VPA Admission Controllerê°€ ê¶Œì¥ ë¦¬ì†ŒìŠ¤ ì£¼ì…
3. ê¸°ì¡´ ì‹¤í–‰ ì¤‘ì¸ PodëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€

#### Auto ëª¨ë“œ (ì™„ì „ ìë™í™”)

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
  namespace: development
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  updatePolicy:
    updateMode: "Auto"    # ìë™ìœ¼ë¡œ Pod ì¬ì‹œì‘ ë° ë¦¬ì†ŒìŠ¤ ì¡°ì •
    minReplicas: 2        # ìµœì†Œ 2ê°œ Pod ìœ ì§€
  resourcePolicy:
    containerPolicies:
    - containerName: api
      minAllowed:
        cpu: "200m"
        memory: "256Mi"
      maxAllowed:
        cpu: "2000m"
        memory: "4Gi"
      controlledResources:
      - cpu
      - memory
      controlledValues: RequestsAndLimits  # requestsì™€ limits ëª¨ë‘ ì¡°ì •
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
- ê°œë°œ/ìŠ¤í…Œì´ì§• í™˜ê²½
- Stateless ì• í”Œë¦¬ì¼€ì´ì…˜
- PodDisruptionBudget ì„¤ì •ëœ ì›Œí¬ë¡œë“œ

:::warning Auto ëª¨ë“œ ì£¼ì˜ì‚¬í•­
Auto ëª¨ë“œëŠ” **Podë¥¼ ì¬ì‹œì‘**í•©ë‹ˆë‹¤:
- Eviction APIë¥¼ í†µí•œ ì¬ì‹œì‘
- ë‹¤ìš´íƒ€ì„ ë°œìƒ ê°€ëŠ¥
- PodDisruptionBudget (PDB) í•„ìˆ˜ ì„¤ì •
- í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ì‹ ì¤‘íˆ ì‚¬ìš©

**ê¶Œì¥:** í”„ë¡œë•ì…˜ì—ì„œëŠ” **Off ë˜ëŠ” Initial ëª¨ë“œ** ì‚¬ìš©
:::

### 4.4 VPA + HPA ê³µì¡´ ì „ëµ

VPAì™€ HPAë¥¼ í•¨ê»˜ ì‚¬ìš©í•  ë•ŒëŠ” ì¶©ëŒì„ ë°©ì§€í•´ì•¼ í•©ë‹ˆë‹¤.

#### ì¶©ëŒ ì‹œë‚˜ë¦¬ì˜¤ (âŒ ê¸ˆì§€)

```yaml
# âŒ ì˜ëª»ëœ ì„¤ì •: VPA Auto + HPA CPU ë™ì‹œ ì‚¬ìš©
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: bad-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Auto"    # âŒ Auto ëª¨ë“œ
  resourcePolicy:
    containerPolicies:
    - containerName: app
      controlledResources:
      - cpu                # âŒ CPU ì œì–´
      - memory

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: bad-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu          # âŒ CPU ë©”íŠ¸ë¦­ ì‚¬ìš©
      target:
        type: Utilization
        averageUtilization: 70
```

**ë¬¸ì œ:**
- VPAê°€ CPU requestsë¥¼ ë³€ê²½ â†’ HPAì˜ CPU ì‚¬ìš©ë¥  ê³„ì‚°ì´ ë³€ê²½ë¨
- HPAê°€ ìŠ¤ì¼€ì¼ ì•„ì›ƒ â†’ VPAê°€ ë‹¤ì‹œ ë¦¬ì†ŒìŠ¤ ì¡°ì • â†’ ë¬´í•œ ë£¨í”„

#### íŒ¨í„´ 1: VPA Off + HPA (âœ… ê¶Œì¥)

```yaml
# âœ… ì˜¬ë°”ë¥¸ ì„¤ì •: VPAëŠ” ê¶Œì¥ë§Œ, HPAë¡œ ìŠ¤ì¼€ì¼ë§
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"    # âœ… ê¶Œì¥ì‚¬í•­ë§Œ ì œê³µ
  resourcePolicy:
    containerPolicies:
    - containerName: app
      controlledResources:
      - cpu
      - memory

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

**ìš´ì˜ ì›Œí¬í”Œë¡œìš°:**
1. VPAê°€ ê¶Œì¥ì‚¬í•­ ìƒì„±
2. ì£¼ê°„ ë¦¬ë·°ì—ì„œ VPA ê¶Œì¥ì‚¬í•­ í™•ì¸
3. Deployment ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì— ìˆ˜ë™ ë°˜ì˜
4. HPAê°€ ë¶€í•˜ì— ë”°ë¼ ìˆ˜í‰ í™•ì¥

#### íŒ¨í„´ 2: VPA Memory + HPA CPU (âœ… ê¶Œì¥)

```yaml
# âœ… ë©”íŠ¸ë¦­ ë¶„ë¦¬: VPAëŠ” Memory, HPAëŠ” CPU
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  updatePolicy:
    updateMode: "Auto"    # Memoryë§Œ ìë™ ì¡°ì •
  resourcePolicy:
    containerPolicies:
    - containerName: api
      controlledResources:
      - memory            # âœ… Memoryë§Œ ì œì–´
      minAllowed:
        memory: "256Mi"
      maxAllowed:
        memory: "8Gi"

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 5
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu          # âœ… CPU ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš©
      target:
        type: Utilization
        averageUtilization: 60
```

**ì¥ì :**
- VPAê°€ Memory ìµœì í™” (Vertical)
- HPAê°€ ë¶€í•˜ì— ë”°ë¼ ìˆ˜í‰ í™•ì¥ (Horizontal)
- ì¶©ëŒ ì—†ìŒ

#### íŒ¨í„´ 3: VPA + HPA + Custom Metrics (âœ… ê³ ê¸‰)

```yaml
# âœ… HPAëŠ” ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì‚¬ìš©
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: worker-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: queue-worker
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: worker
      controlledResources:
      - cpu
      - memory

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: queue-worker
  minReplicas: 2
  maxReplicas: 50
  metrics:
  - type: External
    external:
      metric:
        name: sqs_queue_depth    # âœ… ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ (CPU/Memory ì•„ë‹˜)
        selector:
          matchLabels:
            queue: "tasks"
      target:
        type: AverageValue
        averageValue: "30"
```

**ì ìš© ì‚¬ë¡€:**
- í ê¸°ë°˜ ì›Œí¬ë¡œë“œ (SQS, RabbitMQ, Kafka)
- ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜
- ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§

### 4.5 VPA ì œí•œì‚¬í•­ê³¼ ì£¼ì˜ì 

:::danger VPA ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­

**1. Pod ì¬ì‹œì‘ í•„ìš” (Auto/Recreate ëª¨ë“œ)**
- VPAëŠ” ì‹¤í–‰ ì¤‘ì¸ Podì˜ ë¦¬ì†ŒìŠ¤ë¥¼ **in-place ë³€ê²½ ë¶ˆê°€**
- Podë¥¼ Evictí•˜ê³  ìƒˆë¡œ ìƒì„± (ë‹¤ìš´íƒ€ì„ ë°œìƒ)
- í•´ê²°: PodDisruptionBudget ì„¤ì • í•„ìˆ˜

**2. JVM í™ ì‚¬ì´ì¦ˆ ë¶ˆì¼ì¹˜**
```yaml
# ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤
containers:
- name: java-app
  env:
  - name: JAVA_OPTS
    value: "-Xmx2g"    # ê³ ì •ê°’
  resources:
    requests:
      memory: "3Gi"    # VPAê°€ ë‚˜ì¤‘ì— 4Gië¡œ ë³€ê²½
    limits:
      memory: "3Gi"    # VPAê°€ ë‚˜ì¤‘ì— 4Gië¡œ ë³€ê²½

# VPAê°€ memoryë¥¼ 4Gië¡œ ë³€ê²½í•´ë„ JVMì€ ì—¬ì „íˆ 2Gi í™ ì‚¬ìš©
# â†’ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„
```

**í•´ê²°:**
```yaml
containers:
- name: java-app
  env:
  - name: MEM_LIMIT
    valueFrom:
      resourceFieldRef:
        resource: limits.memory
  - name: JAVA_OPTS
    value: "-XX:MaxRAMPercentage=75.0"  # ë™ì  ê³„ì‚°
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"
```

**3. StatefulSet ì£¼ì˜**
- StatefulSet PodëŠ” ìˆœì°¨ì  ì¬ì‹œì‘
- ë°ì´í„° ì†ì‹¤ ìœ„í—˜
- ê¶Œì¥: **Initial ëª¨ë“œë§Œ ì‚¬ìš©**

**4. Metrics Server ì˜ì¡´ì„±**
- VPAëŠ” Metrics Server í•„ìˆ˜
- Metrics Server ì¥ì•  ì‹œ ê¶Œì¥ì‚¬í•­ ì—…ë°ì´íŠ¸ ì¤‘ë‹¨

**5. ê¶Œì¥ì‚¬í•­ ê³„ì‚° ì‹œê°„**
- ìµœì†Œ 24ì‹œê°„ ë°ì´í„° í•„ìš”
- íŠ¸ë˜í”½ íŒ¨í„´ ë³€í™” ë°˜ì˜ì— ì‹œê°„ ì†Œìš”
:::

## HPA ê³ ê¸‰ íŒ¨í„´

### 5.1 HPA Behavior ì„¤ì •

HPA v2ëŠ” ìŠ¤ì¼€ì¼ë§ ë™ì‘ì„ ì„¸ë°€í•˜ê²Œ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: advanced-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 5
  maxReplicas: 100

  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0    # ì¦‰ì‹œ ìŠ¤ì¼€ì¼ ì—…
      policies:
      - type: Percent
        value: 100                     # 100% ì¦ê°€ í—ˆìš© (2ë°°)
        periodSeconds: 15              # 15ì´ˆë§ˆë‹¤ í‰ê°€
      - type: Pods
        value: 10                      # ë˜ëŠ” 10ê°œ Pod ì¦ê°€
        periodSeconds: 15
      selectPolicy: Max                # ë” í° ê°’ ì„ íƒ

    scaleDown:
      stabilizationWindowSeconds: 300  # 5ë¶„ ì•ˆì •í™” (ê¸‰ê²©í•œ ê°ì†Œ ë°©ì§€)
      policies:
      - type: Percent
        value: 10                      # 10% ê°ì†Œ
        periodSeconds: 60              # 1ë¶„ë§ˆë‹¤ í‰ê°€
      - type: Pods
        value: 5                       # ë˜ëŠ” 5ê°œ Pod ê°ì†Œ
        periodSeconds: 60
      selectPolicy: Min                # ë” ì‘ì€ ê°’ ì„ íƒ (ë³´ìˆ˜ì )
```

**íŒŒë¼ë¯¸í„° ì„¤ëª…:**

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¶Œì¥ê°’ |
|---------|------|--------|
| `stabilizationWindowSeconds` | ë©”íŠ¸ë¦­ ì•ˆì •í™” ëŒ€ê¸° ì‹œê°„ | ScaleUp: 0-30s, ScaleDown: 300-600s |
| `type: Percent` | í˜„ì¬ ë ˆí”Œë¦¬ì¹´ì˜ %ë¡œ ì¦ê° | ScaleUp: 100%, ScaleDown: 10-25% |
| `type: Pods` | ì ˆëŒ€ Pod ìˆ˜ë¡œ ì¦ê° | ì›Œí¬ë¡œë“œ í¬ê¸°ì— ë”°ë¼ ì¡°ì • |
| `periodSeconds` | ì •ì±… í‰ê°€ ì£¼ê¸° | 15-60ì´ˆ |
| `selectPolicy` | Max(ê³µê²©ì ), Min(ë³´ìˆ˜ì ), Disabled | ScaleUp: Max, ScaleDown: Min |

:::info karpenter-autoscaling.md ì°¸ì¡°
HPAì™€ Karpenterë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ì „ì²´ ì•„í‚¤í…ì²˜ëŠ” [Karpenter ì˜¤í† ìŠ¤ì¼€ì¼ë§ ê°€ì´ë“œ](/docs/infrastructure-optimization/karpenter-autoscaling)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
:::

### 5.2 ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ê¸°ë°˜ HPA

#### Prometheus Adapter ì‚¬ìš©

```bash
# Prometheus Adapter ì„¤ì¹˜
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace monitoring \
  --set prometheus.url=http://prometheus-server.monitoring.svc \
  --set prometheus.port=80
```

**ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì„¤ì •:**

```yaml
# values.yaml for prometheus-adapter
rules:
  default: false
  custom:
  - seriesQuery: 'http_requests_total{namespace!="",pod!=""}'
    resources:
      overrides:
        namespace: {resource: "namespace"}
        pod: {resource: "pod"}
    name:
      matches: "^(.*)_total$"
      as: "${1}_per_second"
    metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'
```

**HPA ì„¤ì •:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: custom-metric-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"    # Podë‹¹ 1000 req/s
```

#### KEDA ScaledObject

```bash
# KEDA ì„¤ì¹˜
helm repo add kedacore https://kedacore.github.io/charts
helm install keda kedacore/keda --namespace keda --create-namespace
```

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prometheus-scaledobject
spec:
  scaleTargetRef:
    name: api-server
  minReplicaCount: 2
  maxReplicaCount: 100
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-server.monitoring.svc:80
      metricName: http_requests_per_second
      threshold: "1000"
      query: sum(rate(http_requests_total{app="api-server"}[2m]))
```

### 5.3 ë‹¤ì¤‘ ë©”íŠ¸ë¦­ HPA

ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì„ ì¡°í•©í•˜ì—¬ ìŠ¤ì¼€ì¼ë§:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: multi-metric-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 5
  maxReplicas: 100

  metrics:
  # 1. CPU ë©”íŠ¸ë¦­
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # 2. Memory ë©”íŠ¸ë¦­
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

  # 3. ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ - RPS
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"

  # 4. ì™¸ë¶€ ë©”íŠ¸ë¦­ - ALB Target Response Time
  - type: External
    external:
      metric:
        name: alb_target_response_time
        selector:
          matchLabels:
            targetgroup: "web-app-tg"
      target:
        type: Value
        value: "100"    # 100ms

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 50
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

**ë‹¤ì¤‘ ë©”íŠ¸ë¦­ í‰ê°€:**
- HPAëŠ” **ê° ë©”íŠ¸ë¦­ì„ ë…ë¦½ì ìœ¼ë¡œ í‰ê°€**
- **ê°€ì¥ ë†’ì€ ë ˆí”Œë¦¬ì¹´ ìˆ˜**ë¥¼ ì„ íƒ (ë³´ìˆ˜ì  ì ‘ê·¼)
- ì˜ˆ: CPU ê¸°ì¤€ 10ê°œ, Memory ê¸°ì¤€ 15ê°œ, RPS ê¸°ì¤€ 20ê°œ â†’ **20ê°œ ì„ íƒ**

## Node Readiness Controllerì™€ ë¦¬ì†ŒìŠ¤ ìµœì í™”

### 5.3 ì¤€ë¹„ë˜ì§€ ì•Šì€ ë…¸ë“œì—ì„œì˜ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„

Kubernetes í´ëŸ¬ìŠ¤í„°ì—ì„œ ìƒˆ ë…¸ë“œê°€ í”„ë¡œë¹„ì €ë‹ë˜ë©´, CNI í”ŒëŸ¬ê·¸ì¸, CSI ë“œë¼ì´ë²„, GPU ë“œë¼ì´ë²„ ë“±ì˜ ì¸í”„ë¼ ì»´í¬ë„ŒíŠ¸ê°€ ì¤€ë¹„ë˜ê¸° ì „ì— Podê°€ ìŠ¤ì¼€ì¤„ë§ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤:

**ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ì‹œë‚˜ë¦¬ì˜¤:**

1. **CrashLoopBackOff ë°˜ë³µ**
   - ì¤€ë¹„ë˜ì§€ ì•Šì€ ë…¸ë“œì— Pod ìŠ¤ì¼€ì¤„ë§ â†’ ì‹¤íŒ¨ â†’ ì¬ì‹œì‘ ë°˜ë³µ
   - ë¶ˆí•„ìš”í•œ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš© ë° ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ì¬ë‹¤ìš´ë¡œë“œ

2. **ë¶ˆí•„ìš”í•œ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹**
   - Podê°€ Pending ìƒíƒœë¡œ ëŒ€ê¸° â†’ Karpenter/Cluster Autoscalerê°€ ì¶”ê°€ ë…¸ë“œ ìƒì„±
   - ì‹¤ì œë¡œëŠ” ê¸°ì¡´ ë…¸ë“œê°€ ì¤€ë¹„ë˜ë©´ ìˆ˜ìš© ê°€ëŠ¥í•œ ìƒí™©

3. **ì¬ìŠ¤ì¼€ì¤„ë§ ì˜¤ë²„í—¤ë“œ**
   - ì‹¤íŒ¨í•œ Podë¥¼ ë‹¤ë¥¸ ë…¸ë“œë¡œ ì´ë™ â†’ ë„¤íŠ¸ì›Œí¬/ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„
   - ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” ë¹„ìš© ì¤‘ë³µ ë°œìƒ

### 5.4 Node Readiness Controller (NRC) ê°œìš”

Node Readiness ControllerëŠ” Kubernetes 1.32ì— ë„ì…ëœ ê¸°ëŠ¥ìœ¼ë¡œ, ì¸í”„ë¼ ì¤€ë¹„ ì™„ë£Œ ì „ê¹Œì§€ Pod ìŠ¤ì¼€ì¤„ë§ì„ ì°¨ë‹¨í•˜ì—¬ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

**í•µì‹¬ ê¸°ëŠ¥:**

| ê¸°ëŠ¥ | ì„¤ëª… | ë¦¬ì†ŒìŠ¤ ìµœì í™” íš¨ê³¼ |
|------|------|-------------------|
| **Readiness Gate** | íŠ¹ì • ì¡°ê±´ ì¶©ì¡± ì „ ë…¸ë“œë¥¼ NotReady ìƒíƒœë¡œ ìœ ì§€ | Pod ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨ìœ¼ë¡œ CrashLoop ë°©ì§€ |
| **Custom Taint** | ì¤€ë¹„ë˜ì§€ ì•Šì€ ë…¸ë“œì— ìë™ taint ì¶”ê°€ | ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ë°©ì§€ (NoSchedule íš¨ê³¼) |
| **Enforcement Mode** | `bootstrap-only` ë˜ëŠ” `continuous` ëª¨ë“œ ì„ íƒ | ì´ˆê¸° ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹œì—ë§Œ ë˜ëŠ” ì§€ì†ì  ê²€ì¦ |

**API êµ¬ì¡°:**

```yaml
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
```

### 5.5 Karpenter ì—°ë™ ìµœì í™”

Karpenterì™€ Node Readiness Controllerë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ íš¨ìœ¨ì„±ì´ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.

**ìµœì í™” íŒ¨í„´:**

```mermaid
graph TB
    A[Karpenter: ìƒˆ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹] --> B[NRC: Taint ìë™ ì¶”ê°€]
    B --> C{CNI/CSI ì¤€ë¹„ ì™„ë£Œ?}
    C -->|No| D[Pod Pending ìœ ì§€]
    D --> E[Karpenter: ì¶”ê°€ ë…¸ë“œ ìƒì„± ì•ˆ í•¨]
    C -->|Yes| F[NRC: Taint ì œê±°]
    F --> G[Pod ìŠ¤ì¼€ì¤„ë§ ì‹œì‘]
    G --> H[ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì  ë°°ì¹˜]

    style B fill:#a8dadc
    style E fill:#457b9d
    style H fill:#1d3557
```

**Karpenter NodePoolê³¼ NRC ì—°ë™:**

```yaml
# 1. CSI Driver ì¤€ë¹„ í™•ì¸ (EBS)
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: ebs-csi-readiness
spec:
  conditions:
    - type: "ebs.csi.aws.com/driver-ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/storage-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "bootstrap-only"  # ì´ˆê¸° ë¶€íŠ¸ìŠ¤íŠ¸ë©ë§Œ ê²€ì¦

---
# 2. VPC CNI ì¤€ë¹„ í™•ì¸
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: vpc-cni-readiness
spec:
  conditions:
    - type: "vpc.amazonaws.com/cni-ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/network-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "bootstrap-only"

---
# 3. GPU Driver ì¤€ë¹„ í™•ì¸ (GPU ë…¸ë“œìš©)
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-driver-readiness
spec:
  conditions:
    - type: "nvidia.com/gpu-driver-ready"
      requiredStatus: "True"
    - type: "nvidia.com/cuda-ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/gpu-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "bootstrap-only"
  # GPU ë“œë¼ì´ë²„ ë¡œë”©ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼ (30-60ì´ˆ)
  # NRCë¡œ ì´ ì‹œê°„ ë™ì•ˆ Pod ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨
```

### 5.6 ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± ê°œì„  íš¨ê³¼

Node Readiness Controller ì ìš© ì „í›„ ë¹„êµ:

| ì§€í‘œ | ì ìš© ì „ | ì ìš© í›„ | ê°œì„ ìœ¨ |
|------|---------|---------|--------|
| **CrashLoopBackOff ë°œìƒë¥ ** | 15-20% | < 2% | 90% ê°ì†Œ |
| **ë¶ˆí•„ìš”í•œ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹** | í‰ê·  2-3ê°œ/ì‹œê°„ | < 0.5ê°œ/ì‹œê°„ | 75% ê°ì†Œ |
| **Pod ì‹œì‘ ì‹¤íŒ¨ìœ¨** | 8-12% | < 1% | 90% ê°ì†Œ |
| **ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ì¬ë‹¤ìš´ë¡œë“œ** | 100-200GB/ì¼ | 20-30GB/ì¼ | 80% ê°ì†Œ |

**ë¹„ìš© ì˜í–¥ (100ê°œ ë…¸ë“œ í´ëŸ¬ìŠ¤í„° ê¸°ì¤€):**

```
ì ìš© ì „:
- ë¶ˆí•„ìš”í•œ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹: í‰ê·  3ê°œ Ã— $0.384/ì‹œê°„ Ã— 24ì‹œê°„ Ã— 30ì¼ = $829/ì›”
- ì´ë¯¸ì§€ ì¬ë‹¤ìš´ë¡œë“œ ë°ì´í„° ì „ì†¡ ë¹„ìš©: 150GB/ì¼ Ã— 30ì¼ Ã— $0.09/GB = $405/ì›”
- ì´ ë‚­ë¹„ ë¹„ìš©: $1,234/ì›”

ì ìš© í›„:
- ë¶ˆí•„ìš”í•œ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹: í‰ê·  0.5ê°œ Ã— $0.384/ì‹œê°„ Ã— 24ì‹œê°„ Ã— 30ì¼ = $138/ì›”
- ì´ë¯¸ì§€ ì¬ë‹¤ìš´ë¡œë“œ ë°ì´í„° ì „ì†¡ ë¹„ìš©: 25GB/ì¼ Ã— 30ì¼ Ã— $0.09/GB = $67.5/ì›”
- ì´ ë¹„ìš©: $205.5/ì›”

ì ˆê°ì•¡: $1,234 - $205.5 = $1,028.5/ì›” (83% ì ˆê°)
```

### 5.7 ì‹¤ì „ êµ¬í˜„ ê°€ì´ë“œ

#### Step 1: Feature Gate í™œì„±í™”

```bash
# EKS 1.32+ í´ëŸ¬ìŠ¤í„°ì—ì„œ Feature Gate í™•ì¸
kubectl get --raw /metrics | grep node_readiness_controller

# Karpenter ì„¤ì •ì—ì„œ Feature Gate í™œì„±í™”
# values.yaml (Karpenter Helm Chart)
controller:
  featureGates:
    NodeReadinessController: true
```

#### Step 2: NodeReadinessRule ì ìš©

```yaml
# production-nrc.yaml
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: production-readiness
spec:
  # ì—¬ëŸ¬ ì¡°ê±´ì„ ANDë¡œ ê²€ì¦
  conditions:
    - type: "ebs.csi.aws.com/driver-ready"
      requiredStatus: "True"
    - type: "vpc.amazonaws.com/cni-ready"
      requiredStatus: "True"

  taint:
    key: "readiness.k8s.io/not-ready"
    effect: "NoSchedule"
    value: "pending"

  # bootstrap-only: ë…¸ë“œ ì´ˆê¸° ë¶€íŠ¸ìŠ¤íŠ¸ë©ë§Œ ê²€ì¦
  # continuous: ì§€ì†ì ìœ¼ë¡œ ê²€ì¦ (ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì‹œì—ë„ ëŒ€ì‘)
  enforcementMode: "bootstrap-only"
```

```bash
kubectl apply -f production-nrc.yaml

# ì ìš© í™•ì¸
kubectl get nodereadinessrule
kubectl describe nodereadinessrule production-readiness
```

#### Step 3: ë…¸ë“œ ì¡°ê±´ ëª¨ë‹ˆí„°ë§

```bash
# ìƒˆ ë…¸ë“œê°€ í”„ë¡œë¹„ì €ë‹ë˜ë©´ ì¡°ê±´ í™•ì¸
kubectl get nodes -o json | jq '.items[] | {
  name: .metadata.name,
  conditions: [.status.conditions[] | select(.type |
    test("ebs.csi.aws.com|vpc.amazonaws.com")) |
    {type: .type, status: .status}]
}'

# Taint ìƒíƒœ í™•ì¸
kubectl get nodes -o json | jq '.items[] | {
  name: .metadata.name,
  taints: .spec.taints
}'
```

#### Step 4: Karpenter NodePool ìµœì í™”

```yaml
# Karpenter NodePool with NRC
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: optimized-pool
spec:
  template:
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64", "arm64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]

      # NRCê°€ taintë¥¼ ìë™ ê´€ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œì™¸
      # taints: []  # NRCê°€ ê´€ë¦¬

      # ë…¸ë“œ ë¶€íŠ¸ìŠ¤íŠ¸ë© ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
      kubelet:
        maxPods: 110
        # NRCë¡œ ì¸í•´ ë…¸ë“œ Readyê¹Œì§€ ì‹œê°„ ì¦ê°€ (30ì´ˆ â†’ 60ì´ˆ)
        # Karpenterê°€ ë„ˆë¬´ ë¹¨ë¦¬ íƒ€ì„ì•„ì›ƒí•˜ì§€ ì•Šë„ë¡ ì„¤ì •
        systemReserved:
          cpu: 100m
          memory: 512Mi

  disruption:
    consolidationPolicy: WhenUnderutilized
    # NRCë¡œ ì¸í•´ ë…¸ë“œ ì‹œì‘ì´ ëŠë ¤ì§€ë¯€ë¡œ consolidation ê°„ê²© ì¦ê°€
    consolidateAfter: 60s  # ê¸°ë³¸ 30s â†’ 60s
```

:::warning GPU ë…¸ë“œ íŠ¹ë³„ ê³ ë ¤ì‚¬í•­
GPU ë“œë¼ì´ë²„ ë¡œë”©ì€ 30-60ì´ˆ ì†Œìš”ë˜ë¯€ë¡œ, GPU NodePoolì—ëŠ” ë°˜ë“œì‹œ NRCë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ìƒíƒœì—ì„œ Podê°€ ìŠ¤ì¼€ì¤„ë§ë˜ì–´ ì§€ì†ì ìœ¼ë¡œ ì‹¤íŒ¨í•©ë‹ˆë‹¤.

```yaml
# GPU ì „ìš© NRC
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-readiness
spec:
  nodeSelector:
    matchExpressions:
      - key: nvidia.com/gpu
        operator: Exists
  conditions:
    - type: "nvidia.com/gpu-driver-ready"
      requiredStatus: "True"
  taint:
    key: "nvidia.com/gpu-not-ready"
    effect: "NoSchedule"
  enforcementMode: "bootstrap-only"
```
:::

### 5.8 ë¬¸ì œ í•´ê²° ë° ëª¨ë‹ˆí„°ë§

#### ì¼ë°˜ì ì¸ ë¬¸ì œ

**1. ë…¸ë“œê°€ ê³„ì† NotReady ìƒíƒœ:**

```bash
# ë…¸ë“œ ì¡°ê±´ ìƒì„¸ í™•ì¸
kubectl describe node <node-name> | grep -A 10 "Conditions:"

# NRC ì´ë²¤íŠ¸ í™•ì¸
kubectl get events --all-namespaces --field-selector involvedObject.kind=Node,involvedObject.name=<node-name>

# ë“œë¼ì´ë²„ DaemonSet ìƒíƒœ í™•ì¸
kubectl get pods -n kube-system | grep -E "aws-node|ebs-csi|nvidia"
```

**2. Taintê°€ ì œê±°ë˜ì§€ ì•ŠìŒ:**

```bash
# NRCê°€ ë™ì‘ ì¤‘ì¸ì§€ í™•ì¸
kubectl logs -n kube-system -l app=karpenter -c controller | grep "NodeReadiness"

# ìˆ˜ë™ìœ¼ë¡œ taint ì œê±° (ì„ì‹œ í•´ê²°)
kubectl taint nodes <node-name> readiness.k8s.io/not-ready:NoSchedule-
```

#### Prometheus ë©”íŠ¸ë¦­

```yaml
# ServiceMonitor for NRC metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-readiness-controller
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: karpenter
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s

# ì£¼ìš” ë©”íŠ¸ë¦­:
# - node_readiness_controller_reconcile_duration_seconds
# - node_readiness_controller_condition_evaluation_total
# - node_readiness_controller_taint_operations_total
```

:::tip ì°¸ê³  ìë£Œ
- **ê³µì‹ ë¸”ë¡œê·¸**: [Introducing Node Readiness Controller](https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/)
- **KEP (Kubernetes Enhancement Proposal)**: KEP-4403
- **API ë¬¸ì„œ**: `readiness.node.x-k8s.io/v1alpha1`
:::

## Right-Sizing ë°©ë²•ë¡ 

### 6.1 í˜„ì¬ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„

#### kubectl top ì‚¬ìš©

```bash
# ë…¸ë“œë³„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
kubectl top nodes

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ Pod ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
kubectl top pods -n production --sort-by=cpu
kubectl top pods -n production --sort-by=memory

# íŠ¹ì • Podì˜ ì»¨í…Œì´ë„ˆë³„ ì‚¬ìš©ëŸ‰
kubectl top pods <pod-name> --containers -n production
```

#### Metrics Server API ì§ì ‘ ì¿¼ë¦¬

```bash
# CPU ì‚¬ìš©ëŸ‰
kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/production/pods | jq '.items[] | {name: .metadata.name, cpu: .containers[0].usage.cpu}'

# Memory ì‚¬ìš©ëŸ‰
kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/production/pods | jq '.items[] | {name: .metadata.name, memory: .containers[0].usage.memory}'
```

#### Container Insights (AWS)

```bash
# CloudWatch Logs Insights ì¿¼ë¦¬
fields @timestamp, PodName, ContainerName, pod_cpu_utilization, pod_memory_utilization
| filter Namespace = "production"
| stats avg(pod_cpu_utilization) as avg_cpu,
        max(pod_cpu_utilization) as max_cpu,
        avg(pod_memory_utilization) as avg_mem,
        max(pod_memory_utilization) as max_mem
  by PodName
| sort max_cpu desc
```

#### 6.1.5 CloudWatch Observability Operator ê¸°ë°˜ ìë™ ë¶„ì„

AWSëŠ” 2025ë…„ 12ì›” **CloudWatch Observability Operator**ë¥¼ í†µí•´ EKS Control Plane ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¦¬ì†ŒìŠ¤ ë³‘ëª©ì„ ì„ ì œì ìœ¼ë¡œ ê°ì§€í•˜ê³  ìë™í™”ëœ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

**CloudWatch Observability Operator ì„¤ì¹˜:**

```bash
# 1. Helm ë ˆí¬ì§€í† ë¦¬ ì¶”ê°€
helm repo add eks https://aws.github.io/eks-charts
helm repo update

# 2. Operator ì„¤ì¹˜ (Amazon CloudWatch Observability namespace)
helm install amazon-cloudwatch-observability eks/amazon-cloudwatch-observability \
  --namespace amazon-cloudwatch \
  --create-namespace \
  --set clusterName=<cluster-name> \
  --set region=<region>

# 3. ì„¤ì¹˜ í™•ì¸
kubectl get pods -n amazon-cloudwatch

# ì˜ˆìƒ ì¶œë ¥:
# NAME                                                     READY   STATUS    RESTARTS   AGE
# amazon-cloudwatch-observability-controller-manager-xxx   2/2     Running   0          2m
# cloudwatch-agent-xxx                                     1/1     Running   0          2m
# dcgm-exporter-xxx                                        1/1     Running   0          2m
# fluent-bit-xxx                                           1/1     Running   0          2m
```

**Container Insights Enhanced ê¸°ëŠ¥:**

CloudWatch Observability OperatorëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

| ê¸°ëŠ¥ | ì„¤ëª… | í™œìš© |
|------|------|------|
| **ì´ìƒ íƒì§€** | CloudWatch Anomaly Detectionìœ¼ë¡œ ë¹„ì •ìƒ íŒ¨í„´ ìë™ ì‹ë³„ | CPU/Memory ìŠ¤íŒŒì´í¬ ì‚¬ì „ ê°ì§€ |
| **ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œê°í™”** | ì‹œê³„ì—´ ê·¸ë˜í”„ì—ì„œ ì§€ì†ì  ì¦ê°€ íŒ¨í„´ ê°•ì¡° í‘œì‹œ | ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì¡°ê¸° ë°œê²¬ |
| **ë“œë¦´ë‹¤ìš´ ë¶„ì„** | Namespace â†’ Deployment â†’ Pod â†’ Container ê³„ì¸µ íƒìƒ‰ | ë¦¬ì†ŒìŠ¤ ë³‘ëª© ê·¼ë³¸ ì›ì¸ ë¶„ì„ |
| **Control Plane ë©”íŠ¸ë¦­** | API Server, etcd, Scheduler ì„±ëŠ¥ ë©”íŠ¸ë¦­ | í´ëŸ¬ìŠ¤í„° ìŠ¤ì¼€ì¼ë§ ë³‘ëª© ì‚¬ì „ ê°ì§€ |
| **ì•ŒëŒ ìë™ ìƒì„±** | ê¶Œì¥ ì„ê³„ê°’ ê¸°ë°˜ CloudWatch ì•ŒëŒ ìë™ êµ¬ì„± | ìš´ì˜ ìë™í™” |

**EKS Control Plane ë©”íŠ¸ë¦­ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ë³‘ëª© ì„ ì œ ê°ì§€:**

Control Plane ë©”íŠ¸ë¦­ì„ í†µí•´ Pod ìŠ¤ì¼€ì¤„ë§ ì§€ì—°, API Server ê³¼ë¶€í•˜ ë“± ë¦¬ì†ŒìŠ¤ ìµœì í™”ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í´ëŸ¬ìŠ¤í„° ìˆ˜ì¤€ ë¬¸ì œë¥¼ ì‚¬ì „ì— ê°ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# CloudWatch Insights ì¿¼ë¦¬ - Control Plane API Server ë¶€í•˜ ë¶„ì„
fields @timestamp, apiserver_request_duration_seconds_sum, apiserver_request_total
| filter @logStream like /kube-apiserver/
| stats avg(apiserver_request_duration_seconds_sum) as avg_latency,
        max(apiserver_request_total) as max_requests
  by bin(5m)
| sort @timestamp desc
```

**ì£¼ìš” Control Plane ë©”íŠ¸ë¦­:**

| ë©”íŠ¸ë¦­ | ì˜ë¯¸ | ì„ê³„ê°’ | ëŒ€ì‘ |
|--------|------|--------|------|
| `apiserver_request_duration_seconds` | API ìš”ì²­ ë ˆì´í„´ì‹œ | P95 > 1ì´ˆ | Provisioned Control Plane ê³ ë ¤ |
| `etcd_request_duration_seconds` | etcd ì‘ë‹µ ì‹œê°„ | P95 > 100ms | ë…¸ë“œ/Pod ìˆ˜ ì¤„ì´ê¸° |
| `scheduler_schedule_attempts_total` | ìŠ¤ì¼€ì¤„ë§ ì‹œë„ íšŸìˆ˜ | ì‹¤íŒ¨ìœ¨ > 5% | ë¦¬ì†ŒìŠ¤ ë¶€ì¡±, Node Affinity ê²€í†  |
| `workqueue_depth` | Control Plane ì‘ì—… í ê¹Šì´ | > 100 | í´ëŸ¬ìŠ¤í„° ê³¼ë¶€í•˜ ì‹ í˜¸ |

**Data-Driven ìµœì í™”ì˜ 3ê°€ì§€ ë‚­ë¹„ íŒ¨í„´ (AWS ê³µì‹ ê°€ì´ë“œ):**

AWSê°€ 2025ë…„ 11ì›” ê³µê°œí•œ [Data-driven Amazon EKS cost optimization](https://aws.amazon.com/blogs/containers/data-driven-amazon-eks-cost-optimization-a-practical-guide-to-workload-analysis/) ê°€ì´ë“œì—ì„œëŠ” ì‹¤ì œ ë°ì´í„° ë¶„ì„ì„ í†µí•´ ë‹¤ìŒ 3ê°€ì§€ ì£¼ìš” ë‚­ë¹„ íŒ¨í„´ì„ ì‹ë³„í–ˆìŠµë‹ˆë‹¤:

```mermaid
graph TB
    A[ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ íŒ¨í„´ ë¶„ì„] --> B[1. Greedy Workloads]
    A --> C[2. Pet Workloads]
    A --> D[3. Isolated Workloads]

    B --> B1[ê³¼ë„í•œ ë¦¬ì†ŒìŠ¤ ìš”ì²­]
    B1 --> B2[ì‹¤ì œ ì‚¬ìš©ëŸ‰ì˜ 3-5ë°° requests]
    B2 --> B3[ë…¸ë“œ íŒŒí¸í™” ìœ ë°œ]

    C --> C1[ì—„ê²©í•œ PodDisruptionBudget]
    C1 --> C2[ë…¸ë“œ ë“œë ˆì´ë‹ ì°¨ë‹¨]
    C2 --> C3[í´ëŸ¬ìŠ¤í„° ìŠ¤ì¼€ì¼ ë‹¤ìš´ ë°©í•´]

    D --> D1[íŠ¹ì • ë…¸ë“œì— ê³ ì •ëœ ì›Œí¬ë¡œë“œ]
    D1 --> D2[Node Affinity/Selector ê³¼ë‹¤ ì‚¬ìš©]
    D2 --> D3[ë…¸ë“œ í’€ íŒŒí¸í™”]

    style B fill:#ff6b6b
    style C fill:#ffa94d
    style D fill:#ffd43b
```

**1. Greedy Workloads (íƒìš•ìŠ¤ëŸ¬ìš´ ì›Œí¬ë¡œë“œ):**

ê³¼ë„í•˜ê²Œ ë¦¬ì†ŒìŠ¤ë¥¼ ìš”ì²­í•˜ëŠ” Podë¡œ ì¸í•´ ë…¸ë“œ í™œìš©ë¥ ì´ ë‚®ì•„ì§€ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤.

```bash
# CloudWatch Insights ì¿¼ë¦¬ - Over-requesting ì»¨í…Œì´ë„ˆ ì‹ë³„
fields @timestamp, PodName, ContainerName, pod_cpu_request, pod_cpu_utilization_over_pod_limit
| filter Namespace = "production"
| stats avg(pod_cpu_request) as avg_requested,
        avg(pod_cpu_utilization_over_pod_limit) as avg_utilization
  by PodName
| filter avg_utilization < 30  # ìš”ì²­ëŸ‰ì˜ 30% ë¯¸ë§Œ ì‚¬ìš©
| sort avg_requested desc
```

**ì‹ë³„ ê¸°ì¤€:**
- CPU requestsì˜ 30% ë¯¸ë§Œ ì‚¬ìš©
- Memory requestsì˜ 50% ë¯¸ë§Œ ì‚¬ìš©
- ì§€ì† ê¸°ê°„: 7ì¼ ì´ìƒ

**ëŒ€ì‘ ë°©ë²•:**
```yaml
# Before (Greedy)
resources:
  requests:
    cpu: "2000m"       # ì‹¤ì œ ì‚¬ìš©ëŸ‰: 400m (20%)
    memory: "4Gi"      # ì‹¤ì œ ì‚¬ìš©ëŸ‰: 1Gi (25%)

# After (Right-Sized)
resources:
  requests:
    cpu: "500m"        # P95 400m + 20% = 480m â†’ 500m
    memory: "1280Mi"   # P95 1Gi + 20% = 1.2Gi â†’ 1280Mi
  limits:
    memory: "2Gi"
```

**2. Pet Workloads (ì• ì™„ë™ë¬¼ ì›Œí¬ë¡œë“œ):**

ì—„ê²©í•œ PodDisruptionBudget(PDB)ë¡œ ì¸í•´ í´ëŸ¬ìŠ¤í„° ìŠ¤ì¼€ì¼ ë‹¤ìš´ì´ ì°¨ë‹¨ë˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤.

```bash
# PDBë¡œ ì¸í•œ ë…¸ë“œ ë“œë ˆì´ë‹ ì‹¤íŒ¨ í™•ì¸
kubectl get events --all-namespaces \
  --field-selector reason=EvictionFailed \
  --sort-by='.lastTimestamp'

# ì˜ˆìƒ ì¶œë ¥:
# NAMESPACE   LAST SEEN   TYPE      REASON           MESSAGE
# production  5m          Warning   EvictionFailed   Cannot evict pod as it would violate the pod's disruption budget
```

**ì‹ë³„ ê¸°ì¤€:**
- `minAvailable: 100%` ë˜ëŠ” `maxUnavailable: 0` ì„¤ì •
- ì¥ê¸°ê°„(>30ë¶„) Pending ìƒíƒœ ë…¸ë“œ ì¡´ì¬
- Karpenter/Cluster Autoscaler ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì‹¤íŒ¨ ë¡œê·¸

**ëŒ€ì‘ ë°©ë²•:**
```yaml
# Before (Pet)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
spec:
  minAvailable: 100%  # ëª¨ë“  Pod ë³´í˜¸ â†’ ìŠ¤ì¼€ì¼ ë‹¤ìš´ ë¶ˆê°€

# After (Balanced)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
spec:
  minAvailable: 80%   # 20% ì—¬ìœ ë¡œ ìŠ¤ì¼€ì¼ ë‹¤ìš´ í—ˆìš©
  selector:
    matchLabels:
      app: critical-app
```

**3. Isolated Workloads (ê³ ë¦½ëœ ì›Œí¬ë¡œë“œ):**

ê³¼ë„í•œ Node Affinity, Taints/Tolerationsë¡œ ì¸í•´ ë…¸ë“œ í’€ì´ íŒŒí¸í™”ë˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤.

```bash
# ë…¸ë“œë³„ Pod ìˆ˜ì™€ í™œìš©ë¥  ë¶„ì„
kubectl get nodes -o json | jq -r '
  .items[] |
  {
    name: .metadata.name,
    pods: (.status.allocatable.pods | tonumber),
    cpu_capacity: (.status.capacity.cpu | tonumber),
    cpu_allocatable: (.status.allocatable.cpu | tonumber)
  }
' | jq -s 'sort_by(.pods) | .[]'
```

**ì‹ë³„ ê¸°ì¤€:**
- ë…¸ë“œë‹¹ í‰ê·  Pod ìˆ˜ < 10ê°œ
- ë…¸ë“œ ìˆ˜ > í•„ìš” ìš©ëŸ‰ì˜ 150%
- NodeSelector/Affinity ì‚¬ìš©ë¥  > 50%

**ëŒ€ì‘ ë°©ë²•:**
```yaml
# Before (Isolated)
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: workload-type
          operator: In
          values:
          - api-server-v2  # ë„ˆë¬´ êµ¬ì²´ì  â†’ ë…¸ë“œ íŒŒí¸í™”

# After (Flexible)
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:  # required â†’ preferred
    - weight: 100
      preference:
        matchExpressions:
        - key: workload-class
          operator: In
          values:
          - compute-optimized  # ë” ë„“ì€ ë²”ì£¼
```

**Data-Driven ìµœì í™” í”Œë¡œìš°:**

```mermaid
graph LR
    A[1. ë°ì´í„° ìˆ˜ì§‘] --> B[2. íŒ¨í„´ ë¶„ì„]
    B --> C[3. ë‚­ë¹„ ì‹ë³„]
    C --> D[4. ìµœì í™” ì ìš©]
    D --> E[5. ê²€ì¦]
    E --> F{ëª©í‘œ ë‹¬ì„±?}
    F -->|Yes| G[ì§€ì†ì  ëª¨ë‹ˆí„°ë§]
    F -->|No| B

    A1[CloudWatch Container Insights] --> A
    A2[Prometheus Metrics] --> A
    A3[Cost Explorer] --> A

    C1[Greedy Workloads] --> C
    C2[Pet Workloads] --> C
    C3[Isolated Workloads] --> C

    D1[Right-Sizing] --> D
    D2[PDB ì™„í™”] --> D
    D3[Affinity ìµœì í™”] --> D

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style G fill:#c8e6c9
```

**ì‹¤ì œ íš¨ê³¼ ì‚¬ë¡€ (AWS ê³µì‹ ê°€ì´ë“œ):**

| ì¡°ì§ | ë‚­ë¹„ íŒ¨í„´ | ì ìš© ì¡°ì¹˜ | ì ˆê° íš¨ê³¼ |
|------|----------|----------|----------|
| í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… | Greedy Workloads 40% | VPA ê¶Œì¥ì‚¬í•­ ì ìš© | ë…¸ë“œ ìˆ˜ 35% ê°ì†Œ |
| ì´ì»¤ë¨¸ìŠ¤ ê¸°ì—… | Pet Workloads 25% | PDB minAvailable 80%ë¡œ ì™„í™” | ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì†ë„ 3ë°° í–¥ìƒ |
| SaaS í”Œë«í¼ | Isolated Workloads 30% | NodeSelector ì œê±°, Spot í™œìš© | ë¹„ìš© 45% ì ˆê° |

:::tip ìë™í™”ëœ ë‚­ë¹„ íŒ¨í„´ íƒì§€
CloudWatch Contributor Insightsë¥¼ ì‚¬ìš©í•˜ë©´ ìœ„ 3ê°€ì§€ íŒ¨í„´ì„ ìë™ìœ¼ë¡œ íƒì§€í•˜ëŠ” ê·œì¹™ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# Contributor Insights ê·œì¹™ ìƒì„± (Greedy Workloads)
aws cloudwatch put-insight-rule \
  --rule-name "EKS-GreedyWorkloads" \
  --rule-definition file://greedy-workloads-rule.json
```

ê·œì¹™ ì •ì˜ ì˜ˆì‹œ:
```json
{
  "Schema": {
    "Name": "CloudWatchLogRule",
    "Version": 1
  },
  "LogGroupNames": ["/aws/containerinsights/<cluster-name>/performance"],
  "LogFormat": "JSON",
  "Contribution": {
    "Keys": ["PodName"],
    "Filters": [
      {
        "Match": "$.Type",
        "In": ["Pod"]
      },
      {
        "Match": "$.pod_cpu_utilization_over_pod_limit",
        "LessThan": 30
      }
    ],
    "ValueOf": "pod_cpu_request"
  },
  "AggregateOn": "Sum"
}
```
:::

#### Prometheus ì¿¼ë¦¬

```promql
# CPU ì‚¬ìš©ëŸ‰ (P95, 7ì¼ê°„)
quantile_over_time(0.95,
  sum by (pod, namespace) (
    rate(container_cpu_usage_seconds_total{namespace="production"}[5m])
  )[7d:5m]
)

# Memory ì‚¬ìš©ëŸ‰ (P95, 7ì¼ê°„)
quantile_over_time(0.95,
  sum by (pod, namespace) (
    container_memory_working_set_bytes{namespace="production"}
  )[7d:5m]
)

# CPU Requestsì™€ ì‹¤ì œ ì‚¬ìš©ëŸ‰ ë¹„êµ
sum by (pod) (rate(container_cpu_usage_seconds_total[5m]))
/
sum by (pod) (kube_pod_container_resource_requests{resource="cpu"})

# Memory Requestsì™€ ì‹¤ì œ ì‚¬ìš©ëŸ‰ ë¹„êµ
sum by (pod) (container_memory_working_set_bytes)
/
sum by (pod) (kube_pod_container_resource_requests{resource="memory"})
```

### 6.2 Goldilocksë¥¼ í™œìš©í•œ ìë™ Right-Sizing

GoldilocksëŠ” VPA Recommenderë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì‹œë³´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

#### ì„¤ì¹˜

```bash
# Helmìœ¼ë¡œ ì„¤ì¹˜
helm repo add fairwinds-stable https://charts.fairwinds.com/stable
helm repo update

helm install goldilocks fairwinds-stable/goldilocks \
  --namespace goldilocks \
  --create-namespace \
  --set dashboard.service.type=LoadBalancer
```

#### ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™œì„±í™”

```bash
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë ˆì´ë¸” ì¶”ê°€
kubectl label namespace production goldilocks.fairwinds.com/enabled=true
kubectl label namespace staging goldilocks.fairwinds.com/enabled=true

# Goldilocksê°€ ìë™ìœ¼ë¡œ VPA ìƒì„± (Off ëª¨ë“œ)
kubectl get vpa -n production
```

#### ëŒ€ì‹œë³´ë“œ ì ‘ê·¼

```bash
# ëŒ€ì‹œë³´ë“œ URL í™•ì¸
kubectl get svc -n goldilocks goldilocks-dashboard

# í¬íŠ¸ í¬ì›Œë”©
kubectl port-forward -n goldilocks svc/goldilocks-dashboard 8080:80

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8080 ì ‘ì†
```

**ëŒ€ì‹œë³´ë“œ ê¸°ëŠ¥:**
- ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë¦¬ì†ŒìŠ¤ ê¶Œì¥ì‚¬í•­
- VPA Lower Bound, Target, Upper Bound í‘œì‹œ
- í˜„ì¬ ì„¤ì •ê³¼ ê¶Œì¥ê°’ ë¹„êµ
- QoS í´ë˜ìŠ¤ í‘œì‹œ

### 6.3 Container Insights Enhanced ì´ìƒ íƒì§€ í™œìš©

AWS Container Insights EnhancedëŠ” ê¸°ì¡´ Container Insightsë³´ë‹¤ í–¥ìƒëœ ê´€ì°°ì„± ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, íŠ¹íˆ **ìë™ ì´ìƒ íƒì§€**ì™€ **ë“œë¦´ë‹¤ìš´ ë¶„ì„** ê¸°ëŠ¥ì„ í†µí•´ ë¦¬ì†ŒìŠ¤ ë¬¸ì œë¥¼ ì¡°ê¸°ì— ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 6.3.1 Container Insights Enhanced ê°œìš”

**ê¸°ì¡´ Container Insights ëŒ€ë¹„ í–¥ìƒëœ ê¸°ëŠ¥:**

| ê¸°ëŠ¥ | ê¸°ì¡´ Container Insights | Enhanced |
|------|------------------------|----------|
| **ë©”íŠ¸ë¦­ ìˆ˜ì§‘** | Pod/Container ë ˆë²¨ | Pod/Container + ë„¤íŠ¸ì›Œí¬ ì„¸ë¶„í™” |
| **ì´ìƒ íƒì§€** | ìˆ˜ë™ (ì‚¬ìš©ìê°€ ì„ê³„ê°’ ì„¤ì •) | **ìë™ (ML ê¸°ë°˜ anomaly detection)** |
| **ë“œë¦´ë‹¤ìš´** | ì œí•œì  | **ì™„ì „í•œ ê³„ì¸µ êµ¬ì¡° (Cluster â†’ Node â†’ Pod â†’ Container)** |
| **ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€** | ìˆ˜ë™ ë¶„ì„ í•„ìš” | **ì‹œê°ì  íŒ¨í„´ ìë™ ì‹ë³„** |
| **CPU Throttling** | ë©”íŠ¸ë¦­ë§Œ ì œê³µ | **ìë™ ê²½ê³  + ì›ì¸ ë¶„ì„** |
| **ë„¤íŠ¸ì›Œí¬ ê´€ì°°ì„±** | ê¸°ë³¸ | **Pod-to-Pod íë¦„ ë¶„ì„** |

**í™œì„±í™” ë°©ë²•:**

```bash
# CloudWatch Observability Operator ë°°í¬
kubectl apply -f https://raw.githubusercontent.com/aws-observability/aws-cloudwatch-observability-operator/main/deploy/operator.yaml

# Container Insights Enhanced í™œì„±í™”
cat <<EOF | kubectl apply -f -
apiVersion: cloudwatch.aws.amazon.com/v1alpha1
kind: CloudWatchObservability
metadata:
  name: cloudwatch-observability
spec:
  enableContainerInsights: true
  enableEnhancedContainerInsights: true  # Enhanced í™œì„±í™”
  enableAutoInstrumentation: true
EOF

# í™œì„±í™” í™•ì¸
kubectl get cloudwatchobservability cloudwatch-observability -o yaml
```

#### 6.3.2 ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œê°ì  ì‹ë³„ íŒ¨í„´

Container Insights EnhancedëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì˜ **ì ì§„ì  ì¦ê°€ íŒ¨í„´**ì„ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤.

**ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€ ì‹œë‚˜ë¦¬ì˜¤:**

```mermaid
graph TB
    subgraph "Container Insights Enhanced ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€"
        A[ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘] --> B[CloudWatch Anomaly Detection]
        B --> C{ì´ìƒ íŒ¨í„´ ê°ì§€?}

        C -->|ì •ìƒ| D[ì •ìƒ ëª¨ë‹ˆí„°ë§ ì§€ì†]
        C -->|ë©”ëª¨ë¦¬ ì ì§„ì  ì¦ê°€| E[ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬]

        E --> F[ìë™ ì•Œë¦¼ ë°œì†¡<br/>SNS/Slack/PagerDuty]
        F --> G[ë“œë¦´ë‹¤ìš´ ë¶„ì„ ì‹œì‘]

        G --> H[Pod ë ˆë²¨ ë©”íŠ¸ë¦­ í™•ì¸]
        H --> I[Container ë ˆë²¨ ìƒì„¸ ë¶„ì„]
        I --> J[ì›ì¸ Container ì‹ë³„]

        J --> K[ë¦¬ì†ŒìŠ¤ Right-Sizing ë˜ëŠ”<br/>ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ì •]
    end

    style E fill:#ff6b6b
    style J fill:#ffa94d
    style K fill:#51cf66
```

**CloudWatch Consoleì—ì„œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸:**

1. **CloudWatch â†’ Container Insights â†’ Performance monitoring**
2. **View: EKS Pods** ì„ íƒ
3. **ë©”íŠ¸ë¦­: Memory Utilization (%)** ì„ íƒ
4. **Anomaly Detection Band í™œì„±í™”**

```
ì •ìƒ íŒ¨í„´:
Memory (%) â–²
100% |                    â”Œâ”€â”€â”€â”€â”
     |        â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”˜    â””â”€â”€â”
 50% |   â”Œâ”€â”€â”€â”˜    â””â”€â”€â”˜           â””â”€â”€â”€â”
     |â”€â”€â”€â”˜                            â””â”€â”€â”€
  0% +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
     0h    6h   12h   18h   24h        Time

ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íŒ¨í„´ (ğŸš¨):
Memory (%) â–²
100% |                          â”Œâ”€â”€â”€â”€OOM Kill
     |                    â”Œâ”€â”€â”€â”€â”¤
 50% |           â”Œâ”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
     |      â”Œâ”€â”€â”€â”€â”¤       â”‚     â”‚
  0% +â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
     0h    6h   12h   18h   24h        Time
     ì ì§„ì  ìƒìŠ¹ (Anomaly Detectionì´ ìë™ ê°ì§€)
```

**ìë™ ì•Œë¦¼ ì„¤ì • ì˜ˆì‹œ:**

```yaml
# CloudWatch Alarm with Anomaly Detection
apiVersion: v1
kind: ConfigMap
metadata:
  name: memory-leak-alarm
data:
  alarm.json: |
    {
      "AlarmName": "EKS-MemoryLeak-Detection",
      "ComparisonOperator": "LessThanLowerOrGreaterThanUpperThreshold",
      "EvaluationPeriods": 3,
      "Metrics": [
        {
          "Id": "m1",
          "ReturnData": true,
          "MetricStat": {
            "Metric": {
              "Namespace": "ContainerInsights",
              "MetricName": "pod_memory_utilization",
              "Dimensions": [
                {
                  "Name": "ClusterName",
                  "Value": "production-eks"
                }
              ]
            },
            "Period": 300,
            "Stat": "Average"
          }
        },
        {
          "Id": "ad1",
          "Expression": "ANOMALY_DETECTION_BAND(m1, 2)",
          "Label": "MemoryUsage (Expected)"
        }
      ],
      "ThresholdMetricId": "ad1",
      "ActionsEnabled": true,
      "AlarmActions": [
        "arn:aws:sns:us-east-1:123456789012:ops-alerts"
      ]
    }
```

**AWS CLIë¡œ ì•Œë¦¼ ìƒì„±:**

```bash
# Anomaly Detection ê¸°ë°˜ ë©”ëª¨ë¦¬ ì•Œë¦¼
aws cloudwatch put-metric-alarm \
  --alarm-name eks-memory-leak-detection \
  --alarm-description "Detects memory leak patterns in EKS pods" \
  --comparison-operator LessThanLowerOrGreaterThanUpperThreshold \
  --evaluation-periods 3 \
  --metrics '[
    {
      "Id": "m1",
      "ReturnData": true,
      "MetricStat": {
        "Metric": {
          "Namespace": "ContainerInsights",
          "MetricName": "pod_memory_utilization",
          "Dimensions": [
            {"Name": "ClusterName", "Value": "production-eks"}
          ]
        },
        "Period": 300,
        "Stat": "Average"
      }
    },
    {
      "Id": "ad1",
      "Expression": "ANOMALY_DETECTION_BAND(m1, 2)"
    }
  ]' \
  --threshold-metric-id ad1 \
  --alarm-actions arn:aws:sns:us-east-1:123456789012:ops-alerts
```

#### 6.3.3 CPU Throttling ìë™ íƒì§€

Container Insights EnhancedëŠ” CPU throttlingì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³ , **ê³¼ë„í•œ CPU limit ì„¤ì •**ì„ ê²½ê³ í•©ë‹ˆë‹¤.

**CPU Throttling ë©”íŠ¸ë¦­:**

```
throttled_time_percentage = (container_cpu_cfs_throttled_seconds_total / container_cpu_cfs_periods_total) * 100

ì •ìƒ: <5%
ì£¼ì˜: 5-10% âš ï¸
ì‹¬ê°: >10% ğŸš¨ (HPA ë˜ëŠ” CPU limits ì œê±° í•„ìš”)
```

**CloudWatch Insights ì¿¼ë¦¬ë¡œ Throttling ë¶„ì„:**

```sql
# CloudWatch Logs Insights ï¿½ery
fields @timestamp, kubernetes.pod_name, cpu_limit_millicores, cpu_usage_millicores, throttled_time_ms
| filter kubernetes.namespace_name = "production"
| filter throttled_time_ms > 100  # 100ms ì´ìƒ throttling
| stats
    avg(cpu_usage_millicores) as avg_cpu,
    max(cpu_usage_millicores) as max_cpu,
    avg(throttled_time_ms) as avg_throttled,
    count(*) as throttling_count
  by kubernetes.pod_name
| sort throttling_count desc
| limit 20

# ê²°ê³¼ ì˜ˆì‹œ:
# pod_name            avg_cpu  max_cpu  avg_throttled  throttling_count
# web-app-abc123      450m     800m     250ms          150
# api-server-def456   600m     1000m    180ms          120
```

**Throttling ìë™ ê²½ê³  CloudWatch Alarm:**

```bash
aws cloudwatch put-metric-alarm \
  --alarm-name eks-cpu-throttling-high \
  --alarm-description "Alerts when CPU throttling exceeds 10%" \
  --namespace ContainerInsights \
  --metric-name pod_cpu_throttled_percentage \
  --dimensions Name=ClusterName,Value=production-eks \
  --statistic Average \
  --period 300 \
  --threshold 10 \
  --comparison-operator GreaterThanThreshold \
  --evaluation-periods 2 \
  --alarm-actions arn:aws:sns:us-east-1:123456789012:ops-alerts
```

#### 6.3.4 ì´ìƒ íƒì§€ ë°´ë“œ (Anomaly Detection Band) ì„¤ì •

CloudWatch Anomaly Detectionì€ ML ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì •ìƒ ë²”ìœ„ë¥¼ ìë™ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

**Anomaly Detection ì‘ë™ ì›ë¦¬:**

```
1. í•™ìŠµ ê¸°ê°„: ìµœì†Œ 2ì£¼ ë°ì´í„° ìˆ˜ì§‘
2. ML ëª¨ë¸ í›ˆë ¨: ì‹œê°„ëŒ€ë³„, ìš”ì¼ë³„ íŒ¨í„´ í•™ìŠµ
3. ì˜ˆì¸¡ ë²”ìœ„ ìƒì„±: ì˜ˆìƒ ìƒí•œ/í•˜í•œ ê³„ì‚°
4. ì‹¤ì‹œê°„ ë¹„êµ: ì‹¤ì œê°’ì´ ë²”ìœ„ ë°–ì´ë©´ ì•Œë¦¼
```

**ë°´ë“œ í­ ì¡°ì • (Standard Deviation):**

```yaml
# 2 Standard Deviations (ê¸°ë³¸, 95% ì‹ ë¢°êµ¬ê°„)
Expression: ANOMALY_DETECTION_BAND(m1, 2)

# 3 Standard Deviations (99.7% ì‹ ë¢°êµ¬ê°„, ë” ë³´ìˆ˜ì )
Expression: ANOMALY_DETECTION_BAND(m1, 3)

# 1 Standard Deviation (68% ì‹ ë¢°êµ¬ê°„, ë¯¼ê°í•˜ê²Œ ê°ì§€)
Expression: ANOMALY_DETECTION_BAND(m1, 1)
```

**ì‹œê°ì  ì˜ˆì‹œ:**

```
ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ â–²
              |     â”Œâ”€â”€â”€â”€ Upper Band (ì˜ˆì¸¡ ìƒí•œ)
              |    /
         100% | â”€â”€â—â”€â”€â”€â”€  ì‹¤ì œ ì‚¬ìš©ëŸ‰ (ì´ìƒ ì—†ìŒ)
              |  / â”‚
              | /  â”‚
          50% |â”€â”€â”€â”€â—â”€â”€â”€â”€  ì‹¤ì œ ì‚¬ìš©ëŸ‰ (ì •ìƒ)
              | \  â”‚
              |  \ â”‚
           0% | â”€â”€â—â”€â”€â”€â”€  Lower Band (ì˜ˆì¸¡ í•˜í•œ)
              +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
              0h   6h   12h   18h   24h
```

#### 6.3.5 ì‹¤ì „ ì›Œí¬í”Œë¡œìš°: ì´ìƒ íƒì§€ â†’ ì¡°ì‚¬ â†’ Right-Sizing

**Step 1: CloudWatch Alarm íŠ¸ë¦¬ê±°**

```
[CloudWatch Alarm] â†’ [SNS Topic] â†’ [Slack Webhook]

ì•Œë¦¼ ì˜ˆì‹œ:
ğŸš¨ EKS Memory Anomaly Detected
Cluster: production-eks
Pod: web-app-7d8c9f-abc123
Memory Usage: 1.8Gi (Expected: 1.2Gi Â± 200Mi)
Duration: 15 minutes
Action: Investigate memory leak
```

**Step 2: Container Insights ë“œë¦´ë‹¤ìš´ ë¶„ì„**

```bash
# 1. CloudWatch Consoleì—ì„œ í•´ë‹¹ Pod ì„ íƒ
# 2. "View in Container Insights" í´ë¦­
# 3. ê³„ì¸µ êµ¬ì¡° ë“œë¦´ë‹¤ìš´:
#    Cluster â†’ Node â†’ Pod â†’ Container

# ë˜ëŠ” AWS CLIë¡œ ë©”íŠ¸ë¦­ ì¡°íšŒ:
aws cloudwatch get-metric-statistics \
  --namespace ContainerInsights \
  --metric-name pod_memory_utilization \
  --dimensions \
    Name=ClusterName,Value=production-eks \
    Name=Namespace,Value=production \
    Name=PodName,Value=web-app-7d8c9f-abc123 \
  --start-time 2026-02-12T00:00:00Z \
  --end-time 2026-02-12T23:59:59Z \
  --period 300 \
  --statistics Average,Maximum
```

**Step 3: ì›ì¸ ì‹ë³„**

```bash
# ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸
kubectl top pod web-app-7d8c9f-abc123 -n production --containers

# ë¡œê·¸ í™•ì¸ (OOM ê²½ê³ )
kubectl logs web-app-7d8c9f-abc123 -n production | grep -i "memory\|heap\|oom"

# ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë¡œíŒŒì¼ë§ (Java ì˜ˆì‹œ)
kubectl exec web-app-7d8c9f-abc123 -n production -- jmap -heap 1
```

**Step 4: Right-Sizing ì ìš©**

```yaml
# VPA Off ëª¨ë“œë¡œ ê¶Œì¥ì‚¬í•­ í™•ì¸
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"

# VPA ê¶Œì¥ì‚¬í•­ í™•ì¸ í›„ Deployment ì—…ë°ì´íŠ¸
resources:
  requests:
    memory: "2Gi"    # VPA Target 1.8Gi + 20% ë²„í¼
  limits:
    memory: "3Gi"    # Upper Bound 2.5Gi + ì—¬ìœ 
```

**Step 5: ì§€ì†ì  ëª¨ë‹ˆí„°ë§**

```bash
# CloudWatch Alarm ìƒíƒœ í™•ì¸
aws cloudwatch describe-alarms \
  --alarm-names eks-memory-leak-detection \
  --query 'MetricAlarms[0].StateValue'

# ì¶œë ¥: "OK" (ì •ìƒ) ë˜ëŠ” "ALARM" (ì´ìƒ)
```

:::tip Container Insights Enhanced vs Prometheus
Container Insights EnhancedëŠ” **AWS ë„¤ì´í‹°ë¸Œ í†µí•©**ê³¼ **ì œë¡œ ì„¤ì • ì´ìƒ íƒì§€**ê°€ ê°•ì ì…ë‹ˆë‹¤. PrometheusëŠ” ë” ì„¸ë°€í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì´ìƒ íƒì§€ ML ëª¨ë¸ì„ ì§ì ‘ êµ¬ì¶•í•´ì•¼ í•©ë‹ˆë‹¤. ë‘ ë„êµ¬ë¥¼ ë³‘í–‰í•˜ë©´ ìµœìƒì˜ ê´€ì°°ì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

:::warning ì´ìƒ íƒì§€ì˜ í•œê³„
ML ê¸°ë°˜ ì´ìƒ íƒì§€ëŠ” **ê³¼ê±° íŒ¨í„´**ì„ í•™ìŠµí•˜ë¯€ë¡œ, ë‹¤ìŒ ìƒí™©ì—ì„œëŠ” ì˜¤íƒ(False Positive)ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- ì‹ ê·œ ë°°í¬ ì§í›„ (í•™ìŠµ ë°ì´í„° ë¶€ì¡±)
- ë§ˆì¼€íŒ… ìº í˜ì¸ ë“± ê³„íšëœ íŠ¸ë˜í”½ ì¦ê°€
- ê³„ì ˆì„± ì´ë²¤íŠ¸ (ë¸”ë™ í”„ë¼ì´ë°ì´, ì—°ë§ ê²°ì‚° ë“±)

ì´ëŸ¬í•œ ê²½ìš° **ì¼ì‹œì ìœ¼ë¡œ ì•Œë¦¼ì„ ìŒì†Œê±°**í•˜ê±°ë‚˜, **ì˜ˆìƒ ì´ë²¤íŠ¸ë¥¼ Anomaly Detection ëª¨ë¸ì— ë°˜ì˜**í•´ì•¼ í•©ë‹ˆë‹¤.
:::

### 6.4 Right-Sizing í”„ë¡œì„¸ìŠ¤

5ë‹¨ê³„ ì²´ê³„ì  Right-Sizing í”„ë¡œì„¸ìŠ¤:

```mermaid
graph TB
    A[1ë‹¨ê³„: ë² ì´ìŠ¤ë¼ì¸ ìˆ˜ë¦½] --> B[2ë‹¨ê³„: VPA Off ëª¨ë“œ ë°°í¬]
    B --> C[3ë‹¨ê³„: 7-14ì¼ ë°ì´í„° ìˆ˜ì§‘]
    C --> D[4ë‹¨ê³„: ê¶Œì¥ì‚¬í•­ ë¶„ì„]
    D --> E[5ë‹¨ê³„: ë‹¨ê³„ì  ì ìš©]

    E --> F{ê²€ì¦}
    F -->|ì„±ëŠ¥ ì´ìŠˆ| G[ë¡¤ë°±]
    F -->|ì •ìƒ| H[ë‹¤ìŒ ì›Œí¬ë¡œë“œ]

    G --> D
    H --> I[ì§€ì†ì  ëª¨ë‹ˆí„°ë§]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#f3e5f5
    style H fill:#c8e6c9
```

#### 1ë‹¨ê³„: ë² ì´ìŠ¤ë¼ì¸ ìˆ˜ë¦½

```bash
# í˜„ì¬ ë¦¬ì†ŒìŠ¤ ì„¤ì • ë°±ì—…
kubectl get deploy -n production -o yaml > deployments-backup.yaml

# í˜„ì¬ ì‚¬ìš©ëŸ‰ ìŠ¤ëƒ…ìƒ·
kubectl top pods -n production --containers > baseline-usage.txt
```

#### 2ë‹¨ê³„: VPA Off ëª¨ë“œ ë°°í¬

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"
  resourcePolicy:
    containerPolicies:
    - containerName: '*'    # ëª¨ë“  ì»¨í…Œì´ë„ˆ
      minAllowed:
        cpu: "50m"
        memory: "64Mi"
      maxAllowed:
        cpu: "8000m"
        memory: "32Gi"
```

#### 3ë‹¨ê³„: 7-14ì¼ ë°ì´í„° ìˆ˜ì§‘

```bash
# VPA ìƒíƒœ ëª¨ë‹ˆí„°ë§
watch kubectl describe vpa web-app-vpa -n production

# ìµœì†Œ 7ì¼, ê¶Œì¥ 14ì¼ ëŒ€ê¸°
# íŠ¸ë˜í”½ íŒ¨í„´ì´ ì£¼ê°„ ì‚¬ì´í´ì„ ê°€ì§€ëŠ” ê²½ìš° 14ì¼ í•„ìˆ˜
```

#### 4ë‹¨ê³„: ê¶Œì¥ì‚¬í•­ ë¶„ì„

```bash
# VPA ê¶Œì¥ì‚¬í•­ ì¶”ì¶œ
kubectl get vpa web-app-vpa -n production -o jsonpath='{.status.recommendation.containerRecommendations[0]}' | jq .

# ì¶œë ¥ ì˜ˆì‹œ:
# {
#   "containerName": "web-app",
#   "lowerBound": {
#     "cpu": "150m",
#     "memory": "200Mi"
#   },
#   "target": {
#     "cpu": "250m",
#     "memory": "350Mi"
#   },
#   "uncappedTarget": {
#     "cpu": "300m",
#     "memory": "400Mi"
#   },
#   "upperBound": {
#     "cpu": "500m",
#     "memory": "700Mi"
#   }
# }
```

**ê¶Œì¥ì‚¬í•­ í•´ì„:**

| í•­ëª© | ì˜ë¯¸ | ì‚¬ìš© ì‹œì  |
|------|------|----------|
| **Lower Bound** | ìµœì†Œ í•„ìš” ë¦¬ì†ŒìŠ¤ | ê·¹ë‹¨ì  ë¹„ìš© ì ˆê° (ìœ„í—˜) |
| **Target** | **ê¶Œì¥ ì„¤ì •ê°’** | **ê¸°ë³¸ ì‚¬ìš©** â­ |
| **Uncapped Target** | ì œì•½ ì—†ëŠ” ê¶Œì¥ê°’ | maxAllowed ì¡°ì • ì°¸ê³  |
| **Upper Bound** | ìµœëŒ€ ê´€ì°°ëœ ì‚¬ìš©ëŸ‰ | Limits ì„¤ì • ì°¸ê³  |

:::tip Requests ê³„ì‚° ê³µì‹
**ê¶Œì¥ ê³µì‹**: `Requests = VPA Target + 20% ë²„í¼`

ì´ìœ :
- P95 ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ (5% íŠ¸ë˜í”½ ìŠ¤íŒŒì´í¬ ëŒ€ë¹„)
- ë°°í¬, ì´ˆê¸°í™” ë“± ì¼ì‹œì  ì‚¬ìš©ëŸ‰ ì¦ê°€ ëŒ€ì‘
- Throttling, OOM ë¦¬ìŠ¤í¬ ìµœì†Œí™”

**ì˜ˆì‹œ:**
```
VPA Target CPU: 250m
â†’ Requests: 250m * 1.2 = 300m

VPA Target Memory: 350Mi
â†’ Requests: 350Mi * 1.2 = 420Mi (ë°˜ì˜¬ë¦¼ 512Mi)
```
:::

#### 5ë‹¨ê³„: ë‹¨ê³„ì  ì ìš©

```yaml
# ê¸°ì¡´ ì„¤ì •
resources:
  requests:
    cpu: "1000m"       # ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹
    memory: "2Gi"
  limits:
    cpu: "2000m"
    memory: "2Gi"

# VPA Target: CPU 250m, Memory 350Mi

# Right-Sized ì„¤ì •
resources:
  requests:
    cpu: "300m"        # Target 250m + 20% = 300m
    memory: "512Mi"    # Target 350Mi + 20% â‰ˆ 420Mi â†’ 512Mi
  limits:
    # CPU limits ì œê±° (ì••ì¶• ê°€ëŠ¥ ë¦¬ì†ŒìŠ¤)
    memory: "1Gi"      # Upper Bound 700Mi + ì—¬ìœ  = 1Gi
```

**ì ìš© ì „ëµ:**

```bash
# 1. Canary ë°°í¬ (10% íŠ¸ë˜í”½)
kubectl patch deploy web-app -n production -p '
{
  "spec": {
    "strategy": {
      "type": "RollingUpdate",
      "rollingUpdate": {
        "maxSurge": 1,
        "maxUnavailable": 0
      }
    }
  }
}'

# 2. ë¦¬ì†ŒìŠ¤ ë³€ê²½ ì ìš©
kubectl set resources deploy web-app -n production \
  --limits=memory=1Gi \
  --requests=cpu=300m,memory=512Mi

# 3. ëª¨ë‹ˆí„°ë§ (1-3ì¼)
kubectl top pods -n production -l app=web-app
kubectl get events -n production --field-selector involvedObject.name=web-app

# 4. ì´ìƒ ì—†ìœ¼ë©´ ì „ì²´ ì ìš©
# ì´ìƒ ìˆìœ¼ë©´ ì¦‰ì‹œ ë¡¤ë°±
kubectl rollout undo deploy web-app -n production
```

### 6.5 AI ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ì¶”ì²œ ìë™í™” (ê³ ê¸‰)

AIì™€ LLMì„ í™œìš©í•˜ì—¬ ë¦¬ì†ŒìŠ¤ ìµœì í™” í”„ë¡œì„¸ìŠ¤ë¥¼ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” Amazon Bedrock, Kiro, Amazon Q Developerë¥¼ í™œìš©í•œ ìµœì‹  íŒ¨í„´ì„ ì†Œê°œí•©ë‹ˆë‹¤.

#### 6.5.1 Amazon Bedrock + Prometheus â†’ ìë™ Right-Sizing PR ìƒì„±

ì „í†µì ì¸ ìˆ˜ë™ Right-Sizing í”„ë¡œì„¸ìŠ¤ë¥¼ AIë¡œ ìë™í™”í•˜ëŠ” ì—”ë“œíˆ¬ì—”ë“œ ì›Œí¬í”Œë¡œìš°ì…ë‹ˆë‹¤.

**ì•„í‚¤í…ì²˜ ê°œìš”:**

```mermaid
graph TB
    subgraph "ë°ì´í„° ìˆ˜ì§‘"
        A[EKS Cluster] -->|ë©”íŠ¸ë¦­| B[Prometheus/AMP]
        A -->|VPA ê¶Œì¥ì‚¬í•­| C[VPA Recommender]
    end

    subgraph "AI ë¶„ì„"
        B --> D[Lambda Function]
        C --> D
        D -->|ë©”íŠ¸ë¦­ ì¿¼ë¦¬| E[Amazon Bedrock<br/>Claude/Titan]
        E -->|ë¶„ì„ ê²°ê³¼| F[Right-Sizing ê¶Œì¥ì‚¬í•­]
    end

    subgraph "ìë™ ì ìš©"
        F --> G[GitHub API]
        G -->|Pull Request ìƒì„±| H[GitHub Repository]
        H -->|ìë™ ìŠ¹ì¸/ë³‘í•©| I[ArgoCD/Flux]
        I -->|GitOps ë°°í¬| A
    end

    style E fill:#4dabf7
    style G fill:#51cf66
    style I fill:#ffa94d
```

**êµ¬í˜„ ì˜ˆì‹œ:**

```python
# Lambda Function: AI ê¸°ë°˜ Right-Sizing ì¶”ì²œ
import boto3
import json
import requests
from datetime import datetime, timedelta

bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')
amp_query_url = "https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-xxx/api/v1/query"

def lambda_handler(event, context):
    # 1. Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (7ì¼)
    metrics = collect_prometheus_metrics(
        namespace="production",
        deployment="web-app",
        period_days=7
    )

    # 2. VPA ê¶Œì¥ì‚¬í•­ ìˆ˜ì§‘
    vpa_recommendations = get_vpa_recommendations("web-app-vpa", "production")

    # 3. Amazon Bedrockë¡œ ë¶„ì„
    analysis_prompt = f"""
    ë‹¤ìŒ Kubernetes Deploymentì˜ ë¦¬ì†ŒìŠ¤ ìµœì í™”ë¥¼ ë¶„ì„í•˜ì„¸ìš”:

    í˜„ì¬ ì„¤ì •:
    {json.dumps(metrics['current_resources'], indent=2)}

    7ì¼ê°„ ì‹¤ì œ ì‚¬ìš©ëŸ‰ (P50/P95/P99):
    CPU: {metrics['cpu_p50']}m / {metrics['cpu_p95']}m / {metrics['cpu_p99']}m
    Memory: {metrics['mem_p50']}Mi / {metrics['mem_p95']}Mi / {metrics['mem_p99']}Mi

    VPA ê¶Œì¥ì‚¬í•­:
    {json.dumps(vpa_recommendations, indent=2)}

    ë‹¤ìŒì„ í¬í•¨í•œ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”:
    1. í˜„ì¬ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ë˜ëŠ” ë¶€ì¡± ì—¬ë¶€
    2. ê¶Œì¥ requests/limits ê°’ (êµ¬ì²´ì  ìˆ˜ì¹˜)
    3. ì˜ˆìƒ ë¹„ìš© ì ˆê°ì•¡
    4. ìœ„í—˜ ìš”ì†Œ ë° ì£¼ì˜ì‚¬í•­
    5. ë‹¨ê³„ì  ì ìš© ê³„íš
    """

    response = bedrock.invoke_model(
        modelId='anthropic.claude-3-sonnet-20240229-v1:0',
        contentType='application/json',
        accept='application/json',
        body=json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 2000,
            "messages": [{
                "role": "user",
                "content": analysis_prompt
            }]
        })
    )

    analysis = json.loads(response['body'].read())['content'][0]['text']

    # 4. GitHub Pull Request ìƒì„±
    create_right_sizing_pr(
        deployment="web-app",
        namespace="production",
        analysis=analysis,
        recommended_resources=parse_recommendations(analysis)
    )

    return {
        'statusCode': 200,
        'body': json.dumps({'message': 'Right-sizing PR created', 'analysis': analysis})
    }

def collect_prometheus_metrics(namespace, deployment, period_days):
    """Prometheusì—ì„œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìˆ˜ì§‘"""
    end_time = datetime.now()
    start_time = end_time - timedelta(days=period_days)

    queries = {
        'cpu_p50': f'quantile_over_time(0.50, container_cpu_usage_seconds_total{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) * 1000',
        'cpu_p95': f'quantile_over_time(0.95, container_cpu_usage_seconds_total{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) * 1000',
        'cpu_p99': f'quantile_over_time(0.99, container_cpu_usage_seconds_total{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) * 1000',
        'mem_p50': f'quantile_over_time(0.50, container_memory_working_set_bytes{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) / 1024 / 1024',
        'mem_p95': f'quantile_over_time(0.95, container_memory_working_set_bytes{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) / 1024 / 1024',
        'mem_p99': f'quantile_over_time(0.99, container_memory_working_set_bytes{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) / 1024 / 1024',
    }

    results = {}
    for key, query in queries.items():
        response = requests.get(amp_query_url, params={'query': query})
        results[key] = int(float(response.json()['data']['result'][0]['value'][1]))

    return results

def create_right_sizing_pr(deployment, namespace, analysis, recommended_resources):
    """GitHubì— Right-Sizing PR ìƒì„±"""
    github_token = get_secret('github-token')
    repo_owner = "my-org"
    repo_name = "k8s-manifests"

    # Deployment YAML ìˆ˜ì •
    updated_yaml = update_deployment_resources(
        deployment=deployment,
        namespace=namespace,
        resources=recommended_resources
    )

    # Pull Request ìƒì„±
    pr_body = f"""
## ğŸ¤– AI ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ Right-Sizing ì œì•ˆ

### ë¶„ì„ ê²°ê³¼
{analysis}

### ë³€ê²½ ì‚¬í•­
- Deployment: `{namespace}/{deployment}`
- ë¦¬ì†ŒìŠ¤ requests/limits ì—…ë°ì´íŠ¸

### ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Staging í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì •ìƒ í™•ì¸
- [ ] ë¹„ìš© ì ˆê°ì•¡ ê²€ì¦

### ìë™ ìƒì„± ì •ë³´
- Generator: Amazon Bedrock + VPA Analysis
- Timestamp: {datetime.now().isoformat()}
"""

    headers = {
        'Authorization': f'token {github_token}',
        'Accept': 'application/vnd.github.v3+json'
    }

    # ë¸Œëœì¹˜ ìƒì„± ë° ì»¤ë°‹
    create_branch_and_commit(repo_owner, repo_name, updated_yaml, headers)

    # PR ìƒì„±
    pr_data = {
        'title': f'[AI] Right-Size {namespace}/{deployment}',
        'head': f'right-size-{deployment}-{datetime.now().strftime("%Y%m%d")}',
        'base': 'main',
        'body': pr_body
    }

    response = requests.post(
        f'https://api.github.com/repos/{repo_owner}/{repo_name}/pulls',
        headers=headers,
        json=pr_data
    )

    return response.json()
```

**EventBridge ìŠ¤ì¼€ì¤„ë¡œ ìë™í™”:**

```yaml
# CloudFormation í…œí”Œë¦¿ ì˜ˆì‹œ
Resources:
  RightSizingSchedule:
    Type: AWS::Events::Rule
    Properties:
      Name: weekly-right-sizing-analysis
      Description: "Weekly AI-based right-sizing analysis"
      ScheduleExpression: "cron(0 9 ? * MON *)"  # ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 9ì‹œ
      State: ENABLED
      Targets:
        - Arn: !GetAtt RightSizingLambda.Arn
          Id: RightSizingTarget
          Input: |
            {
              "namespaces": ["production", "staging"],
              "auto_create_pr": true,
              "require_approval": true
            }
```

#### 6.5.2 Kiro + EKS MCPë¥¼ í™œìš©í•œ ë¦¬ì†ŒìŠ¤ ìµœì í™”

**Kiro**ëŠ” AWSì˜ AI ê¸°ë°˜ í´ë¼ìš°ë“œ ìš´ì˜ ë„êµ¬ë¡œ, **ìì—°ì–´ ì§ˆì˜**ë¡œ EKS ë¦¬ì†ŒìŠ¤ ìµœì í™”ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Kiro ì„¤ì¹˜ ë° ì„¤ì •:**

```bash
# Kiro CLI ì„¤ì¹˜
curl -sL https://kiro.aws.dev/install.sh | bash

# EKS MCP (Model Context Protocol) ì—°ê²°
kiro mcp connect eks --cluster production-eks --region us-east-1

# ì—°ê²° í™•ì¸
kiro mcp list
# ì¶œë ¥:
# âœ“ eks-production (connected)
# âœ“ cloudwatch-insights (connected)
# âœ“ cost-explorer (connected)
```

**ìì—°ì–´ ì§ˆì˜ ì˜ˆì‹œ:**

```bash
# 1. ë¦¬ì†ŒìŠ¤ ìµœì í™”ê°€ í•„ìš”í•œ Pod ì°¾ê¸°
kiro ask "production ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ CPU ì‚¬ìš©ë¥ ì´ 30% ë¯¸ë§Œì¸ Podë¥¼ ì°¾ì•„ì„œ Right-Sizing ê¶Œì¥ì‚¬í•­ì„ ì•Œë ¤ì¤˜"

# Kiro ì‘ë‹µ ì˜ˆì‹œ:
# ğŸ“Š ë¶„ì„ ê²°ê³¼: 12ê°œ Podê°€ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ ìƒíƒœì…ë‹ˆë‹¤.
#
# ìƒìœ„ 5ê°œ:
# 1. web-app-7d8c9f (í˜„ì¬: 2 CPU / ì‹¤ì œ P95: 0.4 CPU) â†’ ê¶Œì¥: 0.5 CPU
# 2. api-server-abc123 (í˜„ì¬: 4 CPU / ì‹¤ì œ P95: 0.8 CPU) â†’ ê¶Œì¥: 1 CPU
# 3. worker-def456 (í˜„ì¬: 1 CPU / ì‹¤ì œ P95: 0.2 CPU) â†’ ê¶Œì¥: 0.3 CPU
#
# ğŸ’° ì˜ˆìƒ ì ˆê°ì•¡: $450/ì›” (45% ë¦¬ì†ŒìŠ¤ ê°ì†Œ)
#
# ì ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)

# 2. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬ Pod ì‹ë³„
kiro ask "ì§€ë‚œ 7ì¼ê°„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•œ Podë¥¼ ì°¾ì•„ì¤˜"

# Kiro ì‘ë‹µ:
# ğŸ” ë©”ëª¨ë¦¬ ì¦ê°€ íŒ¨í„´ ê°ì§€:
#
# âš ï¸ cache-service-xyz789
# - ì‹œì‘: 500Mi â†’ í˜„ì¬: 1.8Gi (260% ì¦ê°€)
# - ì¶”ì„¸: í•˜ë£¨ 150Miì”© ì¦ê°€
# - ì˜ˆìƒ OOMê¹Œì§€: 3ì¼
# - ê¶Œì¥ ì¡°ì¹˜: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì¡°ì‚¬ + ì„ì‹œë¡œ limits 2.5Gië¡œ ìƒí–¥
#
# ğŸ“‹ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)

# 3. í´ëŸ¬ìŠ¤í„° ì „ì²´ íš¨ìœ¨ì„± ë¶„ì„
kiro ask "production í´ëŸ¬ìŠ¤í„°ì˜ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±ì„ ë¶„ì„í•˜ê³  ìµœì í™” ìš°ì„ ìˆœìœ„ë¥¼ ì•Œë ¤ì¤˜"

# Kiro ì‘ë‹µ:
# ğŸ“ˆ í´ëŸ¬ìŠ¤í„° íš¨ìœ¨ì„± ë³´ê³ ì„œ
#
# ì „ì²´ íš¨ìœ¨ì„±: 52% (ì—…ê³„ í‰ê· : 65%)
#
# ìµœì í™” ìš°ì„ ìˆœìœ„:
# 1. ğŸ”´ High Priority (ì¦‰ì‹œ ì¡°ì¹˜)
#    - 10ê°œ Deploymentê°€ CPUì˜ 70% ë¯¸ì‚¬ìš©
#    - ì˜ˆìƒ ì ˆê°: $1,200/ì›”
#
# 2. ğŸŸ¡ Medium Priority (1ì£¼ ë‚´)
#    - 5ê°œ StatefulSetì˜ PVC ì‚¬ì´ì¦ˆ ê³¼ë‹¤
#    - ì˜ˆìƒ ì ˆê°: $300/ì›”
#
# 3. ğŸŸ¢ Low Priority (ê³„íš ë‹¨ê³„)
#    - HPA ë¯¸ì„¤ì • Deployment 15ê°œ
#    - íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ í›„ ì ìš© ê¶Œì¥
#
# ìë™ Right-Sizing PRì„ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)
```

**Kiro ì›Œí¬í”Œë¡œìš° ìë™í™”:**

```yaml
# kiro-workflow.yaml
apiVersion: kiro.aws.dev/v1alpha1
kind: Workflow
metadata:
  name: weekly-optimization
spec:
  schedule: "0 9 * * MON"  # ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 9ì‹œ
  steps:
    - name: analyze-underutilized
      action: analyze
      query: "CPU ì‚¬ìš©ë¥  30% ë¯¸ë§Œ ë˜ëŠ” Memory ì‚¬ìš©ë¥  40% ë¯¸ë§Œì¸ ëª¨ë“  Pod ë¶„ì„"
      outputFormat: json

    - name: generate-recommendations
      action: recommend
      input: ${{ steps.analyze-underutilized.output }}
      includeVPA: true
      includePrometheus: true

    - name: create-pr
      action: github-pr
      repository: my-org/k8s-manifests
      branch: kiro-right-sizing-{{ date }}
      title: "[Kiro] Weekly Right-Sizing Recommendations"
      body: ${{ steps.generate-recommendations.output }}
      autoMerge: false  # ìˆ˜ë™ ê²€í†  í•„ìš”

    - name: notify
      action: slack
      webhook: ${{ secrets.SLACK_WEBHOOK }}
      message: |
        ğŸ“Š ì£¼ê°„ Right-Sizing ë¶„ì„ ì™„ë£Œ
        PR: ${{ steps.create-pr.pr_url }}
        ì˜ˆìƒ ì ˆê°: ${{ steps.generate-recommendations.estimated_savings }}
```

#### 6.5.3 Amazon Q Developerë¥¼ í™œìš©í•œ ëŒ€í™”í˜• ìµœì í™”

Amazon Q DeveloperëŠ” IDEì™€ CLIì—ì„œ ì§ì ‘ ë¦¬ì†ŒìŠ¤ ìµœì í™” ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.

**VS Codeì—ì„œ ì‚¬ìš©:**

```yaml
# deployment.yamlì„ ì—´ê³  Q Developerì—ê²Œ ì§ˆë¬¸
# /q optimize-resources

# Q Developer ì‘ë‹µ:
# í˜„ì¬ Deploymentì˜ ë¦¬ì†ŒìŠ¤ ì„¤ì •ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤:
#
# ğŸ” ë°œê²¬ëœ ë¬¸ì œ:
# 1. CPU requestsê°€ ì‹¤ì œ ì‚¬ìš©ëŸ‰ë³´ë‹¤ 3ë°° ë†’ìŠµë‹ˆë‹¤ (1000m â†’ 350m ê¶Œì¥)
# 2. Memory limitsê°€ ì—†ì–´ OOM ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤
# 3. QoS í´ë˜ìŠ¤: Burstable (Guaranteed ê¶Œì¥)
#
# ğŸ’¡ ìµœì í™”ëœ ì„¤ì •:
resources:
  requests:
    cpu: "350m"      # ì‹¤ì œ P95 + 20% ë²„í¼
    memory: "512Mi"  # ì‹¤ì œ P95 400Mi + 20%
  limits:
    memory: "1Gi"    # Upper Bound + ì—¬ìœ 
    # CPU limits ì œê±° (Google/Datadog íŒ¨í„´)
#
# ì´ ë³€ê²½ì‚¬í•­ì„ ì ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Apply / Dismiss)
```

**CLIì—ì„œ ì‚¬ìš©:**

```bash
# Amazon Q CLIë¥¼ í†µí•œ ì§ˆì˜
q ask "ì´ Deploymentì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ìµœì í™”í•´ì¤˜" --file deployment.yaml

# ì¶œë ¥:
# ë¶„ì„ ì¤‘... âœ“
#
# í˜„ì¬ ì„¤ì • ë¬¸ì œ:
# - CPU over-provisioned by 65%
# - Memory under-provisioned (OOM risk)
#
# ê¶Œì¥ ë³€ê²½ì‚¬í•­ì´ deployment-optimized.yamlì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.
# ì°¨ì´ì ì„ í™•ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)

# y ì…ë ¥ ì‹œ:
diff deployment.yaml deployment-optimized.yaml
```

#### 6.5.4 ì£¼ì˜ì‚¬í•­ ë° í•œê³„

AI ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ì¶”ì²œì€ ê°•ë ¥í•˜ì§€ë§Œ, ë‹¤ìŒ í•œê³„ë¥¼ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤:

| í•œê³„ | ì„¤ëª… | ëŒ€ì‘ ë°©ë²• |
|------|------|----------|
| **ê³¼ê±° ë°ì´í„° ì˜ì¡´** | ê³¼ê±°ì— ì—†ë˜ íŠ¸ë˜í”½ íŒ¨í„´ ì˜ˆì¸¡ ë¶ˆê°€ | HPA ë³‘í–‰, ì—¬ìœ  ë²„í¼ í™•ë³´ |
| **ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡±** | ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ (SLA, ê·œì œ) ë¯¸ë°˜ì˜ | ìˆ˜ë™ ê²€í†  ë‹¨ê³„ í•„ìˆ˜ |
| **ì¼ì‹œì  ìŠ¤íŒŒì´í¬** | ë§ˆì¼€íŒ… ìº í˜ì¸ ë“± ê³„íšëœ ë¶€í•˜ ë¯¸ê³ ë ¤ | ì´ë²¤íŠ¸ ê¸°ê°„ ìˆ˜ë™ ìŠ¤ì¼€ì¼ ì—… |
| **ë¹„ìš© ìµœì í™” í¸í–¥** | ì•ˆì •ì„±ë³´ë‹¤ ë¹„ìš© ì ˆê° ìš°ì„  ê°€ëŠ¥ì„± | Critical ì›Œí¬ë¡œë“œ ì œì™¸ ì„¤ì • |

:::warning AI ì¶”ì²œì€ ë³´ì¡° ë„êµ¬ë¡œ í™œìš©
AI ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ì¶”ì²œì€ **ìµœì¢… ì˜ì‚¬ê²°ì • ë„êµ¬ê°€ ì•„ë‹Œ ë³´ì¡° ë„êµ¬**ì…ë‹ˆë‹¤. í”„ë¡œë•ì…˜ ì ìš© ì „ ë°˜ë“œì‹œ:

1. **Staging í™˜ê²½ì—ì„œ ê²€ì¦** (ìµœì†Œ 3ì¼)
2. **ì„±ëŠ¥ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§** (Latency P99, Error Rate)
3. **ì ì§„ì  ë¡¤ì•„ì›ƒ** (Canary 10% â†’ 50% â†’ 100%)
4. **ë¡¤ë°± ê³„íš ìˆ˜ë¦½** (1ë¶„ ë‚´ ì´ì „ ë²„ì „ ë³µêµ¬ ê°€ëŠ¥)

íŠ¹íˆ ë‹¤ìŒ ì›Œí¬ë¡œë“œëŠ” **AI ì¶”ì²œì„ ì ìš©í•˜ì§€ ë§ê³  ìˆ˜ë™ìœ¼ë¡œ ê´€ë¦¬**í•˜ì„¸ìš”:
- ê¸ˆìœµ ê±°ë˜ ì‹œìŠ¤í…œ
- ì˜ë£Œ ì •ë³´ ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤
- Stateful ë°ì´í„°ë² ì´ìŠ¤
:::

**AI ì¶”ì²œ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**

```yaml
# í”„ë¡œë•ì…˜ ì ìš© ì „ í•„ìˆ˜ ê²€ì¦
ai_recommendation_validation:
  staging_test:
    duration_days: 3
    success_criteria:
      - p99_latency_increase: "<5%"
      - error_rate_increase: "<0.1%"
      - no_oom_kills: true
      - no_cpu_throttling: "<10%"

  canary_rollout:
    initial_percentage: 10
    increment_percentage: 20
    increment_interval_hours: 6
    auto_rollback_threshold:
      error_rate: 1.0  # 1% ì—ëŸ¬ìœ¨ ì´ˆê³¼ ì‹œ ìë™ ë¡¤ë°±
      latency_p99_ms: 500  # P99 ì§€ì—° 500ms ì´ˆê³¼ ì‹œ ë¡¤ë°±

  monitoring:
    dashboard_url: "https://grafana.example.com/d/right-sizing"
    alert_channels: ["slack://ops-team", "pagerduty://oncall"]
    review_required: true  # ìë™ ë³‘í•© ê¸ˆì§€, ìˆ˜ë™ ê²€í†  í•„ìˆ˜
```

:::tip AI + Human í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
ìµœìƒì˜ ê²°ê³¼ëŠ” **AI ì¶”ì²œ + ì¸ê°„ ì „ë¬¸ê°€ ê²€í† **ì˜ ì¡°í•©ì—ì„œ ë‚˜ì˜µë‹ˆë‹¤:

1. AIê°€ ìˆ˜ì²œ ê°œ Pod ì¤‘ ìµœì í™” ëŒ€ìƒ ì„ ë³„ (ì†ë„)
2. ì¸ê°„ì´ Critical ì›Œí¬ë¡œë“œ ì œì™¸ ë° ê²€ì¦ (ì‹ ë¢°ì„±)
3. AIê°€ ì´ˆì•ˆ PR ìƒì„± (ìë™í™”)
4. ì¸ê°„ì´ Staging í…ŒìŠ¤íŠ¸ í›„ ìŠ¹ì¸ (ì•ˆì „ì„±)
5. GitOpsê°€ ì ì§„ì  ë°°í¬ (ìš´ì˜ íš¨ìœ¨)

ì´ í”„ë¡œì„¸ìŠ¤ë¡œ **ìˆ˜ë™ ëŒ€ë¹„ 80% ì‹œê°„ ì ˆê°**, **ì•ˆì •ì„±ì€ ë™ì¼** ìœ ì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
:::

## Resource Quota & LimitRange

### 7.1 Namespace ìˆ˜ì¤€ ë¦¬ì†ŒìŠ¤ ì œí•œ

ResourceQuotaë¡œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì „ì²´ ë¦¬ì†ŒìŠ¤ë¥¼ ì œí•œí•©ë‹ˆë‹¤:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    # ì´ ë¦¬ì†ŒìŠ¤ ì œí•œ
    requests.cpu: "100"           # 100 CPU cores
    requests.memory: "200Gi"      # 200GB RAM
    limits.cpu: "200"             # CPU limits í•©ê³„
    limits.memory: "400Gi"        # Memory limits í•©ê³„

    # ì˜¤ë¸Œì íŠ¸ ìˆ˜ ì œí•œ
    pods: "500"                   # ìµœëŒ€ 500ê°œ Pod
    services: "50"                # ìµœëŒ€ 50ê°œ Service
    persistentvolumeclaims: "100" # ìµœëŒ€ 100ê°œ PVC

    # ìŠ¤í† ë¦¬ì§€ ì œí•œ
    requests.storage: "2Ti"       # ì´ 2TB ìŠ¤í† ë¦¬ì§€

---
# í™˜ê²½ë³„ ì¿¼í„° ì˜ˆì‹œ
apiVersion: v1
kind: ResourceQuota
metadata:
  name: development-quota
  namespace: development
spec:
  hard:
    requests.cpu: "20"
    requests.memory: "40Gi"
    limits.cpu: "40"
    limits.memory: "80Gi"
    pods: "100"

---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: staging-quota
  namespace: staging
spec:
  hard:
    requests.cpu: "50"
    requests.memory: "100Gi"
    limits.cpu: "100"
    limits.memory: "200Gi"
    pods: "200"
```

**ì¿¼í„° ì‚¬ìš©ëŸ‰ í™•ì¸:**

```bash
# í˜„ì¬ ì¿¼í„° ì‚¬ìš©ëŸ‰
kubectl describe resourcequota production-quota -n production

# ì¶œë ¥ ì˜ˆì‹œ:
# Name:            production-quota
# Namespace:       production
# Resource         Used   Hard
# --------         ----   ----
# limits.cpu       150    200
# limits.memory    300Gi  400Gi
# pods             342    500
# requests.cpu     75     100
# requests.memory  150Gi  200Gi
```

### 7.2 LimitRangeë¡œ ê¸°ë³¸ê°’ ì„¤ì •

LimitRangeë¡œ Pod/Containerì— ìë™ìœ¼ë¡œ ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ë¥¼ ì£¼ì…í•©ë‹ˆë‹¤:

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limitrange
  namespace: production
spec:
  limits:
  # Container ë ˆë²¨ ì œì•½
  - type: Container
    default:                    # limits ë¯¸ì„¤ì • ì‹œ ê¸°ë³¸ê°’
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:             # requests ë¯¸ì„¤ì • ì‹œ ê¸°ë³¸ê°’
      cpu: "100m"
      memory: "128Mi"
    max:                        # ìµœëŒ€ í—ˆìš©ê°’
      cpu: "4000m"
      memory: "8Gi"
    min:                        # ìµœì†Œ ìš”êµ¬ê°’
      cpu: "50m"
      memory: "64Mi"
    maxLimitRequestRatio:       # limits/requests ìµœëŒ€ ë¹„ìœ¨
      cpu: "4"                  # limitsëŠ” requestsì˜ ìµœëŒ€ 4ë°°
      memory: "2"               # limitsëŠ” requestsì˜ ìµœëŒ€ 2ë°°

  # Pod ë ˆë²¨ ì œì•½
  - type: Pod
    max:
      cpu: "8000m"
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"

  # PVC ì œì•½
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "1Gi"

---
# ê°œë°œ í™˜ê²½ LimitRange
apiVersion: v1
kind: LimitRange
metadata:
  name: development-limitrange
  namespace: development
spec:
  limits:
  - type: Container
    default:
      cpu: "200m"
      memory: "256Mi"
    defaultRequest:
      cpu: "50m"
      memory: "64Mi"
    max:
      cpu: "2000m"
      memory: "4Gi"
```

**ë™ì‘ ì˜ˆì‹œ:**

```yaml
# ê°œë°œìê°€ ì‘ì„±í•œ YAML (ë¦¬ì†ŒìŠ¤ ë¯¸ì§€ì •)
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: production
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    # resources ì„¹ì…˜ ì—†ìŒ

# LimitRangeê°€ ìë™ ì£¼ì…í•œ ê²°ê³¼
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: production
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    resources:
      requests:           # defaultRequest ì ìš©
        cpu: "100m"
        memory: "128Mi"
      limits:             # default ì ìš©
        cpu: "500m"
        memory: "512Mi"
```

**ê²€ì¦:**

```bash
# LimitRange í™•ì¸
kubectl describe limitrange production-limitrange -n production

# Podì— ì ìš©ëœ ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl get pod test-pod -n production -o jsonpath='{.spec.containers[0].resources}' | jq .
```

### 7.3 DRA (Dynamic Resource Allocation) - GPU/íŠ¹ìˆ˜ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

Kubernetes 1.31+ì—ì„œ ë„ì…ëœ **DRA (Dynamic Resource Allocation)**ëŠ” GPU, FPGA, NPU ê°™ì€ íŠ¹ìˆ˜ ë¦¬ì†ŒìŠ¤ë¥¼ ë³´ë‹¤ ìœ ì—°í•˜ê²Œ í• ë‹¹í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.

#### ê¸°ì¡´ Device Plugin vs DRA

| íŠ¹ì„± | Device Plugin (ê¸°ì¡´) | DRA (K8s 1.31+) |
|------|---------------------|-----------------|
| **ë¦¬ì†ŒìŠ¤ í‘œí˜„** | ë‹¨ìˆœ ìˆ«ì (`nvidia.com/gpu: 1`) | êµ¬ì¡°í™”ëœ íŒŒë¼ë¯¸í„° (ë©”ëª¨ë¦¬, ì»´í“¨íŒ… ëª¨ë“œ) |
| **ê³µìœ  ê°€ëŠ¥ì„±** | ë¶ˆê°€ëŠ¥ (1 Pod = 1 GPU) | ê°€ëŠ¥ (ì‹œê°„ ë¶„í• , MIG ì§€ì›) |
| **ë™ì  í• ë‹¹** | ìŠ¤ì¼€ì¤„ë§ ì‹œ ê²°ì • | ëŸ°íƒ€ì„ ë™ì  í• ë‹¹ |
| **ë³µì¡í•œ í† í´ë¡œì§€** | ì œí•œì  | NUMA, PCIe í† í´ë¡œì§€ ê³ ë ¤ |
| **ë©€í‹° í…Œë„ŒíŠ¸** | ì–´ë ¤ì›€ | ë„¤ì´í‹°ë¸Œ ì§€ì› |

**DRAì˜ í•µì‹¬ ê°œë…:**

```mermaid
graph LR
    A[Pod with ResourceClaim] --> B[Scheduler]
    B --> C[ResourceClass ë§¤ì¹­]
    C --> D[DRA Driver]
    D --> E[ë¬¼ë¦¬ GPU í• ë‹¹]
    E --> F[Pod ì‹¤í–‰]

    style A fill:#4dabf7
    style E fill:#51cf66
```

#### DRA êµ¬ì„± ìš”ì†Œ

**1. ResourceClass (í´ëŸ¬ìŠ¤í„° ìˆ˜ì¤€ ë¦¬ì†ŒìŠ¤ ì •ì˜)**

```yaml
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClass
metadata:
  name: nvidia-a100-gpu
spec:
  driverName: gpu.nvidia.com
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClassParameters
    name: a100-80gb
---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClassParameters
metadata:
  name: a100-80gb
spec:
  # GPU íŠ¹ì„± ì •ì˜
  memory: "80Gi"
  computeCapability: "8.0"
  # MIG (Multi-Instance GPU) ì§€ì›
  migEnabled: true
  migProfile: "1g.10gb"  # 1/7 GPU ìŠ¬ë¼ì´ìŠ¤
```

**2. ResourceClaim (Podê°€ ìš”ì²­í•˜ëŠ” ë¦¬ì†ŒìŠ¤)**

```yaml
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaim
metadata:
  name: ml-training-gpu
  namespace: ml-team
spec:
  resourceClassName: nvidia-a100-gpu
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClaimParameters
    name: training-config
---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: training-config
spec:
  # ìš”ì²­í•  GPU ì‚¬ì–‘
  count: 2  # 2ê°œ GPU ìš”ì²­
  sharing: "TimeSlicing"  # ì‹œê°„ ë¶„í•  ê³µìœ  í—ˆìš©
  selector:
    matchLabels:
      gpu.nvidia.com/memory: "80Gi"
```

**3. Podì—ì„œ ResourceClaim ì‚¬ìš©**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pytorch-training
  namespace: ml-team
spec:
  containers:
  - name: trainer
    image: pytorch/pytorch:2.1.0-cuda12.1
    command: ["python", "train.py"]
    resources:
      requests:
        cpu: "8"
        memory: "32Gi"
      limits:
        memory: "64Gi"

  # DRAë¥¼ í†µí•œ GPU í• ë‹¹
  resourceClaims:
  - name: gpu
    source:
      resourceClaimName: ml-training-gpu

  # ì»¨í…Œì´ë„ˆì—ì„œ claim ì°¸ì¡°
  containers:
  - name: trainer
    # ...
    resources:
      claims:
      - name: gpu
```

#### EKSì—ì„œ DRA í™œì„±í™” ë° GPU í• ë‹¹ ì˜ˆì‹œ

**Step 1: EKS í´ëŸ¬ìŠ¤í„°ì—ì„œ DRA Feature Gate í™œì„±í™”**

```bash
# EKS 1.31+ í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ
eksctl create cluster \
  --name dra-enabled-cluster \
  --version 1.31 \
  --region us-west-2 \
  --nodegroup-name gpu-nodes \
  --node-type p4d.24xlarge \
  --nodes 2 \
  --kubernetes-feature-gates DynamicResourceAllocation=true
```

**Step 2: NVIDIA GPU Operator ì„¤ì¹˜ (DRA ë“œë¼ì´ë²„ í¬í•¨)**

```bash
# Helmìœ¼ë¡œ GPU Operator ì„¤ì¹˜ (DRA ì§€ì› ë²„ì „)
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update

helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator \
  --create-namespace \
  --set driver.enabled=true \
  --set toolkit.enabled=true \
  --set devicePlugin.enabled=false \  # ê¸°ì¡´ device plugin ë¹„í™œì„±í™”
  --set dra.enabled=true \             # DRA í™œì„±í™”
  --set migManager.enabled=true        # MIG ì§€ì›
```

**Step 3: ResourceClaimTemplateë¡œ ìë™ Claim ìƒì„±**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference
  namespace: ml-team
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: model-server
        image: tritonserver:24.01
        resources:
          requests:
            cpu: "4"
            memory: "16Gi"
          claims:
          - name: gpu

      # ResourceClaimTemplateë¡œ ê° Podë§ˆë‹¤ ìë™ ìƒì„±
      resourceClaims:
      - name: gpu
        source:
          resourceClaimTemplateName: shared-gpu-template

---
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaimTemplate
metadata:
  name: shared-gpu-template
  namespace: ml-team
spec:
  spec:
    resourceClassName: nvidia-a100-gpu
    parametersRef:
      apiGroup: gpu.nvidia.com
      kind: GpuClaimParameters
      name: shared-inference-config

---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: shared-inference-config
spec:
  count: 1
  sharing: "TimeSlicing"  # ì—¬ëŸ¬ Podê°€ ì‹œê°„ ë¶„í• ë¡œ ê³µìœ 
  requests:
    memory: "10Gi"        # GPU ë©”ëª¨ë¦¬ 10GBë§Œ ìš”ì²­
```

**DRA ì¥ì  ìš”ì•½:**

1. **GPU ê³µìœ **: MIG ë˜ëŠ” Time-Slicingìœ¼ë¡œ 1ê°œ GPUë¥¼ ì—¬ëŸ¬ Podê°€ ì‚¬ìš©
2. **ì„¸ë°€í•œ ì œì–´**: GPU ë©”ëª¨ë¦¬, ì»´í“¨íŒ… ëª¨ë“œ, í† í´ë¡œì§€ ì§€ì • ê°€ëŠ¥
3. **ë™ì  í• ë‹¹**: Pod ìƒì„± í›„ì—ë„ ë¦¬ì†ŒìŠ¤ ì¶”ê°€/ì œê±° ê°€ëŠ¥
4. **ë¹„ìš© ì ˆê°**: GPU í™œìš©ë¥  í–¥ìƒ (ê¸°ì¡´ 30-40% â†’ DRAë¡œ 70-80%)

:::warning EKS DRA ì§€ì› ìƒíƒœ (2026ë…„ 2ì›” ê¸°ì¤€)
- Kubernetes 1.31+ì—ì„œ alpha ê¸°ëŠ¥ìœ¼ë¡œ ì œê³µ
- EKSì—ì„œëŠ” Feature Gate ìˆ˜ë™ í™œì„±í™” í•„ìš”
- í”„ë¡œë•ì…˜ ì‚¬ìš© ì‹œ NVIDIA GPU Operator ìµœì‹  ë²„ì „(v24.9.0+) í™•ì¸
- MIG ì§€ì›ì€ A100/H100 GPUì—ì„œë§Œ ê°€ëŠ¥
:::

### 7.3.1 Setu: Kueue-Karpenter í†µí•©ìœ¼ë¡œ GPU ìœ íœ´ ë¹„ìš© ì œê±°

AI/ML ì›Œí¬ë¡œë“œì—ì„œ GPUëŠ” ê°€ì¥ ë¹„ì‹¼ ë¦¬ì†ŒìŠ¤ì´ì§€ë§Œ, ê¸°ì¡´ ë°˜ì‘í˜• í”„ë¡œë¹„ì €ë‹ ë°©ì‹ì€ ì‹¬ê°í•œ ë‚­ë¹„ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. **Setu**ëŠ” Kueueì˜ ì¿¼í„° ê´€ë¦¬ì™€ Karpenterì˜ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ì„ ì—°ê²°í•˜ì—¬ í”„ë¡œì•¡í‹°ë¸Œ ë¦¬ì†ŒìŠ¤ í• ë‹¹ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

#### ë°˜ì‘í˜• í”„ë¡œë¹„ì €ë‹ì˜ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ë¬¸ì œ

**ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤:**
1. 4-GPU íŠ¸ë ˆì´ë‹ Jobì´ Queueì— ì§„ì…
2. Karpenterê°€ ë…¸ë“œë¥¼ í•˜ë‚˜ì”© í”„ë¡œë¹„ì €ë‹ (5-10ë¶„ ì†Œìš”)
3. 2ê°œ ë…¸ë“œë§Œ ì¤€ë¹„ëœ ìƒíƒœì—ì„œ Podê°€ ìŠ¤ì¼€ì¤„ë§ ì‹œë„ â†’ ì‹¤íŒ¨
4. **2ê°œ GPUëŠ” ìœ íœ´ ìƒíƒœë¡œ ëŒ€ê¸°í•˜ë©° ë¹„ìš© ë°œìƒ**
5. ë‚˜ë¨¸ì§€ ë…¸ë“œ ì¤€ë¹„ í›„ì—ì•¼ ì›Œí¬ë¡œë“œ ì‹œì‘

**ë¹„ìš© ì˜í–¥:**
- p4d.24xlarge (8x A100) = $32.77/ì‹œê°„
- 10ë¶„ ìœ íœ´ ëŒ€ê¸° Ã— 2ë…¸ë“œ = **$10.92 ë‚­ë¹„**
- ì¼ 100ê±´ ì‹¤í–‰ ì‹œ ì›” $32,760 ë¶ˆí•„ìš” ë¹„ìš©

#### Setuì˜ All-or-Nothing í”„ë¡œë¹„ì €ë‹

```mermaid
graph LR
    A[Job ì œì¶œ] --> B[Kueue: ì¿¼í„° ê²€ì¦]
    B --> C[Setu: NodePool ìš©ëŸ‰ ì‚¬ì „ í™•ì¸]
    C -->|ì¶©ë¶„| D[ëª¨ë“  ë…¸ë“œ ë™ì‹œ í”„ë¡œë¹„ì €ë‹]
    C -->|ë¶ˆì¶©ë¶„| E[ì¦‰ì‹œ ì‹¤íŒ¨ - ëŒ€ê¸° ì‹œê°„ 0]
    D --> F[ëª¨ë“  ë…¸ë“œ Ready í™•ì¸]
    F --> G[Job ì‹¤í–‰ - ìœ íœ´ ì—†ìŒ]

    style C fill:#4dabf7
    style G fill:#51cf66
    style E fill:#ff6b6b
```

**Setu ì‘ë™ ë°©ì‹:**

1. **ì‚¬ì „ ìš©ëŸ‰ ê²€ì¦**: Karpenter NodePoolì— í•„ìš”í•œ ë…¸ë“œ ìš©ëŸ‰ì´ ìˆëŠ”ì§€ í™•ì¸
2. **ë™ì‹œ í”„ë¡œë¹„ì €ë‹**: ëª¨ë“  ë…¸ë“œë¥¼ ë™ì‹œì— ìš”ì²­ (ìˆœì°¨ ëŒ€ê¸° ì—†ìŒ)
3. **Gang Scheduling ë³´ì¥**: ëª¨ë“  ë…¸ë“œê°€ Ready ìƒíƒœê°€ ëœ í›„ì—ë§Œ ì›Œí¬ë¡œë“œ ì‹œì‘
4. **ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ**: ìš©ëŸ‰ ë¶€ì¡± ì‹œ ì¦‰ì‹œ ì‹¤íŒ¨í•˜ì—¬ ë¬´ì˜ë¯¸í•œ ëŒ€ê¸° ì œê±°

#### Kueue ClusterQueueì™€ í†µí•©

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: gpu-cluster-queue
spec:
  namespaceSelector: {}
  resourceGroups:
  - coveredResources: ["cpu", "memory", "nvidia.com/gpu"]
    flavors:
    - name: a100-spot
      resources:
      - name: "nvidia.com/gpu"
        nominalQuota: 32  # 4ê°œ ë…¸ë“œ Ã— 8 GPU
      - name: "cpu"
        nominalQuota: 384
      - name: "memory"
        nominalQuota: 1536Gi
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: ml-team-queue
  namespace: ml-training
spec:
  clusterQueue: gpu-cluster-queue
---
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: a100-spot-pool
spec:
  template:
    spec:
      requirements:
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["p4d.24xlarge"]
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot", "on-demand"]
      nodeClassRef:
        name: a100-nodeclass
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m
  # Setuê°€ ì´ NodePoolì˜ ìš©ëŸ‰ì„ ì‚¬ì „ ê²€ì¦
  limits:
    cpu: "384"
    memory: "1536Gi"
```

**Setu Controller ë™ì‘:**

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: llm-training
  namespace: ml-training
  labels:
    kueue.x-k8s.io/queue-name: ml-team-queue
    setu.io/enabled: "true"  # Setu í™œì„±í™”
spec:
  parallelism: 4  # 4ê°œ ë…¸ë“œ í•„ìš”
  completions: 4
  template:
    spec:
      schedulerName: default-scheduler
      containers:
      - name: trainer
        image: pytorch/pytorch:2.1-cuda12.1
        resources:
          requests:
            nvidia.com/gpu: 8  # ë…¸ë“œë‹¹ 8 GPU
            memory: 384Gi
          limits:
            nvidia.com/gpu: 8
```

**Setu ë™ì‘ íë¦„:**

1. Jobì´ Kueue Queueì— ì§„ì…
2. Kueueê°€ ì¿¼íƒ€ í™•ì¸ (32 GPU ì¤‘ ì‚¬ìš© ê°€ëŠ¥ í™•ì¸)
3. **Setu ê°œì…**: Karpenter NodePool `a100-spot-pool`ì—ì„œ 4ê°œ p4d.24xlarge ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ê°€ëŠ¥ ì—¬ë¶€ ê²€ì¦
4. **ê°€ëŠ¥í•˜ë©´**: 4ê°œ ë…¸ë“œ ë™ì‹œ í”„ë¡œë¹„ì €ë‹ ìš”ì²­ + Jobì€ ëŒ€ê¸°
5. **ë¶ˆê°€ëŠ¥í•˜ë©´**: Job ì¦‰ì‹œ ì‹¤íŒ¨ (ë‹¤ë¥¸ Queueë¡œ ì¬ë¼ìš°íŒ… ë˜ëŠ” ì¬ì‹œë„)
6. ëª¨ë“  ë…¸ë“œ Ready í›„ Job ìŠ¤ì¼€ì¤„ë§ â†’ **ìœ íœ´ GPU 0ê°œ**

#### ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± ë¹„êµ

| ìƒí™© | ê¸°ì¡´ ë°©ì‹ | Setu ë°©ì‹ | ì ˆê° íš¨ê³¼ |
|------|----------|-----------|----------|
| **4-GPU Job ì‹œì‘ ì‹œê°„** | ë…¸ë“œ 1ê°œì”© í”„ë¡œë¹„ì €ë‹ (15ë¶„) | ë™ì‹œ í”„ë¡œë¹„ì €ë‹ (7ë¶„) | **53% ë‹¨ì¶•** |
| **ìœ íœ´ GPU ë¹„ìš©** | 2ê°œ ë…¸ë“œ Ã— 10ë¶„ ëŒ€ê¸° = $10.92 | 0 (ë™ì‹œ ì‹œì‘) | **100% ì ˆê°** |
| **ìš©ëŸ‰ ë¶€ì¡± ì‹œ ëŒ€ê¸°** | 10ë¶„ ëŒ€ê¸° í›„ ì‹¤íŒ¨ | ì¦‰ì‹œ ì‹¤íŒ¨ (0ì´ˆ) | **ëŒ€ê¸° ì‹œê°„ ì œê±°** |
| **Spot ì¤‘ë‹¨ ì‹œ ì¬ì‹œì‘** | ë¶€ë¶„ ë…¸ë“œ ì¬ìƒì„± â†’ ìœ íœ´ ë°œìƒ | Gang ë³´ì¥ ì¬í”„ë¡œë¹„ì €ë‹ | **ì¤‘ë‹¨ ë¹„ìš© ìµœì†Œí™”** |

**ì›”ê°„ ë¹„ìš© ì ˆê° (100 Job ì‹¤í–‰ ê¸°ì¤€):**
- ìœ íœ´ ë¹„ìš© ì ˆê°: **$32,760/ì›”**
- Cold start ì œê±°: **$16,380/ì›”** (ì‹œì‘ ì‹œê°„ 53% ë‹¨ì¶•)
- **ì´ ì ˆê°: $49,140/ì›”**

#### ë©€í‹° í…Œë„ŒíŠ¸ í™˜ê²½ì—ì„œ ê³µì •ì„± + íš¨ìœ¨ì„±

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: shared-gpu-queue
spec:
  preemption:
    withinClusterQueue: LowerPriority
    reclaimWithinCohort: Any
  resourceGroups:
  - coveredResources: ["nvidia.com/gpu"]
    flavors:
    - name: a100-80gb
      resources:
      - name: "nvidia.com/gpu"
        nominalQuota: 64
        borrowingLimit: 32  # ë‹¤ë¥¸ íŒ€ ìœ íœ´ ì‹œ 32 GPU ì¶”ê°€ ì‚¬ìš© ê°€ëŠ¥
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: research-team
  namespace: research
spec:
  clusterQueue: shared-gpu-queue
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: production-team
  namespace: production
spec:
  clusterQueue: shared-gpu-queue
```

**Setu + Kueue í†µí•© ì¥ì :**

1. **ê³µì •í•œ ì¿¼íƒ€ ê´€ë¦¬**: Kueueê°€ íŒ€ë³„ GPU í• ë‹¹ëŸ‰ ê´€ë¦¬
2. **íš¨ìœ¨ì  í”„ë¡œë¹„ì €ë‹**: Setuê°€ NodePool ìš©ëŸ‰ ê¸°ë°˜ ì‚¬ì „ ê²€ì¦
3. **Borrowing ìµœì í™”**: ìœ íœ´ GPUë¥¼ ë‹¤ë¥¸ íŒ€ì´ ì‚¬ìš©í•  ë•Œë„ Gang Scheduling ë³´ì¥
4. **Spot í™œìš© ê·¹ëŒ€í™”**: ë¶€ë¶„ í• ë‹¹ ë°©ì§€ë¡œ Spot ì¤‘ë‹¨ ì˜í–¥ ìµœì†Œí™”

:::tip Setu ì ìš© ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤
- **ëŒ€ê·œëª¨ GPU ì›Œí¬ë¡œë“œ**: 4+ GPU í•„ìš” ì‹œ ìœ íœ´ ë¹„ìš© ì‹¬ê°
- **Spot ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©**: Gang schedulingìœ¼ë¡œ Spot ì¤‘ë‹¨ ëŒ€ì‘ë ¥ í–¥ìƒ
- **ë©€í‹° í…Œë„ŒíŠ¸ í™˜ê²½**: Kueue ê³µì •ì„± + Karpenter íš¨ìœ¨ì„± ë™ì‹œ í™•ë³´
- **ë¹„ìš© ë¯¼ê°**: GPU ìœ íœ´ ì‹œê°„ì´ ì›” ìˆ˜ì²œ ë‹¬ëŸ¬ ë¹„ìš© ì´ˆë˜
:::

**ì°¸ê³  ìë£Œ:**
- [Setu GitHub Repository](https://github.com/sanjeevrg89/Setu)
- [Kueue ê³µì‹ ë¬¸ì„œ](https://kueue.sigs.k8s.io/)
- [Karpenter NodePool ì„¤ì • ê°€ì´ë“œ](https://karpenter.sh/)

### 7.4 EKS Blueprints IaC íŒ¨í„´ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ì •ì±… í‘œì¤€í™”

Terraform EKS Blueprintsë¥¼ ì‚¬ìš©í•˜ë©´ ResourceQuota, LimitRange, Policy Enforcementë¥¼ ì½”ë“œë¡œ í‘œì¤€í™”í•˜ì—¬ ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì— ì¼ê´€ë˜ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Terraform EKS Blueprints AddOn êµ¬ì¡°

```hcl
# main.tf - EKS Blueprintsë¡œ ë¦¬ì†ŒìŠ¤ ì •ì±… ìë™ ë°°í¬
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 20.0"

  cluster_name    = "production-eks"
  cluster_version = "1.31"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  enable_irsa = true

  eks_managed_node_groups = {
    general = {
      desired_size = 3
      min_size     = 2
      max_size     = 10
      instance_types = ["m6i.xlarge"]
    }
  }
}

# EKS Blueprints AddOnsë¡œ ë¦¬ì†ŒìŠ¤ ì •ì±… ë°°í¬
module "eks_blueprints_addons" {
  source  = "aws-ia/eks-blueprints-addons/aws"
  version = "~> 1.16"

  cluster_name      = module.eks.cluster_name
  cluster_endpoint  = module.eks.cluster_endpoint
  cluster_version   = module.eks.cluster_version
  oidc_provider_arn = module.eks.oidc_provider_arn

  # Metrics Server (VPA ì‚¬ì „ ìš”êµ¬ì‚¬í•­)
  enable_metrics_server = true

  # Karpenter (ë…¸ë“œ ì˜¤í† ìŠ¤ì¼€ì¼ë§)
  enable_karpenter = true
  karpenter = {
    repository_username = data.aws_ecrpublic_authorization_token.token.user_name
    repository_password = data.aws_ecrpublic_authorization_token.token.password
  }

  # Kyverno (ë¦¬ì†ŒìŠ¤ ì •ì±… ê°•ì œ)
  enable_kyverno = true
  kyverno = {
    values = [templatefile("${path.module}/kyverno-policies.yaml", {
      default_cpu_request    = "100m"
      default_memory_request = "128Mi"
      max_cpu_limit          = "4000m"
      max_memory_limit       = "8Gi"
    })]
  }
}

# ResourceQuotaë¥¼ Helm Chartë¡œ ë°°í¬
resource "helm_release" "resource_quotas" {
  name      = "resource-quotas"
  namespace = "kube-system"

  chart = "${path.module}/charts/resource-quotas"

  values = [
    yamlencode({
      quotas = {
        production = {
          cpu    = "100"
          memory = "200Gi"
          pods   = "500"
        }
        staging = {
          cpu    = "50"
          memory = "100Gi"
          pods   = "200"
        }
        development = {
          cpu    = "20"
          memory = "40Gi"
          pods   = "100"
        }
      }
    })
  ]
}
```

#### Kyverno ì •ì±…ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ìš”ì²­ ê°•ì œ

```yaml
# kyverno-policies.yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-resource-requests
  annotations:
    policies.kyverno.io/title: Require Resource Requests
    policies.kyverno.io/severity: medium
    policies.kyverno.io/description: |
      ëª¨ë“  PodëŠ” CPUì™€ Memory requestsë¥¼ ë°˜ë“œì‹œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
spec:
  validationFailureAction: Enforce  # Audit (ê²½ê³ ë§Œ) ë˜ëŠ” Enforce (ì°¨ë‹¨)
  background: true
  rules:
  - name: check-cpu-memory-requests
    match:
      any:
      - resources:
          kinds:
          - Pod
    validate:
      message: "CPUì™€ Memory requestsëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤"
      pattern:
        spec:
          containers:
          - resources:
              requests:
                memory: "?*"  # ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                cpu: "?*"

  - name: enforce-memory-limits
    match:
      any:
      - resources:
          kinds:
          - Pod
    validate:
      message: "Memory limitsëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤ (OOM Kill ë°©ì§€)"
      pattern:
        spec:
          containers:
          - resources:
              limits:
                memory: "?*"

  - name: prevent-excessive-resources
    match:
      any:
      - resources:
          kinds:
          - Pod
    validate:
      message: "CPUëŠ” ìµœëŒ€ {{ max_cpu_limit }}, MemoryëŠ” ìµœëŒ€ {{ max_memory_limit }}ê¹Œì§€ í—ˆìš©"
      deny:
        conditions:
          any:
          - key: "{{ request.object.spec.containers[].resources.requests.cpu }}"
            operator: GreaterThan
            value: "{{ max_cpu_limit }}"
          - key: "{{ request.object.spec.containers[].resources.requests.memory }}"
            operator: GreaterThan
            value: "{{ max_memory_limit }}"
```

#### OPA Gatekeeper ì •ì±… ì˜ˆì‹œ (ëŒ€ì•ˆ)

```yaml
# ConstraintTemplate - ë¦¬ì†ŒìŠ¤ ìš”ì²­ ê°•ì œ
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequireresources
spec:
  crd:
    spec:
      names:
        kind: K8sRequireResources
      validation:
        openAPIV3Schema:
          type: object
          properties:
            exemptNamespaces:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequireresources

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.requests.cpu
          msg := sprintf("ì»¨í…Œì´ë„ˆ %vëŠ” CPU requestsê°€ ì—†ìŠµë‹ˆë‹¤", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.requests.memory
          msg := sprintf("ì»¨í…Œì´ë„ˆ %vëŠ” Memory requestsê°€ ì—†ìŠµë‹ˆë‹¤", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.limits.memory
          msg := sprintf("ì»¨í…Œì´ë„ˆ %vëŠ” Memory limitsê°€ ì—†ìŠµë‹ˆë‹¤ (OOM ìœ„í—˜)", [container.name])
        }

---
# Constraint - ConstraintTemplate ì ìš©
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequireResources
metadata:
  name: require-resources-production
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces: ["production", "staging"]
  parameters:
    exemptNamespaces: ["kube-system", "kube-node-lease"]
```

#### GitOps ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ì •ì±… ê´€ë¦¬ íŒ¨í„´

**ArgoCD ApplicationSetìœ¼ë¡œ í™˜ê²½ë³„ ResourceQuota ë°°í¬:**

```yaml
# argocd/applicationset-resource-policies.yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: resource-policies
  namespace: argocd
spec:
  generators:
  - list:
      elements:
      - env: production
        cpu: "100"
        memory: "200Gi"
        pods: "500"
      - env: staging
        cpu: "50"
        memory: "100Gi"
        pods: "200"
      - env: development
        cpu: "20"
        memory: "40Gi"
        pods: "100"

  template:
    metadata:
      name: "resource-quota-{{env}}"
    spec:
      project: platform
      source:
        repoURL: https://github.com/myorg/k8s-manifests
        targetRevision: main
        path: resource-policies/{{env}}
        helm:
          parameters:
          - name: quota.cpu
            value: "{{cpu}}"
          - name: quota.memory
            value: "{{memory}}"
          - name: quota.pods
            value: "{{pods}}"
      destination:
        server: https://kubernetes.default.svc
        namespace: "{{env}}"
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
```

**ë¦¬í¬ì§€í† ë¦¬ êµ¬ì¡°:**

```
k8s-manifests/
â”œâ”€â”€ resource-policies/
â”‚   â”œâ”€â”€ production/
â”‚   â”‚   â”œâ”€â”€ resource-quota.yaml
â”‚   â”‚   â”œâ”€â”€ limit-range.yaml
â”‚   â”‚   â””â”€â”€ kyverno-policies.yaml
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ development/
â”‚       â””â”€â”€ ...
â””â”€â”€ argocd/
    â””â”€â”€ applicationset-resource-policies.yaml
```

:::tip EKS Blueprints + GitOps ê¶Œì¥ íŒ¨í„´
1. **Terraformìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° í”„ë¡œë¹„ì €ë‹** (VPC, EKS, AddOns)
2. **Kyverno/OPAë¡œ ì •ì±… ê°•ì œ** (ë¦¬ì†ŒìŠ¤ ìš”ì²­ í•„ìˆ˜, ê³¼ë„í•œ í• ë‹¹ ì°¨ë‹¨)
3. **ArgoCD ApplicationSetìœ¼ë¡œ í™˜ê²½ë³„ ì •ì±… ë°°í¬** (GitOps)
4. **Prometheus + Grafanaë¡œ ì •ì±… ì¤€ìˆ˜ìœ¨ ëª¨ë‹ˆí„°ë§**

ì´ ì¡°í•©ìœ¼ë¡œ **"í´ëŸ¬ìŠ¤í„°ëŠ” Terraformìœ¼ë¡œ, ì •ì±…ì€ Gitìœ¼ë¡œ"** ê´€ë¦¬í•˜ì—¬ ì¸í”„ë¼ í‘œì¤€í™”ì™€ ìš´ì˜ ìë™í™”ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.
:::

## ë¹„ìš© ì˜í–¥ ë¶„ì„

### 8.1 ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ê³„ì‚°

**ì‹œë‚˜ë¦¬ì˜¤:**
- í´ëŸ¬ìŠ¤í„°: 100ê°œ ë…¸ë“œ (m5.2xlarge, $0.384/ì‹œê°„)
- ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±: 40% (60% ë‚­ë¹„)

```
ì›”ë³„ ë¹„ìš©:
100 ë…¸ë“œ Ã— $0.384/ì‹œê°„ Ã— 730ì‹œê°„/ì›” = $28,032/ì›”

ë‚­ë¹„ ë¹„ìš©:
$28,032 Ã— 60% = $16,819/ì›”

Right-Sizing í›„ (íš¨ìœ¨ì„± 70%):
í•„ìš” ë…¸ë“œ: 100 Ã— (40% / 70%) = 57 ë…¸ë“œ
ì›”ë³„ ë¹„ìš©: 57 Ã— $0.384 Ã— 730 = $15,978/ì›”
ì ˆê°ì•¡: $28,032 - $15,978 = $12,054/ì›” (43% ì ˆê°)
```

### 8.2 í´ëŸ¬ìŠ¤í„° íš¨ìœ¨ì„± ë©”íŠ¸ë¦­

```promql
# CPU íš¨ìœ¨ì„±
sum(rate(container_cpu_usage_seconds_total{container!=""}[5m]))
/
sum(kube_pod_container_resource_requests{resource="cpu"}) * 100

# Memory íš¨ìœ¨ì„±
sum(container_memory_working_set_bytes{container!=""})
/
sum(kube_pod_container_resource_requests{resource="memory"}) * 100

# ëª©í‘œ: CPU 60% ì´ìƒ, Memory 70% ì´ìƒ
```

### 8.3 Right-Sizing ì ˆê° íš¨ê³¼

| ìµœì í™” í•­ëª© | ë¹„ìš© ì ˆê°ë¥  | êµ¬í˜„ ë‚œì´ë„ | ì˜ˆìƒ ì‹œê°„ |
|------------|-----------|-----------|----------|
| VPA ê¶Œì¥ì‚¬í•­ ì ìš© | 20-30% | ë‚®ìŒ | 1-2ì£¼ |
| CPU Limits ì œê±° | 5-10% | ë‚®ìŒ | 1ì£¼ |
| QoS í´ë˜ìŠ¤ ìµœì í™” | 10-15% | ì¤‘ê°„ | 2-3ì£¼ |
| HPA + ì ì ˆí•œ Requests | 15-25% | ì¤‘ê°„ | 2-4ì£¼ |
| ì „ì²´ Right-Sizing | 30-50% | ë†’ìŒ | 1-3ê°œì›” |

### 8.4 FinOps í†µí•© ë¹„ìš© ìµœì í™”

FinOps(Financial Operations)ëŠ” í´ë¼ìš°ë“œ ë¹„ìš© ê´€ë¦¬ë¥¼ ì¡°ì§ ë¬¸í™”ë¡œ ì •ì°©ì‹œí‚¤ëŠ” ë°©ë²•ë¡ ì…ë‹ˆë‹¤. Kubernetes í™˜ê²½ì—ì„œëŠ” ë¦¬ì†ŒìŠ¤ ê°€ì‹œì„±, ë¹„ìš© í• ë‹¹, ì§€ì†ì  ìµœì í™”ê°€ í•µì‹¬ì…ë‹ˆë‹¤.

#### 8.4.1 Kubecost + AWS Cost Explorer ì—°ê³„

**Kubecost ì„¤ì¹˜ ë° EKS í†µí•©:**

```bash
# 1. Kubecost ì„¤ì¹˜ (Prometheus í¬í•¨)
helm repo add kubecost https://kubecost.github.io/cost-analyzer/
helm repo update

helm install kubecost kubecost/cost-analyzer \
  --namespace kubecost \
  --create-namespace \
  --set kubecostToken="<your-token>" \
  --set prometheus.server.global.external_labels.cluster_id=<cluster-name> \
  --set prometheus.nodeExporter.enabled=true \
  --set prometheus.serviceAccounts.nodeExporter.create=true

# 2. AWS Cost and Usage Report (CUR) í†µí•© ì„¤ì •
# values.yamlì— ì¶”ê°€:
# kubecostProductConfigs:
#   awsServiceKeyName: <secret-name>
#   awsServiceKeyPassword: <secret-key>
#   awsSpotDataBucket: <s3-bucket>
#   awsSpotDataRegion: <region>
#   curExportPath: <cur-export-path>

# 3. ëŒ€ì‹œë³´ë“œ ì ‘ì†
kubectl port-forward -n kubecost deployment/kubecost-cost-analyzer 9090:9090

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:9090 ì ‘ì†
```

**ë„¤ì„ìŠ¤í˜ì´ìŠ¤/ì›Œí¬ë¡œë“œë³„ ë¹„ìš© ê°€ì‹œì„±:**

KubecostëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì°¨ì›ìœ¼ë¡œ ë¹„ìš©ì„ ë¶„í•´í•©ë‹ˆë‹¤:

| ì°¨ì› | ì„¤ëª… | í™œìš© |
|------|------|------|
| **Namespace** | ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë¹„ìš© | íŒ€/í”„ë¡œì íŠ¸ë³„ ì²­êµ¬ |
| **Deployment** | ì›Œí¬ë¡œë“œë³„ ë¹„ìš© | ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ TCO ë¶„ì„ |
| **Pod** | ê°œë³„ Pod ë¹„ìš© | Over-provisioning ì‹ë³„ |
| **Label** | ì»¤ìŠ¤í…€ ë ˆì´ë¸”ë³„ ë¹„ìš© | í™˜ê²½(dev/staging/prod), ë¹„ìš©ì„¼í„°ë³„ ë¶„ë¥˜ |
| **Node** | ë…¸ë“œë³„ ë¹„ìš© | ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìµœì í™” |

**AWS Cost Explorerì™€ì˜ ë°ì´í„° ì¼ê´€ì„± í™•ë³´:**

```mermaid
graph LR
    subgraph "AWS Billing"
        A[AWS Cost and Usage Report] --> B[S3 Bucket]
    end

    subgraph "Kubecost"
        B --> C[Kubecost ETL]
        C --> D[Cost Allocation]
        D --> E[Namespace ë¹„ìš©]
        D --> F[Pod ë¹„ìš©]
        D --> G[Label ë¹„ìš©]
    end

    subgraph "ê²€ì¦"
        H[AWS Cost Explorer<br/>í´ëŸ¬ìŠ¤í„° ì´ ë¹„ìš©] --> I{ì¼ì¹˜ í™•ì¸}
        E --> J[Kubecost í•©ê³„]
        F --> J
        G --> J
        J --> I
        I -->|ì°¨ì´ < 5%| K[ì •ìƒ]
        I -->|ì°¨ì´ > 5%| L[CUR ì„¤ì • í™•ì¸]
    end

    style K fill:#51cf66
    style L fill:#ff6b6b
```

**ì¼ê´€ì„± ê²€ì¦ ì¿¼ë¦¬:**

```bash
# Kubecost API - í´ëŸ¬ìŠ¤í„° ì´ ë¹„ìš© (ì§€ë‚œ 7ì¼)
curl "http://localhost:9090/model/allocation?window=7d&aggregate=cluster" | jq '.data[].totalCost'

# AWS CLI - Cost Explorer ì´ ë¹„ìš© (ì§€ë‚œ 7ì¼)
aws ce get-cost-and-usage \
  --time-period Start=$(date -d '7 days ago' +%Y-%m-%d),End=$(date +%Y-%m-%d) \
  --granularity DAILY \
  --metrics BlendedCost \
  --filter file://eks-filter.json

# eks-filter.json:
# {
#   "Tags": {
#     "Key": "eks:cluster-name",
#     "Values": ["<cluster-name>"]
#   }
# }
```

**20-60% ë¹„ìš© ì ˆê° ê°€ëŠ¥ ì˜ì—­ ì‹ë³„ íŒ¨í„´:**

Kubecost ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ìŒ ì§€í‘œë¡œ ìµœì í™” ê¸°íšŒë¥¼ ì‹ë³„í•©ë‹ˆë‹¤:

| ì§€í‘œ | ê¸°ì¤€ | ì˜ˆìƒ ì ˆê° | ì¡°ì¹˜ |
|------|------|----------|------|
| **CPU Efficiency** | < 50% | 20-30% | Right-Sizing (VPA) |
| **Memory Efficiency** | < 60% | 15-25% | Right-Sizing (VPA) |
| **Idle Cost** | > 30% | 30-50% | HPA + Cluster Autoscaler/Karpenter |
| **Over-Provisioned Pods** | Requests ì‚¬ìš©ë¥  < 50% | 10-20% | Goldilocks ê¶Œì¥ì‚¬í•­ ì ìš© |
| **Spot Adoption** | < 30% | 40-60% | Spot + Graviton ì „í™˜ |

**Kubecost Savings Insights í™œìš©:**

```bash
# Kubecost API - Savings ê¶Œì¥ì‚¬í•­ ì¡°íšŒ
curl "http://localhost:9090/model/savings" | jq '.data[] | {
  type: .savingsType,
  monthly_savings: .monthlySavings,
  resource: .resourceName
}'

# ì˜ˆìƒ ì¶œë ¥:
# {
#   "type": "rightsize-deployment",
#   "monthly_savings": 1240.50,
#   "resource": "production/web-app"
# }
# {
#   "type": "adopt-spot",
#   "monthly_savings": 3450.20,
#   "resource": "batch/worker-pool"
# }
```

#### 8.4.2 Goldilocks vs Kubecost ë„êµ¬ ë¹„êµ

| í•­ëª© | Goldilocks | Kubecost |
|------|-----------|----------|
| **ì£¼ìš” ê¸°ëŠ¥** | VPA ê¶Œì¥ì‚¬í•­ ì‹œê°í™” | ì „ì²´ ë¹„ìš© ê°€ì‹œì„± + ìµœì í™” ê¶Œì¥ì‚¬í•­ |
| **ë¹„ìš©** | ë¬´ë£Œ (ì˜¤í”ˆì†ŒìŠ¤) | ë¬´ë£Œ (ê¸°ë³¸), Enterprise (ìœ ë£Œ) |
| **ì„¤ì¹˜ ë³µì¡ë„** | ë‚®ìŒ (Helm 1ì¤„) | ì¤‘ê°„ (Prometheus ì„¤ì • í•„ìš”) |
| **ë°ì´í„° ì†ŒìŠ¤** | Metrics Server, VPA | Prometheus, AWS CUR, í´ë¼ìš°ë“œ ë¹Œë§ API |
| **ê¶Œì¥ì‚¬í•­ ë²”ìœ„** | CPU/Memory Right-Sizing | Right-Sizing, Spot, Graviton, Idle Resource, Cluster Sizing |
| **ë¹„ìš© í• ë‹¹** | ì—†ìŒ | Namespace, Label, Pod, Deployment ë ˆë²¨ |
| **ì˜ˆì‚° ê´€ë¦¬** | ì—†ìŒ | ì˜ˆì‚° ì•ŒëŒ, ë¹„ìš© ì¶”ì„¸ ì˜ˆì¸¡ |
| **ë©€í‹° í´ëŸ¬ìŠ¤í„°** | í´ëŸ¬ìŠ¤í„°ë³„ ë…ë¦½ | í†µí•© ëŒ€ì‹œë³´ë“œ ì§€ì› |
| **AWS í†µí•©** | ì—†ìŒ | Cost Explorer, CUR, Savings Plans ë¶„ì„ |
| **ë¦¬í¬íŠ¸** | ì›¹ UIë§Œ | PDF, CSV, Slack/Teams ì•ŒëŒ |

**ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤:**

| ìƒí™© | ì¶”ì²œ ë„êµ¬ | ì´ìœ  |
|------|----------|------|
| **ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°, ë¦¬ì†ŒìŠ¤ ìµœì í™”ë§Œ** | Goldilocks | ê°€ë³ê³  ë¹ ë¥¸ ì‹œì‘ |
| **ë©€í‹° í´ëŸ¬ìŠ¤í„°, ë¹„ìš© ì²­êµ¬** | Kubecost | ì „ì‚¬ì  ë¹„ìš© ê´€ë¦¬ í•„ìš” |
| **ìŠ¤íƒ€íŠ¸ì—…, ë¹ ë¥¸ ì ˆê° í•„ìš”** | Goldilocks â†’ Kubecost | ë‹¨ê³„ì  ë„ì… |
| **ì—”í„°í”„ë¼ì´ì¦ˆ, FinOps íŒ€ ì¡´ì¬** | Kubecost Enterprise | ê³ ê¸‰ ê¸°ëŠ¥ (ì˜ˆì‚°, ì•ŒëŒ, ì •ì±…) |
| **ì˜¤í”ˆì†ŒìŠ¤ë§Œ ì‚¬ìš©** | Goldilocks + Prometheus | ë¹„ìš© 0ì› |

**ë³‘í–‰ ì‚¬ìš© íŒ¨í„´:**

```bash
# Goldilocksë¡œ ë¹ ë¥¸ Right-Sizing
kubectl label namespace production goldilocks.fairwinds.com/enabled=true

# Kubecostë¡œ ì „ì²´ ë¹„ìš© ì¶”ì  ë° ê²€ì¦
# 1. Goldilocks ê¶Œì¥ì‚¬í•­ ì ìš© ì „ ë¹„ìš© ê¸°ë¡
curl "http://localhost:9090/model/allocation?window=7d&aggregate=namespace&accumulate=true" \
  | jq '.data[] | select(.name=="production") | .totalCost'

# 2. Right-Sizing ì ìš©
kubectl set resources deployment web-app -n production \
  --requests=cpu=300m,memory=512Mi \
  --limits=memory=1Gi

# 3. 7ì¼ í›„ Kubecostì—ì„œ ì ˆê°ì•¡ í™•ì¸
```

#### 8.4.3 ìë™í™”ëœ ë¹„ìš© ìµœì í™” ë£¨í”„

FinOpsì˜ í•µì‹¬ì€ **ì§€ì†ì ì¸ ë¹„ìš© ê°€ì‹œì„± â†’ ìµœì í™” â†’ ê²€ì¦ ë£¨í”„**ì…ë‹ˆë‹¤. GitOpsì™€ ê²°í•©í•˜ë©´ ì™„ì „ ìë™í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

**ë¹„ìš© ìµœì í™” ë£¨í”„ ì•„í‚¤í…ì²˜:**

```mermaid
graph TB
    subgraph "1. ë¹„ìš© ê°€ì‹œì„±"
        A[Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘] --> B[Kubecost ë¹„ìš© ë¶„ì„]
        B --> C[Over-provisioned ì‹ë³„]
    end

    subgraph "2. ë¦¬ì†ŒìŠ¤ ìµœì í™”"
        C --> D[VPA ê¶Œì¥ì‚¬í•­ ìƒì„±]
        D --> E[GitOps PR ìë™ ìƒì„±]
        E --> F[íŒ€ ë¦¬ë·°]
    end

    subgraph "3. ë¹„ìš© ê²€ì¦"
        F --> G[Merge â†’ ArgoCD ë°°í¬]
        G --> H[Kubecost ë¹„ìš© ì¶”ì ]
        H --> I{ì ˆê° í™•ì¸}
    end

    I -->|ì ˆê° ì„±ê³µ| J[ì•ŒëŒ: Slack í†µì§€]
    I -->|ì ˆê° ë¯¸ë‹¬| K[ë¡¤ë°± ê²€í† ]
    J --> L[ë‹¤ìŒ ìµœì í™” ëŒ€ìƒ ì„ ì •]
    K --> L
    L --> A

    style A fill:#e3f2fd
    style E fill:#fff3e0
    style I fill:#f3e5f5
    style J fill:#c8e6c9
    style K fill:#ff6b6b
```

**GitOps ê¸°ë°˜ ìë™ Right-Sizing PR ìƒì„± íŒ¨í„´:**

```python
# automation/right-sizing-bot.py
import requests
import yaml
import subprocess
from datetime import datetime

# 1. Kubecost APIì—ì„œ ê¶Œì¥ì‚¬í•­ ì¡°íšŒ
def get_kubecost_recommendations():
    response = requests.get("http://kubecost:9090/model/savings")
    savings = response.json()["data"]
    return [s for s in savings if s["savingsType"] == "rightsize-deployment"]

# 2. Deployment ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
def update_deployment(namespace, name, cpu_request, memory_request):
    file_path = f"k8s/{namespace}/{name}.yaml"
    with open(file_path, 'r') as f:
        manifest = yaml.safe_load(f)

    # ë¦¬ì†ŒìŠ¤ ì—…ë°ì´íŠ¸
    manifest["spec"]["template"]["spec"]["containers"][0]["resources"] = {
        "requests": {
            "cpu": cpu_request,
            "memory": memory_request
        },
        "limits": {
            "memory": str(int(memory_request.rstrip('Mi')) * 1.5) + 'Mi'
        }
    }

    with open(file_path, 'w') as f:
        yaml.dump(manifest, f)

# 3. Git PR ìƒì„±
def create_pr(recommendations):
    branch = f"right-sizing-{datetime.now().strftime('%Y%m%d')}"
    subprocess.run(["git", "checkout", "-b", branch])

    for rec in recommendations:
        update_deployment(
            rec["namespace"],
            rec["resourceName"],
            rec["recommendedCPU"],
            rec["recommendedMemory"]
        )
        subprocess.run(["git", "add", f"k8s/{rec['namespace']}/{rec['resourceName']}.yaml"])

    subprocess.run([
        "git", "commit", "-m",
        f"chore: apply Kubecost right-sizing (estimated savings: ${sum(r['monthlySavings'] for r in recommendations):.2f}/month)"
    ])
    subprocess.run(["git", "push", "origin", branch])

    # GitHub PR ìƒì„±
    subprocess.run([
        "gh", "pr", "create",
        "--title", f"Cost Optimization: Right-Sizing Recommendations",
        "--body", f"Estimated monthly savings: ${sum(r['monthlySavings'] for r in recommendations):.2f}\n\nAuto-generated by Kubecost",
        "--label", "cost-optimization"
    ])

# ì‹¤í–‰
if __name__ == "__main__":
    recommendations = get_kubecost_recommendations()
    if recommendations:
        create_pr(recommendations)
```

**ìë™í™” ì‹¤í–‰ (CronJob):**

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: right-sizing-bot
  namespace: automation
spec:
  schedule: "0 9 * * MON"  # ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 9ì‹œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: right-sizing-bot
          containers:
          - name: bot
            image: right-sizing-bot:v1
            env:
            - name: KUBECOST_URL
              value: "http://kubecost.kubecost.svc:9090"
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: github-token
                  key: token
          restartPolicy: OnFailure
```

**Prometheus + Bedrock + GitOps ìë™í™” ì°¸ì¡°:**

AWS re:Invent 2025ì˜ [CNS421 ì„¸ì…˜](https://www.youtube.com/watch?v=4s-a0jY4kSE)ì—ì„œëŠ” Amazon Bedrockê³¼ Model Context Protocol(MCP)ì„ í™œìš©í•œ ê³ ê¸‰ ìë™í™” íŒ¨í„´ì„ ì†Œê°œí–ˆìŠµë‹ˆë‹¤:

```python
# ê³ ê¸‰ íŒ¨í„´: AI ê¸°ë°˜ ìµœì í™” ì˜ì‚¬ê²°ì •
from anthropic import Anthropic

client = Anthropic()

# Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
metrics = get_prometheus_metrics()

# Bedrockì—ê²Œ ìµœì í™” ì „ëµ ìš”ì²­
response = client.messages.create(
    model="claude-3-sonnet-20240229",
    max_tokens=1024,
    messages=[{
        "role": "user",
        "content": f"""
        ë‹¤ìŒ Kubernetes í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ì„ ë¶„ì„í•˜ê³  ìµœì í™” ì „ëµì„ ì œì•ˆí•˜ì„¸ìš”:

        {metrics}

        ë‹¤ìŒì„ í¬í•¨í•˜ì„¸ìš”:
        1. ë¹„ìš© ì ˆê° ìš°ì„ ìˆœìœ„
        2. ë¦¬ìŠ¤í¬ í‰ê°€
        3. ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš
        """
    }]
)

# AI ì œì•ˆì„ PR ì„¤ëª…ì— í¬í•¨
create_pr_with_ai_context(response.content)
```

#### 8.4.4 Graviton + Spot ë¹„ìš© ì ˆê° ì‹œë‚˜ë¦¬ì˜¤

**ì‹¤ì œ ë¹„ìš© ë¹„êµ í‘œ (2026ë…„ 2ì›” ê¸°ì¤€, us-east-1):**

| ì‹œë‚˜ë¦¬ì˜¤ | ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | vCPU | Memory | ì‹œê°„ë‹¹ ë¹„ìš© | ì›”ê°„ ë¹„ìš© (730h) | ì ˆê°ë¥  |
|---------|-------------|------|--------|-----------|-----------------|--------|
| **Baseline: x86 On-Demand** | m6i.2xlarge | 8 | 32 GB | $0.384 | $280.32 | - |
| **Graviton On-Demand** | m7g.2xlarge | 8 | 32 GB | $0.3264 | $238.27 | **15%** |
| **x86 Spot** | m6i.2xlarge | 8 | 32 GB | $0.1152 (70% í• ì¸) | $84.10 | **70%** |
| **Graviton Spot** | m7g.2xlarge | 8 | 32 GB | $0.0979 (70% í• ì¸) | $71.47 | **75%** |

**100ê°œ ë…¸ë“œ í´ëŸ¬ìŠ¤í„° ê¸°ì¤€ ì—°ê°„ ë¹„ìš©:**

| êµ¬ì„± | ì›”ê°„ ë¹„ìš© | ì—°ê°„ ë¹„ìš© | ì—°ê°„ ì ˆê°ì•¡ |
|------|----------|----------|-----------|
| x86 On-Demand (100 nodes) | $28,032 | $336,384 | - |
| Graviton On-Demand (100 nodes) | $23,827 | $285,924 | $50,460 (15%) |
| x86 Spot (100 nodes) | $8,410 | $100,920 | $235,464 (70%) |
| **Graviton Spot (100 nodes)** | **$7,147** | **$85,764** | **$250,620 (75%)** â­ |

**ì›Œí¬ë¡œë“œ ìœ í˜•ë³„ ê¶Œì¥ ì¡°í•©:**

| ì›Œí¬ë¡œë“œ ìœ í˜• | ê¶Œì¥ êµ¬ì„± | ì´ìœ  | ì˜ˆìƒ ì ˆê° |
|-------------|----------|------|----------|
| **í”„ë¡œë•ì…˜ API (ìƒì‹œ)** | Graviton On-Demand 70% + Graviton Spot 30% | ì•ˆì •ì„± ìš°ì„ , ì¼ë¶€ Spot í™œìš© | 25-35% |
| **ë°°ì¹˜ ì‘ì—…** | Graviton Spot 100% | ì¤‘ë‹¨ í—ˆìš©, ë¹„ìš© ìµœìš°ì„  | 70-75% |
| **ê°œë°œ/ìŠ¤í…Œì´ì§•** | Graviton Spot 100% | ì¤‘ë‹¨ í—ˆìš©, ë¹ ë¥¸ ì¬ì‹œì‘ | 70-75% |
| **ë°ì´í„°ë² ì´ìŠ¤** | Graviton On-Demand 100% | ì¤‘ë‹¨ ë¶ˆê°€, ì•ˆì •ì„± ìµœìš°ì„  | 15% |
| **í ì›Œì»¤ (Stateless)** | Graviton Spot 80% + Graviton On-Demand 20% | ì¤‘ë‹¨ ì‹œ ì¬ì‹œì‘, ëŒ€ë¶€ë¶„ Spot | 60-65% |
| **ML ì¶”ë¡ ** | Graviton Spot 100% (GPU ì›Œí¬ë¡œë“œëŠ” p4d Spot) | ì¤‘ë‹¨ í—ˆìš©, ê³ ë¹„ìš© ì¸ìŠ¤í„´ìŠ¤ ì ˆê° | 70-75% |

**Karpenter NodePoolì—ì„œ Graviton ìš°ì„  ì„¤ì • YAML:**

```yaml
# Production API - Graviton ìš°ì„ , Spot/On-Demand í˜¼í•©
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: production-api-pool
spec:
  template:
    spec:
      requirements:
      # Graviton ìš°ì„ 
      - key: kubernetes.io/arch
        operator: In
        values: ["arm64"]

      # Spot 70%, On-Demand 30% (ê°€ì¤‘ì¹˜ë¡œ ì œì–´)
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot", "on-demand"]

      # ë²”ìš© ì›Œí¬ë¡œë“œìš© ì¸ìŠ¤í„´ìŠ¤ íŒ¨ë°€ë¦¬
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["m7g.large", "m7g.xlarge", "m7g.2xlarge"]

      nodeClassRef:
        name: default

  # Spot ì¤‘ë‹¨ ì‹œ ìë™ êµì²´
  disruption:
    consolidationPolicy: WhenUnderutilized
    expireAfter: 720h

  limits:
    cpu: "200"
    memory: "400Gi"

  weight: 100  # ìµœê³  ìš°ì„ ìˆœìœ„

---
# Batch Jobs - Graviton Spot 100%
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: batch-jobs-pool
spec:
  template:
    spec:
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values: ["arm64"]

      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot"]  # Spotë§Œ

      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["c7g.large", "c7g.xlarge", "c7g.2xlarge", "c7g.4xlarge"]

      nodeClassRef:
        name: default

      # Batch ì‘ì—…ìš© Taints
      taints:
      - key: workload-type
        value: batch
        effect: NoSchedule

  disruption:
    consolidationPolicy: WhenUnderutilized
    expireAfter: 1h  # ë°°ì¹˜ ì‘ì—…ì€ ì§§ì€ ìˆ˜ëª…

  limits:
    cpu: "500"

  weight: 50

---
# Database - Graviton On-Demand 100%
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: database-pool
spec:
  template:
    spec:
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values: ["arm64"]

      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand"]  # On-Demandë§Œ

      # ë©”ëª¨ë¦¬ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["r7g.xlarge", "r7g.2xlarge", "r7g.4xlarge"]

      nodeClassRef:
        name: default

      taints:
      - key: workload-type
        value: database
        effect: NoSchedule

  disruption:
    consolidationPolicy: WhenEmpty  # ë¹„ì–´ìˆì„ ë•Œë§Œ êµì²´
    expireAfter: 2160h  # 90ì¼ (ì¥ê¸° ì‹¤í–‰)

  limits:
    cpu: "100"
    memory: "800Gi"

  weight: 200  # ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„
```

**Podì—ì„œ NodePool ì„ íƒ:**

```yaml
# API ì„œë²„ - production-api-pool ì‚¬ìš©
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 20
  template:
    spec:
      nodeSelector:
        karpenter.sh/nodepool: production-api-pool
      containers:
      - name: api
        image: api-server:v1-arm64  # Gravitonìš© ì´ë¯¸ì§€
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"

---
# ë°°ì¹˜ ì‘ì—… - batch-jobs-pool ì‚¬ìš©
apiVersion: batch/v1
kind: CronJob
metadata:
  name: nightly-report
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          nodeSelector:
            karpenter.sh/nodepool: batch-jobs-pool
          tolerations:
          - key: workload-type
            operator: Equal
            value: batch
            effect: NoSchedule
          containers:
          - name: report-gen
            image: report-generator:v1-arm64
            resources:
              requests:
                cpu: "2000m"
                memory: "4Gi"
          restartPolicy: OnFailure

---
# ë°ì´í„°ë² ì´ìŠ¤ - database-pool ì‚¬ìš©
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  replicas: 3
  template:
    spec:
      nodeSelector:
        karpenter.sh/nodepool: database-pool
      tolerations:
      - key: workload-type
        operator: Equal
        value: database
        effect: NoSchedule
      containers:
      - name: postgres
        image: postgres:16-arm64
        resources:
          requests:
            cpu: "4000m"
            memory: "16Gi"
          limits:
            cpu: "4000m"
            memory: "16Gi"  # Guaranteed QoS
```

**Spot ì¤‘ë‹¨ ëŒ€ì‘ ì „ëµ:**

```yaml
# PodDisruptionBudgetìœ¼ë¡œ ìµœì†Œ ê°€ìš©ì„± ë³´ì¥
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-server-pdb
spec:
  minAvailable: 80%  # ìµœì†Œ 80% Pod ìœ ì§€
  selector:
    matchLabels:
      app: api-server

---
# Spot ì¤‘ë‹¨ 2ë¶„ ì „ ì•Œë¦¼ ì²˜ë¦¬ (DaemonSet)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: spot-termination-handler
spec:
  selector:
    matchLabels:
      app: spot-termination-handler
  template:
    spec:
      serviceAccountName: spot-termination-handler
      containers:
      - name: handler
        image: aws/aws-node-termination-handler:v1.21.0
        env:
        - name: ENABLE_SPOT_INTERRUPTION_DRAINING
          value: "true"
        - name: ENABLE_SCHEDULED_EVENT_DRAINING
          value: "true"
```

**ì‹¤ì œ ì ˆê° ì‚¬ë¡€ (AWS ê³µì‹ ë¸”ë¡œê·¸):**

| ì¡°ì§ | ì›Œí¬ë¡œë“œ | ì´ì „ êµ¬ì„± | ìµœì í™” í›„ | ì ˆê°ì•¡ |
|------|---------|----------|----------|--------|
| Fintech ìŠ¤íƒ€íŠ¸ì—… | API ì„œë²„ 100 nodes | x86 On-Demand | Graviton Spot 70% + On-Demand 30% | $8,500/ì›” (30%) |
| ì´ì»¤ë¨¸ìŠ¤ ê¸°ì—… | ë°°ì¹˜ ì‘ì—… 200 nodes | x86 On-Demand | Graviton Spot 100% | $42,000/ì›” (75%) |
| SaaS í”Œë«í¼ | ì „ì²´ í´ëŸ¬ìŠ¤í„° 300 nodes | x86 í˜¼í•© | Graviton 90% + Spot 60% | $65,000/ì›” (65%) |

:::tip Auto Modeì—ì„œì˜ Graviton + Spot
EKS Auto ModeëŠ” ìœ„ì™€ ê°™ì€ NodePool êµ¬ì„± ì—†ì´ë„, Podì˜ ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ **ìë™ìœ¼ë¡œ Graviton Spot ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìš°ì„  ì„ íƒ**í•©ë‹ˆë‹¤. ë‹¨, ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ê°€ arm64 ì•„í‚¤í…ì²˜ë¥¼ ì§€ì›í•´ì•¼ í•©ë‹ˆë‹¤.

```yaml
# Auto Mode í™˜ê²½ - NodePool ë¶ˆí•„ìš”
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 20
  template:
    spec:
      containers:
      - name: api
        image: api-server:v1  # multi-arch ì´ë¯¸ì§€ (arm64/amd64 ëª¨ë‘ ì§€ì›)
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"

      # Auto Modeê°€ ìë™ìœ¼ë¡œ:
      # 1. Graviton Spot ìš°ì„  ì‹œë„
      # 2. Spot ë¶ˆê°€ ì‹œ Graviton On-Demand
      # 3. Graviton ë¶ˆê°€ ì‹œ x86 Spot
      # 4. ìµœí›„ x86 On-Demand
```
:::

:::info ì „ì²´ ë¹„ìš© ì „ëµì€ cost-management.md ì°¸ì¡°
ì´ ë¬¸ì„œëŠ” Pod ë¦¬ì†ŒìŠ¤ ìµœì í™”ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„° ì „ì²´ ë¹„ìš© ê´€ë¦¬ ì „ëµì€ [EKS ë¹„ìš© ê´€ë¦¬ ê°€ì´ë“œ](/docs/infrastructure-optimization/cost-management)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
:::

## ì¢…í•© ì²´í¬ë¦¬ìŠ¤íŠ¸ & ì°¸ê³  ìë£Œ

### ë¦¬ì†ŒìŠ¤ ì„¤ì • ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | í™•ì¸ ì‚¬í•­ | ê¶Œì¥ ì„¤ì • |
|------|----------|----------|
| **CPU Requests** | âœ… P95 ì‚¬ìš©ëŸ‰ + 20% | VPA Target ê¸°ë°˜ |
| **CPU Limits** | âœ… ì¼ë°˜ ì›Œí¬ë¡œë“œëŠ” ë¯¸ì„¤ì • | ë°°ì¹˜ ì‘ì—…ë§Œ ì„¤ì • |
| **Memory Requests** | âœ… P95 ì‚¬ìš©ëŸ‰ + 20% | VPA Target ê¸°ë°˜ |
| **Memory Limits** | âœ… ë°˜ë“œì‹œ ì„¤ì • | Requests Ã— 1.5~2 |
| **QoS í´ë˜ìŠ¤** | âœ… í”„ë¡œë•ì…˜ì€ Guaranteed/Burstable | BestEffort ê¸ˆì§€ |
| **VPA** | âœ… Off ë˜ëŠ” Initial ëª¨ë“œ | Auto ëª¨ë“œ ì‹ ì¤‘ |
| **HPA** | âœ… Behavior ì„¤ì • | ScaleUp ê³µê²©ì , ScaleDown ë³´ìˆ˜ì  |
| **ResourceQuota** | âœ… ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ì„¤ì • | í™˜ê²½ë³„ ì°¨ë“± ì ìš© |
| **LimitRange** | âœ… ê¸°ë³¸ê°’ ì„¤ì • | ê°œë°œì í¸ì˜ì„± |
| **PDB** | âœ… VPA Auto ì‚¬ìš© ì‹œ í•„ìˆ˜ | minAvailable 80% |

### ê´€ë ¨ ë¬¸ì„œ

**ë‚´ë¶€ ë¬¸ì„œ:**
- [Karpenter ì˜¤í† ìŠ¤ì¼€ì¼ë§](/docs/infrastructure-optimization/karpenter-autoscaling) - ë…¸ë“œ ë ˆë²¨ ìŠ¤ì¼€ì¼ë§
- [EKS ë¹„ìš© ê´€ë¦¬](/docs/infrastructure-optimization/cost-management) - ì „ì²´ ë¹„ìš© ìµœì í™” ì „ëµ
- [EKS Resiliency ê°€ì´ë“œ](/docs/operations-observability/eks-resiliency-guide) - ì•ˆì •ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸

**ì™¸ë¶€ ì°¸ì¡°:**
- [Kubernetes Resource Management](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- [Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)
- [AWS EKS Best Practices - Resource Management](https://docs.aws.amazon.com/eks/latest/best-practices/reliability.html)
- [Goldilocks](https://github.com/FairwindsOps/goldilocks)
- [CAST.ai Resource Optimization](https://cast.ai/blog/kubernetes-resource-optimization/)

**Red Hat OpenShift ë¬¸ì„œ:**
- [Automatically Scaling Pods with HPA](https://docs.openshift.com/container-platform/4.18/nodes/pods/nodes-pods-autoscaling.html) â€” HPA êµ¬ì„± ë° ìš´ì˜
- [Vertical Pod Autoscaler](https://docs.openshift.com/container-platform/4.18/nodes/pods/nodes-pods-vertical-autoscaler.html) â€” VPA ëª¨ë“œë³„ ì„¤ì • ë° ìš´ì˜
- [Quotas and Limit Ranges](https://docs.openshift.com/container-platform/4.18/applications/quotas/quotas-setting-per-project.html) â€” ResourceQuota, LimitRange ì„¤ì •
- [Using CPU Manager](https://docs.openshift.com/container-platform/4.18/scalability_and_performance/using-cpu-manager.html) â€” CPU ë¦¬ì†ŒìŠ¤ ê³ ê¸‰ ê´€ë¦¬

---

**í”¼ë“œë°± ë° ê¸°ì—¬**

ì´ ë¬¸ì„œì— ëŒ€í•œ í”¼ë“œë°±ì´ë‚˜ ê°œì„  ì œì•ˆì€ [GitHub Issues](https://github.com/devfloor9/engineering-playbook/issues)ì— ë“±ë¡í•´ì£¼ì„¸ìš”.

**ë¬¸ì„œ ë²„ì „**: v1.0 (2026-02-12)
**ë‹¤ìŒ ë¦¬ë·°**: 2026-05-12
