---
title: "Ray Serveì™€ vLLM í†µí•© ì•„í‚¤í…ì²˜"
sidebar_label: "Ray Serve + vLLM"
description: "Ray Serve ê¸°ë°˜ ë¶„ì‚° ì¶”ë¡  ì•„í‚¤í…ì²˜ì™€ vLLM í†µí•© ì „ëµ, ìë™ ìŠ¤ì¼€ì¼ë§, í”„ë¡œë•ì…˜ ë°°í¬ íŒ¨í„´"
sidebar_position: 6
date: 2025-02-09
---

# Ray Serveì™€ vLLM í†µí•© ì•„í‚¤í…ì²˜

> **ğŸ“Œ ê¸°ì¤€ ë²„ì „**: Ray 2.44.0+, vLLM v0.15.1, KubeRay Operator 1.2+

ëŒ€ê·œëª¨ LLM ì„œë¹™ í™˜ê²½ì—ì„œ ë‹¨ì¼ vLLM ì¸ìŠ¤í„´ìŠ¤ë§Œìœ¼ë¡œëŠ” ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ê¸° ì–´ë µë‹¤. ë‹¤ì¤‘ ëª¨ë¸ ê´€ë¦¬, ë™ì  ìŠ¤ì¼€ì¼ë§, ë³µì¡í•œ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸, ê·¸ë¦¬ê³  ì¥ì•  ë³µêµ¬ê°€ í•„ìš”í•œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë¶„ì‚° ì„œë¹™ í”„ë ˆì„ì›Œí¬ê°€ í•„ìˆ˜ë‹¤. Ray ServeëŠ” ì´ëŸ¬í•œ ìš”êµ¬ì‚¬í•­ì„ í•´ê²°í•˜ëŠ” ë¶„ì‚° ëª¨ë¸ ì„œë¹™ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, vLLMê³¼ ê²°í•©í•˜ë©´ ê°ê°ì˜ ê°•ì ì„ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆë‹¤.

ë³¸ ë¬¸ì„œì—ì„œëŠ” Ray Serveì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•˜ê³ , vLLMê³¼ì˜ í†µí•©ì´ í•„ìš”í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹ë³„í•˜ë©°, Amazon EKS í™˜ê²½ì—ì„œì˜ ì‹¤ë¬´ ë°°í¬ íŒ¨í„´ì„ ë‹¤ë£¬ë‹¤.

## Ray Serve í•µì‹¬ ì•„í‚¤í…ì²˜

### ë¶„ì‚° ì„œë¹™ì˜ í•„ìš”ì„±

ë‹¨ì¼ vLLM ì¸ìŠ¤í„´ìŠ¤ ë°°í¬ëŠ” ë‹¤ìŒ ìƒí™©ì—ì„œ í•œê³„ì— ì§ë©´í•œë‹¤.

ì²«ì§¸, ë‹¤ì¤‘ ëª¨ë¸ ì„œë¹™ì´ë‹¤. ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì™€ íŠ¹ì„±ì˜ ëª¨ë¸ë“¤ì„ ë™ì‹œì— ì„œë¹„ìŠ¤í•´ì•¼ í•˜ëŠ” ê²½ìš°, ê° ëª¨ë¸ì— ëŒ€í•œ ê°œë³„ ì—”ë“œí¬ì¸íŠ¸ ê´€ë¦¬, ë²„ì „ ê´€ë¦¬, íŠ¸ë˜í”½ ë¼ìš°íŒ…ì´ ë³µì¡í•´ì§„ë‹¤.

ë‘˜ì§¸, ë³µí•© ì¶”ë¡  íŒŒì´í”„ë¼ì¸ì´ë‹¤. ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìƒì„±ì„ ë„˜ì–´ ì„ë² ë”© ìƒì„±, ë¦¬ë­í‚¹, RAG íŒŒì´í”„ë¼ì¸, ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ë“± ì—¬ëŸ¬ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ë¡œì§ì´ ì—°ê²°ëœ ì›Œí¬í”Œë¡œìš°ê°€ í•„ìš”í•˜ë‹¤.

ì…‹ì§¸, ì„¸ë°€í•œ ìë™ ìŠ¤ì¼€ì¼ë§ì´ë‹¤. Kubernetes HPAëŠ” Pod ìˆ˜ì¤€ì—ì„œë§Œ ì‘ë™í•˜ì§€ë§Œ, GPU í™œìš©ë¥ , ìš”ì²­ í ê¸¸ì´, ë°°ì¹˜ í¬ê¸° ë“± ì¶”ë¡  íŠ¹í™” ë©”íŠ¸ë¦­ ê¸°ë°˜ì˜ ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•˜ë‹¤.

ë„·ì§¸, ì¥ì•  ê²©ë¦¬ì™€ ë³µêµ¬ë‹¤. íŠ¹ì • ëª¨ë¸ì˜ ì¥ì• ê°€ ì „ì²´ ì„œë¹„ìŠ¤ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šë„ë¡ ê²©ë¦¬í•˜ê³ , ìë™ìœ¼ë¡œ ë³µêµ¬í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì´ í•„ìš”í•˜ë‹¤.

### Ray Serve êµ¬ì„±ìš”ì†Œ

Ray ServeëŠ” Ray ë¶„ì‚° ì»´í“¨íŒ… í”„ë ˆì„ì›Œí¬ ìœ„ì— êµ¬ì¶•ëœ ëª¨ë¸ ì„œë¹™ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤.

**Deployment**: ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥í•œ ì„œë¹„ìŠ¤ ë‹¨ìœ„. ê° DeploymentëŠ” ë…ë¦½ì ì¸ ë³µì œë³¸(replica)ì„ ê°€ì§€ë©°, ìì²´ì ì¸ ìë™ ìŠ¤ì¼€ì¼ë§ ì •ì±…ì„ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.

**Replica**: Deploymentì˜ ì‹¤ì œ ì¸ìŠ¤í„´ìŠ¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ë¡œ, ìˆ˜í‰ í™•ì¥ì˜ ê¸°ë³¸ ë‹¨ìœ„ë‹¤.

**Ingress Deployment**: HTTP ìš”ì²­ì„ ë°›ì•„ ì ì ˆí•œ Deploymentë¡œ ë¼ìš°íŒ…í•˜ëŠ” ì§„ì…ì . ë³µì¡í•œ ìš”ì²­ íŒŒì´í”„ë¼ì¸ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•œë‹¤.

**ServeHandle**: Deployment ê°„ ë‚´ë¶€ í†µì‹ ì„ ìœ„í•œ ì¸í„°í˜ì´ìŠ¤. ë¹„ë™ê¸° í˜¸ì¶œê³¼ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•œë‹¤.

```python
from ray import serve
from ray.serve.handle import DeploymentHandle

@serve.deployment(num_replicas=2, ray_actor_options={"num_gpus": 1})
class VLLMDeployment:
    def __init__(self, model_id: str):
        from vllm import LLM
        self.llm = LLM(model=model_id)

    async def generate(self, prompt: str, **kwargs):
        outputs = self.llm.generate([prompt], **kwargs)
        return outputs[0].outputs[0].text

@serve.deployment
class Router:
    def __init__(self, model_7b: DeploymentHandle, model_70b: DeploymentHandle):
        self.model_7b = model_7b
        self.model_70b = model_70b

    async def __call__(self, request):
        data = await request.json()
        model_size = data.get("model_size", "7b")
        prompt = data["prompt"]

        if model_size == "70b":
            return await self.model_70b.generate.remote(prompt)
        return await self.model_7b.generate.remote(prompt)
```

## ì–¸ì œ Ray Serve + vLLM í†µí•©ì´ í•„ìš”í•œê°€

### ë‹¨ë… vLLMì´ ì í•©í•œ ê²½ìš°

ëª¨ë“  ìƒí™©ì—ì„œ Ray Serveê°€ í•„ìš”í•˜ì§€ëŠ” ì•Šë‹¤. ë‹¤ìŒ ì¡°ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•˜ë©´ ë‹¨ë… vLLM ë°°í¬ê°€ ë” íš¨ìœ¨ì ì´ë‹¤.

- ë‹¨ì¼ ëª¨ë¸ë§Œ ì„œë¹™
- ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìƒì„± APIë§Œ í•„ìš”
- Kubernetes HPAë¡œ ì¶©ë¶„í•œ ìŠ¤ì¼€ì¼ë§
- ë³µì¡í•œ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ì—†ìŒ
- ìš´ì˜ ë³µì¡ë„ ìµœì†Œí™” ìš°ì„ 

ì´ ê²½ìš° vLLMì˜ OpenAI í˜¸í™˜ ì„œë²„ë¥¼ ì§ì ‘ ë°°í¬í•˜ê³ , Kubernetes Service/Ingressì™€ HPAë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ê°„ë‹¨í•˜ë‹¤.

### Ray Serve í†µí•©ì´ í•„ìš”í•œ ê²½ìš°

ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ ì¤‘ í•˜ë‚˜ë¼ë„ í•´ë‹¹ë˜ë©´ Ray Serve í†µí•©ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.

**ë‹¤ì¤‘ ëª¨ë¸ ê²Œì´íŠ¸ì›¨ì´**: ì—¬ëŸ¬ í¬ê¸°/ì¢…ë¥˜ì˜ ëª¨ë¸ì„ ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸ë¡œ í†µí•© ê´€ë¦¬. ëª¨ë¸ë³„ ë²„ì „ ê´€ë¦¬, A/B í…ŒìŠ¤íŠ¸, ì¹´ë‚˜ë¦¬ ë°°í¬ê°€ í•„ìš”í•œ ê²½ìš°.

```python
@serve.deployment
class ModelGateway:
    def __init__(self):
        self.models = {
            "llama-7b": VLLMDeployment.bind("meta-llama/Llama-3.2-7B"),
            "llama-70b": VLLMDeployment.bind("meta-llama/Llama-3.3-70B-Instruct"),
            "qwen-32b": VLLMDeployment.bind("Qwen/Qwen3-32B"),
        }

    async def route(self, model_name: str, prompt: str):
        if model_name not in self.models:
            raise ValueError(f"Unknown model: {model_name}")
        return await self.models[model_name].generate.remote(prompt)
```

**RAG íŒŒì´í”„ë¼ì¸**: ì„ë² ë”© ìƒì„±, ë²¡í„° ê²€ìƒ‰, ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±, LLM ìƒì„±ì„ í•˜ë‚˜ì˜ ì„œë¹„ìŠ¤ë¡œ í†µí•©.

```python
@serve.deployment
class RAGPipeline:
    def __init__(self, embedder: DeploymentHandle, llm: DeploymentHandle):
        self.embedder = embedder
        self.llm = llm
        self.vector_db = MilvusClient()

    async def query(self, question: str):
        # 1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
        embedding = await self.embedder.embed.remote(question)

        # 2. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        docs = self.vector_db.search(embedding, top_k=5)

        # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° LLM ìƒì„±
        context = "\n".join([doc.text for doc in docs])
        prompt = f"Context:\n{context}\n\nQuestion: {question}\nAnswer:"

        return await self.llm.generate.remote(prompt)
```

**ì„¸ë°€í•œ ìë™ ìŠ¤ì¼€ì¼ë§**: GPU í™œìš©ë¥ , í ê¸¸ì´, ì§€ì—° ì‹œê°„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ ì •ì±….

```python
@serve.deployment(
    autoscaling_config={
        "min_replicas": 1,
        "max_replicas": 10,
        "target_ongoing_requests": 5,  # ë³µì œë³¸ë‹¹ ëª©í‘œ ë™ì‹œ ìš”ì²­
        "upscale_delay_s": 30,
        "downscale_delay_s": 300,
    },
    ray_actor_options={"num_gpus": 1}
)
class AutoScaledLLM:
    ...
```

**ë³µí•© ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°**: ì—¬ëŸ¬ LLM í˜¸ì¶œ, ë„êµ¬ ì‚¬ìš©, ì¡°ê±´ë¶€ ë¶„ê¸°ê°€ í¬í•¨ëœ ì—ì´ì „í‹± ì›Œí¬í”Œë¡œìš°.

## Kubernetesì—ì„œì˜ Ray Serve ë°°í¬

### KubeRay Operator

KubeRayëŠ” Kubernetesì—ì„œ Ray í´ëŸ¬ìŠ¤í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” ì˜¤í¼ë ˆì´í„°ë‹¤. RayService CRDë¥¼ í†µí•´ Ray Serve ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì„ ì–¸ì ìœ¼ë¡œ ë°°í¬í•œë‹¤.

```yaml
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: llm-serving
  namespace: ray-system
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 300
  serveConfigV2: |
    applications:
      - name: llm-app
        route_prefix: /
        import_path: serve_app:deployment
        deployments:
          - name: VLLMDeployment
            num_replicas: 2
            ray_actor_options:
              num_gpus: 1
            user_config:
              model_id: "meta-llama/Llama-3.2-7B-Instruct"
  rayClusterConfig:
    rayVersion: '2.44.0'
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          containers:
            - name: ray-head
              image: rayproject/ray-ml:2.44.0-py311-gpu
              resources:
                limits:
                  cpu: "4"
                  memory: "16Gi"
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 8000
                  name: serve
    workerGroupSpecs:
      - replicas: 2
        minReplicas: 1
        maxReplicas: 10
        groupName: gpu-worker
        rayStartParams: {}
        template:
          spec:
            nodeSelector:
              karpenter.sh/instance-family: g6e
            containers:
              - name: ray-worker
                image: rayproject/ray-ml:2.44.0-py311-gpu
                resources:
                  limits:
                    cpu: "8"
                    memory: "32Gi"
                    nvidia.com/gpu: "1"
            tolerations:
              - key: nvidia.com/gpu
                operator: Exists
                effect: NoSchedule
```

### vLLM Ray Serve í†µí•© ë°°í¬

[Scalable Model Inference and Agentic AI on Amazon EKS](https://github.com/aws-solutions-library-samples/guidance-for-scalable-model-inference-and-agentic-ai-on-amazon-eks) ì•„í‚¤í…ì²˜ì—ì„œëŠ” Ray Serveë¥¼ í†µí•œ ë¶„ì‚° ì¶”ë¡ ì„ êµ¬í˜„í•œë‹¤.

:::info vLLM v0.15.x í˜¸í™˜ì„±
vLLM v0.15.1ì€ Ray Serveì™€ì˜ í†µí•©ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤:
- **ìë™ í…ì„œ ë³‘ë ¬í™”**: Rayì˜ GPU í• ë‹¹ì— ë”°ë¼ ìë™ ì„¤ì •
- **Multi-LoRA on Ray**: Ray Serve ë ˆí”Œë¦¬ì¹´ë³„ LoRA ì–´ëŒ‘í„° ë™ì  ë¡œë”©
- **í–¥ìƒëœ ë©”íŠ¸ë¦­**: Prometheus í˜¸í™˜ ë©”íŠ¸ë¦­ ìë™ ë…¸ì¶œ
:::

```python
# serve_app.py
import os
from typing import Dict, List, Optional
from ray import serve
from vllm import LLM, SamplingParams

@serve.deployment(
    ray_actor_options={"num_gpus": 1},
    autoscaling_config={
        "min_replicas": 1,
        "max_replicas": 8,
        "target_ongoing_requests": 3,
    }
)
class VLLMDeployment:
    def __init__(self, model_id: str, **vllm_kwargs):
        self.model_id = model_id
        self.llm = LLM(
            model=model_id,
            trust_remote_code=True,
            **vllm_kwargs
        )

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.95,
    ) -> str:
        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        outputs = self.llm.generate([prompt], sampling_params)
        return outputs[0].outputs[0].text

    async def batch_generate(
        self,
        prompts: List[str],
        **kwargs
    ) -> List[str]:
        sampling_params = SamplingParams(**kwargs)
        outputs = self.llm.generate(prompts, sampling_params)
        return [o.outputs[0].text for o in outputs]

    def reconfigure(self, config: Dict):
        """ë™ì  ì„¤ì • ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ë©”ì„œë“œ"""
        pass

# ë°°í¬ êµ¬ì„±
deployment = VLLMDeployment.bind(
    model_id=os.environ.get("MODEL_ID", "meta-llama/Llama-3.2-7B-Instruct"),
    gpu_memory_utilization=0.9,
    max_model_len=4096,
)
```

### ë‹¤ì¤‘ GPU í…ì„œ ë³‘ë ¬ ë°°í¬

ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ê²½ìš° Rayì˜ placement groupì„ í™œìš©í•˜ì—¬ ë‹¤ì¤‘ GPU í…ì„œ ë³‘ë ¬í™”ë¥¼ êµ¬ì„±í•œë‹¤.

```python
from ray import serve
from ray.util.placement_group import placement_group
from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy

@serve.deployment(
    ray_actor_options={
        "num_gpus": 4,
        "scheduling_strategy": PlacementGroupSchedulingStrategy(
            placement_group=placement_group([{"GPU": 4}]),
            placement_group_capture_child_tasks=True,
        )
    }
)
class LargeModelDeployment:
    def __init__(self):
        from vllm import LLM
        self.llm = LLM(
            model="meta-llama/Llama-3.3-70B-Instruct",
            tensor_parallel_size=4,
            gpu_memory_utilization=0.9,
        )

    async def generate(self, prompt: str, **kwargs):
        outputs = self.llm.generate([prompt], **kwargs)
        return outputs[0].outputs[0].text
```

## ìë™ ìŠ¤ì¼€ì¼ë§ ì „ëµ

### Ray Serve ì˜¤í† ìŠ¤ì¼€ì¼ë§ ê°œì„ 

ìµœì‹  Ray ë²„ì „ì—ì„œì˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì£¼ìš” ê°œì„ :

- **GPU-aware ìŠ¤ì¼€ì¼ë§**: GPU í™œìš©ë¥  ê¸°ë°˜ ë ˆí”Œë¦¬ì¹´ ìˆ˜ ì¡°ì •
- **ì˜ˆì¸¡ì  ìŠ¤ì¼€ì¼ë§**: ìš”ì²­ íŒ¨í„´ í•™ìŠµ ê¸°ë°˜ ì„ ì œì  ìŠ¤ì¼€ì¼ì•„ì›ƒ
- **Karpenter í†µí•©**: Ray ë…¸ë“œ ìš”ì²­ ì‹œ Karpenterê°€ ìµœì ì˜ GPU ì¸ìŠ¤í„´ìŠ¤ ìë™ í”„ë¡œë¹„ì €ë‹

### Ray Serve ìë™ ìŠ¤ì¼€ì¼ë§

Ray Serveì˜ ìë™ ìŠ¤ì¼€ì¼ë§ì€ ìš”ì²­ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•œë‹¤. Kubernetes HPAì™€ ë‹¬ë¦¬ ì¶”ë¡  ì›Œí¬ë¡œë“œì— íŠ¹í™”ëœ ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•œë‹¤.

```python
autoscaling_config = {
    # ë³µì œë³¸ ìˆ˜ ë²”ìœ„
    "min_replicas": 1,
    "max_replicas": 20,

    # ìŠ¤ì¼€ì¼ë§ íŠ¸ë¦¬ê±°
    "target_ongoing_requests": 5,  # ë³µì œë³¸ë‹¹ ëª©í‘œ ë™ì‹œ ìš”ì²­

    # ìŠ¤ì¼€ì¼ë§ ì§€ì—° (ì•ˆì •ì„±)
    "upscale_delay_s": 30,    # í™•ì¥ ì „ ëŒ€ê¸°
    "downscale_delay_s": 300,  # ì¶•ì†Œ ì „ ëŒ€ê¸°

    # ìŠ¤ì¼€ì¼ë§ ì†ë„
    "upscaling_factor": 1.0,   # í•œ ë²ˆì— 100%ê¹Œì§€ í™•ì¥
    "downscaling_factor": 0.5, # í•œ ë²ˆì— 50%ê¹Œì§€ ì¶•ì†Œ
}
```

### Karpenterì™€ì˜ í†µí•©

Ray ì›Œì»¤ ë…¸ë“œëŠ” Karpenterë¡œ ë™ì  í”„ë¡œë¹„ì €ë‹í•œë‹¤. Ray í´ëŸ¬ìŠ¤í„° í™•ì¥ ì‹œ í•„ìš”í•œ GPU ë…¸ë“œë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•œë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: ray-gpu-workers
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/instance-family
          operator: In
          values: ["g6e", "g5", "p4d"]
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 100
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m
```

### ê³„ì¸µì  ìŠ¤ì¼€ì¼ë§ ì•„í‚¤í…ì²˜

1. **Ray Serve ìˆ˜ì¤€**: ë³µì œë³¸ ìë™ ìŠ¤ì¼€ì¼ë§ (ìš”ì²­ ê¸°ë°˜)
2. **Ray í´ëŸ¬ìŠ¤í„° ìˆ˜ì¤€**: ì›Œì»¤ ë…¸ë“œ ìë™ ìŠ¤ì¼€ì¼ë§ (ë¦¬ì†ŒìŠ¤ ê¸°ë°˜)
3. **Kubernetes ìˆ˜ì¤€**: Karpenter ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ (Pod ìŠ¤ì¼€ì¤„ë§ ê¸°ë°˜)

```
ìš”ì²­ ì¦ê°€ â†’ Ray Serve ë³µì œë³¸ í™•ì¥ â†’ Ray ì›Œì»¤ ë¶€ì¡± â†’
Ray Autoscalerê°€ ì›Œì»¤ ìš”ì²­ â†’ Pending Pod ë°œìƒ â†’ Karpenterê°€ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
```

## ì„±ëŠ¥ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ

### ì•„í‚¤í…ì²˜ë³„ íŠ¹ì„± ë¹„êµ

| íŠ¹ì„± | ë‹¨ë… vLLM | Ray Serve + vLLM |
|-----|---------|-----------------|
| ìš´ì˜ ë³µì¡ë„ | ë‚®ìŒ | ë†’ìŒ |
| ì´ˆê¸° ì„¤ì • ë¹„ìš© | ìµœì†Œ | ìƒë‹¹í•¨ |
| ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› | ëª¨ë¸ë‹¹ ë³„ë„ ë°°í¬ | í†µí•© ê´€ë¦¬ |
| ìŠ¤ì¼€ì¼ë§ ì„¸ë¶„í™” | Pod ìˆ˜ì¤€ | ë³µì œë³¸ ìˆ˜ì¤€ |
| ë³µí•© íŒŒì´í”„ë¼ì¸ | ì™¸ë¶€ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í•„ìš” | ë„¤ì´í‹°ë¸Œ ì§€ì› |
| ì¥ì•  ê²©ë¦¬ | Pod ë‹¨ìœ„ | Deployment ë‹¨ìœ„ |
| ë¦¬ì†ŒìŠ¤ ì˜¤ë²„í—¤ë“œ | ìµœì†Œ | Ray í´ëŸ¬ìŠ¤í„° ì˜¤ë²„í—¤ë“œ |
| ì§€ì—° ì‹œê°„ | ìµœì†Œ | ì•½ê°„ì˜ ì˜¤ë²„í—¤ë“œ |

### ì˜ì‚¬ê²°ì • í”Œë¡œìš°ì°¨íŠ¸

```
ì‹œì‘
  â”‚
  â”œâ”€ ë‹¨ì¼ ëª¨ë¸ë§Œ ì„œë¹™? â”€â”€Yesâ”€â”€â†’ ë³µì¡í•œ íŒŒì´í”„ë¼ì¸? â”€â”€Noâ”€â”€â†’ ë‹¨ë… vLLM
  â”‚       â”‚                            â”‚
  â”‚      No                           Yes
  â”‚       â”‚                            â”‚
  â”‚       â–¼                            â–¼
  â”‚   Ray Serve + vLLM          Ray Serve + vLLM
  â”‚
  â”œâ”€ ì„¸ë°€í•œ ìë™ ìŠ¤ì¼€ì¼ë§ í•„ìš”? â”€â”€Yesâ”€â”€â†’ Ray Serve + vLLM
  â”‚       â”‚
  â”‚      No
  â”‚       â”‚
  â”œâ”€ ìš´ì˜ ë³µì¡ë„ ìµœì†Œí™” ìš°ì„ ? â”€â”€Yesâ”€â”€â†’ ë‹¨ë… vLLM
  â”‚       â”‚
  â”‚      No
  â”‚       â”‚
  â””â”€â”€â”€â”€â”€â”€â†’ Ray Serve + vLLM
```

### ì›Œí¬ë¡œë“œë³„ ê¶Œì¥ ì•„í‚¤í…ì²˜

| ì›Œí¬ë¡œë“œ | ê¶Œì¥ ì•„í‚¤í…ì²˜ | ì´ìœ  |
|---------|-------------|-----|
| ì±—ë´‡ API | ë‹¨ë… vLLM | ë‹¨ìˆœ ìš”ì²­-ì‘ë‹µ, ë‚®ì€ ë³µì¡ë„ |
| ë‹¤ì¤‘ ëª¨ë¸ ê²Œì´íŠ¸ì›¨ì´ | Ray Serve + vLLM | í†µí•© ë¼ìš°íŒ…, ë²„ì „ ê´€ë¦¬ |
| RAG ì‹œìŠ¤í…œ | Ray Serve + vLLM | ì„ë² ë”©-ê²€ìƒ‰-ìƒì„± íŒŒì´í”„ë¼ì¸ |
| ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° | Ray Serve + vLLM | ë³µí•© ì¶”ë¡ , ë„êµ¬ í˜¸ì¶œ |
| ë°°ì¹˜ ì²˜ë¦¬ | Ray Serve + vLLM | ëŒ€ê·œëª¨ ë³‘ë ¬ ì²˜ë¦¬ |
| ì‹¤ì‹œê°„ ì €ì§€ì—° | ë‹¨ë… vLLM | ìµœì†Œ ì˜¤ë²„í—¤ë“œ |

## ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±

### Ray Dashboard

RayëŠ” í´ëŸ¬ìŠ¤í„° ìƒíƒœ, ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰, Serve ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ì„ ë³´ì—¬ì£¼ëŠ” ëŒ€ì‹œë³´ë“œë¥¼ ì œê³µí•œë‹¤.

```yaml
# Ray Dashboard Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ray-dashboard
  namespace: ray-system
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
spec:
  rules:
    - host: ray-dashboard.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: llm-serving-head-svc
                port:
                  number: 8265
```

### Prometheus ë©”íŠ¸ë¦­ í†µí•©

Ray ServeëŠ” Prometheus í˜•ì‹ì˜ ë©”íŠ¸ë¦­ì„ ë…¸ì¶œí•œë‹¤.

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ray-serve-monitor
  namespace: ray-system
spec:
  selector:
    matchLabels:
      ray.io/serve: "true"
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics
```

ì£¼ìš” ë©”íŠ¸ë¦­:
- `ray_serve_deployment_replica_starts`: ë³µì œë³¸ ì‹œì‘ íšŸìˆ˜
- `ray_serve_deployment_queued_queries`: ëŒ€ê¸° ì¤‘ì¸ ì¿¼ë¦¬ ìˆ˜
- `ray_serve_deployment_request_counter`: ì´ ìš”ì²­ ìˆ˜
- `ray_serve_deployment_error_counter`: ì—ëŸ¬ ìˆ˜
- `ray_serve_deployment_processing_latency_ms`: ì²˜ë¦¬ ì§€ì—° ì‹œê°„

### Grafana ëŒ€ì‹œë³´ë“œ ì˜ˆì‹œ

```json
{
  "panels": [
    {
      "title": "Request Rate by Deployment",
      "expr": "sum(rate(ray_serve_deployment_request_counter[5m])) by (deployment)"
    },
    {
      "title": "P99 Latency",
      "expr": "histogram_quantile(0.99, rate(ray_serve_deployment_processing_latency_ms_bucket[5m]))"
    },
    {
      "title": "Queue Depth",
      "expr": "sum(ray_serve_deployment_queued_queries) by (deployment)"
    },
    {
      "title": "GPU Utilization",
      "expr": "avg(DCGM_FI_DEV_GPU_UTIL) by (pod)"
    }
  ]
}
```

## í”„ë¡œë•ì…˜ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Ray Serve + vLLM í†µí•© ë°°í¬ ì‹œ í™•ì¸ì‚¬í•­

1. **í´ëŸ¬ìŠ¤í„° êµ¬ì„±**
   - KubeRay Operator ì„¤ì¹˜ ë° CRD ë“±ë¡
   - Ray ë²„ì „ê³¼ vLLM ë²„ì „ í˜¸í™˜ì„± í™•ì¸
   - GPU ë“œë¼ì´ë²„ ë° CUDA ë²„ì „ ê²€ì¦

2. **ë¦¬ì†ŒìŠ¤ ê³„íš**
   - Head ë…¸ë“œ: CPU ì „ìš©, í´ëŸ¬ìŠ¤í„° ì¡°ìœ¨
   - Worker ë…¸ë“œ: GPU íƒ‘ì¬, ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰
   - ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ì‚°ì • (Ray ì˜¤ë²„í—¤ë“œ í¬í•¨)

3. **ë„¤íŠ¸ì›Œí‚¹**
   - Ray GCS í¬íŠ¸(6379) ë‚´ë¶€ í†µì‹  í—ˆìš©
   - Serve í¬íŠ¸(8000) ì™¸ë¶€ ë…¸ì¶œ
   - Dashboard í¬íŠ¸(8265) ì ‘ê·¼ ì œì–´

4. **ìë™ ìŠ¤ì¼€ì¼ë§**
   - Ray Serve autoscaling_config íŠœë‹
   - Karpenter NodePool GPU ì œí•œ ì„¤ì •
   - ìŠ¤ì¼€ì¼ë§ ì§€ì—° ì‹œê°„ ìµœì í™”

5. **ëª¨ë‹ˆí„°ë§**
   - Ray Dashboard ì ‘ê·¼ êµ¬ì„±
   - Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
   - ì•Œë¦¼ ê·œì¹™ ì„¤ì •

6. **ì¥ì•  ë³µêµ¬**
   - Head ë…¸ë“œ ì¥ì•  ì‹œ í´ëŸ¬ìŠ¤í„° ì¬ì‹œì‘ ì „ëµ
   - ì›Œì»¤ ë…¸ë“œ ì„ ì  ëŒ€ì‘
   - ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë³µêµ¬

## ì°¸ê³  ìë£Œ

- [Ray Serve ê³µì‹ ë¬¸ì„œ](https://docs.ray.io/en/latest/serve/index.html)
- [KubeRay GitHub](https://github.com/ray-project/kuberay)
- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/en/latest/)
- [Scalable Model Inference on Amazon EKS](https://github.com/aws-solutions-library-samples/guidance-for-scalable-model-inference-and-agentic-ai-on-amazon-eks)
- [GenAI on EKS Starter Kit](https://github.com/aws-samples/sample-genai-on-eks-starter-kit)
