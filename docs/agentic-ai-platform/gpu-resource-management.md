---
title: "GPU í´ëŸ¬ìŠ¤í„° ë™ì  ë¦¬ì†ŒìŠ¤ ê´€ë¦¬"
sidebar_label: "4. GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬"
description: "ë³µìˆ˜ GPU í´ëŸ¬ìŠ¤í„° í™˜ê²½ì—ì„œì˜ ë™ì  ë¦¬ì†ŒìŠ¤ í• ë‹¹ ë° Karpenter ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§"
tags: [eks, gpu, karpenter, autoscaling, resource-management, dcgm]
category: "genai-aiml"
last_update:
  date: 2026-02-13
  author: devfloor9
sidebar_position: 4
---

import { SpecificationTable, ComparisonTable } from '@site/src/components/tables';
import { DraLimitationsTable, ScalingDecisionTable } from '@site/src/components/GpuResourceTables';

# GPU í´ëŸ¬ìŠ¤í„° ë™ì  ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

> ğŸ“… **ì‘ì„±ì¼**: 2025-02-09 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 9ë¶„


## ê°œìš”

ëŒ€ê·œëª¨ GenAI ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œëŠ” ë³µìˆ˜ì˜ GPU í´ëŸ¬ìŠ¤í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³ , íŠ¸ë˜í”½ ë³€í™”ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ë¥¼ ì¬í• ë‹¹í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” Amazon EKS í™˜ê²½ì—ì„œ Karpenterë¥¼ í™œìš©í•œ GPU ë…¸ë“œ ìë™ ìŠ¤ì¼€ì¼ë§ê³¼ DCGM(Data Center GPU Manager) ê¸°ë°˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ê·¸ë¦¬ê³  KEDAë¥¼ í†µí•œ ì›Œí¬ë¡œë“œ ìë™ ìŠ¤ì¼€ì¼ë§ ì „ëµì„ ë‹¤ë£¹ë‹ˆë‹¤.

### ì£¼ìš” ëª©í‘œ

- **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±**: GPU ë¦¬ì†ŒìŠ¤ì˜ ìœ íœ´ ì‹œê°„ ìµœì†Œí™”
- **ë¹„ìš© ìµœì í™”**: Spot ì¸ìŠ¤í„´ìŠ¤ í™œìš© ë° Consolidationì„ í†µí•œ ë¹„ìš© ì ˆê°
- **ìë™í™”ëœ ìŠ¤ì¼€ì¼ë§**: íŠ¸ë˜í”½ íŒ¨í„´ì— ë”°ë¥¸ ìë™ ë¦¬ì†ŒìŠ¤ ì¡°ì •
- **ì„œë¹„ìŠ¤ ì•ˆì •ì„±**: SLA ì¤€ìˆ˜ë¥¼ ìœ„í•œ ì ì ˆí•œ ë¦¬ì†ŒìŠ¤ í™•ë³´

---

## Kubernetes 1.33/1.34 GPU ê´€ë¦¬ ê°œì„ ì‚¬í•­

Kubernetes 1.33ê³¼ 1.34 ë²„ì „ì—ì„œëŠ” GPU ì›Œí¬ë¡œë“œ ê´€ë¦¬ë¥¼ ìœ„í•œ ì—¬ëŸ¬ ì¤‘ìš”í•œ ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.

### Kubernetes 1.33+ ì£¼ìš” ê¸°ëŠ¥

<SpecificationTable
  headers={['ê¸°ëŠ¥', 'ì„¤ëª…', 'GPU ì›Œí¬ë¡œë“œ ì˜í–¥']}
  rows={[
    { id: '1', cells: ['Stable Sidecar Containers', 'Init ì»¨í…Œì´ë„ˆê°€ Pod ì „ì²´ ë¼ì´í”„ì‚¬ì´í´ ë™ì•ˆ ì‹¤í–‰ ê°€ëŠ¥', 'GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ë¡œê¹… ì‚¬ì´ë“œì¹´ ì•ˆì •í™”'] },
    { id: '2', cells: ['Topology-Aware Routing', 'ë…¸ë“œ í† í´ë¡œì§€ ê¸°ë°˜ íŠ¸ë˜í”½ ë¼ìš°íŒ…', 'GPU ë…¸ë“œ ê°„ ìµœì  ê²½ë¡œ ì„ íƒ, ì§€ì—° ì‹œê°„ ê°ì†Œ'] },
    { id: '3', cells: ['In-Place Resource Resizing', 'Pod ì¬ì‹œì‘ ì—†ì´ ë¦¬ì†ŒìŠ¤ ì¡°ì •', 'GPU ë©”ëª¨ë¦¬ ë™ì  ì¡°ì • (ì œí•œì )'] },
    { id: '4', cells: ['DRA v1beta1 ì•ˆì •í™”', 'Dynamic Resource Allocation API ì•ˆì •í™”', 'í”„ë¡œë•ì…˜ GPU íŒŒí‹°ì…”ë‹ ì§€ì›'] }
  ]}
/>

### Kubernetes 1.34+ ì£¼ìš” ê¸°ëŠ¥

<SpecificationTable
  headers={['ê¸°ëŠ¥', 'ì„¤ëª…', 'GPU ì›Œí¬ë¡œë“œ ì˜í–¥']}
  rows={[
    { id: '1', cells: ['Projected Service Account Tokens', 'í–¥ìƒëœ ì„œë¹„ìŠ¤ ê³„ì • í† í° ê´€ë¦¬', 'GPU Podì˜ ë³´ì•ˆ ê°•í™”'] },
    { id: '2', cells: ['DRA Prioritized Alternatives', 'ë¦¬ì†ŒìŠ¤ í• ë‹¹ ìš°ì„ ìˆœìœ„ ëŒ€ì•ˆ', 'GPU ë¦¬ì†ŒìŠ¤ ê²½í•© ì‹œ ì§€ëŠ¥ì  ìŠ¤ì¼€ì¤„ë§'] },
    { id: '3', cells: ['Improved Resource Quota', 'ë¦¬ì†ŒìŠ¤ ì¿¼í„° ì„¸ë¶„í™”', 'GPU í…Œë„ŒíŠ¸ë³„ ì •ë°€í•œ í• ë‹¹ ì œì–´'] }
  ]}
/>

:::info kubectl ë²„ì „ ìš”êµ¬ì‚¬í•­
Kubernetes 1.33+ í´ëŸ¬ìŠ¤í„°ë¥¼ ê´€ë¦¬í•˜ë ¤ë©´ kubectl 1.33 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ API ê¸°ëŠ¥ì„ í™œìš©í•˜ë ¤ë©´ ìµœì‹  kubectl ë²„ì „ì„ ì‚¬ìš©í•˜ì„¸ìš”.

```bash
# kubectl ë²„ì „ í™•ì¸
kubectl version --client

# kubectl 1.33+ ì„¤ì¹˜ (Linux)
curl -LO "https://dl.k8s.io/release/v1.33.0/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```
:::

### Sidecar Containersë¥¼ í™œìš©í•œ GPU ëª¨ë‹ˆí„°ë§

Kubernetes 1.33+ì˜ ì•ˆì •í™”ëœ Sidecar Containersë¥¼ ì‚¬ìš©í•˜ì—¬ GPU ë©”íŠ¸ë¦­ì„ ì§€ì†ì ìœ¼ë¡œ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: vllm-with-monitoring
  namespace: ai-inference
spec:
  initContainers:
    # Sidecarë¡œ ì‹¤í–‰ë˜ëŠ” DCGM Exporter
    - name: dcgm-sidecar
      image: nvcr.io/nvidia/k8s/dcgm-exporter:4.2.2-4.1.3-ubuntu22.04
      restartPolicy: Always  # K8s 1.33+ Sidecar ê¸°ëŠ¥
      ports:
        - name: metrics
          containerPort: 9400
      securityContext:
        capabilities:
          add: ["SYS_ADMIN"]
  containers:
    - name: vllm
      image: vllm/vllm-openai:latest
      resources:
        requests:
          nvidia.com/gpu: 2
        limits:
          nvidia.com/gpu: 2
```

### Topology-Aware Routing í™œìš©

GPU ë…¸ë“œ ê°„ ìµœì  ê²½ë¡œë¥¼ ì„ íƒí•˜ì—¬ ì§€ì—° ì‹œê°„ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: vllm-inference
  namespace: ai-inference
  annotations:
    # K8s 1.33+ Topology-Aware Routing
    service.kubernetes.io/topology-mode: "Auto"
spec:
  selector:
    app: vllm
  ports:
    - port: 8000
      targetPort: 8000
  # í† í´ë¡œì§€ ì¸ì‹ ë¼ìš°íŒ… í™œì„±í™”
  trafficDistribution: PreferClose
```

---

## ë©€í‹° GPU í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì²˜

### ì „ì²´ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
flowchart TB
    subgraph "Traffic Layer"
        ALB[Application Load Balancer]
        IGW[Ingress Gateway]
    end

    subgraph "EKS Control Plane"
        API[Kubernetes API Server]
        KARP[Karpenter Controller]
        KEDA_OP[KEDA Operator]
    end

    subgraph "GPU Node Pool A - Model Serving"
        NPA1[p4d.24xlarge Node 1]
        NPA2[p4d.24xlarge Node 2]
        NPA3[p5.48xlarge Node 3]
        
        subgraph "Model A Pods"
            MA1[vLLM Pod A-1]
            MA2[vLLM Pod A-2]
            MA3[vLLM Pod A-3]
        end
    end

    subgraph "GPU Node Pool B - Model Serving"
        NPB1[g5.48xlarge Node 1]
        NPB2[g5.48xlarge Node 2]
        
        subgraph "Model B Pods"
            MB1[vLLM Pod B-1]
            MB2[vLLM Pod B-2]
        end
    end

    subgraph "Monitoring Stack"
        DCGM[DCGM Exporter]
        PROM[Prometheus]
        GRAF[Grafana]
    end

    ALB --> IGW
    IGW --> MA1 & MA2 & MA3
    IGW --> MB1 & MB2
    
    API --> KARP
    API --> KEDA_OP
    
    KARP --> NPA1 & NPA2 & NPA3
    KARP --> NPB1 & NPB2
    
    DCGM --> PROM
    PROM --> KEDA_OP
    PROM --> GRAF
```

### ë¦¬ì†ŒìŠ¤ ê³µìœ  ì•„í‚¤í…ì²˜

ë³µìˆ˜ ëª¨ë¸ ê°„ GPU ë¦¬ì†ŒìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³µìœ í•˜ê¸° ìœ„í•œ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph "Shared GPU Pool"
        direction TB
        GPU1[GPU 1-4<br/>Model A Primary]
        GPU2[GPU 5-8<br/>Model B Primary]
        GPU3[GPU 9-12<br/>Elastic Pool]
    end

    subgraph "Resource Allocator"
        RA[Dynamic Resource<br/>Allocator]
        METRICS[GPU Metrics<br/>Collector]
    end

    subgraph "Workloads"
        WA[Model A<br/>Workload]
        WB[Model B<br/>Workload]
    end

    METRICS --> RA
    RA --> GPU1 & GPU2 & GPU3
    WA --> GPU1
    WB --> GPU2
    WA -.->|Traffic Surge| GPU3
    WB -.->|Traffic Surge| GPU3
```

:::info ë¦¬ì†ŒìŠ¤ ê³µìœ  ì›ì¹™

- **Primary Pool**: ê° ëª¨ë¸ì— í• ë‹¹ëœ ê¸°ë³¸ GPU ë¦¬ì†ŒìŠ¤
- **Elastic Pool**: íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œ ë™ì ìœ¼ë¡œ í• ë‹¹ë˜ëŠ” ê³µìœ  ë¦¬ì†ŒìŠ¤
- **Priority-based Allocation**: ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ í• ë‹¹ìœ¼ë¡œ ì¤‘ìš” ì›Œí¬ë¡œë“œ ë³´í˜¸

:::

---

## ë™ì  ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì „ëµ

### íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œë‚˜ë¦¬ì˜¤

ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œë‚˜ë¦¬ì˜¤ì™€ ëŒ€ì‘ ì „ëµì…ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì íŠ¸ë˜í”½
    participant LB as Load Balancer
    participant ModelA as Model A Service
    participant ModelB as Model B Service
    participant KEDA as KEDA Controller
    participant Karpenter as Karpenter
    participant AWS as AWS EC2

    Note over User,AWS: ì •ìƒ ìƒíƒœ: Model A 40%, Model B 30% GPU ì‚¬ìš©ë¥ 
    
    User->>LB: íŠ¸ë˜í”½ ê¸‰ì¦ (Model A)
    LB->>ModelA: ìš”ì²­ ì „ë‹¬
    ModelA->>KEDA: GPU ì‚¬ìš©ë¥  85% ê°ì§€
    
    KEDA->>ModelA: HPA íŠ¸ë¦¬ê±° - Pod ìŠ¤ì¼€ì¼ ì•„ì›ƒ
    KEDA->>Karpenter: ì¶”ê°€ ë…¸ë“œ ìš”ì²­
    
    Karpenter->>AWS: p4d.24xlarge í”„ë¡œë¹„ì €ë‹
    AWS-->>Karpenter: ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ
    
    Note over ModelA,ModelB: Model B ë¦¬ì†ŒìŠ¤ ì¼ë¶€ë¥¼ Model Aë¡œ ì¬í• ë‹¹
    
    Karpenter->>ModelA: ìƒˆ ë…¸ë“œì— Pod ìŠ¤ì¼€ì¤„ë§
    ModelA-->>User: ì‘ë‹µ ì§€ì—° ì‹œê°„ ì •ìƒí™”
```

### ëª¨ë¸ ê°„ ë¦¬ì†ŒìŠ¤ ì¬í• ë‹¹ ì ˆì°¨

Model Aì— íŠ¸ë˜í”½ì´ ê¸‰ì¦í•  ë•Œ Model Bì˜ ìœ íœ´ ë¦¬ì†ŒìŠ¤ë¥¼ Model Aì— í• ë‹¹í•˜ëŠ” êµ¬ì²´ì ì¸ ì ˆì°¨ì…ë‹ˆë‹¤.

#### ë‹¨ê³„ 1: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„

```yaml
# DCGM Exporterê°€ ìˆ˜ì§‘í•˜ëŠ” ì£¼ìš” ë©”íŠ¸ë¦­
# - DCGM_FI_DEV_GPU_UTIL: GPU ì‚¬ìš©ë¥ 
# - DCGM_FI_DEV_MEM_COPY_UTIL: ë©”ëª¨ë¦¬ ë³µì‚¬ ì‚¬ìš©ë¥ 
# - DCGM_FI_DEV_FB_USED: í”„ë ˆì„ë²„í¼ ì‚¬ìš©ëŸ‰
```

#### ë‹¨ê³„ 2: ìŠ¤ì¼€ì¼ë§ ê²°ì •

<ScalingDecisionTable />

#### ë‹¨ê³„ 3: ë¦¬ì†ŒìŠ¤ ì¬í• ë‹¹ ì‹¤í–‰

```bash
# Model Bì˜ replica ìˆ˜ ê°ì†Œ (ìœ íœ´ ë¦¬ì†ŒìŠ¤ í™•ë³´)
kubectl scale deployment model-b-serving --replicas=1 -n inference

# Model Aì˜ replica ìˆ˜ ì¦ê°€
kubectl scale deployment model-a-serving --replicas=5 -n inference

# ë˜ëŠ” KEDAê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
```

#### ë‹¨ê³„ 4: ë…¸ë“œ ë ˆë²¨ ìŠ¤ì¼€ì¼ë§

Karpenterê°€ ìë™ìœ¼ë¡œ ì¶”ê°€ ë…¸ë“œë¥¼ í”„ë¡œë¹„ì €ë‹í•˜ê±°ë‚˜ ìœ íœ´ ë…¸ë“œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

:::warning ì£¼ì˜ì‚¬í•­

ë¦¬ì†ŒìŠ¤ ì¬í• ë‹¹ ì‹œ Model Bì˜ ìµœì†Œ SLAë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•´ `minReplicas`ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì™„ì „í•œ ë¦¬ì†ŒìŠ¤ íšŒìˆ˜ëŠ” ì„œë¹„ìŠ¤ ì¤‘ë‹¨ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

:::

---

## Karpenter ê¸°ë°˜ ë…¸ë“œ ìŠ¤ì¼€ì¼ë§

:::info Karpenter v1.0+ GA ìƒíƒœ
KarpenterëŠ” v1.0ë¶€í„° GA(Generally Available) ìƒíƒœë¡œ, í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ë¬¸ì„œì˜ ëª¨ë“  ì˜ˆì œëŠ” Karpenter v1 API (`karpenter.sh/v1`)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
:::

### NodePool ì„¤ì •

GPU ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ Karpenter NodePool ì„¤ì • ì˜ˆì œì…ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-inference
        workload: genai
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge    # 8x A100 40GB
            - p5.48xlarge     # 8x H100 80GB
            - g5.48xlarge     # 8x A10G 24GB
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
  limits:
    cpu: 1000
    memory: 4000Gi
    nvidia.com/gpu: 64
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
  weight: 100
```

### EC2NodeClass ì„¤ì •

GPU ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìœ„í•œ EC2NodeClass ì„¤ì •ì…ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu-nodeclass
spec:
  role: KarpenterNodeRole-${CLUSTER_NAME}
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 500Gi
        volumeType: gp3
        iops: 10000
        throughput: 500
        encrypted: true
        deleteOnTermination: true
  instanceStorePolicy: RAID0
  userData: |
    #!/bin/bash
    # NVIDIA ë“œë¼ì´ë²„ ë° Container Toolkit ì„¤ì •
    nvidia-smi
    
    # GPU ë©”ëª¨ë¦¬ ëª¨ë“œ ì„¤ì • (Persistence Mode)
    nvidia-smi -pm 1
    
    # EFA ë“œë¼ì´ë²„ ë¡œë“œ (p4d, p5 ì¸ìŠ¤í„´ìŠ¤ìš©)
    modprobe efa
  tags:
    Environment: production
    Workload: genai-inference
```

### GPU ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë¹„êµ

<ComparisonTable
  headers={['ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…', 'GPU', 'GPU ë©”ëª¨ë¦¬', 'vCPU', 'ë©”ëª¨ë¦¬', 'ë„¤íŠ¸ì›Œí¬', 'ìš©ë„']}
  rows={[
    { id: '1', cells: ['p4d.24xlarge', '8x A100', '40GB x 8', '96', '1152 GiB', '400 Gbps EFA', 'ëŒ€ê·œëª¨ LLM ì¶”ë¡ '], recommended: true },
    { id: '2', cells: ['p5.48xlarge', '8x H100', '80GB x 8', '192', '2048 GiB', '3200 Gbps EFA', 'ì´ˆëŒ€ê·œëª¨ ëª¨ë¸, í•™ìŠµ'] },
    { id: '3', cells: ['p5e.48xlarge', '8x H200', '141GB x 8', '192', '2048 GiB', '3200 Gbps EFA', 'ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ/ì¶”ë¡ '] },
    { id: '4', cells: ['g5.48xlarge', '8x A10G', '24GB x 8', '192', '768 GiB', '100 Gbps', 'ì¤‘ì†Œê·œëª¨ ëª¨ë¸ ì¶”ë¡ '] },
    { id: '5', cells: ['g6e.xlarge ~ g6e.48xlarge', 'NVIDIA L40S', 'ìµœëŒ€ 8Ã—48GB', 'ìµœëŒ€ 192', 'ìµœëŒ€ 768 GiB', 'ìµœëŒ€ 100 Gbps', 'ë¹„ìš© íš¨ìœ¨ì  ì¶”ë¡ '] },
    { id: '6', cells: ['trn2.48xlarge', '16x Trainium2', '-', '192', '2048 GiB', '1600 Gbps', 'AWS ë„¤ì´í‹°ë¸Œ í•™ìŠµ'] }
  ]}
/>

:::tip ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ ê°€ì´ë“œ

- **p5e.48xlarge**: 100B+ íŒŒë¼ë¯¸í„° ëª¨ë¸, H200ì˜ ìµœëŒ€ ë©”ëª¨ë¦¬ í™œìš©
- **p5.48xlarge**: 70B+ íŒŒë¼ë¯¸í„° ëª¨ë¸, ìµœê³  ì„±ëŠ¥ ìš”êµ¬ ì‹œ
- **p4d.24xlarge**: 13B-70B íŒŒë¼ë¯¸í„° ëª¨ë¸, ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ ê· í˜•
- **g6e.xlarge~48xlarge**: 13B-70B ëª¨ë¸, L40Sì˜ ë¹„ìš© íš¨ìœ¨ì  ì¶”ë¡ 
- **g5.48xlarge**: 7B ì´í•˜ ëª¨ë¸, ë¹„ìš© íš¨ìœ¨ì ì¸ ì¶”ë¡ 
- **trn2.48xlarge**: AWS ë„¤ì´í‹°ë¸Œ í•™ìŠµ ì›Œí¬ë¡œë“œ, Trainium2 ìµœì í™”

:::

:::tip EKS Auto Mode GPU ìŠ¤ì¼€ì¤„ë§
EKS Auto ModeëŠ” GPU ì›Œí¬ë¡œë“œë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì ì ˆí•œ GPU ì¸ìŠ¤í„´ìŠ¤ë¥¼ í”„ë¡œë¹„ì €ë‹í•©ë‹ˆë‹¤. NodePool ì„¤ì • ì—†ì´ë„ GPU Podì˜ ë¦¬ì†ŒìŠ¤ ìš”ì²­ì— ë”°ë¼ ìµœì ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
:::

---

## GPU ë©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§

### DCGM Exporter ì„¤ì •

NVIDIA DCGM Exporterë¥¼ í†µí•´ GPU ë©”íŠ¸ë¦­ì„ Prometheusë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: dcgm-exporter
  namespace: gpu-monitoring
  labels:
    app: dcgm-exporter
spec:
  selector:
    matchLabels:
      app: dcgm-exporter
  template:
    metadata:
      labels:
        app: dcgm-exporter
    spec:
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: dcgm-exporter
          image: nvcr.io/nvidia/k8s/dcgm-exporter:3.3.8-3.6.0-ubuntu22.04
          ports:
            - name: metrics
              containerPort: 9400
          env:
            - name: DCGM_EXPORTER_LISTEN
              value: ":9400"
            - name: DCGM_EXPORTER_KUBERNETES
              value: "true"
            - name: DCGM_EXPORTER_COLLECTORS
              value: "/etc/dcgm-exporter/dcp-metrics-included.csv"
          volumeMounts:
            - name: pod-resources
              mountPath: /var/lib/kubelet/pod-resources
              readOnly: true
          securityContext:
            runAsNonRoot: false
            runAsUser: 0
            capabilities:
              add: ["SYS_ADMIN"]
      volumes:
        - name: pod-resources
          hostPath:
            path: /var/lib/kubelet/pod-resources
```

:::info DCGM Exporter 3.3+ ê¸°ëŠ¥
DCGM Exporter 3.3+ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í–¥ìƒëœ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
- **H100/H200 ì§€ì›**: ìµœì‹  GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- **í–¥ìƒëœ ë©”íŠ¸ë¦­**: ë” ì„¸ë°€í•œ GPU ìƒíƒœ ëª¨ë‹ˆí„°ë§
- **ì„±ëŠ¥ ê°œì„ **: ë‚®ì€ ì˜¤ë²„í—¤ë“œë¡œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
:::

### ì£¼ìš” GPU ë©”íŠ¸ë¦­

DCGM Exporterê°€ ìˆ˜ì§‘í•˜ëŠ” í•µì‹¬ ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤.

<SpecificationTable
  headers={['ë©”íŠ¸ë¦­ ì´ë¦„', 'ì„¤ëª…', 'ìŠ¤ì¼€ì¼ë§ í™œìš©']}
  rows={[
    { id: '1', cells: ['DCGM_FI_DEV_GPU_UTIL', 'GPU ì½”ì–´ ì‚¬ìš©ë¥  (%)', 'HPA íŠ¸ë¦¬ê±° ê¸°ì¤€'] },
    { id: '2', cells: ['DCGM_FI_DEV_MEM_COPY_UTIL', 'ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì‚¬ìš©ë¥  (%)', 'ë©”ëª¨ë¦¬ ë³‘ëª© ê°ì§€'] },
    { id: '3', cells: ['DCGM_FI_DEV_FB_USED', 'í”„ë ˆì„ë²„í¼ ì‚¬ìš©ëŸ‰ (MB)', 'OOM ë°©ì§€'] },
    { id: '4', cells: ['DCGM_FI_DEV_FB_FREE', 'í”„ë ˆì„ë²„í¼ ì—¬ìœ ëŸ‰ (MB)', 'ìš©ëŸ‰ ê³„íš'] },
    { id: '5', cells: ['DCGM_FI_DEV_POWER_USAGE', 'ì „ë ¥ ì‚¬ìš©ëŸ‰ (W)', 'ë¹„ìš© ëª¨ë‹ˆí„°ë§'] },
    { id: '6', cells: ['DCGM_FI_DEV_SM_CLOCK', 'SM í´ëŸ­ ì†ë„ (MHz)', 'ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§'] },
    { id: '7', cells: ['DCGM_FI_DEV_GPU_TEMP', 'GPU ì˜¨ë„ (Â°C)', 'ì—´ ê´€ë¦¬'] }
  ]}
/>

### Prometheus ServiceMonitor ì„¤ì •

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: dcgm-exporter
  namespace: gpu-monitoring
spec:
  selector:
    matchLabels:
      app: dcgm-exporter
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics
  namespaceSelector:
    matchNames:
      - gpu-monitoring
```

### KEDA ScaledObject ì„¤ì •

KEDAë¥¼ ì‚¬ìš©í•˜ì—¬ GPU ë©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: model-a-gpu-scaler
  namespace: inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: model-a-serving
  pollingInterval: 15
  cooldownPeriod: 60
  minReplicaCount: 2
  maxReplicaCount: 10
  fallback:
    failureThreshold: 3
    replicas: 3
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: Percent
              value: 25
              periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
            - type: Pods
              value: 4
              periodSeconds: 15
          selectPolicy: Max
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus-server.monitoring:9090
        metricName: gpu_utilization
        query: |
          avg(DCGM_FI_DEV_GPU_UTIL{pod=~"model-a-.*"})
        threshold: "70"
        activationThreshold: "50"
```

### ìë™ ìŠ¤ì¼€ì¼ë§ ì„ê³„ê°’ ì„¤ì •

ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë”°ë¥¸ ê¶Œì¥ ì„ê³„ê°’ì…ë‹ˆë‹¤.

<SpecificationTable
  headers={['ì›Œí¬ë¡œë“œ ìœ í˜•', 'Scale Up ì„ê³„ê°’', 'Scale Down ì„ê³„ê°’', 'Cooldown']}
  rows={[
    { id: '1', cells: ['ì‹¤ì‹œê°„ ì¶”ë¡ ', 'GPU 70%', 'GPU 30%', '60ì´ˆ'] },
    { id: '2', cells: ['ë°°ì¹˜ ì²˜ë¦¬', 'GPU 85%', 'GPU 40%', '300ì´ˆ'] },
    { id: '3', cells: ['ëŒ€í™”í˜• ì„œë¹„ìŠ¤', 'GPU 60%', 'GPU 25%', '30ì´ˆ'] }
  ]}
/>

:::tip ì„ê³„ê°’ íŠœë‹ ê°€ì´ë“œ

1. **ì´ˆê¸° ì„¤ì •**: ë³´ìˆ˜ì ì¸ ê°’(Scale Up 80%, Scale Down 20%)ìœ¼ë¡œ ì‹œì‘
2. **ëª¨ë‹ˆí„°ë§**: 2-3ì¼ê°„ ì‹¤ì œ íŠ¸ë˜í”½ íŒ¨í„´ ê´€ì°°
3. **ì¡°ì •**: ì‘ë‹µ ì‹œê°„ SLAì™€ ë¹„ìš©ì„ ê³ ë ¤í•˜ì—¬ ì ì§„ì  ì¡°ì •
4. **ê²€ì¦**: ë¶€í•˜ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì„¤ì • ê²€ì¦

:::

### HPAì™€ KEDA ì—°ë™

ê¸°ë³¸ HPAì™€ KEDAë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì˜ ì„¤ì •ì…ë‹ˆë‹¤.

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: model-a-hpa
  namespace: inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: model-a-serving
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: External
      external:
        metric:
          name: gpu_utilization
          selector:
            matchLabels:
              scaledobject.keda.sh/name: model-a-gpu-scaler
        target:
          type: AverageValue
          averageValue: "70"
```

---

## ë¹„ìš© ìµœì í™” ì „ëµ

### Spot ì¸ìŠ¤í„´ìŠ¤ í™œìš©

GPU Spot ì¸ìŠ¤í„´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ë¹„ìš©ì„ ìµœëŒ€ 90%ê¹Œì§€ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-spot-pool
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.12xlarge
            - g5.24xlarge
            - g5.48xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-spot-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: karpenter.sh/capacity-type
          value: "spot"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 32
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 30s
  weight: 50
```

:::warning Spot ì¸ìŠ¤í„´ìŠ¤ ì£¼ì˜ì‚¬í•­

- **ì¤‘ë‹¨ ì²˜ë¦¬**: Spot ì¸ìŠ¤í„´ìŠ¤ëŠ” 2ë¶„ ì „ ì¤‘ë‹¨ ì•Œë¦¼ì„ ë°›ìŠµë‹ˆë‹¤. ì ì ˆí•œ graceful shutdown êµ¬í˜„ í•„ìš”
- **ì›Œí¬ë¡œë“œ ì í•©ì„±**: ìƒíƒœ ë¹„ì €ì¥(stateless) ì¶”ë¡  ì›Œí¬ë¡œë“œì— ì í•©
- **ê°€ìš©ì„±**: íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì˜ Spot ê°€ìš©ì„±ì´ ë‚®ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ì–‘í•œ íƒ€ì… ì§€ì • ê¶Œì¥

:::

### Spot ì¤‘ë‹¨ ì²˜ë¦¬

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-serving-spot
  namespace: inference
spec:
  template:
    spec:
      terminationGracePeriodSeconds: 120
      containers:
        - name: vllm
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    # ìƒˆ ìš”ì²­ ìˆ˜ì‹  ì¤‘ë‹¨
                    curl -X POST localhost:8000/drain
                    # ì§„í–‰ ì¤‘ì¸ ìš”ì²­ ì™„ë£Œ ëŒ€ê¸°
                    sleep 90
      tolerations:
        - key: karpenter.sh/capacity-type
          operator: Equal
          value: "spot"
          effect: NoSchedule
```

### Consolidation ì •ì±…

ìœ íœ´ ë…¸ë“œë¥¼ ìë™ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ë¹„ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-pool
spec:
  disruption:
    # ë…¸ë“œê°€ ë¹„ì–´ìˆê±°ë‚˜ í™œìš©ë„ê°€ ë‚®ì„ ë•Œ í†µí•©
    consolidationPolicy: WhenEmptyOrUnderutilized
    # í†µí•© ëŒ€ê¸° ì‹œê°„
    consolidateAfter: 30s
    # ì˜ˆì‚° ì„¤ì • - ë™ì‹œì— ì¤‘ë‹¨ ê°€ëŠ¥í•œ ë…¸ë“œ ìˆ˜ ì œí•œ
    budgets:
      - nodes: "20%"
      - nodes: "0"
        schedule: "0 9 * * 1-5"  # í‰ì¼ ì—…ë¬´ ì‹œê°„ì—ëŠ” ì¤‘ë‹¨ ë°©ì§€
        duration: 8h
```

### ë¹„ìš© ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

<SpecificationTable
  headers={['í•­ëª©', 'ì„¤ëª…', 'ì˜ˆìƒ ì ˆê°']}
  rows={[
    { id: '1', cells: ['Spot ì¸ìŠ¤í„´ìŠ¤ í™œìš©', 'ë¹„í”„ë¡œë•ì…˜ ë° ë‚´ê²°í•¨ì„± ì›Œí¬ë¡œë“œ', '60-90%'] },
    { id: '2', cells: ['Consolidation í™œì„±í™”', 'ìœ íœ´ ë…¸ë“œ ìë™ ì •ë¦¬', '20-30%'] },
    { id: '3', cells: ['Right-sizing', 'ì›Œí¬ë¡œë“œì— ë§ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ', '15-25%'] },
    { id: '4', cells: ['ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§', 'ë¹„ì—…ë¬´ ì‹œê°„ ë¦¬ì†ŒìŠ¤ ì¶•ì†Œ', '30-40%'] }
  ]}
/>

:::tip ë¹„ìš© ëª¨ë‹ˆí„°ë§

Kubecost ë˜ëŠ” AWS Cost Explorerë¥¼ í™œìš©í•˜ì—¬ GPU ì›Œí¬ë¡œë“œë³„ ë¹„ìš©ì„ ì¶”ì í•˜ê³ , ì •ê¸°ì ìœ¼ë¡œ ìµœì í™” ê¸°íšŒë¥¼ ê²€í† í•˜ì„¸ìš”.

:::

---

## ìš´ì˜ ëª¨ë²” ì‚¬ë¡€

### GPU ë¦¬ì†ŒìŠ¤ ìš”ì²­ ì„¤ì •

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-a-serving
  namespace: inference
spec:
  template:
    spec:
      containers:
        - name: vllm
          resources:
            requests:
              nvidia.com/gpu: 1
              memory: "32Gi"
              cpu: "8"
            limits:
              nvidia.com/gpu: 1
              memory: "64Gi"
              cpu: "16"
```

### ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„±

Grafana ëŒ€ì‹œë³´ë“œì—ì„œ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•  í•µì‹¬ íŒ¨ë„:

1. **GPU ì‚¬ìš©ë¥  íŠ¸ë Œë“œ**: ì‹œê°„ë³„ GPU ì‚¬ìš©ë¥  ë³€í™”
2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì—¬ìœ  ê³µê°„
3. **Pod ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸**: HPA/KEDA ìŠ¤ì¼€ì¼ë§ ì´ë ¥
4. **ë…¸ë“œ í”„ë¡œë¹„ì €ë‹**: Karpenter ë…¸ë“œ ìƒì„±/ì‚­ì œ ì´ë²¤íŠ¸
5. **ë¹„ìš© ì¶”ì **: ì‹œê°„ë‹¹/ì¼ë³„ GPU ë¹„ìš©

### ì•Œë¦¼ ì„¤ì •

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: gpu-alerts
  namespace: monitoring
spec:
  groups:
    - name: gpu-alerts
      rules:
        - alert: HighGPUUtilization
          expr: avg(DCGM_FI_DEV_GPU_UTIL) > 90
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "GPU ì‚¬ìš©ë¥ ì´ 90%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤"
            
        - alert: GPUMemoryPressure
          expr: (DCGM_FI_DEV_FB_USED / (DCGM_FI_DEV_FB_USED + DCGM_FI_DEV_FB_FREE)) > 0.9
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜"
```

---

## ìš”ì•½

GPU í´ëŸ¬ìŠ¤í„°ì˜ ë™ì  ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ëŠ” GenAI ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ê³¼ ë¹„ìš© íš¨ìœ¨ì„±ì„ ê²°ì •í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.

### í•µì‹¬ í¬ì¸íŠ¸

1. **Karpenter í™œìš©**: GPU ë…¸ë“œì˜ ìë™ í”„ë¡œë¹„ì €ë‹ ë° ì •ë¦¬ë¡œ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
2. **DCGM ë©”íŠ¸ë¦­**: ì •í™•í•œ GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ë°ì´í„° ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ ê²°ì •
3. **KEDA ì—°ë™**: GPU ë©”íŠ¸ë¦­ ê¸°ë°˜ ì›Œí¬ë¡œë“œ ìë™ ìŠ¤ì¼€ì¼ë§
4. **Spot ì¸ìŠ¤í„´ìŠ¤**: ì ì ˆí•œ ì›Œí¬ë¡œë“œì— Spot í™œìš©ìœ¼ë¡œ ë¹„ìš© ì ˆê°
5. **Consolidation**: ìœ íœ´ ë¦¬ì†ŒìŠ¤ ìë™ ì •ë¦¬ë¡œ ë¹„ìš© ìµœì í™”

### ë‹¤ìŒ ë‹¨ê³„

- [Agentic AI í”Œë«í¼ ì•„í‚¤í…ì²˜](./agentic-platform-architecture.md) - ì „ì²´ í”Œë«í¼ êµ¬ì„±
- [Agentic AI ì¸í”„ë¼](./agentic-ai-challenges.md) - AI ì—ì´ì „íŠ¸ ìš´ì˜ ì „ëµ

---

## DRA ì‹¬ì¸µ ë¶„ì„: Dynamic Resource Allocation

### DRAì˜ ë“±ì¥ ë°°ê²½ê³¼ í•„ìš”ì„±

:::info DRA (Dynamic Resource Allocation) ìƒíƒœ ì—…ë°ì´íŠ¸

- **K8s 1.26-1.30**: Alpha (feature gate í•„ìš”, `v1alpha2` API)
- **K8s 1.31**: Betaë¡œ ìŠ¹ê²©, ê¸°ë³¸ í™œì„±í™” (`v1alpha2` API)
- **K8s 1.32**: ìƒˆë¡œìš´ êµ¬í˜„(KEP #4381)ìœ¼ë¡œ ì „í™˜, `v1beta1` API (ê¸°ë³¸ ë¹„í™œì„±í™”)
- **K8s 1.33+**: `v1beta1` API ì•ˆì •í™”, ì„±ëŠ¥ ëŒ€í­ ê°œì„ , í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ
- **K8s 1.34+**: DRA ìš°ì„ ìˆœìœ„ ëŒ€ì•ˆ(prioritized alternatives) ì§€ì›, í–¥ìƒëœ ìŠ¤ì¼€ì¤„ë§
- EKS 1.32+ì—ì„œ DRAë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `DynamicResourceAllocation` feature gateë¥¼ ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™”í•´ì•¼ í•©ë‹ˆë‹¤.
- EKS 1.33+ì—ì„œëŠ” DRAê°€ ê¸°ë³¸ í™œì„±í™”ë˜ë©°, ì•ˆì •ì ì¸ í”„ë¡œë•ì…˜ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
:::

Kubernetes ì´ˆê¸° ë‹¨ê³„ì—ì„œ GPU ë¦¬ì†ŒìŠ¤ í• ë‹¹ì€ **Device Plugin** ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ê·¼ë³¸ì ì¸ í•œê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

<DraLimitationsTable />

**DRA (Dynamic Resource Allocation)**ëŠ” Kubernetes 1.26ì—ì„œ Alphaë¡œ ë„ì…ë˜ì—ˆìœ¼ë©°, 1.31+ì—ì„œ Betaë¡œ ìŠ¹ê²©ë˜ì–´ ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤.

### DRAì˜ í•µì‹¬ ê°œë…

DRAëŠ” **ì„ ì–¸ì  ë¦¬ì†ŒìŠ¤ ìš”ì²­ê³¼ ì¦‰ì‹œ í• ë‹¹**ì„ ë¶„ë¦¬í•˜ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì…ë‹ˆë‹¤:

```mermaid
graph LR
    A["Pod ìƒì„±<br/>(ResourceClaim ìš”ì²­)"] -->|Pending| B["Karpenter<br/>(ë…¸ë“œ ë¶„ì„)"]
    B -->|ë¦¬ì†ŒìŠ¤ ë¶€ì¡±| C["ìƒˆ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹"]
    C -->|í• ë‹¹ ì¤€ë¹„| D["DRA Controller<br/>(ë¦¬ì†ŒìŠ¤ ì˜ˆì•½)"]
    D -->|Allocated| E["Pod Binding"]
    E -->|Reserved| F["Pod ìŠ¤ì¼€ì¤„ë§"]
    F -->|InUse| G["Pod ì‹¤í–‰"]

    H["Resource Quota<br/>í™•ì¸"] -->|ì ìš©| D
    I["GPU íŒŒí‹°ì…”ë‹<br/>ì •ì±…"] -->|ì ìš©| D

    style A fill:#e8f4f8
    style D fill:#326ce5
    style E fill:#76b900
    style G fill:#ffd93d
```

### ResourceClaim ë¼ì´í”„ì‚¬ì´í´

DRAì˜ í•µì‹¬ì€ **ResourceClaim**ì´ë¼ëŠ” ìƒˆë¡œìš´ Kubernetes ë¦¬ì†ŒìŠ¤ì…ë‹ˆë‹¤:

:::warning API ë²„ì „ ì£¼ì˜
ì•„ë˜ ì˜ˆì‹œëŠ” K8s 1.31 ì´í•˜ì˜ `v1alpha2` API ê¸°ì¤€ì…ë‹ˆë‹¤. 

**K8s 1.32+**: `resource.k8s.io/v1beta1` APIë¡œ ì „í™˜, ResourceClass ëŒ€ì‹  DeviceClass ì‚¬ìš©, ResourceClaim ìŠ¤í™ êµ¬ì¡° ë³€ê²½

**K8s 1.33+**: `v1beta1` API ì•ˆì •í™”, í”„ë¡œë•ì…˜ ì‚¬ìš© ê¶Œì¥

**K8s 1.34+**: DRA ìš°ì„ ìˆœìœ„ ëŒ€ì•ˆ ì§€ì›, í–¥ìƒëœ ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§

í”„ë¡œë•ì…˜ ë°°í¬ ì‹œ í´ëŸ¬ìŠ¤í„° ë²„ì „ì— ë§ëŠ” APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
:::

```yaml
# 1. ë¼ì´í”„ì‚¬ì´í´ ìƒíƒœ ì„¤ëª…

# PENDING ìƒíƒœ: ë¦¬ì†ŒìŠ¤ í• ë‹¹ ëŒ€ê¸° ì¤‘
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: gpu-claim-vllm
  namespace: ai-inference
spec:
  resourceClassName: gpu.nvidia.com
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClaimParameters
    name: h100-params
status:
  phase: Pending  # ì•„ì§ í• ë‹¹ë˜ì§€ ì•ŠìŒ

---

# ALLOCATED ìƒíƒœ: DRA ì»¨íŠ¸ë¡¤ëŸ¬ê°€ ë¦¬ì†ŒìŠ¤ ì˜ˆì•½ ì™„ë£Œ
status:
  phase: Allocated
  allocation:
    resourceHandle: "gpu-handle-12345"
    shareable: false

---

# RESERVED ìƒíƒœ: Podì´ ë°”ì¸ë”©ë  ì¤€ë¹„ ì™„ë£Œ
status:
  phase: Reserved
  allocation:
    resourceHandle: "gpu-handle-12345"
    nodeName: "gpu-node-01"

---

# INUSE ìƒíƒœ: Podì´ í™œì„± ì‹¤í–‰ ì¤‘
status:
  phase: InUse
  allocation:
    resourceHandle: "gpu-handle-12345"
    nodeName: "gpu-node-01"
  reservedFor:
    - kind: Pod
      name: vllm-inference
      namespace: ai-inference
      uid: "abc123"
```

ê° ìƒíƒœì—ì„œ ë‹¤ìŒ ìƒíƒœë¡œ ì „í™˜ë˜ë ¤ë©´ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤:

- **Pending â†’ Allocated**: DRA ë“œë¼ì´ë²„ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ë° ì˜ˆì•½
- **Allocated â†’ Reserved**: Podì´ ResourceClaimì„ ì§€ì •í•˜ê³  ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ë…¸ë“œ ê²°ì •
- **Reserved â†’ InUse**: Podì´ ì‹¤ì œë¡œ ë…¸ë“œì—ì„œ ì‹¤í–‰ ì‹œì‘

### DRA vs Device Plugin ìƒì„¸ ë¹„êµ

<ComparisonTable
  headers={['í•­ëª©', 'Device Plugin', 'DRA']}
  rows={[
    { id: '1', cells: ['ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì‹œì ', 'ë…¸ë“œ ì‹œì‘ ì‹œ (ì •ì )', 'Pod ìŠ¤ì¼€ì¤„ë§ ì‹œ (ë™ì )'] },
    { id: '2', cells: ['í• ë‹¹ ë‹¨ìœ„', 'ì „ì²´ GPUë§Œ ê°€ëŠ¥', 'GPU ë¶„í•  ê°€ëŠ¥ (MIG, time-slicing)'] },
    { id: '3', cells: ['ìš°ì„ ìˆœìœ„ ì§€ì›', 'ì—†ìŒ (ì„ ì°©ìˆœ)', 'ResourceClaimì˜ ìš°ì„ ìˆœìœ„ ì§€ì›'] },
    { id: '4', cells: ['ë©€í‹° ë¦¬ì†ŒìŠ¤ ì¡°ìœ¨', 'ë¶ˆê°€ëŠ¥', 'Pod ìˆ˜ì¤€ì—ì„œ ì—¬ëŸ¬ ë¦¬ì†ŒìŠ¤ ì¡°ìœ¨'] },
    { id: '5', cells: ['ì„±ëŠ¥ ì œì•½ ì •ì±…', 'ì—†ìŒ', 'ResourceClassë¡œ ì„±ëŠ¥ ì •ì±… ì •ì˜ ê°€ëŠ¥'] },
    { id: '6', cells: ['í• ë‹¹ ë³µì›ë ¥', 'ë…¸ë“œ ì¥ì•  ì‹œ ìˆ˜ë™ ì •ë¦¬', 'ìë™ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜'] },
    { id: '7', cells: ['Kubernetes ë²„ì „', '1.8+', '1.26+ (Alpha), 1.32+ (v1beta1)'] },
    { id: '8', cells: ['ì„±ìˆ™ë„', 'í”„ë¡œë•ì…˜', '1.33+ í”„ë¡œë•ì…˜ ì¤€ë¹„'], recommended: true }
  ]}
/>

:::tip DRA ì„ íƒ ê°€ì´ë“œ
**DRAë¥¼ ì‚¬ìš©í•´ì•¼ í•  ë•Œ:**

- GPU íŒŒí‹°ì…”ë‹ì´ í•„ìš”í•œ ê²½ìš° (MIG, time-slicing)
- ë©€í‹° í…Œë„ŒíŠ¸ í™˜ê²½ì—ì„œ ê³µì •í•œ ë¦¬ì†ŒìŠ¤ ë°°ë¶„ í•„ìš”
- ë¦¬ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ë¥¼ ì ìš©í•´ì•¼ í•˜ëŠ” ê²½ìš°
- ë™ì  ìŠ¤ì¼€ì¼ë§ì´ ì¤‘ìš”í•œ ê²½ìš°
- **K8s 1.33+ í™˜ê²½**: DRA `v1beta1` API ì•ˆì •í™”, í”„ë¡œë•ì…˜ ì‚¬ìš© ê¶Œì¥
- **K8s 1.34+ í™˜ê²½**: DRA ìš°ì„ ìˆœìœ„ ëŒ€ì•ˆìœ¼ë¡œ í–¥ìƒëœ ìŠ¤ì¼€ì¤„ë§ í™œìš©

**Device Pluginì´ ì¶©ë¶„í•œ ê²½ìš°:**

- ë‹¨ìˆœíˆ GPUë¥¼ ì „ì²´ ë‹¨ìœ„ë¡œë§Œ í• ë‹¹
- ë ˆê±°ì‹œ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„± ì¤‘ìš”
- Kubernetes ë²„ì „ì´ 1.32 ì´í•˜
:::

### ê³ ê¸‰ GPU íŒŒí‹°ì…”ë‹ ì „ëµ

#### 1. MIG (Multi-Instance GPU) ê¸°ë°˜ íŒŒí‹°ì…”ë‹

MIGëŠ” H100, A100 ê°™ì€ ìµœì‹  GPUë¥¼ ìµœëŒ€ 7ê°œì˜ ë…ë¦½ì ì¸ GPUë¡œ ë¶„í• í•©ë‹ˆë‹¤:

```yaml
# MIG í”„ë¡œí•„ ì •ì˜
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: a100-mig-1g.5gb
  namespace: ai-inference
spec:
  # MIG í”„ë¡œí•„ ì„ íƒ: 1g.5gb, 2g.10gb, 3g.20gb, 7g.40gb
  mig:
    profile: "1g.5gb"  # 5GB ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ MIG ì¸ìŠ¤í„´ìŠ¤
    count: 1

---

# MIG ê¸°ë°˜ ResourceClass
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClass
metadata:
  name: gpu.nvidia.com/mig
driverName: nvidia.com/gpu
structuredParameters: true
parametersSchema:
  openAPIV3Schema:
    type: object
    properties:
      gpuProfile:
        type: string
        enum: ["1g.5gb", "2g.10gb", "3g.20gb", "7g.40gb"]
        default: "1g.5gb"

---

# MIG ResourceClaim ì‚¬ìš© ì˜ˆì‹œ
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: inference-gpu-mig
  namespace: ai-inference
spec:
  resourceClassName: gpu.nvidia.com/mig
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClaimParameters
    name: a100-mig-1g.5gb

---

# Podì—ì„œ MIG ResourceClaim ì‚¬ìš©
apiVersion: v1
kind: Pod
metadata:
  name: vllm-mig-inference
  namespace: ai-inference
spec:
  containers:
    - name: vllm
      image: vllm/vllm-openai:latest
      command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
      args:
        - "--model"
        - "meta-llama/Llama-2-7b-hf"
        - "--gpu-memory-utilization"
        - "0.9"
      resources:
        requests:
          memory: "4Gi"
          cpu: "4"
        claims:
          - name: mig-gpu
  resourceClaims:
    - name: mig-gpu
      source:
        resourceClaimTemplateName: mig-template
```

**MIG í”„ë¡œí•„ ì„±ëŠ¥ ì§€í‘œ:**

<SpecificationTable
  headers={['í”„ë¡œí•„', 'ë©”ëª¨ë¦¬', 'SM ìˆ˜', 'ìš©ë„', 'ì˜ˆìƒ ì²˜ë¦¬ëŸ‰']}
  rows={[
    { id: '1', cells: ['1g.5gb', '5GB', '14', 'ì†Œí˜• ëª¨ë¸ (3B-7B)', '~20 tok/s'] },
    { id: '2', cells: ['2g.10gb', '10GB', '28', 'ì¤‘í˜• ëª¨ë¸ (7B-13B)', '~50 tok/s'] },
    { id: '3', cells: ['3g.20gb', '20GB', '42', 'ëŒ€í˜• ëª¨ë¸ (13B-70B)', '~100 tok/s'] },
    { id: '4', cells: ['7g.40gb', '40GB', '84', 'ì´ˆëŒ€í˜• ëª¨ë¸ (70B+)', '~200 tok/s'] }
  ]}
/>

#### 2. Time-Slicing ê¸°ë°˜ íŒŒí‹°ì…”ë‹

Time-Slicingì€ ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ GPU ì‹œê°„ì„ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ Podì´ ë™ì¼ GPUë¥¼ ê³µìœ í•©ë‹ˆë‹¤:

```yaml
# Time-Slicing ResourceSlice ì •ì˜
apiVersion: gpu.nvidia.com/v1alpha1
kind: ResourceSlice
metadata:
  name: gpu-node-timeslice
  namespace: ai-inference
spec:
  nodeName: gpu-node-01
  devices:
    - id: 0  # GPU 0
      vendor: nvidia
      model: "A100-SXM4-80GB"
      # Time-slicing ì„¤ì •: ìµœëŒ€ 4ê°œ Podì´ ë™ì¼ GPU ì‚¬ìš© ê°€ëŠ¥
      timeSlicing:
        replicas: 4
        # GPU ìŠ¤ì¼€ì¤„ë§ ì •ì±…: "aggressive", "default", "conservative"
        schedulingPolicy: "default"
        # ì»¨í…ìŠ¤íŠ¸ ìŠ¤ìœ„ì¹­ ì˜¤ë²„í—¤ë“œ ì„¤ì • (ms)
        contextSwitchInterval: 100

---

# Time-Slicing ResourceClass
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClass
metadata:
  name: gpu.nvidia.com/timeslice
driverName: nvidia.com/gpu
structuredParameters: true

---

# Time-Slicing ResourceClaim ì‚¬ìš©
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: inference-gpu-slice
  namespace: ai-inference
spec:
  resourceClassName: gpu.nvidia.com/timeslice

---

# ì—¬ëŸ¬ Podì´ ë™ì¼ GPUë¥¼ time-sliceë¡œ ê³µìœ 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-timeslice-replicas
  namespace: ai-inference
spec:
  replicas: 3  # 3ê°œ Podì´ ë™ì¼ GPU ê³µìœ 
  selector:
    matchLabels:
      app: vllm-slice
  template:
    metadata:
      labels:
        app: vllm-slice
    spec:
      containers:
        - name: vllm
          image: vllm/vllm-openai:latest
          resources:
            requests:
              memory: "8Gi"
              cpu: "2"
            claims:
              - name: gpu-slice
      resourceClaims:
        - name: gpu-slice
          source:
            resourceClaimTemplateName: timeslice-template
```

**Time-Slicing ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­:**

```mermaid
graph TB
    subgraph "Time-Slicing ì˜¤ë²„í—¤ë“œ"
        A["GPU ì»¨í…ìŠ¤íŠ¸ ìŠ¤ìœ„ì¹­"] -->|~100-500ms| B["L2 ìºì‹œ í”ŒëŸ¬ì‹œ"]
        B --> C["ìƒˆ ì»¤ë„ ë¡œë“œ"]
        C --> D["ë©”ëª¨ë¦¬ ì¬êµ¬ì„±"]
        D --> E["ì„±ëŠ¥ ì €í•˜ 5-15%"]
    end

    F["ì¶”ì²œ ì‚¬ìš© ì‚¬ë¡€"] -->|ë°°ì¹˜ ì¶”ë¡ | G["ì²˜ë¦¬ëŸ‰ ì¤‘ì‹¬"]
    F -->|ê°œë°œ/í…ŒìŠ¤íŠ¸| H["ë¹„ìš© ìµœì í™”"]
    F -->|ë‚®ì€ QoS ìš”êµ¬| I["ë¹„ê¸´ê¸‰ ì‘ì—…"]

    J["í”¼í•´ì•¼ í•  ì‚¬ìš© ì‚¬ë¡€"] -->|ì‹¤ì‹œê°„ ì¶”ë¡ | K["ë‚®ì€ ì§€ì—° ìš”êµ¬"]
    J -->|ê³ ì„±ëŠ¥ í•™ìŠµ| L["ë†’ì€ ì²˜ë¦¬ëŸ‰ í•„ìš”"]
    J -->|ë¯¼ê°í•œ ì• í”Œë¦¬ì¼€ì´ì…˜| M["ì„±ëŠ¥ ë³´ì¥ í•„ìš”"]

    style E fill:#ff6b6b
    style G fill:#76b900
    style K fill:#ff6b6b
```

---

## ì°¸ê³  ìë£Œ

- [Karpenter ê³µì‹ ë¬¸ì„œ](https://karpenter.sh/)
- [NVIDIA DCGM Exporter](https://github.com/NVIDIA/dcgm-exporter)
- [KEDA ê³µì‹ ë¬¸ì„œ](https://keda.sh/)
- [AWS GPU ì¸ìŠ¤í„´ìŠ¤ ê°€ì´ë“œ](https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing)
- [NVIDIA GPU Operator Documentation](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/)
