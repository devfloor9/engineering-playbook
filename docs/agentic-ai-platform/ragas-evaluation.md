---
title: "Ragas RAG í‰ê°€ í”„ë ˆì„ì›Œí¬"
sidebar_label: "Ragas í‰ê°€"
description: "Ragasë¥¼ í™œìš©í•œ RAG íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í‰ê°€ ë° ì§€ì†ì  ê°œì„  ë°©ë²•"
sidebar_position: 9
category: "genai-aiml"
date: 2025-02-05
authors:
  - devfloor9
tags:
  - ragas
  - rag
  - evaluation
  - llm
  - quality
  - genai
  - testing
---

# Ragas RAG í‰ê°€ í”„ë ˆì„ì›Œí¬

> ğŸ“… **ì‘ì„±ì¼**: 2025-02-05 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 28ë¶„

Ragas(RAG Assessment)ëŠ” RAG(Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸ì˜ í’ˆì§ˆì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Agentic AI í”Œë«í¼ì—ì„œ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.

## ê°œìš”

### RAG í‰ê°€ê°€ í•„ìš”í•œ ì´ìœ 

RAG ì‹œìŠ¤í…œì€ ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸(ê²€ìƒ‰, ìƒì„±, ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬)ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ ì „ì²´ í’ˆì§ˆì„ ì¸¡ì •í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤:

```mermaid
graph LR
    subgraph "RAG Pipeline"
        Q["ì§ˆë¬¸"]
        R["ê²€ìƒ‰<br/>(Retrieval)"]
        C["ì»¨í…ìŠ¤íŠ¸<br/>(Context)"]
        G["ìƒì„±<br/>(Generation)"]
        A["ë‹µë³€"]
    end
    
    subgraph "í‰ê°€ í¬ì¸íŠ¸"
        E1["ê²€ìƒ‰ í’ˆì§ˆ"]
        E2["ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±"]
        E3["ë‹µë³€ ì •í™•ì„±"]
        E4["ë‹µë³€ ì¶©ì‹¤ë„"]
    end
    
    Q --> R
    R --> C
    C --> G
    G --> A
    
    R -.-> E1
    C -.-> E2
    A -.-> E3
    A -.-> E4
    
    style E1 fill:#4285f4,stroke:#333
    style E2 fill:#34a853,stroke:#333
    style E3 fill:#fbbc04,stroke:#333
    style E4 fill:#ea4335,stroke:#333
```

### Ragas í•µì‹¬ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì¸¡ì • ëŒ€ìƒ | ì„¤ëª… |
| --- | --- | --- |
| Faithfulness | ìƒì„± í’ˆì§ˆ | ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œì§€ |
| Answer Relevancy | ìƒì„± í’ˆì§ˆ | ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ì§€ |
| Context Precision | ê²€ìƒ‰ í’ˆì§ˆ | ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë°€ë„ |
| Context Recall | ê²€ìƒ‰ í’ˆì§ˆ | í•„ìš”í•œ ì •ë³´ê°€ ê²€ìƒ‰ë˜ì—ˆëŠ”ì§€ |
| Context Relevancy | ê²€ìƒ‰ í’ˆì§ˆ | ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ì§€ |
| Answer Correctness | ì¢…í•© í’ˆì§ˆ | ë‹µë³€ì˜ ì •í™•ì„± |

## ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •

### Python í™˜ê²½ ì„¤ì •

```bash
# Ragas ì„¤ì¹˜
pip install ragas langchain-openai datasets

# ì¶”ê°€ ì˜ì¡´ì„±
pip install pandas numpy
```

### ê¸°ë³¸ í‰ê°€ ì½”ë“œ

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset

# í‰ê°€ ë°ì´í„°ì…‹ ì¤€ë¹„
eval_data = {
    "question": [
        "Kubernetesì—ì„œ GPU ìŠ¤ì¼€ì¤„ë§ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
        "Karpenterì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    ],
    "answer": [
        "Kubernetesì—ì„œ GPU ìŠ¤ì¼€ì¤„ë§ì€ NVIDIA Device Pluginì„ í†µí•´ ìˆ˜í–‰ë©ë‹ˆë‹¤...",
        "KarpenterëŠ” ìë™ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹, í†µí•©(consolidation), ë“œë¦¬í”„íŠ¸ ê°ì§€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤...",
    ],
    "contexts": [
        ["GPU ìŠ¤ì¼€ì¤„ë§ì€ Device Pluginì„ í†µí•´...", "NVIDIA GPU OperatorëŠ”..."],
        ["KarpenterëŠ” Kubernetes ë…¸ë“œ ìë™ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ...", "NodePool CRDë¥¼ í†µí•´..."],
    ],
    "ground_truth": [
        "NVIDIA Device Pluginê³¼ GPU Operatorë¥¼ ì‚¬ìš©í•˜ì—¬ GPU ë¦¬ì†ŒìŠ¤ë¥¼ ìŠ¤ì¼€ì¤„ë§í•©ë‹ˆë‹¤.",
        "KarpenterëŠ” ìë™ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹, í†µí•©, ë“œë¦¬í”„íŠ¸ ê°ì§€, ì¤‘ë‹¨ ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.",
    ],
}

dataset = Dataset.from_dict(eval_data)

# í‰ê°€ ì‹¤í–‰
results = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ],
)

print(results)
```

## í•µì‹¬ ë©”íŠ¸ë¦­ ìƒì„¸ ì„¤ëª…

### 1. Faithfulness (ì¶©ì‹¤ë„)

ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ ì¸¡ì •í•©ë‹ˆë‹¤. í™˜ê°(hallucination)ì„ ê°ì§€í•˜ëŠ” ë° í•µì‹¬ì ì¸ ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤.

```python
from ragas.metrics import faithfulness

# Faithfulness ê³„ì‚° ê³¼ì •:
# 1. ë‹µë³€ì„ ê°œë³„ ì£¼ì¥(claims)ìœ¼ë¡œ ë¶„í•´
# 2. ê° ì£¼ì¥ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ë¡  ê°€ëŠ¥í•œì§€ ê²€ì¦
# 3. ê²€ì¦ëœ ì£¼ì¥ ìˆ˜ / ì „ì²´ ì£¼ì¥ ìˆ˜ = Faithfulness ì ìˆ˜

# ì ìˆ˜ í•´ì„:
# 1.0: ëª¨ë“  ì£¼ì¥ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§€ì›ë¨
# 0.5: ì ˆë°˜ì˜ ì£¼ì¥ë§Œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§€ì›ë¨
# 0.0: ì–´ë–¤ ì£¼ì¥ë„ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ (ì‹¬ê°í•œ í™˜ê°)
```

### 2. Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±)

ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€ ì¸¡ì •í•©ë‹ˆë‹¤.

```python
from ragas.metrics import answer_relevancy

# Answer Relevancy ê³„ì‚° ê³¼ì •:
# 1. ë‹µë³€ì—ì„œ ì—­ìœ¼ë¡œ ì§ˆë¬¸ì„ ìƒì„±
# 2. ìƒì„±ëœ ì§ˆë¬¸ê³¼ ì›ë³¸ ì§ˆë¬¸ì˜ ìœ ì‚¬ë„ ê³„ì‚°
# 3. ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ í‰ê·  ê³„ì‚°

# ì ìˆ˜ í•´ì„:
# ë†’ì€ ì ìˆ˜: ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë¨
# ë‚®ì€ ì ìˆ˜: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ë™ë–¨ì–´ì§„ ë‚´ìš©ì„ í¬í•¨
```

### 3. Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„)

ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì¤‘ ì‹¤ì œë¡œ ìœ ìš©í•œ ì •ë³´ì˜ ë¹„ìœ¨ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

```python
from ragas.metrics import context_precision

# Context Precision ê³„ì‚°:
# - Ground truth ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì‹ë³„
# - ìƒìœ„ ë­í‚¹ ì»¨í…ìŠ¤íŠ¸ì— ìœ ìš©í•œ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
# - ë†’ì€ ìˆœìœ„ì— ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
```

### 4. Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨)

ì •ë‹µì„ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ì •ë³´ê°€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì¸¡ì •í•©ë‹ˆë‹¤.

```python
from ragas.metrics import context_recall

# Context Recall ê³„ì‚°:
# 1. Ground truthë¥¼ ê°œë³„ ë¬¸ì¥ìœ¼ë¡œ ë¶„í•´
# 2. ê° ë¬¸ì¥ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ë¡  ê°€ëŠ¥í•œì§€ í™•ì¸
# 3. ì¶”ë¡  ê°€ëŠ¥í•œ ë¬¸ì¥ ìˆ˜ / ì „ì²´ ë¬¸ì¥ ìˆ˜ = Recall ì ìˆ˜
```

## ì¢…í•© í‰ê°€ íŒŒì´í”„ë¼ì¸

### ì „ì²´ RAG ì‹œìŠ¤í…œ í‰ê°€

```python
import os
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    context_relevancy,
    answer_correctness,
)
from datasets import Dataset
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# LLM ì„¤ì • (í‰ê°€ìš©)
os.environ["OPENAI_API_KEY"] = "your-api-key"

def evaluate_rag_pipeline(questions, rag_chain, ground_truths):
    """RAG íŒŒì´í”„ë¼ì¸ ì¢…í•© í‰ê°€"""
    
    answers = []
    contexts = []
    
    for question in questions:
        # RAG ì²´ì¸ ì‹¤í–‰
        result = rag_chain.invoke({"query": question})
        answers.append(result["result"])
        contexts.append([doc.page_content for doc in result["source_documents"]])
    
    # í‰ê°€ ë°ì´í„°ì…‹ êµ¬ì„±
    eval_dataset = Dataset.from_dict({
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths,
    })
    
    # ì „ì²´ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‰ê°€
    results = evaluate(
        eval_dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
            context_relevancy,
            answer_correctness,
        ],
    )
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
questions = [
    "EKSì—ì„œ Karpenterë¥¼ ì„¤ì •í•˜ëŠ” ë°©ë²•ì€?",
    "GPU ë…¸ë“œ ìë™ ìŠ¤ì¼€ì¼ë§ êµ¬ì„± ë°©ë²•ì€?",
    "Inference Gatewayì˜ ë™ì  ë¼ìš°íŒ… ì„¤ì •ì€?",
]

ground_truths = [
    "KarpenterëŠ” Helm ì°¨íŠ¸ë¡œ ì„¤ì¹˜í•˜ê³  NodePool CRDë¥¼ ì •ì˜í•˜ì—¬ ì„¤ì •í•©ë‹ˆë‹¤.",
    "DCGM Exporter ë©”íŠ¸ë¦­ê³¼ KEDAë¥¼ ì—°ë™í•˜ì—¬ GPU ì‚¬ìš©ë¥  ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ì„ êµ¬ì„±í•©ë‹ˆë‹¤.",
    "Gateway APIì˜ HTTPRouteë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ ê¸°ë°˜ íŠ¸ë˜í”½ ë¶„ë°°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.",
]

# í‰ê°€ ì‹¤í–‰
results = evaluate_rag_pipeline(questions, rag_chain, ground_truths)
print(results.to_pandas())
```

### í‰ê°€ ê²°ê³¼ ë¶„ì„

```python
import pandas as pd
import matplotlib.pyplot as plt

def analyze_evaluation_results(results):
    """í‰ê°€ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”"""
    
    df = results.to_pandas()
    
    # ë©”íŠ¸ë¦­ë³„ í‰ê·  ì ìˆ˜
    metrics_summary = df.mean(numeric_only=True)
    print("=== ë©”íŠ¸ë¦­ë³„ í‰ê·  ì ìˆ˜ ===")
    print(metrics_summary)
    
    # ë¬¸ì œ ì˜ì—­ ì‹ë³„
    print("\n=== ê°œì„  í•„ìš” ì˜ì—­ ===")
    for metric, score in metrics_summary.items():
        if score < 0.7:
            print(f"âš ï¸ {metric}: {score:.2f} - ê°œì„  í•„ìš”")
        elif score < 0.85:
            print(f"ğŸ“Š {metric}: {score:.2f} - ì–‘í˜¸")
        else:
            print(f"âœ… {metric}: {score:.2f} - ìš°ìˆ˜")
    
    # ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(10, 6))
    metrics_summary.plot(kind='bar', ax=ax, color=['#4285f4', '#34a853', '#fbbc04', '#ea4335', '#9c27b0', '#00bcd4'])
    ax.set_ylabel('Score')
    ax.set_title('RAG Pipeline Evaluation Results')
    ax.set_ylim(0, 1)
    ax.axhline(y=0.7, color='r', linestyle='--', label='Minimum Threshold')
    ax.legend()
    plt.tight_layout()
    plt.savefig('rag_evaluation_results.png')
    
    return metrics_summary

# ë¶„ì„ ì‹¤í–‰
summary = analyze_evaluation_results(results)
```

## CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©

### GitHub Actions ì›Œí¬í”Œë¡œìš°

```yaml
# .github/workflows/rag-evaluation.yml
name: RAG Pipeline Evaluation

on:
  push:
    paths:
      - 'src/rag/**'
      - 'data/knowledge_base/**'
  pull_request:
    paths:
      - 'src/rag/**'
  schedule:
    - cron: '0 0 * * *'  # ë§¤ì¼ ìì •

jobs:
  evaluate:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install ragas langchain-openai datasets pandas
    
    - name: Run RAG Evaluation
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python scripts/evaluate_rag.py --output results/evaluation.json
    
    - name: Check Quality Gates
      run: |
        python scripts/check_quality_gates.py results/evaluation.json
    
    - name: Upload Results
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-results
        path: results/
    
    - name: Comment PR with Results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const results = JSON.parse(fs.readFileSync('results/evaluation.json'));
          
          let comment = '## RAG Evaluation Results\n\n';
          comment += '| Metric | Score | Status |\n';
          comment += '|--------|-------|--------|\n';
          
          for (const [metric, score] of Object.entries(results.metrics)) {
            const status = score >= 0.7 ? 'âœ…' : 'âš ï¸';
            comment += `| ${metric} | ${score.toFixed(2)} | ${status} |\n`;
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
```

### í’ˆì§ˆ ê²Œì´íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/check_quality_gates.py
import json
import sys

QUALITY_GATES = {
    "faithfulness": 0.8,
    "answer_relevancy": 0.75,
    "context_precision": 0.7,
    "context_recall": 0.7,
}

def check_quality_gates(results_file):
    with open(results_file) as f:
        results = json.load(f)
    
    failed_gates = []
    
    for metric, threshold in QUALITY_GATES.items():
        score = results["metrics"].get(metric, 0)
        if score < threshold:
            failed_gates.append({
                "metric": metric,
                "score": score,
                "threshold": threshold,
            })
    
    if failed_gates:
        print("âŒ Quality gates failed:")
        for gate in failed_gates:
            print(f"  - {gate['metric']}: {gate['score']:.2f} < {gate['threshold']}")
        sys.exit(1)
    else:
        print("âœ… All quality gates passed!")
        sys.exit(0)

if __name__ == "__main__":
    check_quality_gates(sys.argv[1])
```

## Kubernetes Jobìœ¼ë¡œ ì •ê¸° í‰ê°€

### í‰ê°€ Job ì •ì˜

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: rag-evaluation
  namespace: genai-platform
spec:
  schedule: "0 6 * * *"  # ë§¤ì¼ ì˜¤ì „ 6ì‹œ
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: evaluator
            image: your-registry/rag-evaluator:latest
            env:
            - name: OPENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: openai-credentials
                  key: api-key
            - name: MILVUS_HOST
              value: "milvus-proxy.milvus.svc.cluster.local"
            - name: RESULTS_BUCKET
              value: "s3://rag-evaluation-results"
            command:
            - python
            - /app/evaluate.py
            - --config=/app/config/evaluation.yaml
            - --output=s3
            resources:
              requests:
                cpu: "1"
                memory: "2Gi"
              limits:
                cpu: "2"
                memory: "4Gi"
          restartPolicy: OnFailure
          serviceAccountName: rag-evaluator
```

### í‰ê°€ ì„¤ì • ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: rag-evaluation-config
  namespace: genai-platform
data:
  evaluation.yaml: |
    evaluation:
      metrics:
        - faithfulness
        - answer_relevancy
        - context_precision
        - context_recall
      
      test_sets:
        - name: "general_knowledge"
          path: "s3://test-data/general.json"
          weight: 0.4
        - name: "technical_docs"
          path: "s3://test-data/technical.json"
          weight: 0.6
      
      quality_gates:
        faithfulness: 0.8
        answer_relevancy: 0.75
        context_precision: 0.7
        context_recall: 0.7
      
      alerts:
        slack_webhook: "https://hooks.slack.com/..."
        threshold_drop: 0.1  # 10% ì´ìƒ í•˜ë½ ì‹œ ì•Œë¦¼
```

## í‰ê°€ ê²°ê³¼ í•´ì„ ë° ê°œì„  ê°€ì´ë“œ

### ë©”íŠ¸ë¦­ë³„ ê°œì„  ë°©í–¥

```mermaid
graph TD
    subgraph "ë‚®ì€ Faithfulness"
        F1["í”„ë¡¬í”„íŠ¸ì— ì»¨í…ìŠ¤íŠ¸ ê°•ì¡° ì¶”ê°€"]
        F2["Temperature ë‚®ì¶”ê¸°"]
        F3["ë” ê°•ë ¥í•œ LLM ì‚¬ìš©"]
    end
    
    subgraph "ë‚®ì€ Context Precision"
        CP1["ì„ë² ë”© ëª¨ë¸ ê°œì„ "]
        CP2["ì²­í‚¹ ì „ëµ ì¡°ì •"]
        CP3["ë¦¬ë­í‚¹ ëª¨ë¸ ì¶”ê°€"]
    end
    
    subgraph "ë‚®ì€ Context Recall"
        CR1["ê²€ìƒ‰ kê°’ ì¦ê°€"]
        CR2["í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì ìš©"]
        CR3["ì§€ì‹ ë² ì´ìŠ¤ í™•ì¥"]
    end
    
    subgraph "ë‚®ì€ Answer Relevancy"
        AR1["í”„ë¡¬í”„íŠ¸ ëª…í™•í™”"]
        AR2["Few-shot ì˜ˆì œ ì¶”ê°€"]
        AR3["ì¶œë ¥ í˜•ì‹ ì§€ì •"]
    end
```

### ê°œì„  ì²´í¬ë¦¬ìŠ¤íŠ¸

| ë¬¸ì œ | ê°€ëŠ¥í•œ ì›ì¸ | í•´ê²° ë°©ì•ˆ |
| --- | --- | --- |
| Faithfulness < 0.7 | LLMì´ ì»¨í…ìŠ¤íŠ¸ ë¬´ì‹œ | í”„ë¡¬í”„íŠ¸ì— "ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©" ê°•ì¡° |
| Context Precision < 0.6 | ê²€ìƒ‰ í’ˆì§ˆ ë‚®ìŒ | ì„ë² ë”© ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ, ë¦¬ë­í‚¹ ì¶”ê°€ |
| Context Recall < 0.6 | ê´€ë ¨ ë¬¸ì„œ ëˆ„ë½ | kê°’ ì¦ê°€, ì§€ì‹ ë² ì´ìŠ¤ ë³´ê°• |
| Answer Relevancy < 0.7 | ë‹µë³€ì´ ì‚°ë§Œí•¨ | í”„ë¡¬í”„íŠ¸ êµ¬ì¡°í™”, ì¶œë ¥ í˜•ì‹ ì§€ì • |

## ê´€ë ¨ ë¬¸ì„œ

- [Milvus ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤](./milvus-vector-database.md)
- [Agent ëª¨ë‹ˆí„°ë§](./agent-monitoring.md)
- [Agentic AI í”Œë«í¼ ì•„í‚¤í…ì²˜](./agentic-platform-architecture.md)

:::tip ê¶Œì¥ ì‚¬í•­
- í‰ê°€ ë°ì´í„°ì…‹ì€ ìµœì†Œ 50ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”
- Ground truthëŠ” ë„ë©”ì¸ ì „ë¬¸ê°€ê°€ ê²€ì¦í•œ ì •ë‹µì„ ì‚¬ìš©í•˜ì„¸ìš”
- ì •ê¸°ì ì¸ í‰ê°€ë¥¼ í†µí•´ ì‹œê°„ì— ë”°ë¥¸ í’ˆì§ˆ ë³€í™”ë¥¼ ì¶”ì í•˜ì„¸ìš”
:::

:::warning ì£¼ì˜ì‚¬í•­
- Ragas í‰ê°€ëŠ” LLM API í˜¸ì¶œì´ í•„ìš”í•˜ë¯€ë¡œ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤
- ëŒ€ê·œëª¨ í‰ê°€ ì‹œ ë°°ì¹˜ ì²˜ë¦¬ì™€ ìºì‹±ì„ í™œìš©í•˜ì„¸ìš”
- í‰ê°€ ê²°ê³¼ëŠ” ì‚¬ìš©ëœ LLMì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤
:::
