---
title: "llm-d ê¸°ë°˜ EKS Auto Mode ì¶”ë¡  ë°°í¬ ê°€ì´ë“œ"
sidebar_label: "llm-d EKS Auto Mode"
description: "llm-dë¥¼ í™œìš©í•œ EKS Auto Mode í™˜ê²½ì—ì„œì˜ Kubernetes ë„¤ì´í‹°ë¸Œ ë¶„ì‚° ì¶”ë¡  ë°°í¬ ë° ìš´ì˜ ê°€ì´ë“œ"
tags: [eks, llm-d, vllm, inference-gateway, gpu, auto-mode, qwen, kv-cache]
category: "genai-aiml"
last_update:
  date: 2026-02-10
  author: devfloor9
sidebar_position: 8
---

# llm-d ê¸°ë°˜ EKS Auto Mode ì¶”ë¡  ë°°í¬ ê°€ì´ë“œ

> **ğŸ“Œ í˜„ì¬ ë²„ì „**: llm-d v0.4 (2025). ë³¸ ë¬¸ì„œì˜ ë°°í¬ ì˜ˆì‹œëŠ” Intelligent Inference Scheduling well-lit path ê¸°ì¤€ì…ë‹ˆë‹¤.

> ğŸ“… **ì‘ì„±ì¼**: 2026-02-10 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 15ë¶„

## ê°œìš”

llm-dëŠ” Red Hatì´ ì£¼ë„í•˜ëŠ” Apache 2.0 ë¼ì´ì„ ìŠ¤ì˜ Kubernetes ë„¤ì´í‹°ë¸Œ ë¶„ì‚° ì¶”ë¡  ìŠ¤íƒì…ë‹ˆë‹¤. vLLM ì¶”ë¡  ì—”ì§„, Envoy ê¸°ë°˜ Inference Gateway, ê·¸ë¦¬ê³  Kubernetes Gateway APIë¥¼ ê²°í•©í•˜ì—¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ëŠ¥ì ì¸ ì¶”ë¡  ë¼ìš°íŒ…ì„ ì œê³µí•©ë‹ˆë‹¤.

ê¸°ì¡´ vLLM ë°°í¬ê°€ ë‹¨ìˆœí•œ Round-Robin ë¡œë“œ ë°¸ëŸ°ì‹±ì— ì˜ì¡´í•˜ëŠ” ë°˜ë©´, llm-dëŠ” KV Cache ìƒíƒœë¥¼ ì¸ì‹í•˜ëŠ” ì§€ëŠ¥ì  ë¼ìš°íŒ…ì„ í†µí•´ ë™ì¼í•œ prefixë¥¼ ê°€ì§„ ìš”ì²­ì„ ì´ë¯¸ í•´ë‹¹ KV Cacheë¥¼ ë³´ìœ í•œ Podë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Time To First Token(TTFT)ì„ í¬ê²Œ ë‹¨ì¶•í•˜ê³  GPU ì—°ì‚°ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë³¸ ë¬¸ì„œì—ì„œëŠ” Amazon EKS Auto Mode í™˜ê²½ì—ì„œ llm-dë¥¼ ë°°í¬í•˜ê³  Qwen3-32B ëª¨ë¸ë¡œ ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤. EKS Auto ModeëŠ” Karpenter ê¸°ë°˜ ë…¸ë“œ ìë™ í”„ë¡œë¹„ì €ë‹ê³¼ NVIDIA GPU ë“œë¼ì´ë²„ ìë™ ê´€ë¦¬ë¥¼ ì œê³µí•˜ì—¬ GPU ì¸í”„ë¼ êµ¬ì„±ì˜ ë³µì¡ì„±ì„ í¬ê²Œ ì¤„ì—¬ì¤ë‹ˆë‹¤.

### ì£¼ìš” ëª©í‘œ

- **llm-d ì•„í‚¤í…ì²˜ ì´í•´**: Inference Gatewayì™€ KV Cache-aware ë¼ìš°íŒ…ì˜ ë™ì‘ ì›ë¦¬
- **EKS Auto Mode GPU êµ¬ì„±**: p5.48xlarge ë…¸ë“œ ìë™ í”„ë¡œë¹„ì €ë‹ ì„¤ì •
- **Qwen3-32B ë°°í¬**: helmfile ê¸°ë°˜ í†µí•© ë°°í¬ ë° ê²€ì¦
- **ì¶”ë¡  í…ŒìŠ¤íŠ¸**: OpenAI í˜¸í™˜ APIë¥¼ í†µí•œ ì¶”ë¡  ìš”ì²­ ë° ìŠ¤íŠ¸ë¦¬ë°
- **ìš´ì˜ ìµœì í™”**: ëª¨ë‹ˆí„°ë§, ë¹„ìš© ìµœì í™”, íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### llm-dì˜ 3ê°€ì§€ Well-Lit Path

llm-dëŠ” ì„¸ ê°€ì§€ ê²€ì¦ëœ ë°°í¬ ê²½ë¡œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

| Well-Lit Path | ì„¤ëª… | ì í•©í•œ ì›Œí¬ë¡œë“œ |
| --- | --- | --- |
| **Intelligent Inference Scheduling** | KV Cache-aware ë¼ìš°íŒ…ìœ¼ë¡œ ì§€ëŠ¥ì  ìš”ì²­ ë¶„ë°° | ë²”ìš© LLM ì„œë¹™ (ë³¸ ê°€ì´ë“œ) |
| **Prefill/Decode Disaggregation** | Prefillê³¼ Decode ë‹¨ê³„ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬ | ëŒ€ê·œëª¨ ë°°ì¹˜, ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ |
| **Wide Expert-Parallelism** | MoE ëª¨ë¸ì˜ Expertë¥¼ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ì‚° | MoE ëª¨ë¸ (Mixtral, DeepSeek ë“±) |

---

## ì•„í‚¤í…ì²˜

llm-dì˜ Intelligent Inference Scheduling ì•„í‚¤í…ì²˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë©ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph "Client Layer"
        CLIENT[Client Application<br/>OpenAI Compatible API]
    end
    
    subgraph "Gateway Layer"
        GW[Inference Gateway<br/>Envoy-based]
        IM[InferenceModel CRD<br/>ëª¨ë¸ â†” Pool ë§¤í•‘]
        IP[InferencePool CRD<br/>vLLM Pod ê·¸ë£¹ ê´€ë¦¬]
    end
    
    subgraph "Inference Layer"
        subgraph "vLLM Pod 1 â€” GPU 0,1"
            V1[vLLM Engine<br/>Qwen3-32B TP=2]
            KV1[KV Cache]
        end
        subgraph "vLLM Pod 2 â€” GPU 2,3"
            V2[vLLM Engine<br/>Qwen3-32B TP=2]
            KV2[KV Cache]
        end
        subgraph "vLLM Pod N â€” GPU 14,15"
            VN[vLLM Engine<br/>Qwen3-32B TP=2]
            KVN[KV Cache]
        end
    end
    
    subgraph "EKS Auto Mode"
        NP[Karpenter NodePool<br/>p5.48xlarge]
        NC[NodeClass default<br/>ìë™ GPU ë“œë¼ì´ë²„]
    end
    
    CLIENT --> GW
    GW --> IM
    IM --> IP
    IP --> V1 & V2 & VN
    NP --> NC
    
    style CLIENT fill:#34a853,color:#fff
    style GW fill:#4286f4,color:#fff
    style NP fill:#fbbc04,color:#000
```

### llm-d vs ê¸°ì¡´ vLLM ë°°í¬ ë¹„êµ

| íŠ¹ì„± | ê¸°ì¡´ vLLM ë°°í¬ | llm-d ë°°í¬ |
| --- | --- | --- |
| ë¼ìš°íŒ… ë°©ì‹ | Round-Robin / Random | KV Cache-aware Intelligent Routing |
| Gateway í†µí•© | ë³„ë„ Ingress/Service êµ¬ì„± | Gateway API ë„¤ì´í‹°ë¸Œ í†µí•© |
| ìŠ¤ì¼€ì¼ë§ ê´€ë¦¬ | ìˆ˜ë™ HPA êµ¬ì„± | InferencePool ê¸°ë°˜ ìë™ ê´€ë¦¬ |
| KV Cache í™œìš© | Podë³„ ë…ë¦½ì  ê´€ë¦¬ | Cross-pod prefix ì¬ì‚¬ìš©ìœ¼ë¡œ TTFT ë‹¨ì¶• |
| ì„¤ì¹˜ ë°©ì‹ | ê°œë³„ Helm chart ì¡°í•© | helmfile í†µí•© ë°°í¬ (ì›ì»¤ë§¨ë“œ) |
| ëª¨ë¸ ì •ì˜ | Deployment YAML ì§ì ‘ ì‘ì„± | InferenceModel CRD ì„ ì–¸ì  ê´€ë¦¬ |

### Qwen3-32B ëª¨ë¸ ì„ ì • ì´ìœ 

| í•­ëª© | ë‚´ìš© |
| --- | --- |
| ëª¨ë¸ëª… | Qwen/Qwen3-32B |
| íŒŒë¼ë¯¸í„° | 32B (Dense) |
| ë¼ì´ì„ ìŠ¤ | Apache 2.0 |
| ì •ë°€ë„ | BF16 (~65GB VRAM) |
| ì»¨í…ìŠ¤íŠ¸ | ìµœëŒ€ 32,768 í† í° |
| íŠ¹ì§• | llm-d ê³µì‹ ê¸°ë³¸ ëª¨ë¸, ë‹¤êµ­ì–´ ì§€ì› ìš°ìˆ˜, ì˜¤í”ˆì†ŒìŠ¤ LLM ì¤‘ ìµœê³  ì¸ê¸° |

:::info Qwen3-32B ì„ ì • ë°°ê²½
Qwen3-32BëŠ” llm-dì˜ ê³µì‹ ê¸°ë³¸ ëª¨ë¸ì´ë©°, Apache 2.0 ë¼ì´ì„ ìŠ¤ë¡œ ìƒì—…ì  ì‚¬ìš©ì´ ììœ ë¡­ìŠµë‹ˆë‹¤. BF16 ê¸°ì¤€ ì•½ 65GB VRAMì´ í•„ìš”í•˜ì—¬ TP=2 (2Ã— GPU)ë¡œ H100 80GBì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì„œë¹™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

---

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

| í•­ëª© | ìš”êµ¬ì‚¬í•­ | ë¹„ê³  |
| --- | --- | --- |
| AWS ê³„ì • | p5.48xlarge ì¿¼í„° ìŠ¹ì¸ | Service Quotas â†’ Running On-Demand P instances â‰¥ 192 |
| eksctl | >= 0.200.0 | EKS Auto Mode ì§€ì› ë²„ì „ |
| kubectl | >= 1.31 | EKS 1.31 í˜¸í™˜ |
| Helm | >= 3.0 | Helm chart ë°°í¬ìš© |
| helmfile | ìµœì‹  ë²„ì „ | llm-d í†µí•© ë°°í¬ ë„êµ¬ |
| yq | >= 4.0 | YAML ì²˜ë¦¬ ë„êµ¬ |
| HuggingFace Token | Qwen3-32B ì ‘ê·¼ ê¶Œí•œ | https://huggingface.co/settings/tokens |
| AWS CLI | v2 ìµœì‹  | ìê²© ì¦ëª… êµ¬ì„± ì™„ë£Œ |

### í´ë¼ì´ì–¸íŠ¸ ë„êµ¬ ì„¤ì¹˜

```bash
# eksctl ì„¤ì¹˜ (macOS)
brew tap weaveworks/tap
brew install weaveworks/tap/eksctl

# helmfile ì„¤ì¹˜
brew install helmfile

# yq ì„¤ì¹˜
brew install yq

# ë²„ì „ í™•ì¸
eksctl version
kubectl version --client
helm version
helmfile --version
yq --version
```

:::warning p5.48xlarge ì¿¼í„° í™•ì¸
p5.48xlargeëŠ” 192 vCPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. AWS Service Quotasì—ì„œ **Running On-Demand P instances** í•œë„ê°€ ìµœì†Œ 192 ì´ìƒì¸ì§€ í™•ì¸í•˜ì„¸ìš”. ì¿¼í„° ì¦ê°€ ìš”ì²­ì€ ìŠ¹ì¸ê¹Œì§€ 1-3 ì˜ì—…ì¼ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# í˜„ì¬ P ì¸ìŠ¤í„´ìŠ¤ ì¿¼í„° í™•ì¸
aws service-quotas get-service-quota \
  --service-code ec2 \
  --quota-code L-417A185B \
  --region us-west-2 \
  --query 'Quota.Value'
```
:::

---

## EKS Auto Mode í´ëŸ¬ìŠ¤í„° ìƒì„±

### í´ëŸ¬ìŠ¤í„° êµ¬ì„± íŒŒì¼

```yaml
# cluster-config.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: llm-d-cluster
  region: us-west-2
  version: "1.31"
autoModeConfig:
  enabled: true
```

```bash
# í´ëŸ¬ìŠ¤í„° ìƒì„± (ì•½ 15-20ë¶„ ì†Œìš”)
eksctl create cluster -f cluster-config.yaml

# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl get nodes
kubectl cluster-info
```

### GPU NodePool ìƒì„±

EKS Auto Modeì—ì„œ p5.48xlarge ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìë™ í”„ë¡œë¹„ì €ë‹í•˜ê¸° ìœ„í•œ Karpenter NodePoolì„ ìƒì„±í•©ë‹ˆë‹¤.

```yaml
# gpu-nodepool.yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-p5
spec:
  template:
    spec:
      requirements:
        - key: eks.amazonaws.com/instance-family
          operator: In
          values: ["p5"]
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      taints:
        - key: nvidia.com/gpu
          effect: NoSchedule
  limits:
    cpu: "384"
    memory: 4096Gi
    nvidia.com/gpu: "16"
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 30s
```

```bash
kubectl apply -f gpu-nodepool.yaml

# NodePool ìƒíƒœ í™•ì¸
kubectl get nodepool gpu-p5
```

:::info EKS Auto Modeì˜ GPU ì§€ì›
EKS Auto ModeëŠ” NVIDIA GPU ë“œë¼ì´ë²„ë¥¼ ìë™ìœ¼ë¡œ ì„¤ì¹˜í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤. ë³„ë„ì˜ GPU Operatorë‚˜ NVIDIA Device Plugin ì„¤ì¹˜ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤. NodeClass `default`ë¥¼ ì‚¬ìš©í•˜ë©´ Auto Modeê°€ ìµœì ì˜ AMIì™€ ë“œë¼ì´ë²„ ë²„ì „ì„ ìë™ ì„ íƒí•©ë‹ˆë‹¤.
:::

### p5.48xlarge ì¸ìŠ¤í„´ìŠ¤ ì‚¬ì–‘

| í•­ëª© | ì‚¬ì–‘ |
| --- | --- |
| GPU | 8Ã— NVIDIA H100 80GB HBM3 |
| GPU ë©”ëª¨ë¦¬ | ì´ 640GB |
| vCPU | 192 |
| ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ | 2,048 GiB |
| GPU ì¸í„°ì»¤ë„¥íŠ¸ | NVSwitch (900 GB/s) |
| ë„¤íŠ¸ì›Œí¬ | EFA 3,200 Gbps |
| ìŠ¤í† ë¦¬ì§€ | 8Ã— 3.84TB NVMe SSD |

---

## llm-d ë°°í¬

### 5.1 ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° ì‹œí¬ë¦¿ ìƒì„±

```bash
export NAMESPACE=llm-d
kubectl create namespace ${NAMESPACE}

# HuggingFace í† í° ì‹œí¬ë¦¿ ìƒì„±
kubectl create secret generic llm-d-hf-token \
  --from-literal=HF_TOKEN=<your-huggingface-token> \
  -n ${NAMESPACE}

# ì‹œí¬ë¦¿ ìƒì„± í™•ì¸
kubectl get secret llm-d-hf-token -n ${NAMESPACE}
```

### 5.2 llm-d ì €ì¥ì†Œ í´ë¡ 

```bash
git clone https://github.com/llm-d/llm-d.git
cd llm-d/guides/inference-scheduling
```

ë””ë ‰í† ë¦¬ êµ¬ì¡°:
```
guides/inference-scheduling/
â”œâ”€â”€ helmfile.yaml          # í†µí•© ë°°í¬ ì •ì˜
â”œâ”€â”€ values/
â”‚   â”œâ”€â”€ vllm-values.yaml   # vLLM ì„œë²„ ì„¤ì •
â”‚   â”œâ”€â”€ gateway-values.yaml # Gateway ì„¤ì •
â”‚   â””â”€â”€ ...
â””â”€â”€ README.md
```

### 5.3 Gateway API CRD ì„¤ì¹˜

llm-dëŠ” Kubernetes Gateway APIì™€ Inference Extension CRDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```bash
# Gateway API í‘œì¤€ CRD ì„¤ì¹˜
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.1/standard-install.yaml

# Inference Extension CRD ì„¤ì¹˜ (InferencePool, InferenceModel)
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/v0.3.0/manifests.yaml
```

ì„¤ì¹˜ë˜ëŠ” CRD:

| CRD | ì—­í•  |
| --- | --- |
| `Gateway` | Envoy ê¸°ë°˜ í”„ë¡ì‹œ ì¸ìŠ¤í„´ìŠ¤ ì •ì˜ |
| `HTTPRoute` | ë¼ìš°íŒ… ê·œì¹™ ì •ì˜ |
| `InferencePool` | vLLM Pod ê·¸ë£¹ (ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸ í’€) ì •ì˜ |
| `InferenceModel` | ëª¨ë¸ ì´ë¦„ê³¼ InferencePool ë§¤í•‘ |

```bash
# CRD ì„¤ì¹˜ í™•ì¸
kubectl get crd | grep -E "gateway|inference"
```

### 5.4 Gateway ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ì„¤ì¹˜

```bash
# Istio ê¸°ë°˜ Gateway ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ì„¤ì¹˜
helmfile apply -n ${NAMESPACE} -l component=gateway-control-plane
```

### 5.5 llm-d ì „ì²´ ë°°í¬

```bash
# ì „ì²´ ì»´í¬ë„ŒíŠ¸ ë°°í¬ (vLLM + Gateway + InferencePool + InferenceModel)
helmfile apply -n ${NAMESPACE}
```

ê¸°ë³¸ ë°°í¬ êµ¬ì„±:

| ì„¤ì • | ê¸°ë³¸ê°’ | ì„¤ëª… |
| --- | --- | --- |
| ëª¨ë¸ | Qwen/Qwen3-32B | Apache 2.0, BF16 ~65GB VRAM |
| Tensor Parallelism | TP=2 | replicaë‹¹ 2 GPU ì‚¬ìš© |
| Replicas | 8 | ì´ 16 GPU (2Ã— p5.48xlarge) |
| Max Model Length | 32,768 | ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ |
| GPU Memory Utilization | 0.90 | KV Cache í• ë‹¹ ë¹„ìœ¨ |

:::tip ë¦¬ì†ŒìŠ¤ ì¡°ì •
ê¸°ë³¸ ì„¤ì •ì€ 8 replicas Ã— 2 GPU = 16 GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ëª©ì ì´ë¼ë©´ `helmfile.yaml`ì—ì„œ `replicaCount`ë¥¼ ì¤„ì—¬ ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 4 replicasë¡œ ì„¤ì •í•˜ë©´ ë‹¨ì¼ p5.48xlarge (8 GPU)ë¡œ ìš´ì˜ ê°€ëŠ¥í•©ë‹ˆë‹¤.
:::

### 5.6 ë°°í¬ í™•ì¸

```bash
# Helm ë¦´ë¦¬ì¦ˆ í™•ì¸
helm list -n ${NAMESPACE}

# ì „ì²´ ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl get all -n ${NAMESPACE}

# InferencePool ìƒíƒœ í™•ì¸
kubectl get inferencepool -n ${NAMESPACE}

# InferenceModel ìƒíƒœ í™•ì¸
kubectl get inferencemodel -n ${NAMESPACE}

# vLLM Pod ìƒíƒœ í™•ì¸ (GPU í• ë‹¹ í¬í•¨)
kubectl get pods -n ${NAMESPACE} -o wide

# Podê°€ Ready ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ëª¨ë¸ ë¡œë”©ì— 5-10ë¶„ ì†Œìš”)
kubectl wait --for=condition=Ready pods -l app=vllm \
  -n ${NAMESPACE} --timeout=600s
```

:::warning ëª¨ë¸ ë¡œë”© ì‹œê°„
Qwen3-32B (BF16, ~65GB)ëŠ” HuggingFace Hubì—ì„œ ìµœì´ˆ ë‹¤ìš´ë¡œë“œ ì‹œ ë„¤íŠ¸ì›Œí¬ ì†ë„ì— ë”°ë¼ 10-20ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´í›„ ë°°í¬ì—ì„œëŠ” ë…¸ë“œì˜ ë¡œì»¬ ìºì‹œë¥¼ í™œìš©í•˜ì—¬ ë¡œë”© ì‹œê°„ì´ í¬ê²Œ ë‹¨ì¶•ë©ë‹ˆë‹¤.
:::

---

## ì¶”ë¡  ìš”ì²­ í…ŒìŠ¤íŠ¸

### 6.1 í¬íŠ¸ í¬ì›Œë”©

```bash
# Inference Gateway í¬íŠ¸ í¬ì›Œë”©
kubectl port-forward svc/inference-gateway -n ${NAMESPACE} 8080:8080
```

### 6.2 curl ê¸°ë³¸ í…ŒìŠ¤íŠ¸

```bash
curl -s http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-32B",
    "messages": [
      {
        "role": "user",
        "content": "Kubernetesë€ ë¬´ì—‡ì¸ê°€ìš”? ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
      }
    ],
    "max_tokens": 256,
    "temperature": 0.7
  }' | jq .
```

ì˜ˆìƒ ì‘ë‹µ êµ¬ì¡°:
```json
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "model": "Qwen/Qwen3-32B",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "KubernetesëŠ” ì»¨í…Œì´ë„ˆí™”ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë°°í¬, ìŠ¤ì¼€ì¼ë§..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 128,
    "total_tokens": 143
  }
}
```

### 6.3 Python í´ë¼ì´ì–¸íŠ¸

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8080/v1",
    api_key="not-needed"  # llm-dëŠ” ë³„ë„ ì¸ì¦ ë¶ˆí•„ìš”
)

response = client.chat.completions.create(
    model="Qwen/Qwen3-32B",
    messages=[
        {"role": "system", "content": "ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "EKS Auto Modeì˜ ì¥ì ì„ 3ê°€ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
    ],
    max_tokens=512,
    temperature=0.7
)
print(response.choices[0].message.content)
```

### 6.4 ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í…ŒìŠ¤íŠ¸

```python
stream = client.chat.completions.create(
    model="Qwen/Qwen3-32B",
    messages=[
        {"role": "user", "content": "llm-dì˜ KV Cache-aware ë¼ìš°íŒ…ì´ ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?"}
    ],
    max_tokens=512,
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()
```

### 6.5 ëª¨ë¸ ëª©ë¡ í™•ì¸

```bash
curl -s http://localhost:8080/v1/models | jq .
```

:::info OpenAI í˜¸í™˜ API
llm-dëŠ” OpenAI í˜¸í™˜ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê¸°ì¡´ OpenAI SDKë¥¼ ì‚¬ìš©í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ `base_url`ë§Œ ë³€ê²½í•˜ë©´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `/v1/chat/completions`, `/v1/completions`, `/v1/models` ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
:::

---

## KV Cache-aware ë¼ìš°íŒ… ì´í•´

llm-dì˜ í•µì‹¬ ì°¨ë³„ì ì€ KV Cache ìƒíƒœë¥¼ ì¸ì‹í•˜ëŠ” ì§€ëŠ¥ì  ë¼ìš°íŒ…ì…ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant C as Client
    participant GW as Inference Gateway
    participant P1 as vLLM Pod 1<br/>(Cache: "K8së€")
    participant P2 as vLLM Pod 2<br/>(Cache: "AWSë€")
    participant P3 as vLLM Pod 3<br/>(Empty)
    
    Note over GW: KV Cache-aware Routing
    
    C->>GW: "K8së€ ë¬´ì—‡ì¸ê°€ìš”?"
    GW->>GW: Prefix ë§¤ì¹­ í™•ì¸
    GW->>P1: ë¼ìš°íŒ… (Cache Hit!)
    P1->>C: ë¹ ë¥¸ ì‘ë‹µ (TTFT â†“)
    
    C->>GW: "AWSë€ ë¬´ì—‡ì¸ê°€ìš”?"
    GW->>GW: Prefix ë§¤ì¹­ í™•ì¸
    GW->>P2: ë¼ìš°íŒ… (Cache Hit!)
    P2->>C: ë¹ ë¥¸ ì‘ë‹µ (TTFT â†“)
    
    C->>GW: "ì™„ì „íˆ ìƒˆë¡œìš´ ì§ˆë¬¸"
    GW->>GW: Cache Miss
    GW->>P3: Load Balancing í´ë°±
    P3->>C: ì¼ë°˜ ì‘ë‹µ
```

### ë¼ìš°íŒ… ë™ì‘ ì›ë¦¬

1. **ìš”ì²­ ìˆ˜ì‹ **: í´ë¼ì´ì–¸íŠ¸ê°€ Inference Gatewayë¡œ ì¶”ë¡  ìš”ì²­ ì „ì†¡
2. **Prefix ë¶„ì„**: Gatewayê°€ ìš”ì²­ì˜ prompt prefixë¥¼ í•´ì‹œí•˜ì—¬ ì‹ë³„
3. **Cache ì¡°íšŒ**: ê° vLLM Podì˜ KV Cache ìƒíƒœë¥¼ í™•ì¸í•˜ì—¬ í•´ë‹¹ prefixë¥¼ ë³´ìœ í•œ Pod íƒìƒ‰
4. **ì§€ëŠ¥ì  ë¼ìš°íŒ…**: Cache hit ì‹œ í•´ë‹¹ Podë¡œ ë¼ìš°íŒ…, miss ì‹œ ë¶€í•˜ ê¸°ë°˜ ë¡œë“œ ë°¸ëŸ°ì‹±
5. **ì‘ë‹µ ë°˜í™˜**: vLLMì´ ì¶”ë¡  ê²°ê³¼ë¥¼ Gatewayë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ì— ë°˜í™˜

### KV Cache-aware ë¼ìš°íŒ…ì˜ íš¨ê³¼

| ì§€í‘œ | Cache Miss (ê¸°ì¡´ ë°©ì‹) | Cache Hit (llm-d) | ê°œì„  íš¨ê³¼ |
| --- | --- | --- | --- |
| TTFT (Time To First Token) | ë†’ìŒ (ì „ì²´ prefill í•„ìš”) | ë‚®ìŒ (prefill ìŠ¤í‚µ) | 50-80% ë‹¨ì¶• |
| GPU ì—°ì‚°ëŸ‰ | ì „ì²´ prompt ì²˜ë¦¬ | ìƒˆë¡œìš´ í† í°ë§Œ ì²˜ë¦¬ | ì—°ì‚° ì ˆì•½ |
| ì²˜ë¦¬ëŸ‰ (Throughput) | ê¸°ë³¸ | í–¥ìƒ | 1.5-3x í–¥ìƒ |

:::tip Cache Hit Rate ê·¹ëŒ€í™”
ë™ì¼í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ KV Cache-aware ë¼ìš°íŒ…ì˜ íš¨ê³¼ê°€ ê·¹ëŒ€í™”ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ RAG íŒŒì´í”„ë¼ì¸ì—ì„œ ë™ì¼í•œ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë°˜ë³µ ì°¸ì¡°í•˜ëŠ” ê²½ìš°, í•´ë‹¹ prefixì˜ KV Cacheë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ TTFTë¥¼ í¬ê²Œ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

---

## ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦

### 8.1 vLLM ë©”íŠ¸ë¦­ í™•ì¸

```bash
# vLLM Podì˜ ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ ì ‘ê·¼
VLLM_POD=$(kubectl get pods -n ${NAMESPACE} -l app=vllm -o jsonpath='{.items[0].metadata.name}')
kubectl port-forward ${VLLM_POD} -n ${NAMESPACE} 9090:9090

# ë©”íŠ¸ë¦­ ì¡°íšŒ
curl -s http://localhost:9090/metrics | grep -E "vllm_"
```

### ì£¼ìš” ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ì •ìƒ ë²”ìœ„ |
| --- | --- | --- |
| `vllm_num_requests_running` | í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ìš”ì²­ ìˆ˜ | ì›Œí¬ë¡œë“œì— ë”°ë¼ ë‹¤ë¦„ |
| `vllm_num_requests_waiting` | ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ ìˆ˜ | < 50 |
| `vllm_gpu_cache_usage_perc` | GPU KV Cache ì‚¬ìš©ë¥  | 60-90% |
| `vllm_avg_generation_throughput_toks_per_s` | ì´ˆë‹¹ ìƒì„± í† í° ìˆ˜ | ëª¨ë¸/GPUì— ë”°ë¼ ë‹¤ë¦„ |
| `vllm_avg_prompt_throughput_toks_per_s` | ì´ˆë‹¹ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ í† í° ìˆ˜ | ëª¨ë¸/GPUì— ë”°ë¼ ë‹¤ë¦„ |
| `vllm_e2e_request_latency_seconds` | ìš”ì²­ ì „ì²´ ì§€ì—°ì‹œê°„ | P95 < 30s |

### 8.2 GPU í™œìš©ë¥  í™•ì¸

```bash
# íŠ¹ì • vLLM Podì—ì„œ nvidia-smi ì‹¤í–‰
kubectl exec -it ${VLLM_POD} -n ${NAMESPACE} -- nvidia-smi

# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§ (1ì´ˆ ê°„ê²©)
kubectl exec -it ${VLLM_POD} -n ${NAMESPACE} -- nvidia-smi dmon -s u -d 1
```

### 8.3 Gateway ë¡œê·¸ í™•ì¸

```bash
# Inference Gateway ë¡œê·¸ í™•ì¸
kubectl logs -f deployment/inference-gateway -n ${NAMESPACE}

# InferencePool ìƒíƒœ ìƒì„¸ í™•ì¸
kubectl describe inferencepool -n ${NAMESPACE}
```

### 8.4 Prometheus ServiceMonitor êµ¬ì„±

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-d-vllm-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: vllm
  endpoints:
    - port: metrics
      path: /metrics
      interval: 15s
  namespaceSelector:
    matchNames:
      - llm-d
```

---

## ìš´ì˜ ê³ ë ¤ì‚¬í•­

### 9.1 S3 ëª¨ë¸ ìºì‹±

HuggingFace Hubì—ì„œ ë§¤ë²ˆ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë©´ Cold Start ì‹œê°„ì´ ê¸¸ì–´ì§‘ë‹ˆë‹¤. S3ì— ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ìºì‹±í•˜ì—¬ ë¡œë”© ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
# vLLM í™˜ê²½ ë³€ìˆ˜ì— S3 ìºì‹œ ê²½ë¡œ ì¶”ê°€
env:
  - name: VLLM_S3_MODEL_CACHE
    value: "s3://your-bucket/model-cache/qwen3-32b/"
```

| ë¡œë”© ë°©ì‹ | ì˜ˆìƒ ì‹œê°„ | ë¹„ê³  |
| --- | --- | --- |
| HuggingFace Hub (ìµœì´ˆ) | 10-20ë¶„ | ë„¤íŠ¸ì›Œí¬ ì†ë„ì— ë”°ë¼ ë‹¤ë¦„ |
| S3 ìºì‹œ | 3-5ë¶„ | ê°™ì€ ë¦¬ì „ S3ì—ì„œ ë¡œë”© |
| ë…¸ë“œ ë¡œì»¬ ìºì‹œ | 1-2ë¶„ | ë™ì¼ ë…¸ë“œ ì¬ë°°í¬ ì‹œ |

### 9.2 HPA (Horizontal Pod Autoscaler) êµ¬ì„±

vLLM ëŒ€ê¸° ìš”ì²­ ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìë™ ìŠ¤ì¼€ì¼ë§ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-hpa
  namespace: llm-d
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-deployment
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Pods
      pods:
        metric:
          name: vllm_num_requests_waiting
        target:
          type: AverageValue
          averageValue: "5"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 120
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 180
```

:::info HPAì™€ Karpenter ì—°ë™
HPAê°€ vLLM replicaë¥¼ ì¦ê°€ì‹œí‚¤ë©´, ì¶”ê°€ GPUê°€ í•„ìš”í•œ ê²½ìš° Karpenterê°€ ìë™ìœ¼ë¡œ ìƒˆë¡œìš´ p5.48xlarge ë…¸ë“œë¥¼ í”„ë¡œë¹„ì €ë‹í•©ë‹ˆë‹¤. EKS Auto Modeì—ì„œëŠ” ì´ ê³¼ì •ì´ ì™„ì „íˆ ìë™í™”ë©ë‹ˆë‹¤.
:::

### 9.3 ë¹„ìš© ìµœì í™”

| ì „ëµ | ì„¤ëª… | ì˜ˆìƒ ì ˆê° |
| --- | --- | --- |
| Savings Plans | 1ë…„/3ë…„ Compute Savings Plans ì•½ì • | 30-60% |
| ë¹„í”¼í¬ ì‹œê°„ ìŠ¤ì¼€ì¼ ë‹¤ìš´ | ì•¼ê°„/ì£¼ë§ replicas ì¶•ì†Œ (CronJob í™œìš©) | 40-60% |
| ëª¨ë¸ ì–‘ìí™” | INT8/INT4ë¡œ GPU ìˆ˜ ì ˆê° | GPU ë¹„ìš© 50% |
| Spot Instances | ë‚´ê²°í•¨ì„± ì›Œí¬ë¡œë“œì— ì ìš© (ì¤‘ë‹¨ ìœ„í—˜ ìˆìŒ) | 60-90% |
| TP ìµœì í™” | ëª¨ë¸ í¬ê¸°ì— ë§ëŠ” ìµœì†Œ TP ê°’ ì‚¬ìš© | ë¶ˆí•„ìš”í•œ GPU ì ˆì•½ |

:::warning ë¹„ìš© ì£¼ì˜
p5.48xlargeëŠ” ì‹œê°„ë‹¹ ì•½ $98.32 (us-west-2 On-Demand ê¸°ì¤€)ì…ë‹ˆë‹¤. 2ëŒ€ ìš´ì˜ ì‹œ **ì›” ì•½ $141,580**ì…ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„ ë°˜ë“œì‹œ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•˜ì„¸ìš”.

```bash
# ë¦¬ì†ŒìŠ¤ ì •ë¦¬
helmfile destroy -n ${NAMESPACE}
kubectl delete namespace ${NAMESPACE}
kubectl delete nodepool gpu-p5

# í´ëŸ¬ìŠ¤í„° ì‚­ì œ (í•„ìš” ì‹œ)
eksctl delete cluster --name llm-d-cluster --region us-west-2
```
:::

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²° ë°©ë²•

| ì¦ìƒ | ì›ì¸ | í•´ê²° ë°©ë²• |
| --- | --- | --- |
| GPU ë…¸ë“œê°€ í”„ë¡œë¹„ì €ë‹ë˜ì§€ ì•ŠìŒ | Service Quotas ë¶€ì¡± | AWS Consoleì—ì„œ P instance ì¿¼í„° í™•ì¸ ë° ì¦ê°€ ìš”ì²­ |
| Podê°€ Pending ìƒíƒœ | NodePool ì„¤ì • ì˜¤ë¥˜ ë˜ëŠ” GPU ë¶€ì¡± | `kubectl describe pod`ë¡œ ì´ë²¤íŠ¸ í™•ì¸, NodePoolì˜ instance-family í™•ì¸ |
| CUDA OOM (Out of Memory) | GPU ë©”ëª¨ë¦¬ ë¶€ì¡± | TP ê°’ ì¦ê°€ ë˜ëŠ” `gpu-memory-utilization` ê°’ ë‚®ì¶”ê¸° (0.85) |
| ëª¨ë¸ ë¡œë”© íƒ€ì„ì•„ì›ƒ | HuggingFace ë‹¤ìš´ë¡œë“œ ëŠë¦¼ | S3 ëª¨ë¸ ìºì‹± í™œì„±í™”, `initialDelaySeconds` ì¦ê°€ |
| Gateway ë¼ìš°íŒ… ì‹¤íŒ¨ | CRD ë¯¸ì„¤ì¹˜ | Gateway API CRD ë° Inference Extension CRD ì„¤ì¹˜ í™•ì¸ |
| HuggingFace í† í° ì˜¤ë¥˜ | Secret ë¯¸ìƒì„± ë˜ëŠ” ê¶Œí•œ ë¶€ì¡± | `kubectl get secret -n llm-d` í™•ì¸, HF í† í° ê¶Œí•œ í™•ì¸ |
| NCCL í†µì‹  ì˜¤ë¥˜ | GPU ê°„ í†µì‹  ë¬¸ì œ | `NCCL_DEBUG=INFO` í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€, EFA ì§€ì› í™•ì¸ |
| InferencePoolì´ Readyê°€ ì•„ë‹˜ | vLLM Pod ë¯¸ì¤€ë¹„ | Pod ìƒíƒœ í™•ì¸, ëª¨ë¸ ë¡œë”© ì™„ë£Œ ëŒ€ê¸° |

### ë””ë²„ê¹… ëª…ë ¹ì–´ ëª¨ìŒ

```bash
# Pod ìƒíƒœ ë° ì´ë²¤íŠ¸ í™•ì¸
kubectl describe pod <pod-name> -n llm-d

# vLLM ë¡œê·¸ í™•ì¸ (ìµœê·¼ 100ì¤„)
kubectl logs <vllm-pod> -n llm-d --tail=100

# GPU ìƒíƒœ í™•ì¸
kubectl exec -it <vllm-pod> -n llm-d -- nvidia-smi

# InferencePool ìƒíƒœ ìƒì„¸ í™•ì¸
kubectl describe inferencepool -n llm-d

# InferenceModel ìƒíƒœ í™•ì¸
kubectl describe inferencemodel -n llm-d

# Gateway ë¡œê·¸ í™•ì¸
kubectl logs -f deployment/inference-gateway -n llm-d

# ë…¸ë“œ GPU ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl get nodes -o custom-columns=\
  NAME:.metadata.name,\
  GPU:.status.allocatable.nvidia\\.com/gpu,\
  STATUS:.status.conditions[-1].type

# Karpenter ë¡œê·¸ í™•ì¸ (ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ë¬¸ì œ)
kubectl logs -f deployment/karpenter -n kube-system
```

:::tip NCCL ë””ë²„ê¹…
ë©€í‹° GPU í†µì‹  ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ì—¬ ìƒì„¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”:
```yaml
env:
  - name: NCCL_DEBUG
    value: "INFO"
  - name: NCCL_DEBUG_SUBSYS
    value: "ALL"
```
:::

---

## ë‹¤ìŒ ë‹¨ê³„

ë³¸ ê°€ì´ë“œì—ì„œëŠ” llm-dì˜ Intelligent Inference Scheduling pathë¥¼ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ê³ ê¸‰ ê¸°ëŠ¥ì„ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **Prefill/Decode Disaggregation**: Prefillê³¼ Decode ë‹¨ê³„ë¥¼ ë³„ë„ Pod ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ëŒ€ê·œëª¨ ë°°ì¹˜ ì²˜ë¦¬ì™€ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì›Œí¬ë¡œë“œì˜ ì²˜ë¦¬ëŸ‰ì„ ê·¹ëŒ€í™”
- **Expert Parallelism**: MoE ëª¨ë¸(Mixtral, DeepSeek ë“±)ì˜ Expertë¥¼ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ì‚°í•˜ì—¬ ì´ˆëŒ€ê·œëª¨ ëª¨ë¸ ì„œë¹™
- **LoRA ì–´ëŒ‘í„° í•«ìŠ¤ì™‘**: ë‹¨ì¼ ê¸°ë³¸ ëª¨ë¸ì— ì—¬ëŸ¬ LoRA ì–´ëŒ‘í„°ë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ/ì–¸ë¡œë“œí•˜ì—¬ ë‹¤ì¤‘ íƒœìŠ¤í¬ ì„œë¹™
- **Prometheus + Grafana ëŒ€ì‹œë³´ë“œ**: vLLM ë©”íŠ¸ë¦­ ê¸°ë°˜ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„±
- **ë©€í‹° ëª¨ë¸ ì„œë¹™**: í•˜ë‚˜ì˜ llm-d í´ëŸ¬ìŠ¤í„°ì—ì„œ ì—¬ëŸ¬ ëª¨ë¸ì„ InferenceModel CRDë¡œ ë™ì‹œ ì„œë¹™

### ê´€ë ¨ ë¬¸ì„œ

- [vLLM ê¸°ë°˜ FM ë°°í¬ ë° ì„±ëŠ¥ ìµœì í™”](./vllm-model-serving.md) â€” vLLM ê¸°ë³¸ ê°œë… ë° ë°°í¬
- [MoE ëª¨ë¸ ì„œë¹™ ê°€ì´ë“œ](./moe-model-serving.md) â€” Mixture of Experts ëª¨ë¸ ì„œë¹™
- [Inference Gateway ë° ë™ì  ë¼ìš°íŒ…](./inference-gateway-routing.md) â€” ì¶”ë¡  ë¼ìš°íŒ… ì „ëµ
- [GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬](./gpu-resource-management.md) â€” GPU í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

---

## ì°¸ê³  ìë£Œ

- [llm-d ê³µì‹ ë¬¸ì„œ](https://llm-d.ai/docs)
- [llm-d GitHub](https://github.com/llm-d/llm-d)
- [llm-d Quickstart Guide](https://llm-d.ai/docs/guide/Installation/quickstart)
- [EKS Auto Mode ë¬¸ì„œ](https://docs.aws.amazon.com/eks/latest/userguide/automode.html)
- [Gateway API Inference Extension](https://gateway-api.sigs.k8s.io/geps/gep-3567/)
- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)
- [Qwen3-32B HuggingFace](https://huggingface.co/Qwen/Qwen3-32B)
- [Kubernetes Gateway API](https://gateway-api.sigs.k8s.io/)
