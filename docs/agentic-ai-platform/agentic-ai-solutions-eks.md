---
title: "EKS ê¸°ë°˜ Agentic AI í•´ê²°ë°©ì•ˆ"
sidebar_label: "2. EKS ê¸°ë°˜ í•´ê²°ë°©ì•ˆ"
description: "Amazon EKSì™€ AWS ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•œ Agentic AI ë„ì „ê³¼ì œ í•´ê²° ê°€ì´ë“œ"
tags: [eks, aws, karpenter, genai, agentic-ai, gpu, solutions]
category: "genai-aiml"
last_update:
  date: 2025-02-05
  author: devfloor9
sidebar_position: 3
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

> ğŸ“… **ì‘ì„±ì¼**: 2025-02-05 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 20ë¶„

:::info ì„ í–‰ ë¬¸ì„œ: ê¸°ìˆ ì  ë„ì „ê³¼ì œ
ì´ ë¬¸ì„œë¥¼ ì½ê¸° ì „ì— [Agentic AI ì›Œí¬ë¡œë“œì˜ ê¸°ìˆ ì  ë„ì „ê³¼ì œ](./agentic-ai-challenges.md)ë¥¼ ë¨¼ì € ì½ì–´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤. í•´ë‹¹ ë¬¸ì„œì—ì„œ 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œì™€ Kubernetes ê¸°ë°˜ ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.
:::

## ê°œìš”

Agentic AI í”Œë«í¼ì˜ 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œ(GPU ëª¨ë‹ˆí„°ë§, ë™ì  ìŠ¤ì¼€ì¼ë§, ë¹„ìš© ì»¨íŠ¸ë¡¤, FM íŒŒì¸íŠœë‹)ëŠ” **Amazon EKSì™€ AWS ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ì˜ í†µí•©**ì„ í†µí•´ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ë¬¸ì„œì—ì„œëŠ” **EKS Auto Mode + Karpenter ì¤‘ì‹¬ì˜ êµ¬ì²´ì ì¸ í•´ê²° ë°©ì•ˆ**ê³¼ AWS ì¸í”„ë¼ì™€ì˜ í†µí•© ì•„í‚¤í…ì²˜ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

---

## Amazon EKSì™€ Karpenter: Kubernetesì˜ ì¥ì  ê·¹ëŒ€í™”

Kubernetesê°€ AI í”Œë«í¼ì˜ ê¸°ë°˜ì´ë¼ë©´, **Amazon EKSì™€ Karpenterì˜ ì¡°í•©**ì€ Kubernetesì˜ ì¥ì ì„ ê·¹ëŒ€í™”í•˜ì—¬ **ì™„ì „ ìë™í™”ëœ ìµœì ì˜ ì¸í”„ë¼**ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### EKS + Karpenter + AWS ì¸í”„ë¼ í†µí•© ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "AWS ê´€ë¦¬í˜• ì„œë¹„ìŠ¤"
        EKS["Amazon EKS<br/>ê´€ë¦¬í˜• Kubernetes"]
        EC2["Amazon EC2<br/>GPU ì¸ìŠ¤í„´ìŠ¤"]
        S3["Amazon S3<br/>ëª¨ë¸ ìŠ¤í† ë¦¬ì§€"]
        CW["CloudWatch<br/>í†µí•© ëª¨ë‹ˆí„°ë§"]
    end

    subgraph "Karpenter ìë™í™” ê³„ì¸µ"
        KARP["Karpenter Controller"]
        NP1["GPU Inference NodePool"]
        NP2["GPU Training NodePool"]
        NP3["Spot NodePool"]
    end

    subgraph "AI ì›Œí¬ë¡œë“œ"
        INF["ì¶”ë¡  ì„œë¹„ìŠ¤"]
        TRAIN["í•™ìŠµ ì‘ì—…"]
        BATCH["ë°°ì¹˜ ì²˜ë¦¬"]
    end

    EKS --> KARP
    KARP --> NP1 & NP2 & NP3
    NP1 --> EC2
    NP2 --> EC2
    NP3 --> EC2
    NP1 --> INF
    NP2 --> TRAIN
    NP3 --> BATCH
    INF & TRAIN --> S3
    INF & TRAIN --> CW

    style EKS fill:#ff9900
    style KARP fill:#ffd93d
    style EC2 fill:#ff9900
```

### ì™œ EKS + Karpenterì¸ê°€?

| ê³„ì¸µ | ì—­í•  | ì œê³µ ê°€ì¹˜ |
| --- | --- | --- |
| **Amazon EKS** | ê´€ë¦¬í˜• Kubernetes Control Plane | ìš´ì˜ ë¶€ë‹´ ì œê±°, ê³ ê°€ìš©ì„±, ë³´ì•ˆ |
| **Karpenter** | ì§€ëŠ¥í˜• ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ | Just-in-Time GPU í”„ë¡œë¹„ì €ë‹, ë¹„ìš© ìµœì í™” |
| **AWS ì¸í”„ë¼** | GPU ì¸ìŠ¤í„´ìŠ¤, ìŠ¤í† ë¦¬ì§€, ë„¤íŠ¸ì›Œí¬ | ë‹¤ì–‘í•œ GPU ì˜µì…˜, EFA ê³ ì† ë„¤íŠ¸ì›Œí¬, Spot ì¸ìŠ¤í„´ìŠ¤ |

### Karpenter: AI ì¸í”„ë¼ ìë™í™”ì˜ í•µì‹¬

KarpenterëŠ” ê¸°ì¡´ Cluster Autoscalerì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , **AI ì›Œí¬ë¡œë“œì— ìµœì í™”ëœ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹**ì„ ì œê³µí•©ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant Pod as Pod (Pending)
    participant CA as Cluster Autoscaler
    participant ASG as Auto Scaling Group
    participant EC2 as EC2 Instance

    rect rgb(255, 235, 235)
    Note over Pod,EC2: ê¸°ì¡´ ë°©ì‹ (Cluster Autoscaler) - 5~10ë¶„ ì†Œìš”
    Pod->>CA: 1. Pending ê°ì§€
    CA->>CA: 2. Node Group í™•ì¸
    CA->>ASG: 3. ìŠ¤ì¼€ì¼ ì•„ì›ƒ ìš”ì²­
    ASG->>EC2: 4. ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘
    Note right of EC2: AMI ë¶€íŒ… ëŒ€ê¸°
    EC2->>EC2: 5. ë…¸ë“œ ì¤€ë¹„
    EC2->>Pod: 6. Pod ìŠ¤ì¼€ì¤„ë§
    end
```

```mermaid
sequenceDiagram
    participant Pod as Pod (Pending)
    participant Karp as Karpenter
    participant EC2 as EC2 API

    rect rgb(235, 255, 245)
    Note over Pod,EC2: Karpenter ë°©ì‹ - 2~3ë¶„ ì†Œìš”
    Pod->>Karp: 1. Pending ê°ì§€
    Karp->>Karp: 2. ì›Œí¬ë¡œë“œ ë¶„ì„ & ìµœì  ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ
    Karp->>EC2: 3. EC2 API ì§ì ‘ í˜¸ì¶œ
    EC2->>Pod: 4. ì¦‰ì‹œ í”„ë¡œë¹„ì €ë‹ & ìŠ¤ì¼€ì¤„ë§
    end
```

| ë¹„êµ í•­ëª© | Cluster Autoscaler | Karpenter |
|----------|-------------------|-----------|
| **í”„ë¡œë¹„ì €ë‹ ì‹œê°„** | 5-10ë¶„ | 2-3ë¶„ |
| **ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ** | Node Group ë‚´ ê³ ì • íƒ€ì… | ì›Œí¬ë¡œë“œ ê¸°ë°˜ ë™ì  ì„ íƒ |
| **GPU ì§€ì›** | ìˆ˜ë™ Node Group êµ¬ì„± | NodePool ìë™ ë§¤ì¹­ |
| **ë¹„ìš© ìµœì í™”** | ì œí•œì  | Spot, Consolidation ìë™ |

### Karpenterê°€ ì œê³µí•˜ëŠ” í•µì‹¬ ê°€ì¹˜

| ê¸°ëŠ¥ | ì„¤ëª… | Agentic AI ì ìš© |
| --- | --- | --- |
| **Just-in-Time í”„ë¡œë¹„ì €ë‹** | ì›Œí¬ë¡œë“œ ìš”êµ¬ì— ë”°ë¼ ì¦‰ì‹œ ë…¸ë“œ ìƒì„± | GPU ë…¸ë“œ ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™” |
| **Spot ì¸ìŠ¤í„´ìŠ¤ ì§€ì›** | ìµœëŒ€ 90% ë¹„ìš© ì ˆê° | ì¶”ë¡  ì›Œí¬ë¡œë“œ ë¹„ìš© ìµœì í™” |
| **Consolidation** | ìœ íœ´ ë…¸ë“œ ìë™ ì •ë¦¬ | GPU ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± ê·¹ëŒ€í™” |
| **ë‹¤ì–‘í•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…** | ì›Œí¬ë¡œë“œì— ìµœì í™”ëœ ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ | ëª¨ë¸ í¬ê¸°ë³„ ìµœì  GPU ë§¤ì¹­ |
| **Disruption Budgets** | ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”í•˜ë©° ë…¸ë“œ ê´€ë¦¬ | ì•ˆì •ì ì¸ ìŠ¤ì¼€ì¼ ë‹¤ìš´ |

### EKS Auto Mode: ì™„ì „ ìë™í™”ì˜ ì™„ì„±

**EKS Auto Mode**ëŠ” Karpenterë¥¼ í¬í•¨í•œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ìë™ìœ¼ë¡œ êµ¬ì„±í•˜ê³  ê´€ë¦¬í•˜ì—¬, AI ì¸í”„ë¼ ìë™í™”ì˜ ë§ˆì§€ë§‰ í¼ì¦ì„ ì™„ì„±í•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "EKS Auto Modeê°€ ìë™ ê´€ë¦¬"
        AUTO["EKS Auto Mode"]
        KARP["Karpenter<br/>(ìë™ êµ¬ì„±)"]
        VPC_CNI["VPC CNI<br/>(ìë™ êµ¬ì„±)"]
        CSI["EBS CSI Driver<br/>(ìë™ êµ¬ì„±)"]
        COREDNS["CoreDNS<br/>(ìë™ êµ¬ì„±)"]
        POD_ID["Pod Identity Agent<br/>(ìë™ êµ¬ì„±)"]
    end

    subgraph "ì‚¬ìš©ì ì •ì˜ ì˜ì—­"
        NP["Custom NodePool<br/>(GPU ìµœì í™”)"]
        NC["Custom NodeClass<br/>(EFA, ìŠ¤í† ë¦¬ì§€)"]
        WL["AI ì›Œí¬ë¡œë“œ"]
    end

    AUTO --> KARP
    AUTO --> VPC_CNI
    AUTO --> CSI
    AUTO --> COREDNS
    AUTO --> POD_ID
    KARP --> NP
    NP --> NC
    NC --> WL

    style AUTO fill:#ff9900
    style KARP fill:#ffd93d
```

#### EKS Auto Mode vs ìˆ˜ë™ êµ¬ì„± ë¹„êµ

| êµ¬ì„± ìš”ì†Œ | ìˆ˜ë™ êµ¬ì„± (EKS Standard) | EKS Auto Mode |
| --- | --- | --- |
| **Karpenter ì„¤ì¹˜** | Helm ì°¨íŠ¸ ìˆ˜ë™ ì„¤ì¹˜, IAM ì—­í•  êµ¬ì„± | âœ… ìë™ ì„¤ì¹˜ ë° êµ¬ì„± |
| **NodePool ê´€ë¦¬** | ì§ì ‘ ì •ì˜ í•„ìš” | ê¸°ë³¸ ì œê³µ + ì»¤ìŠ¤í…€ ê°€ëŠ¥ |
| **VPC CNI** | ìˆ˜ë™ ì„¤ì¹˜ ë° ì—…ê·¸ë ˆì´ë“œ | âœ… ìë™ ê´€ë¦¬ |
| **EBS CSI Driver** | ìˆ˜ë™ ì„¤ì¹˜, IRSA êµ¬ì„± | âœ… ìë™ ê´€ë¦¬ |
| **CoreDNS** | ìˆ˜ë™ ìŠ¤ì¼€ì¼ë§ | âœ… ìë™ ìŠ¤ì¼€ì¼ë§ |
| **ë³´ì•ˆ íŒ¨ì¹˜** | ìˆ˜ë™ ì ìš© | âœ… ìë™ ì ìš© |
| **ë²„ì „ ì—…ê·¸ë ˆì´ë“œ** | ìˆ˜ë™ ê³„íš ë° ì‹¤í–‰ | âœ… ìë™ ì—…ê·¸ë ˆì´ë“œ |

#### EKS Auto Modeì˜ AI ì›Œí¬ë¡œë“œ ì´ì 

```mermaid
sequenceDiagram
    participant User as í”Œë«í¼ ì—”ì§€ë‹ˆì–´
    participant Auto as EKS Auto Mode
    participant Karp as Karpenter (ìë™ ê´€ë¦¬)
    participant EC2 as AWS EC2

    Note over User,EC2: EKS Auto Mode í´ëŸ¬ìŠ¤í„° ìƒì„±
    User->>Auto: í´ëŸ¬ìŠ¤í„° ìƒì„± ìš”ì²­
    Auto->>Auto: Karpenter ìë™ ì„¤ì¹˜
    Auto->>Auto: ê¸°ë³¸ NodePool êµ¬ì„±
    Auto-->>User: í´ëŸ¬ìŠ¤í„° ì¤€ë¹„ ì™„ë£Œ

    Note over User,EC2: GPU ì›Œí¬ë¡œë“œ ë°°í¬
    User->>Auto: GPU Pod ë°°í¬
    Auto->>Karp: Pending Pod ê°ì§€
    Karp->>EC2: GPU ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œë¹„ì €ë‹
    EC2-->>Karp: p4d.24xlarge ì¤€ë¹„
    Karp-->>User: Pod ì‹¤í–‰ ì¤‘

    Note over User,EC2: ìë™ ìµœì í™”
    Karp->>Karp: Consolidation ì‹¤í–‰
    Karp->>EC2: ìœ íœ´ ë…¸ë“œ ì •ë¦¬
```

#### GPU ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ EKS Auto Mode ì„¤ì •

EKS Auto Modeì—ì„œ GPU ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ NodePoolì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
# EKS Auto Modeì—ì„œ GPU NodePool ì¶”ê°€
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-inference
        eks-auto-mode: "true"
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.xlarge
            - g5.2xlarge
            - g5.4xlarge
            - g5.12xlarge
            - p4d.24xlarge
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default  # EKS Auto Mode ê¸°ë³¸ NodeClass í™œìš©
  limits:
    nvidia.com/gpu: 50
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
```

:::tip EKS Auto Mode ê¶Œì¥ ì‚¬í•­
EKS Auto ModeëŠ” **ìƒˆë¡œìš´ AI í”Œë«í¼ êµ¬ì¶• ì‹œ ê¶Œì¥ë˜ëŠ” ì˜µì…˜**ì…ë‹ˆë‹¤.

- Karpenter ì„¤ì¹˜ ë° êµ¬ì„± ìë™í™”ë¡œ **ì´ˆê¸° êµ¬ì¶• ì‹œê°„ 80% ë‹¨ì¶•**
- í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ìë™ ì—…ê·¸ë ˆì´ë“œë¡œ **ìš´ì˜ ë¶€ë‹´ ëŒ€í­ ê°ì†Œ**
- GPU NodePoolë§Œ ì»¤ìŠ¤í…€ ì •ì˜í•˜ë©´ **ì¦‰ì‹œ AI ì›Œí¬ë¡œë“œ ë°°í¬ ê°€ëŠ¥**
:::

:::info EKS Auto Modeì™€ GPU ì§€ì›
EKS Auto ModeëŠ” NVIDIA GPUë¥¼ í¬í•¨í•œ ê°€ì† ì»´í“¨íŒ… ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì™„ë²½íˆ ì§€ì›í•©ë‹ˆë‹¤. ê¸°ë³¸ NodeClassì— GPU ë“œë¼ì´ë²„ê°€ í¬í•¨ëœ AMIê°€ ìë™ìœ¼ë¡œ ì„ íƒë˜ë©°, í•„ìš”ì‹œ ì»¤ìŠ¤í…€ NodeClassë¡œ EFA ë„¤íŠ¸ì›Œí¬ ë“± ê³ ê¸‰ ì„¤ì •ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

### Karpenter vs Cluster Autoscaler ìƒì„¸ ë¹„êµ

:::tip Karpenter vs Cluster Autoscaler
KarpenterëŠ” Node Group ì—†ì´ ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­ì„ ì§ì ‘ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. GPU ì›Œí¬ë¡œë“œì˜ ê²½ìš° í”„ë¡œë¹„ì €ë‹ ì‹œê°„ì´ **50% ì´ìƒ ë‹¨ì¶•**ë˜ê³ , Consolidationì„ í†µí•´ **ë¹„ìš©ì´ 20-30% ì ˆê°**ë©ë‹ˆë‹¤.
:::

### ë„ì „ê³¼ì œë³„ Karpenter í•´ê²° ë°©ì•ˆ ë§¤í•‘

```mermaid
graph TB
    subgraph "4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œ"
        C1["ğŸ–¥ï¸ GPU ëª¨ë‹ˆí„°ë§ ë°<br/>ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§"]
        C2["ğŸ”€ Agentic AI ìš”ì²­<br/>ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§"]
        C3["ğŸ“Š í† í°/ì„¸ì…˜ ìˆ˜ì¤€<br/>ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© ì»¨íŠ¸ë¡¤"]
        C4["ğŸ”§ FM íŒŒì¸íŠœë‹ê³¼<br/>ìë™í™” íŒŒì´í”„ë¼ì¸"]
    end

    subgraph "Karpenter ì¤‘ì‹¬ í•´ê²° ë°©ì•ˆ"
        S1["â­ Karpenter NodePool<br/>GPU ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ"]
        S2["Karpenter + KEDA<br/>End-to-End ìë™ ìŠ¤ì¼€ì¼ë§"]
        S3["Spot + Consolidation<br/>ë¹„ìš© 50-70% ì ˆê°"]
        S4["Training NodePool<br/>EFA ë„¤íŠ¸ì›Œí¬ ìµœì í™”"]
    end

    subgraph "ë³´ì¡° ì†”ë£¨ì…˜"
        A1["DCGM Exporter<br/>GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘"]
        A2["Gateway API<br/>ë™ì  ë¼ìš°íŒ…"]
        A3["LangFuse<br/>í† í° ì¶”ì "]
        A4["NeMo + Kubeflow<br/>í•™ìŠµ íŒŒì´í”„ë¼ì¸"]
    end

    C1 --> S1
    C2 --> S2
    C3 --> S3
    C4 --> S4
    S1 --> A1
    S2 --> A2
    S3 --> A3
    S4 --> A4

    style C1 fill:#ff6b6b
    style C2 fill:#4ecdc4
    style C3 fill:#45b7d1
    style C4 fill:#96ceb4
    style S1 fill:#ffd93d
    style S2 fill:#ffd93d
    style S3 fill:#ffd93d
    style S4 fill:#ffd93d
```

:::info ëŒ€ìƒ ë…ì
ì´ ë¬¸ì„œëŠ” Agentic AI Platform ë„ì…ì„ ê²€í† í•˜ëŠ” **ê¸°ìˆ  ì˜ì‚¬ê²°ì •ì**ì™€ **ì†”ë£¨ì…˜ ì•„í‚¤í…íŠ¸**ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤. Kubernetes ê¸°ë°˜ AI ì¸í”„ë¼ì˜ í•„ìš”ì„±ê³¼ EKS + Karpenterë¥¼ í™œìš©í•œ êµ¬ì²´ì ì¸ êµ¬í˜„ ë°©ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.
:::

---

## ë„ì „ê³¼ì œ 1: GPU ëª¨ë‹ˆí„°ë§ ë° ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§

### Karpenter ê¸°ë°˜ í•´ê²° ë°©ì•ˆ

**Karpenter NodePool**ì„ í™œìš©í•˜ë©´ GPU ì›Œí¬ë¡œë“œì— ìµœì í™”ëœ ë…¸ë“œë¥¼ ìë™ìœ¼ë¡œ í”„ë¡œë¹„ì €ë‹í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<Tabs>
<TabItem value="nodepool" label="GPU NodePool ì„¤ì •" default>

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-inference
        workload: genai
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge    # 8x A100 40GB
            - p5.48xlarge     # 8x H100 80GB
            - g5.48xlarge     # 8x A10G 24GB
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 100
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
  weight: 100
```

</TabItem>
<TabItem value="nodeclass" label="EC2NodeClass ì„¤ì •">

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu-nodeclass
spec:
  role: KarpenterNodeRole-${CLUSTER_NAME}
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 500Gi
        volumeType: gp3
        iops: 10000
        throughput: 500
        encrypted: true
  instanceStorePolicy: RAID0
  userData: |
    #!/bin/bash
    nvidia-smi -pm 1
    modprobe efa
```

</TabItem>
</Tabs>

### Karpenterì˜ GPU ì›Œí¬ë¡œë“œ ìµœì í™” ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì„¤ëª… | íš¨ê³¼ |
| --- | --- | --- |
| ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìë™ ì„ íƒ | ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” GPU ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ | ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ë°©ì§€ |
| Spot ì¸ìŠ¤í„´ìŠ¤ í´ë°± | Spot ë¶ˆê°€ ì‹œ On-Demandë¡œ ìë™ ì „í™˜ | ê°€ìš©ì„± ë³´ì¥ |
| Consolidation | ìœ íœ´ GPU ë…¸ë“œ ìë™ ì •ë¦¬ | ë¹„ìš© 30% ì ˆê° |
| ë¹ ë¥¸ í”„ë¡œë¹„ì €ë‹ | Node Group ì—†ì´ ì§ì ‘ EC2 API í˜¸ì¶œ | í”„ë¡œë¹„ì €ë‹ ì‹œê°„ 50% ë‹¨ì¶• |

### ë³´ì¡° ì†”ë£¨ì…˜: NVIDIA GPU Operator

Karpenterì™€ í•¨ê»˜ NVIDIA GPU Operatorë¥¼ ì‚¬ìš©í•˜ì—¬ GPU ë“œë¼ì´ë²„ ë° ëª¨ë‹ˆí„°ë§ ìŠ¤íƒì„ ìë™í™”í•©ë‹ˆë‹¤.

```yaml
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: cluster-policy
spec:
  operator:
    defaultRuntime: containerd
  driver:
    enabled: true
    version: "535.104.05"
  toolkit:
    enabled: true
  devicePlugin:
    enabled: true
  dcgmExporter:
    enabled: true
  migManager:
    enabled: true
```

---

## ë„ì „ê³¼ì œ 2: Agentic AI ìš”ì²­ ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§

### Karpenter + KEDA ì—°ë™ í•´ê²° ë°©ì•ˆ

Karpenterì™€ KEDAë¥¼ ì—°ë™í•˜ë©´ **ì›Œí¬ë¡œë“œ ìŠ¤ì¼€ì¼ë§ê³¼ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ì´ ìë™ìœ¼ë¡œ ì—°ê³„**ë©ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì íŠ¸ë˜í”½
    participant KEDA as KEDA Controller
    participant HPA as HPA
    participant Karpenter as Karpenter
    participant AWS as AWS EC2

    User->>KEDA: íŠ¸ë˜í”½ ê¸‰ì¦ ê°ì§€
    KEDA->>HPA: Pod ìŠ¤ì¼€ì¼ ì•„ì›ƒ íŠ¸ë¦¬ê±°
    HPA->>Karpenter: Pending Pod ê°ì§€
    Karpenter->>AWS: ìµœì  GPU ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œë¹„ì €ë‹
    AWS-->>Karpenter: p4d.24xlarge ì¤€ë¹„ ì™„ë£Œ
    Karpenter-->>HPA: ìƒˆ ë…¸ë“œì— Pod ìŠ¤ì¼€ì¤„ë§
    HPA-->>User: ì‘ë‹µ ì§€ì—° ì‹œê°„ ì •ìƒí™”
```

<Tabs>
<TabItem value="keda" label="KEDA ScaledObject" default>

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: vllm-gpu-scaler
  namespace: ai-inference
spec:
  scaleTargetRef:
    name: vllm-deployment
  minReplicaCount: 2
  maxReplicaCount: 20
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.observability:9090
        metricName: vllm_pending_requests
        threshold: "50"
        query: |
          sum(vllm_pending_requests{namespace="ai-inference"})
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.observability:9090
        metricName: gpu_utilization
        threshold: "70"
        query: |
          avg(DCGM_FI_DEV_GPU_UTIL{namespace="ai-inference"})
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
        scaleDown:
          stabilizationWindowSeconds: 300
```

</TabItem>
<TabItem value="httproute" label="Gateway API HTTPRoute">

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: ai-model-routing
  namespace: ai-inference
spec:
  parentRefs:
    - name: ai-gateway
      namespace: ai-gateway
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /v1/chat/completions
          headers:
            - name: x-model-id
              value: "gpt-4"
      backendRefs:
        - name: vllm-gpt4
          port: 8000
          weight: 80
        - name: vllm-gpt4-canary
          port: 8000
          weight: 20
    - matches:
        - path:
            type: PathPrefix
            value: /v1/chat/completions
          headers:
            - name: x-model-id
              value: "claude-3"
      backendRefs:
        - name: vllm-claude
          port: 8000
```

</TabItem>
</Tabs>

### Karpenter Disruption ì •ì±…ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´

íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œì—ë„ ì„œë¹„ìŠ¤ ì•ˆì •ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ Karpenter ì„¤ì •ì…ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-stable
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
    budgets:
      # ë™ì‹œì— ì¤‘ë‹¨ ê°€ëŠ¥í•œ ë…¸ë“œ ìˆ˜ ì œí•œ
      - nodes: "20%"
      # ì—…ë¬´ ì‹œê°„ì—ëŠ” ì¤‘ë‹¨ ë°©ì§€
      - nodes: "0"
        schedule: "0 9 * * 1-5"
        duration: 10h
```

:::warning ìŠ¤ì¼€ì¼ë§ ì£¼ì˜ì‚¬í•­
GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ì€ ì¼ë°˜ CPU ë…¸ë“œë³´ë‹¤ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. Karpenterì˜ `consolidationPolicy`ë¥¼ ì ì ˆíˆ ì„¤ì •í•˜ì—¬ ë¶ˆí•„ìš”í•œ ìŠ¤ì¼€ì¼ ë‹¤ìš´ì„ ë°©ì§€í•˜ì„¸ìš”.
:::

---

## ë„ì „ê³¼ì œ 3: í† í°/ì„¸ì…˜ ìˆ˜ì¤€ ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© ì»¨íŠ¸ë¡¤

### Karpenter ê¸°ë°˜ ë¹„ìš© ìµœì í™” ì „ëµ

KarpenterëŠ” GPU ì¸í”„ë¼ ë¹„ìš© ìµœì í™”ì˜ **í•µì‹¬ ë ˆë²„**ì…ë‹ˆë‹¤. ë‹¤ìŒ 4ê°€ì§€ ì „ëµì„ ì¡°í•©í•˜ì—¬ ìµœëŒ€ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ì „ëµ 1: Spot ì¸ìŠ¤í„´ìŠ¤ ìš°ì„  í™œìš©

Karpenterì˜ Spot ì¸ìŠ¤í„´ìŠ¤ ì§€ì›ì„ í™œìš©í•˜ë©´ GPU ë¹„ìš©ì„ **ìµœëŒ€ 90%ê¹Œì§€ ì ˆê°**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-spot-inference
spec:
  template:
    metadata:
      labels:
        cost-tier: spot
        workload: inference
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.12xlarge
            - g5.24xlarge
            - g5.48xlarge
            - p4d.24xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-spot-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: karpenter.sh/capacity-type
          value: "spot"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 32
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 30s
  weight: 50  # On-Demandë³´ë‹¤ ìš°ì„  ì„ íƒ
```

#### ì „ëµ 2: ì‹œê°„ëŒ€ë³„ ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ë¹„ìš© ê´€ë¦¬

ì—…ë¬´ ì‹œê°„ê³¼ ë¹„ì—…ë¬´ ì‹œê°„ì— ë”°ë¥¸ ì°¨ë³„í™”ëœ ë¦¬ì†ŒìŠ¤ ì •ì±…ì„ ì ìš©í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-scheduled-pool
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.12xlarge
            - g5.24xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodeclass
  limits:
    nvidia.com/gpu: 16
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
    budgets:
      # ì—…ë¬´ ì‹œê°„: ì•ˆì •ì„± ìš°ì„  (ë…¸ë“œ ì¤‘ë‹¨ ìµœì†Œí™”)
      - nodes: "10%"
        schedule: "0 9 * * 1-5"
        duration: 9h
      # ë¹„ì—…ë¬´ ì‹œê°„: ë¹„ìš© ìš°ì„  (ì ê·¹ì  í†µí•©)
      - nodes: "50%"
        schedule: "0 18 * * 1-5"
        duration: 15h
      # ì£¼ë§: ìµœì†Œ ë¦¬ì†ŒìŠ¤ ìœ ì§€
      - nodes: "80%"
        schedule: "0 0 * * 0,6"
        duration: 24h
```

#### ì „ëµ 3: Consolidationì„ í†µí•œ ìœ íœ´ ë¦¬ì†ŒìŠ¤ ì œê±°

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-consolidation-pool
spec:
  disruption:
    # ë…¸ë“œê°€ ë¹„ì–´ìˆê±°ë‚˜ í™œìš©ë„ê°€ ë‚®ì„ ë•Œ í†µí•©
    consolidationPolicy: WhenEmptyOrUnderutilized
    # ë¹ ë¥¸ í†µí•©ìœ¼ë¡œ ë¹„ìš© ì ˆê° (30ì´ˆ ëŒ€ê¸° í›„ í†µí•©)
    consolidateAfter: 30s
```

#### ì „ëµ 4: ì›Œí¬ë¡œë“œë³„ ì¸ìŠ¤í„´ìŠ¤ ìµœì í™”

```yaml
# ì†Œê·œëª¨ ëª¨ë¸ìš© (7B ì´í•˜) - ë¹„ìš© íš¨ìœ¨ì 
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-small-models
spec:
  template:
    spec:
      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.xlarge      # 1x A10G - $1.01/hr
            - g5.2xlarge     # 1x A10G - $1.21/hr
  weight: 100  # ìµœìš°ì„  ì„ íƒ

---
# ëŒ€ê·œëª¨ ëª¨ë¸ìš© (70B+) - ì„±ëŠ¥ ìš°ì„ 
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-large-models
spec:
  template:
    spec:
      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge   # 8x A100 - $32.77/hr
            - p5.48xlarge    # 8x H100 - $98.32/hr
  weight: 10   # í•„ìš”ì‹œì—ë§Œ ì„ íƒ
```

### ë¹„ìš© ìµœì í™” ì „ëµ ë¹„êµ

| ì „ëµ | êµ¬í˜„ ë°©ë²• | ì˜ˆìƒ ì ˆê°ë¥  | ì ìš© ì›Œí¬ë¡œë“œ | ìœ„í—˜ë„ |
| --- | --- | --- | --- | --- |
| Spot ì¸ìŠ¤í„´ìŠ¤ | Karpenter NodePool | 60-90% | ì¶”ë¡ , ë°°ì¹˜ ì²˜ë¦¬ | ì¤‘ê°„ (ì¤‘ë‹¨ ê°€ëŠ¥) |
| Consolidation | Karpenter disruption | 20-30% | ëª¨ë“  ì›Œí¬ë¡œë“œ | ë‚®ìŒ |
| Right-sizing | Karpenter ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ | 15-25% | ëª¨ë“  ì›Œí¬ë¡œë“œ | ë‚®ìŒ |
| ìŠ¤ì¼€ì¤„ ê¸°ë°˜ | Karpenter budgets | 30-40% | ë¹„ì—…ë¬´ ì‹œê°„ | ë‚®ìŒ |
| ë³µí•© ì ìš© | ìœ„ ì „ëµ ì¡°í•© | 50-70% | ì „ì²´ | ì¤‘ê°„ |

### ë³´ì¡° ì†”ë£¨ì…˜: LangFuse ê¸°ë°˜ í† í° ì¶”ì 

ì¸í”„ë¼ ë¹„ìš©ê³¼ í•¨ê»˜ í† í° ë ˆë²¨ ë¹„ìš©ë„ ì¶”ì í•´ì•¼ ì™„ì „í•œ ë¹„ìš© ê°€ì‹œì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langfuse
  namespace: observability
spec:
  replicas: 2
  selector:
    matchLabels:
      app: langfuse
  template:
    metadata:
      labels:
        app: langfuse
    spec:
      containers:
        - name: langfuse
          image: langfuse/langfuse:latest
          ports:
            - containerPort: 3000
          env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: database-url
            - name: NEXTAUTH_SECRET
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: nextauth-secret
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
```

### ë¹„ìš© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„±

```yaml
# Prometheus ë¹„ìš© ê´€ë ¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê·œì¹™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: gpu-cost-rules
  namespace: monitoring
spec:
  groups:
    - name: gpu-cost
      rules:
        - record: gpu:hourly_cost:sum
          expr: |
            sum(
              karpenter_nodes_total_pod_requests{resource_type="nvidia.com/gpu"}
              * on(instance_type) group_left()
              aws_ec2_instance_hourly_cost
            )
        - alert: HighGPUCostAlert
          expr: gpu:hourly_cost:sum > 100
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "ì‹œê°„ë‹¹ GPU ë¹„ìš©ì´ $100ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤"
```

:::tip ë¹„ìš© ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

1. **Spot ì¸ìŠ¤í„´ìŠ¤ ë¹„ìœ¨**: ì¶”ë¡  ì›Œí¬ë¡œë“œì˜ 70% ì´ìƒì„ Spotìœ¼ë¡œ ìš´ì˜
2. **Consolidation í™œì„±í™”**: 30ì´ˆ ì´ë‚´ ìœ íœ´ ë…¸ë“œ ì •ë¦¬
3. **ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ì •ì±…**: ë¹„ì—…ë¬´ ì‹œê°„ ë¦¬ì†ŒìŠ¤ 50% ì´ìƒ ì¶•ì†Œ
4. **Right-sizing**: ëª¨ë¸ í¬ê¸°ì— ë§ëŠ” ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìë™ ì„ íƒ
:::

:::warning ë¹„ìš© ìµœì í™” ì£¼ì˜ì‚¬í•­

- Spot ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì‹œ ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”ë¥¼ ìœ„í•œ graceful shutdown êµ¬í˜„ í•„ìˆ˜
- ê³¼ë„í•œ Consolidationì€ ìŠ¤ì¼€ì¼ ì•„ì›ƒ ì§€ì—°ì„ ìœ ë°œí•  ìˆ˜ ìˆìŒ
- ë¹„ìš© ì ˆê°ê³¼ SLA ì¤€ìˆ˜ ì‚¬ì´ì˜ ê· í˜•ì  ì„¤ì • í•„ìš”
:::

---

## ë„ì „ê³¼ì œ 4: FM íŒŒì¸íŠœë‹ê³¼ ìë™í™” íŒŒì´í”„ë¼ì¸

### Karpenter ê¸°ë°˜ í•™ìŠµ ì¸í”„ë¼ êµ¬ì„±

#### ì „ëµ 1: í•™ìŠµ ì „ìš© NodePool ë¶„ë¦¬

í•™ìŠµ ì›Œí¬ë¡œë“œëŠ” ì¶”ë¡ ê³¼ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§€ë¯€ë¡œ ë³„ë„ì˜ NodePoolë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-training-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-training
        workload: ml-training
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]  # í•™ìŠµì€ On-Demand ê¶Œì¥ (ì•ˆì •ì„±)
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p5.48xlarge     # 8x H100 80GB - ëŒ€ê·œëª¨ í•™ìŠµ
            - p4d.24xlarge    # 8x A100 40GB - ì¤‘ê·œëª¨ í•™ìŠµ
            - p4de.24xlarge   # 8x A100 80GB - ë©”ëª¨ë¦¬ ì§‘ì•½ì  í•™ìŠµ
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-training-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: workload-type
          value: "training"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 64
  disruption:
    # í•™ìŠµ ì¤‘ì—ëŠ” ë…¸ë“œ ì¤‘ë‹¨ ë°©ì§€
    consolidationPolicy: WhenEmpty
    consolidateAfter: 1h  # í•™ìŠµ ì™„ë£Œ í›„ 1ì‹œê°„ ëŒ€ê¸°
    budgets:
      # í•™ìŠµ ì¤‘ì—ëŠ” ë…¸ë“œ ì¤‘ë‹¨ ì™„ì „ ë°©ì§€
      - nodes: "0"
```

#### ì „ëµ 2: EFA ë„¤íŠ¸ì›Œí¬ ìµœì í™” NodeClass

ë¶„ì‚° í•™ìŠµì˜ ì„±ëŠ¥ì€ GPU ê°„ í†µì‹  ì†ë„ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. EFA(Elastic Fabric Adapter)ë¥¼ í™œìš©í•˜ì—¬ ìµœëŒ€ ì„±ëŠ¥ì„ í™•ë³´í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu-training-nodeclass
spec:
  role: KarpenterNodeRole-${CLUSTER_NAME}
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
        network-type: efa-enabled  # EFA ì§€ì› ì„œë¸Œë„·
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 1000Gi  # ëŒ€ìš©ëŸ‰ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        volumeType: gp3
        iops: 16000
        throughput: 1000
        encrypted: true
        deleteOnTermination: true
  instanceStorePolicy: RAID0  # NVMe ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ í™œìš©
  userData: |
    #!/bin/bash
    set -e

    # NVIDIA ë“œë¼ì´ë²„ ì„¤ì •
    nvidia-smi -pm 1
    nvidia-smi -ac 1593,1410  # H100 ìµœì  í´ëŸ­ ì„¤ì •

    # EFA ë“œë¼ì´ë²„ ë¡œë“œ
    modprobe efa

    # NCCL í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    echo 'export NCCL_DEBUG=INFO' >> /etc/profile.d/nccl.sh
    echo 'export NCCL_SOCKET_IFNAME=eth0' >> /etc/profile.d/nccl.sh
    echo 'export FI_EFA_USE_DEVICE_RDMA=1' >> /etc/profile.d/nccl.sh
    echo 'export FI_PROVIDER=efa' >> /etc/profile.d/nccl.sh

    # ëŒ€ìš©ëŸ‰ í˜ì´ì§€ ì„¤ì • (í•™ìŠµ ì„±ëŠ¥ í–¥ìƒ)
    echo 'vm.nr_hugepages=5120' >> /etc/sysctl.conf
    sysctl -p
  tags:
    Environment: production
    Workload: ml-training
    CostCenter: ml-platform
```

#### ì „ëµ 3: ì‹¤í—˜ìš© Spot ê¸°ë°˜ NodePool

í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ë‚˜ ì‹¤í—˜ì  í•™ìŠµì—ëŠ” Spot ì¸ìŠ¤í„´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ë¹„ìš©ì„ ì ˆê°í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-experiment-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-experiment
        workload: ml-experiment
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge
            - g5.48xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-experiment-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: workload-type
          value: "experiment"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 32
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 10m  # ì‹¤í—˜ ì™„ë£Œ í›„ ë¹ ë¥¸ ì •ë¦¬
  weight: 30  # í”„ë¡œë•ì…˜ í•™ìŠµë³´ë‹¤ ë‚®ì€ ìš°ì„ ìˆœìœ„
```

### NeMo ë¶„ì‚° í•™ìŠµ Job ì˜ˆì œ

Karpenterê°€ í”„ë¡œë¹„ì €ë‹í•œ ë…¸ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” NeMo ë¶„ì‚° í•™ìŠµ Jobì…ë‹ˆë‹¤.

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: nemo-finetune-llama-70b
  namespace: ai-training
spec:
  parallelism: 4  # 4ê°œ ë…¸ë“œ ë³‘ë ¬ ì‹¤í–‰
  completions: 4
  completionMode: Indexed
  template:
    metadata:
      labels:
        app: nemo-training
        model: llama-70b
    spec:
      restartPolicy: OnFailure
      containers:
        - name: nemo
          image: nvcr.io/nvidia/nemo:24.01
          command:
            - /bin/bash
            - -c
            - |
              # ë¶„ì‚° í•™ìŠµ í™˜ê²½ ì„¤ì •
              export MASTER_ADDR=$(hostname -i)
              export MASTER_PORT=29500
              export WORLD_SIZE=32  # 4 nodes x 8 GPUs
              export RANK=$JOB_COMPLETION_INDEX

              python -m torch.distributed.launch \
                --nproc_per_node=8 \
                --nnodes=4 \
                --node_rank=$RANK \
                --master_addr=$MASTER_ADDR \
                --master_port=$MASTER_PORT \
                /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_finetuning.py \
                --config-path=/config \
                --config-name=llama_70b_finetune
          args:
            - model.data.train_ds.file_path=/data/train.jsonl
            - model.data.validation_ds.file_path=/data/val.jsonl
            - trainer.devices=8
            - trainer.num_nodes=4
            - trainer.max_epochs=3
            - trainer.precision=bf16-mixed
            - model.tensor_model_parallel_size=4
            - model.pipeline_model_parallel_size=2
            - exp_manager.checkpoint_callback_params.save_top_k=3
          resources:
            requests:
              nvidia.com/gpu: 8
              memory: "900Gi"
              cpu: "90"
            limits:
              nvidia.com/gpu: 8
              memory: "1100Gi"
              cpu: "96"
          volumeMounts:
            - name: training-data
              mountPath: /data
            - name: checkpoints
              mountPath: /checkpoints
            - name: config
              mountPath: /config
            - name: shm
              mountPath: /dev/shm
      nodeSelector:
        node-type: gpu-training
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - key: workload-type
          operator: Equal
          value: "training"
          effect: NoSchedule
      volumes:
        - name: training-data
          persistentVolumeClaim:
            claimName: training-data-pvc
        - name: checkpoints
          persistentVolumeClaim:
            claimName: checkpoints-pvc
        - name: config
          configMap:
            name: nemo-training-config
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 256Gi  # ëŒ€ìš©ëŸ‰ ê³µìœ  ë©”ëª¨ë¦¬
```

### í•™ìŠµ ì¸í”„ë¼ ë¹„ìš© ìµœì í™” ì „ëµ

| ì „ëµ | ì ìš© ëŒ€ìƒ | ì˜ˆìƒ ì ˆê°ë¥  | êµ¬í˜„ ë°©ë²• |
| --- | --- | --- | --- |
| Spot ì‹¤í—˜ í´ëŸ¬ìŠ¤í„° | í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | 60-80% | ë³„ë„ NodePool |
| ìë™ ë…¸ë“œ ì •ë¦¬ | í•™ìŠµ ì™„ë£Œ í›„ | 20-30% | Consolidation |
| ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì¬ì‹œì‘ | Spot ì¤‘ë‹¨ ëŒ€ì‘ | 10-20% | NeMo ì²´í¬í¬ì¸íŠ¸ |
| ì‹œê°„ëŒ€ë³„ ìŠ¤ì¼€ì¤„ë§ | ë¹„ì—…ë¬´ ì‹œê°„ í•™ìŠµ | 15-25% | CronJob + Karpenter |

:::tip í•™ìŠµ ì¸í”„ë¼ ëª¨ë²” ì‚¬ë¡€

1. **í”„ë¡œë•ì…˜ í•™ìŠµ**: On-Demand ì¸ìŠ¤í„´ìŠ¤ë¡œ ì•ˆì •ì„± í™•ë³´
2. **ì‹¤í—˜/íŠœë‹**: Spot ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¹„ìš© ì ˆê°
3. **ì²´í¬í¬ì¸íŠ¸**: FSx for Lustreì— ì£¼ê¸°ì  ì €ì¥
4. **ëª¨ë‹ˆí„°ë§**: TensorBoard + Prometheusë¡œ í•™ìŠµ ì§„í–‰ ì¶”ì 
:::

:::warning ë¶„ì‚° í•™ìŠµ ì£¼ì˜ì‚¬í•­

- EFA ë„¤íŠ¸ì›Œí¬ê°€ ì§€ì›ë˜ëŠ” ì„œë¸Œë„·ì—ì„œë§Œ ìµœì  ì„±ëŠ¥ ë°œíœ˜
- NCCL í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì´ ì„±ëŠ¥ì— í° ì˜í–¥
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°ì™€ ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ê°„ ê· í˜• í•„ìš”
:::

---

## EKS ê¸°ë°˜ Agentic AI í”Œë«í¼ ê°„í¸ êµ¬ì¶•

ì•ì„œ ì†Œê°œí•œ ì†”ë£¨ì…˜ë“¤ì€ **Amazon EKS í™˜ê²½ì—ì„œ ì†ì‰½ê²Œ ë°°í¬**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. EKS Auto Modeì™€ AWS ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ì˜ í†µí•©ì„ í†µí•´ **ë³µì¡í•œ ì¸í”„ë¼ êµ¬ì„± ì—†ì´** ì™„ì „í•œ Agentic AI í”Œë«í¼ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### EKSì˜ ê°„í¸ ë°°í¬ ì´ì 

```mermaid
graph LR
    subgraph "ì „í†µì  êµ¬ì¶• ë°©ì‹"
        T1["ì¸í”„ë¼ ì„¤ê³„<br/>2-4ì£¼"]
        T2["ì»´í¬ë„ŒíŠ¸ ì„¤ì¹˜<br/>2-3ì£¼"]
        T3["í†µí•© í…ŒìŠ¤íŠ¸<br/>1-2ì£¼"]
        T4["ìš´ì˜ ì¤€ë¹„<br/>1-2ì£¼"]
        T1 --> T2 --> T3 --> T4
    end

    subgraph "EKS ê¸°ë°˜ êµ¬ì¶•"
        E1["EKS Auto Mode<br/>í´ëŸ¬ìŠ¤í„° ìƒì„±<br/>1ì¼"]
        E2["Helm/Addon<br/>ì†”ë£¨ì…˜ ë°°í¬<br/>2-3ì¼"]
        E3["ì›Œí¬ë¡œë“œ ë°°í¬<br/>1-2ì¼"]
        E1 --> E2 --> E3
    end

    style T4 fill:#ff6b6b
    style E3 fill:#4ecdc4
```

| êµ¬ì¶• ë°©ì‹ | ì†Œìš” ì‹œê°„ | ìš´ì˜ ë³µì¡ë„ | ë¹„ìš© íš¨ìœ¨ì„± |
| --- | --- | --- | --- |
| **ì „í†µì  ë°©ì‹** | 6-11ì£¼ | ë†’ìŒ | ë‚®ìŒ |
| **EKS ê¸°ë°˜** | 1-2ì£¼ | ë‚®ìŒ | ë†’ìŒ |

### ì†”ë£¨ì…˜ë³„ EKS ë°°í¬ ë°©ë²•

| ì†”ë£¨ì…˜ | ë°°í¬ ë°©ë²• | EKS í†µí•© ì´ì  |
| --- | --- | --- |
| **Karpenter** | EKS Auto Mode (ìë™) | ì„¤ì¹˜/êµ¬ì„± ë¶ˆí•„ìš”, ìë™ ì—…ê·¸ë ˆì´ë“œ |
| **Kgateway** | Helm Chart | ALB Controller ì—°ë™, ACM ì¸ì¦ì„œ ìë™ ê´€ë¦¬ |
| **LiteLLM** | Helm Chart | Secrets Manager ì—°ë™, IAM ê¸°ë°˜ ì¸ì¦ |
| **vLLM** | Helm Chart | GPU NodePool ìë™ í”„ë¡œë¹„ì €ë‹ |
| **llm-d** | Helm Chart | Karpenter ì—°ë™ ìë™ ìŠ¤ì¼€ì¼ë§ |
| **LangFuse** | Helm Chart | RDS/Aurora ì—°ë™, S3 ìŠ¤í† ë¦¬ì§€ |
| **KAgent** | Helm Chart | Pod Identity ê¸°ë°˜ AWS ì„œë¹„ìŠ¤ ì ‘ê·¼ |
| **KEDA** | EKS Addon | ê´€ë¦¬í˜• ì„¤ì¹˜, CloudWatch ë©”íŠ¸ë¦­ ì—°ë™ |

### EKS í†µí•© ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "AWS ê´€ë¦¬í˜• ì„œë¹„ìŠ¤"
        EKS["Amazon EKS<br/>Auto Mode"]
        ALB["Application<br/>Load Balancer"]
        RDS["Amazon RDS<br/>(LangFuse DB)"]
        S3["Amazon S3<br/>(Model Storage)"]
        SM["Secrets Manager"]
        CW["CloudWatch"]
    end

    subgraph "EKS Cluster"
        subgraph "Karpenter ê´€ë¦¬ ë…¸ë“œ"
            GPU["GPU NodePool"]
            CPU["CPU NodePool"]
        end

        subgraph "AI Platform Stack"
            KGW["Kgateway"]
            LITE["LiteLLM"]
            VLLM["vLLM"]
            LLMD["llm-d"]
            KAGENT["KAgent"]
            LF["LangFuse"]
        end
    end

    EKS --> GPU & CPU
    ALB --> KGW
    KGW --> LITE --> VLLM
    VLLM --> GPU
    LF --> RDS
    VLLM --> S3
    LITE --> SM
    VLLM --> CW

    style EKS fill:#ff9900
    style ALB fill:#ff9900
    style RDS fill:#ff9900
    style S3 fill:#ff9900
```

### ê°„í¸ ë°°í¬ ì˜ˆì‹œ

EKS Auto Mode í´ëŸ¬ìŠ¤í„°ì—ì„œ ì „ì²´ Agentic AI ìŠ¤íƒì„ ë°°í¬í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.

```bash
# 1. EKS Auto Mode í´ëŸ¬ìŠ¤í„° ìƒì„± (Karpenter ìë™ í¬í•¨)
eksctl create cluster --name ai-platform --region us-west-2 --auto-mode

# 2. GPU NodePool ì¶”ê°€
kubectl apply -f gpu-nodepool.yaml

# 3. AI Platform ì†”ë£¨ì…˜ ìŠ¤íƒ ë°°í¬
helm repo add kgateway https://kgateway.io/charts
helm repo add litellm https://litellm.github.io/helm
helm repo add vllm https://vllm-project.github.io/helm
helm repo add langfuse https://langfuse.github.io/helm

helm install kgateway kgateway/kgateway -n ai-gateway --create-namespace
helm install litellm litellm/litellm -n ai-inference --create-namespace
helm install vllm vllm/vllm -n ai-inference
helm install langfuse langfuse/langfuse -n observability --create-namespace

# 4. KEDA ì„¤ì¹˜ (EKS Addon)
aws eks create-addon --cluster-name ai-platform --addon-name keda
```

### EKS ê¸°ë°˜ êµ¬ì¶•ì˜ í•µì‹¬ ì´ì 

:::tip EKSë¡œ Agentic AI í”Œë«í¼ì„ êµ¬ì¶•í•˜ë©´

1. **ì¸í”„ë¼ ìë™í™”**: EKS Auto Mode + Karpenterë¡œ GPU ë…¸ë“œ ìë™ ê´€ë¦¬
2. **ê°„í¸í•œ ë°°í¬**: Helm Chartì™€ EKS Addonìœ¼ë¡œ ì†”ë£¨ì…˜ ìŠ¤íƒ ì›í´ë¦­ ë°°í¬
3. **AWS ì„œë¹„ìŠ¤ í†µí•©**: RDS, S3, Secrets Manager, CloudWatchì™€ ë„¤ì´í‹°ë¸Œ ì—°ë™
4. **ë³´ì•ˆ ê°•í™”**: Pod Identity, Security Groups for Pods, ì•”í˜¸í™” ìë™ ì ìš©
5. **ë¹„ìš© ìµœì í™”**: Spot ì¸ìŠ¤í„´ìŠ¤, Savings Plans, Consolidation ìë™ í™œìš©
:::

:::tip EKS Auto Mode ì‹œì‘í•˜ê¸°
EKS Auto ModeëŠ” AWS ì½˜ì†”, eksctl, ë˜ëŠ” Terraformì—ì„œ ê°„ë‹¨íˆ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# eksctlë¡œ EKS Auto Mode í´ëŸ¬ìŠ¤í„° ìƒì„±
eksctl create cluster --name ai-platform --region us-west-2 --auto-mode
```

í´ëŸ¬ìŠ¤í„° ìƒì„± í›„ GPU NodePoolë§Œ ì¶”ê°€í•˜ë©´ ì¦‰ì‹œ AI ì›Œí¬ë¡œë“œë¥¼ ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

---

## EKS Capability: Agentic AIë¥¼ ìœ„í•œ í†µí•© í”Œë«í¼ ê¸°ëŠ¥

### EKS Capabilityë€?

**EKS Capability**ëŠ” Amazon EKSì—ì„œ íŠ¹ì • ì›Œí¬ë¡œë“œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìš´ì˜í•˜ê¸° ìœ„í•´ **ê²€ì¦ëœ ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬ì™€ AWS ì„œë¹„ìŠ¤ë¥¼ í†µí•©í•˜ì—¬ ì œê³µí•˜ëŠ” í”Œë«í¼ ìˆ˜ì¤€ì˜ ê¸°ëŠ¥**ì…ë‹ˆë‹¤. EKSëŠ” ë‹¨ìˆœí•œ Kubernetes ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ë¥¼ ë„˜ì–´, íŠ¹ì • ë„ë©”ì¸(AI/ML, ë°ì´í„° ë¶„ì„, ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ë“±)ì— ìµœì í™”ëœ **ì—”ë“œ-íˆ¬-ì—”ë“œ ì†”ë£¨ì…˜ ìŠ¤íƒ**ì„ ì œê³µí•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "EKS Capability ê³„ì¸µ êµ¬ì¡°"
        EKS["Amazon EKS<br/>ê´€ë¦¬í˜• Kubernetes"]

        subgraph "Platform Capabilities"
            AUTO["EKS Auto Mode<br/>ì¸í”„ë¼ ìë™í™”"]
            ADDON["EKS Add-ons<br/>í•µì‹¬ ì»´í¬ë„ŒíŠ¸"]
        end

        subgraph "Workload Capabilities"
            AI["AI/ML Capability<br/>Karpenter, GPU, Training Operator"]
            DATA["Data Capability<br/>Spark, Flink, EMR"]
            APP["App Capability<br/>ALB, Service Mesh"]
        end

        subgraph "Integration Capabilities (EKS ê³µì‹ ì§€ì›)"
            ACK_C["ACK<br/>AWS ë¦¬ì†ŒìŠ¤ í†µí•©"]
            KRO_C["KRO<br/>ë¦¬ì†ŒìŠ¤ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"]
            ARGOCD_C["Argo CD<br/>GitOps ë°°í¬"]
        end
    end

    EKS --> AUTO & ADDON
    AUTO --> AI & DATA & APP
    AI --> ACK_C & KRO_C & ARGOCD_C

    style EKS fill:#ff9900
    style AI fill:#76b900
    style ACK_C fill:#326ce5
    style KRO_C fill:#ffd93d
    style ARGOCD_C fill:#e85a25
```

### Agentic AIë¥¼ ìœ„í•œ í•µì‹¬ EKS Capability

Agentic AI ì›Œí¬ë¡œë“œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìš´ì˜í•˜ê¸° ìœ„í•´ EKSëŠ” ë‹¤ìŒ **Integration Capability**ë¥¼ ê³µì‹ ì§€ì›í•©ë‹ˆë‹¤:

| EKS Capability | ì—­í•  | Agentic AI í™œìš© | ì§€ì› ë°©ì‹ |
|----------------|------|-----------------|----------|
| **ACK (AWS Controllers for Kubernetes)** | AWS ì„œë¹„ìŠ¤ì˜ Kubernetes ë„¤ì´í‹°ë¸Œ ê´€ë¦¬ | S3 ëª¨ë¸ ì €ì¥ì†Œ, RDS ë©”íƒ€ë°ì´í„°, SageMaker í•™ìŠµ ì‘ì—… | EKS Add-on |
| **KRO (Kubernetes Resource Orchestrator)** | ë³µí•© ë¦¬ì†ŒìŠ¤ ì¶”ìƒí™” ë° í…œí”Œë¦¿í™” | AI ì¶”ë¡  ìŠ¤íƒ, í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì›í´ë¦­ ë°°í¬ | EKS Add-on |
| **Argo CD** | GitOps ê¸°ë°˜ ì§€ì†ì  ë°°í¬ | ëª¨ë¸ ì„œë¹™ ë°°í¬ ìë™í™”, ë¡¤ë°±, í™˜ê²½ ë™ê¸°í™” | EKS Add-on |

:::warning Argo WorkflowsëŠ” ë³„ë„ ì„¤ì¹˜ í•„ìš”
**Argo Workflows**ëŠ” EKS Capabilityë¡œ ê³µì‹ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ **ì§ì ‘ ì„¤ì¹˜ê°€ í•„ìš”**í•©ë‹ˆë‹¤.
Argo CD(EKS Capability)ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ê°•ë ¥í•œ ML íŒŒì´í”„ë¼ì¸ ìë™í™”ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# Argo Workflows ì„¤ì¹˜
kubectl create namespace argo
kubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/v3.5.0/install.yaml
```

:::

:::info EKS Capabilityì˜ í•µì‹¬ ê°€ì¹˜
ACK, KRO, Argo CD (EKS Capability)ë¥¼ ì¡°í•©í•˜ë©´:

- **ì„ ì–¸ì  ê´€ë¦¬**: ëª¨ë“  ì¸í”„ë¼ì™€ ì›Œí¬ë¡œë“œë¥¼ YAMLë¡œ ì •ì˜
- **GitOps ê¸°ë°˜**: Gitì„ Single Source of Truthë¡œ í™œìš©
- **ì™„ì „ ìë™í™”**: ì½”ë“œ ì»¤ë°‹ë¶€í„° í”„ë¡œë•ì…˜ ë°°í¬ê¹Œì§€ ë¬´ì¤‘ë‹¨ íŒŒì´í”„ë¼ì¸
- **í†µí•© ëª¨ë‹ˆí„°ë§**: AWS CloudWatchì™€ Kubernetes ë©”íŠ¸ë¦­ í†µí•©
:::

---

### ACK (AWS Controllers for Kubernetes)

**ACK**ëŠ” EKS Capabilityì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œë¡œ, Kubernetes Custom Resourceë¥¼ í†µí•´ AWS ì„œë¹„ìŠ¤ë¥¼ ì§ì ‘ í”„ë¡œë¹„ì €ë‹í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. **EKS Add-onìœ¼ë¡œ ê°„í¸í•˜ê²Œ ì„¤ì¹˜**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```mermaid
graph LR
    subgraph "Kubernetes Cluster"
        CR["AWS Custom Resources<br/>(S3, RDS, SageMaker...)"]
        ACK["ACK Controller"]
    end

    subgraph "AWS Services"
        S3["Amazon S3"]
        RDS["Amazon RDS"]
        SM["SageMaker"]
        SEC["Secrets Manager"]
    end

    CR --> ACK
    ACK --> S3 & RDS & SM & SEC

    style ACK fill:#ff9900
    style CR fill:#326ce5
```

**AI í”Œë«í¼ì—ì„œ ACK í™œìš© ì‚¬ë¡€:**

| AWS ì„œë¹„ìŠ¤ | ACK Controller | Agentic AI í™œìš© |
|-----------|---------------|-----------------|
| **S3** | `s3.services.k8s.aws` | ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ì†Œ, í•™ìŠµ ë°ì´í„° ë²„í‚· |
| **RDS/Aurora** | `rds.services.k8s.aws` | LangFuse ë°±ì—”ë“œ, ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œ |
| **SageMaker** | `sagemaker.services.k8s.aws` | ëª¨ë¸ í•™ìŠµ ì‘ì—…, ì—”ë“œí¬ì¸íŠ¸ ë°°í¬ |
| **Secrets Manager** | `secretsmanager.services.k8s.aws` | API í‚¤, ëª¨ë¸ ìê²©ì¦ëª… ê´€ë¦¬ |
| **ECR** | `ecr.services.k8s.aws` | ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë ˆì§€ìŠ¤íŠ¸ë¦¬ |

**ACKë¥¼ ì´ìš©í•œ S3 ë²„í‚· ìƒì„± ì˜ˆì‹œ:**

```yaml
# s3-model-bucket.yaml
apiVersion: s3.services.k8s.aws/v1alpha1
kind: Bucket
metadata:
  name: agentic-ai-models
  namespace: ai-platform
spec:
  name: agentic-ai-models-prod
  versioning:
    status: Enabled
  encryption:
    rules:
    - applyServerSideEncryptionByDefault:
        sseAlgorithm: aws:kms
  tags:
  - key: Project
    value: agentic-ai
  - key: Environment
    value: production
```

### KRO (Kubernetes Resource Orchestrator)

**KRO**ëŠ” ì—¬ëŸ¬ Kubernetes ë¦¬ì†ŒìŠ¤ì™€ AWS ë¦¬ì†ŒìŠ¤ë¥¼ **í•˜ë‚˜ì˜ ì¶”ìƒí™”ëœ ë‹¨ìœ„ë¡œ ì¡°í•©**í•˜ì—¬ ë³µì¡í•œ ì¸í”„ë¼ë¥¼ ë‹¨ìˆœí•˜ê²Œ ë°°í¬í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "KRO ResourceGroup"
        RG["ResourceGroup<br/>ai-inference-stack"]
    end

    subgraph "ìë™ ìƒì„±ë˜ëŠ” ë¦¬ì†ŒìŠ¤"
        S3B["S3 Bucket<br/>(ëª¨ë¸ ì €ì¥ì†Œ)"]
        RDS["RDS Instance<br/>(ë©”íƒ€ë°ì´í„°)"]
        SEC["Secret<br/>(ìê²©ì¦ëª…)"]
        DEP["Deployment<br/>(vLLM)"]
        SVC["Service<br/>(ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸)"]
        HPA["HPA<br/>(ì˜¤í† ìŠ¤ì¼€ì¼ë§)"]
    end

    RG --> S3B & RDS & SEC & DEP & SVC & HPA

    style RG fill:#ffd93d
    style S3B fill:#ff9900
    style RDS fill:#ff9900
```

**KRO ResourceGroup ì •ì˜ ì˜ˆì‹œ:**

```yaml
# ai-inference-stack.yaml
apiVersion: kro.aws.io/v1alpha1
kind: ResourceGroup
metadata:
  name: ai-inference-stack
spec:
  schema:
    apiVersion: v1alpha1
    kind: AIInferenceStack
    spec:
      modelName: string
      gpuType: string | default="g5.xlarge"
      minReplicas: integer | default=1
      maxReplicas: integer | default=10

  resources:
  # S3 ë²„í‚· (ACK)
  - id: modelBucket
    template:
      apiVersion: s3.services.k8s.aws/v1alpha1
      kind: Bucket
      metadata:
        name: ${schema.spec.modelName}-models
      spec:
        name: ${schema.spec.modelName}-models-${schema.metadata.namespace}

  # vLLM Deployment
  - id: inference
    template:
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: ${schema.spec.modelName}-vllm
      spec:
        replicas: ${schema.spec.minReplicas}
        template:
          spec:
            containers:
            - name: vllm
              image: vllm/vllm-openai:latest
              env:
              - name: MODEL_PATH
                value: s3://${modelBucket.status.bucketName}/

  # HPA
  - id: autoscaler
    template:
      apiVersion: autoscaling/v2
      kind: HorizontalPodAutoscaler
      metadata:
        name: ${schema.spec.modelName}-hpa
      spec:
        scaleTargetRef:
          name: ${inference.metadata.name}
        minReplicas: ${schema.spec.minReplicas}
        maxReplicas: ${schema.spec.maxReplicas}
```

**KROë¡œ AI ì¶”ë¡  ìŠ¤íƒ ë°°í¬:**

```yaml
# ë‹¨ì¼ ë¦¬ì†ŒìŠ¤ë¡œ ì „ì²´ ìŠ¤íƒ ë°°í¬
apiVersion: v1alpha1
kind: AIInferenceStack
metadata:
  name: llama-inference
  namespace: ai-platform
spec:
  modelName: llama-3-70b
  gpuType: g5.12xlarge
  minReplicas: 2
  maxReplicas: 20
```

### Argo ê¸°ë°˜ ML íŒŒì´í”„ë¼ì¸ ìë™í™”

**Argo Workflows**ì™€ **Argo CD**ë¥¼ ê²°í•©í•˜ë©´ AI ëª¨ë¸ì˜ í•™ìŠµ, í‰ê°€, ë°°í¬ê¹Œì§€ **ì „ì²´ MLOps íŒŒì´í”„ë¼ì¸ì„ GitOps ë°©ì‹ìœ¼ë¡œ ìë™í™”**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```mermaid
graph LR
    subgraph "GitOps Pipeline"
        GIT["Git Repository<br/>(ëª¨ë¸ ì½”ë“œ + ì„¤ì •)"]
        ARGOCD["Argo CD<br/>(ë°°í¬ ìë™í™”)"]
    end

    subgraph "ML Pipeline (Argo Workflows)"
        PREP["ë°ì´í„° ì „ì²˜ë¦¬"]
        TRAIN["ëª¨ë¸ í•™ìŠµ<br/>(GPU NodePool)"]
        EVAL["ëª¨ë¸ í‰ê°€<br/>(RAGAS)"]
        REG["ëª¨ë¸ ë“±ë¡<br/>(S3/MLflow)"]
    end

    subgraph "Serving"
        CANARY["Canary ë°°í¬"]
        PROD["Production<br/>vLLM Serving"]
    end

    GIT --> ARGOCD
    ARGOCD --> PREP --> TRAIN --> EVAL --> REG
    REG --> CANARY --> PROD

    style ARGOCD fill:#e85a25
    style TRAIN fill:#76b900
```

**Argo Workflowë¥¼ ì´ìš©í•œ FM íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸:**

```yaml
# fine-tuning-pipeline.yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: llm-fine-tuning
  namespace: ai-platform
spec:
  entrypoint: fine-tuning-pipeline

  templates:
  - name: fine-tuning-pipeline
    dag:
      tasks:
      # 1. ë°ì´í„° ì¤€ë¹„
      - name: prepare-data
        template: data-preparation

      # 2. ëª¨ë¸ í•™ìŠµ (GPU ì‚¬ìš©)
      - name: train-model
        template: training
        dependencies: [prepare-data]

      # 3. ëª¨ë¸ í‰ê°€
      - name: evaluate-model
        template: evaluation
        dependencies: [train-model]

      # 4. ëª¨ë¸ ë“±ë¡ (í‰ê°€ í†µê³¼ ì‹œ)
      - name: register-model
        template: registration
        dependencies: [evaluate-model]
        when: "{{tasks.evaluate-model.outputs.parameters.quality-score}} > 0.8"

  - name: training
    nodeSelector:
      karpenter.sh/nodepool: gpu-training
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
    container:
      image: nvcr.io/nvidia/nemo:24.01
      command: [python, train.py]
      resources:
        limits:
          nvidia.com/gpu: 8
      env:
      - name: TRAINING_DATA
        value: s3://agentic-ai-data/training/
      - name: MODEL_OUTPUT
        value: s3://agentic-ai-models/checkpoints/

  - name: evaluation
    container:
      image: ai-platform/ragas-evaluator:latest
      command: [python, evaluate.py]
    outputs:
      parameters:
      - name: quality-score
        valueFrom:
          path: /tmp/quality-score.txt
```

**Argo CDë¥¼ ì´ìš©í•œ ëª¨ë¸ ë°°í¬ ìë™í™”:**

```yaml
# argocd-application.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llm-inference-prod
  namespace: argocd
spec:
  project: ai-platform
  source:
    repoURL: https://github.com/myorg/ai-platform-configs
    targetRevision: main
    path: deployments/llm-inference
  destination:
    server: https://kubernetes.default.svc
    namespace: ai-platform
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
```

### ACK + KRO + Argo í†µí•© ì•„í‚¤í…ì²˜

ì„¸ ê°€ì§€ ë„êµ¬ë¥¼ ì¡°í•©í•˜ë©´ **ì™„ì „ ìë™í™”ëœ AI í”Œë«í¼ ìš´ì˜**ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "ê°œë°œì ê²½í—˜"
        DEV["ê°œë°œì"]
        GIT["Git Push<br/>(ëª¨ë¸ ì½”ë“œ + ì„¤ì •)"]
    end

    subgraph "GitOps Layer"
        ARGOCD["Argo CD<br/>ë°°í¬ ìë™í™”"]
        ARGOWF["Argo Workflows<br/>ML íŒŒì´í”„ë¼ì¸"]
    end

    subgraph "Infrastructure Abstraction"
        KRO["KRO<br/>ë¦¬ì†ŒìŠ¤ ì¡°í•©"]
        ACK["ACK Controllers<br/>AWS ë¦¬ì†ŒìŠ¤ ê´€ë¦¬"]
    end

    subgraph "EKS Platform"
        KARP["Karpenter<br/>GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹"]
        VLLM["vLLM<br/>ëª¨ë¸ ì„œë¹™"]
    end

    subgraph "AWS Services"
        S3["S3"]
        RDS["RDS"]
        SM["SageMaker"]
    end

    DEV --> GIT --> ARGOCD
    ARGOCD --> ARGOWF
    ARGOCD --> KRO
    KRO --> ACK
    ACK --> S3 & RDS & SM
    ARGOWF --> KARP
    KARP --> VLLM

    style ARGOCD fill:#e85a25
    style KRO fill:#ffd93d
    style ACK fill:#ff9900
    style KARP fill:#ffd93d
```

| êµ¬ì„±ìš”ì†Œ | ì—­í•  | ìë™í™” ë²”ìœ„ |
|---------|------|------------|
| **Argo CD** | GitOps ë°°í¬ ìë™í™” | ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬, ë¡¤ë°±, ë™ê¸°í™” |
| **Argo Workflows** | ML íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ | í•™ìŠµ, í‰ê°€, ëª¨ë¸ ë“±ë¡ ì›Œí¬í”Œë¡œ |
| **KRO** | ë³µí•© ë¦¬ì†ŒìŠ¤ ì¶”ìƒí™” | K8s + AWS ë¦¬ì†ŒìŠ¤ë¥¼ ë‹¨ì¼ ë‹¨ìœ„ë¡œ ê´€ë¦¬ |
| **ACK** | AWS ë¦¬ì†ŒìŠ¤ ì„ ì–¸ì  ê´€ë¦¬ | S3, RDS, SageMaker ë“± AWS ì„œë¹„ìŠ¤ |
| **Karpenter** | GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ | Just-in-Time ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œë¹„ì €ë‹ |

:::info ì™„ì „ ìë™í™”ì˜ ì´ì 
ì´ í†µí•© ì•„í‚¤í…ì²˜ë¥¼ í†µí•´:

- **ê°œë°œì**: Git pushë§Œìœ¼ë¡œ ëª¨ë¸ ë°°í¬
- **í”Œë«í¼ íŒ€**: ì¸í”„ë¼ ê´€ë¦¬ ë¶€ë‹´ ìµœì†Œí™”
- **ë¹„ìš© ìµœì í™”**: í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ë§Œ ë™ì  í”„ë¡œë¹„ì €ë‹
- **ì¼ê´€ì„±**: ëª¨ë“  í™˜ê²½ì—ì„œ ë™ì¼í•œ ë°°í¬ ë°©ì‹
:::

---

## ê²°ë¡ : Kubernetes + EKS Auto Modeë¡œ ì™„ì„±í•˜ëŠ” AI ì¸í”„ë¼ ìë™í™”

Agentic AI Platform êµ¬ì¶•ì˜ 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œëŠ” **í´ë¼ìš°ë“œ ì¸í”„ë¼ ìë™í™”ì™€ AI í”Œë«í¼ì˜ ìœ ê¸°ì  í†µí•©**ì„ í†µí•´ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ **EKS Auto Mode**ëŠ” Karpenterë¥¼ í¬í•¨í•œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ìë™ìœ¼ë¡œ ê´€ë¦¬í•˜ì—¬ **ì™„ì „ ìë™í™”ì˜ ë§ˆì§€ë§‰ í¼ì¦**ì„ ì™„ì„±í•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "ë¬¸ì œ ì¸ì‹"
        P["Agentic AI í”Œë«í¼<br/>4ê°€ì§€ ë„ì „ê³¼ì œ"]
    end

    subgraph "í•´ê²° í”„ë ˆì„ì›Œí¬"
        K8S["Kubernetes<br/>ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"]
        AUTO["EKS Auto Mode<br/>ì™„ì „ ê´€ë¦¬í˜• + Karpenter ìë™í™”"]
        AWS["AWS ì¸í”„ë¼<br/>GPU, ë„¤íŠ¸ì›Œí¬, ìŠ¤í† ë¦¬ì§€"]
    end

    subgraph "ë‹¬ì„± ëª©í‘œ"
        G1["âœ… ì™„ì „ ìë™í™”ëœ GPU ê´€ë¦¬"]
        G2["âœ… ë¹„ìš© 50-70% ì ˆê°"]
        G3["âœ… í”„ë¡œë¹„ì €ë‹ ì‹œê°„ 50% ë‹¨ì¶•"]
        G4["âœ… ìš´ì˜ ë¶€ë‹´ 80% ê°ì†Œ"]
    end

    P --> K8S
    K8S --> AUTO
    AUTO --> AWS
    AWS --> G1 & G2 & G3 & G4

    style P fill:#ff6b6b
    style K8S fill:#326ce5
    style AUTO fill:#ff9900
    style G1 fill:#4ecdc4
    style G2 fill:#4ecdc4
    style G3 fill:#4ecdc4
    style G4 fill:#4ecdc4
```

### í•µì‹¬ ë©”ì‹œì§€

1. **KubernetesëŠ” AI ì¸í”„ë¼ì˜ í•„ìˆ˜ ê¸°ë°˜**: ì„ ì–¸ì  ë¦¬ì†ŒìŠ¤ ê´€ë¦¬, ìë™ ìŠ¤ì¼€ì¼ë§, Operator íŒ¨í„´ì„ í†µí•´ ë³µì¡í•œ AI ì›Œí¬ë¡œë“œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬
2. **EKS Auto Modeê°€ ì™„ì „ ìë™í™” ì‹¤í˜„**: Karpenter, VPC CNI, EBS CSI Driver ë“± í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ìë™ ê´€ë¦¬ë¡œ ìš´ì˜ ë¶€ë‹´ ëŒ€í­ ê°ì†Œ
3. **KarpenterëŠ” GPU ì¸í”„ë¼ ìë™í™”ì˜ í•µì‹¬**: Just-in-Time í”„ë¡œë¹„ì €ë‹, Spot ì¸ìŠ¤í„´ìŠ¤, Consolidationìœ¼ë¡œ ë¹„ìš©ê³¼ ì„±ëŠ¥ ìµœì í™”
4. **AWS ì¸í”„ë¼ í†µí•©ì´ ì‹œë„ˆì§€ ê·¹ëŒ€í™”**: EFA ë„¤íŠ¸ì›Œí¬, ë‹¤ì–‘í•œ GPU ì¸ìŠ¤í„´ìŠ¤, FSx ìŠ¤í† ë¦¬ì§€ì™€ì˜ ê¸´ë°€í•œ í†µí•©

### EKS Auto Mode: ê¶Œì¥ ì‹œì‘ì 

ìƒˆë¡œìš´ Agentic AI í”Œë«í¼ì„ êµ¬ì¶•í•œë‹¤ë©´ **EKS Auto Mode**ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

| ì´ì  | ì„¤ëª… |
| --- | --- |
| **ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥** | Karpenter ì„¤ì¹˜/êµ¬ì„± ì—†ì´ í´ëŸ¬ìŠ¤í„° ìƒì„± ì¦‰ì‹œ GPU ì›Œí¬ë¡œë“œ ë°°í¬ |
| **ìë™ ì—…ê·¸ë ˆì´ë“œ** | Karpenter, CNI, CSI ë“± í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ìë™ ì—…ë°ì´íŠ¸ |
| **ë³´ì•ˆ íŒ¨ì¹˜ ìë™í™”** | ë³´ì•ˆ ì·¨ì•½ì  íŒ¨ì¹˜ ìë™ ì ìš© |
| **ì»¤ìŠ¤í…€ í™•ì¥ ê°€ëŠ¥** | GPU NodePool, EFA NodeClass ë“± í•„ìš”ì‹œ ì»¤ìŠ¤í…€ ì„¤ì • ì¶”ê°€ |

### ë„ì „ê³¼ì œë³„ í•´ê²° ë°©ì•ˆ ìµœì¢… ìš”ì•½

| ë„ì „ê³¼ì œ | Kubernetes ê¸°ë°˜ | EKS Auto Mode + Karpenter | ê¸°ëŒ€ íš¨ê³¼ |
| --- | --- | --- | --- |
| **GPU ëª¨ë‹ˆí„°ë§** | DCGM + Prometheus | NodePool ê¸°ë°˜ í†µí•© ê´€ë¦¬ | ë¦¬ì†ŒìŠ¤ í™œìš©ë¥  40% í–¥ìƒ |
| **ë™ì  ìŠ¤ì¼€ì¼ë§** | HPA + KEDA | Just-in-Time í”„ë¡œë¹„ì €ë‹ (ìë™ êµ¬ì„±) | í”„ë¡œë¹„ì €ë‹ ì‹œê°„ 50% ë‹¨ì¶• |
| **ë¹„ìš© ì»¨íŠ¸ë¡¤** | ë„¤ì„ìŠ¤í˜ì´ìŠ¤ Quota | Spot + Consolidation (ìë™ í™œì„±í™”) | ë¹„ìš© 50-70% ì ˆê° |
| **FM íŒŒì¸íŠœë‹** | Kubeflow Operator | Training NodePool + EFA | í•™ìŠµ íš¨ìœ¨ì„± 30% í–¥ìƒ |

### í•µì‹¬ ê¶Œì¥ì‚¬í•­

1. **EKS Auto Modeë¡œ ì‹œì‘**: ìƒˆ í´ëŸ¬ìŠ¤í„°ëŠ” Auto Modeë¡œ ìƒì„±í•˜ì—¬ Karpenter ìë™ êµ¬ì„± í™œìš©
2. **GPU NodePool ì»¤ìŠ¤í…€ ì •ì˜**: ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë§ëŠ” GPU NodePool ì¶”ê°€ (ì¶”ë¡ /í•™ìŠµ/ì‹¤í—˜ ë¶„ë¦¬)
3. **Spot ì¸ìŠ¤í„´ìŠ¤ ì ê·¹ í™œìš©**: ì¶”ë¡  ì›Œí¬ë¡œë“œì˜ 70% ì´ìƒì„ Spotìœ¼ë¡œ ìš´ì˜
4. **Consolidation ê¸°ë³¸ í™œì„±í™”**: EKS Auto Modeì—ì„œ ìë™ í™œì„±í™”ëœ Consolidation í™œìš©
5. **KEDA ì—°ë™**: ë©”íŠ¸ë¦­ ê¸°ë°˜ Pod ìŠ¤ì¼€ì¼ë§ê³¼ Karpenter ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì—°ê³„
6. **EFA NodeClass ì¶”ê°€**: ë¶„ì‚° í•™ìŠµ ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ ê³ ì„±ëŠ¥ ë„¤íŠ¸ì›Œí¬ ì„¤ì •

---

## ì°¸ê³  ìë£Œ

### Kubernetes ë° ì¸í”„ë¼

- [Kubernetes ê³µì‹ ë¬¸ì„œ](https://kubernetes.io/docs/)
- [Karpenter ê³µì‹ ë¬¸ì„œ](https://karpenter.sh/docs/)
- [Amazon EKS Best Practices Guide](https://docs.aws.amazon.com/eks/latest/best-practices/introduction.html)
- [NVIDIA GPU Operator Documentation](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html)
- [KEDA - Kubernetes Event-driven Autoscaling](https://keda.sh/)

### ëª¨ë¸ ì„œë¹™ ë° ì¶”ë¡ 

- [vLLM Documentation](https://docs.vllm.ai/)
- [llm-d Project](https://github.com/llm-d/llm-d)
- [Kgateway Documentation](https://kgateway.io/docs/)
- [LiteLLM Documentation](https://docs.litellm.ai/)

### LLM Observability

- [LangFuse Documentation](https://langfuse.com/docs)
- [LangSmith Documentation](https://docs.smith.langchain.com/)

### Agent í”„ë ˆì„ì›Œí¬ ë° í•™ìŠµ

- [KAgent - Kubernetes Agent Framework](https://github.com/kagent-dev/kagent)
- [NVIDIA NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)
- [Kubeflow Documentation](https://www.kubeflow.org/docs/)

### AWS ì„œë¹„ìŠ¤

- [Amazon EKS Documentation](https://docs.aws.amazon.com/eks/)
- [EKS Auto Mode](https://docs.aws.amazon.com/eks/latest/userguide/automode.html)
- [AWS Elastic Fabric Adapter (EFA)](https://aws.amazon.com/hpc/efa/)
- [Amazon FSx for Lustre](https://aws.amazon.com/fsx/lustre/)

### Agentic AIë¥¼ ìœ„í•œ EKSì˜ ì¥ì 

**EKSê°€ ìµœì ì˜ í”Œë«í¼ì¸ ì´ìœ :**

1. **ì²«ë‚ ë¶€í„° í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ**
   - 99.95% SLAë¥¼ ì œê³µí•˜ëŠ” AWS ê´€ë¦¬í˜• Control Plane
   - ìë™ ë³´ì•ˆ íŒ¨ì¹˜ ë° Kubernetes ì—…ê·¸ë ˆì´ë“œ
   - AWS ì„œë¹„ìŠ¤ì™€ì˜ ê¹Šì€ í†µí•© (IAM, VPC, CloudWatch)

2. **ê°„ì†Œí™”ëœ ìš´ì˜**
   - EKS Auto Modeë¡œ ë…¸ë“œ ê´€ë¦¬ ë¶€ë‹´ ì œê±°
   - Karpenterë¥¼ í†µí•œ GPU í”„ë¡œë¹„ì €ë‹ ìë™í™”
   - CloudWatchë¥¼ í†µí•œ í†µí•© ê´€ì°°ì„± ì œê³µ

3. **ëŒ€ê·œëª¨ ë¹„ìš© ìµœì í™”**
   - Spot ì¸ìŠ¤í„´ìŠ¤ í†µí•©ìœ¼ë¡œ 60-90% ë¹„ìš© ì ˆê°
   - Karpenter Consolidationìœ¼ë¡œ ìœ íœ´ ë‚­ë¹„ 30-40% ê°ì†Œ
   - Right-sizing ë° ì˜¤í† ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ê³¼ë‹¤ í”„ë¡œë¹„ì €ë‹ ìµœì†Œí™”

4. **ì—”í„°í”„ë¼ì´ì¦ˆ ë³´ì•ˆ**
   - Pod ë ˆë²¨ IAM ì—­í•  (IRSA)
   - VPC ë° Security Groupsë¥¼ í†µí•œ ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬
   - ê·œì • ì¤€ìˆ˜ ì¸ì¦ (HIPAA, PCI-DSS, SOC 2)

### ë°°í¬ ê²½ë¡œ ì„ íƒí•˜ê¸°

<Tabs>
<TabItem value="auto-mode" label="EKS Auto Mode (ëŒ€ë¶€ë¶„ì—ê²Œ ê¶Œì¥)">

**ì í•©í•œ ê²½ìš°:**

- ìŠ¤íƒ€íŠ¸ì—… ë° ì†Œê·œëª¨ íŒ€
- Kubernetes ì´ˆë³´ íŒ€
- í‘œì¤€ Agentic AI ì›Œí¬ë¡œë“œ (CPU + ì¤‘ê°„ ìˆ˜ì¤€ GPU)
- ë¹ ë¥¸ ì¶œì‹œ ìš”êµ¬ì‚¬í•­

**ì‹œì‘í•˜ê¸°:**

```bash
aws eks create-cluster \
  --name agentic-ai-auto \
  --region us-west-2 \
  --compute-config enabled=true
```

**ì¥ì :**

- ì¸í”„ë¼ ê´€ë¦¬ ë¶€ë‹´ ì œë¡œ
- AWS ìµœì í™”ëœ ê¸°ë³¸ ì„¤ì •
- ë‚´ì¥ëœ ë¹„ìš© ìµœì í™”
- ìë™ ë³´ì•ˆ íŒ¨ì¹˜

**ë‹¨ì :**

- ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì— ëŒ€í•œ ì œì–´ ê°ì†Œ
- ê·¹ë‹¨ì ì¸ ë¹„ìš© ì‹œë‚˜ë¦¬ì˜¤ ìµœì í™” ì–´ë ¤ì›€
- AWS ê´€ë¦¬í˜• íƒ€ì…ìœ¼ë¡œ GPU ì§€ì› ì œí•œ

</TabItem>
<TabItem value="karpenter" label="EKS + Karpenter (ìµœëŒ€ ì œì–´)">

**ì í•©í•œ ê²½ìš°:**

- ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ ì›Œí¬ë¡œë“œ
- ë³µì¡í•œ GPU ìš”êµ¬ì‚¬í•­ (í˜¼í•© ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…)
- ë¹„ìš© ìµœì í™”ê°€ ìµœìš°ì„  (70%+ ì ˆê°)
- Kubernetes ì „ë¬¸ì„±ì„ ë³´ìœ í•œ íŒ€

**ì‹œì‘í•˜ê¸°:**

```bash
terraform apply -f eks-karpenter-blueprint/
kubectl apply -f karpenter-nodepools/
```

**ì¥ì :**

- ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•œ ì„¸ë°€í•œ ì œì–´
- ìµœëŒ€ ë¹„ìš© ìµœì í™” (70-80% ì ˆê°)
- ìœ ì—°í•œ GPU ìŠ¤ì¼€ì¤„ë§
- ì»¤ìŠ¤í…€ AMI ë° ë…¸ë“œ êµ¬ì„±

**ë‹¨ì :**

- Karpenter ê´€ë¦¬ í•„ìš”
- êµ¬ì„± ë³µì¡ë„ ì¦ê°€
- íŒ€ì— K8s ì „ë¬¸ì„± í•„ìš”

</TabItem>
<TabItem value="hybrid" label="í•˜ì´ë¸Œë¦¬ë“œ (ë‘ ë°©ì‹ì˜ ì¥ì  ê²°í•©)">

**ì í•©í•œ ê²½ìš°:**

- ì„±ì¥í•˜ëŠ” í”Œë«í¼ (ë‹¨ìˆœí•˜ê²Œ ì‹œì‘, ë³µì¡í•˜ê²Œ í™•ì¥)
- í˜¼í•© ì›Œí¬ë¡œë“œ íƒ€ì… (CPU ì—ì´ì „íŠ¸ + GPU LLM)
- Auto Modeì—ì„œ Karpenterë¡œ ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜

**ì•„í‚¤í…ì²˜:**

- Control Planeì€ EKS Auto Mode ì‚¬ìš©
- ì‹œìŠ¤í…œ ì›Œí¬ë¡œë“œëŠ” ê´€ë¦¬í˜• ë…¸ë“œ ê·¸ë£¹ì—ì„œ ì‹¤í–‰
- GPU ì›Œí¬ë¡œë“œëŠ” Karpenter NodePoolì—ì„œ ì‹¤í–‰

**ì‹œì‘í•˜ê¸°:**

```bash
# 1ë‹¨ê³„: Auto Modeë¡œ EKS í´ëŸ¬ìŠ¤í„° ìƒì„±
aws eks create-cluster --name agentic-ai --compute-config enabled=true

# 2ë‹¨ê³„: GPU ë…¸ë“œìš© Karpenter ì„¤ì¹˜
helm install karpenter oci://public.ecr.aws/karpenter/karpenter

# 3ë‹¨ê³„: GPU NodePool ë°°í¬
kubectl apply -f gpu-nodepools.yaml
```

**ì¥ì :**

- ì ì§„ì  ë³µì¡ë„ ì¦ê°€
- ì¤‘ìš”í•œ ë¶€ë¶„(GPU ë¹„ìš©)ì—ì„œ ìµœì í™”
- AWS ê´€ë¦¬í˜• Control Plane + ì»¤ìŠ¤í…€ Data Plane

**ë‹¨ì :**

- Auto Modeì™€ Karpenter ëª¨ë‘ ê´€ë¦¬ í•„ìš”
- ì ì¬ì  êµ¬ì„± ì¶©ëŒ ê°€ëŠ¥ì„±

</TabItem>
</Tabs>

### ë¯¸ë˜: AI ë„¤ì´í‹°ë¸Œ Kubernetes

**ì£¼ìš” íŠ¸ë Œë“œ:**

- **AI ìµœì í™” ìŠ¤ì¼€ì¤„ë§**: ML ê¸°ë°˜ ì¸ìŠ¤í„´ìŠ¤ ì„ íƒì„ í†µí•œ Karpenter
- **ë™ì  ëª¨ë¸ ë¼ìš°íŒ…**: ì‘ì—… ë³µì¡ë„ ê¸°ë°˜ ì§€ëŠ¥í˜• LLM ì„ íƒ
- **ì—°í•© í•™ìŠµ(Federated Learning)**: EKS Anywhereë¥¼ í†µí•œ ë©€í‹° í´ëŸ¬ìŠ¤í„° í•™ìŠµ
- **ì„œë²„ë¦¬ìŠ¤ GPU**: ê¸‰ì¦í•˜ëŠ” ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ AWS Lambda GPU ì¸ìŠ¤í„´ìŠ¤

**EKS ë¡œë“œë§µ í•˜ì´ë¼ì´íŠ¸:**

- ë„¤ì´í‹°ë¸Œ GPU ê³µìœ  (MIG/MPS ì§€ì›)
- í†µí•© ëª¨ë¸ ì„œë¹™ (SageMaker + EKS)
- ë©€í‹° í…Œë„ŒíŠ¸ AI í”Œë«í¼ì„ ìœ„í•œ ë¹„ìš© í• ë‹¹
- LLM ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ í–¥ìƒëœ ê´€ì°°ì„±

### ì§€ê¸ˆ ì‹œì‘í•˜ê¸°

**ì˜¤ëŠ˜ë¶€í„° ì‹œì‘:**

1. **í”„ë¡œí† íƒ€ì…** (1ì£¼)
   - EKS Auto Mode í´ëŸ¬ìŠ¤í„° ë°°í¬
   - ìƒ˜í”Œ Agentic AI ì›Œí¬ë¡œë“œ ì‹¤í–‰
   - ê¸°ì¤€ ë¹„ìš© ë° ì„±ëŠ¥ ì¸¡ì •

2. **ìµœì í™”** (2-4ì£¼)
   - GPU ì›Œí¬ë¡œë“œë¥¼ ìœ„í•´ Karpenterë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
   - KEDA ì˜¤í† ìŠ¤ì¼€ì¼ë§ êµ¬í˜„
   - CloudWatch ëŒ€ì‹œë³´ë“œ ì„¤ì •

3. **í™•ì¥** (ì§€ì†ì )
   - Consolidation ì •ì±… ë¯¸ì„¸ ì¡°ì •
   - í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
   - ë©€í‹° í…Œë„ŒíŠ¸ í”Œë«í¼ êµ¬ì¶•

**ë¦¬ì†ŒìŠ¤:**

- [AWS EKS Best Practices Guide](https://docs.aws.amazon.com/eks/latest/best-practices/introduction.html)
- [Karpenter Documentation](https://karpenter.sh/)
- [KEDA Scalers Reference](https://keda.sh/docs/scalers/)
- [Kubeflow on AWS](https://awslabs.github.io/kubeflow-manifests/)

**ì§ˆë¬¸ì´ ìˆìœ¼ì‹ ê°€ìš”?**

- [AWS Containers Slack](https://aws-containers.slack.com) ì°¸ì—¬
- [EKS Blueprints](https://github.com/aws-ia/terraform-aws-eks-blueprints)ì— ì´ìŠˆ ë“±ë¡
- ì•„í‚¤í…ì²˜ ê²€í† ë¥¼ ìœ„í•´ AWS Solutions Architectì—ê²Œ ë¬¸ì˜

---

**ë‹¤ìŒ ë‹¨ê³„:**

- ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ì•ˆì„ í™•ì¸í•˜ë ¤ë©´ [ê¸°ìˆ ì  ë„ì „ê³¼ì œ ë¬¸ì„œ](./agentic-ai-challenges.md)ë¥¼ ê²€í† í•˜ì„¸ìš”
- ì‹¤ìŠµì„ ìœ„í•´ [AWS EKS Workshop](https://eksworkshop.com/)ì„ íƒìƒ‰í•˜ì„¸ìš”
- ìµœì‹  íŠ¸ë Œë“œë¥¼ ìœ„í•´ [Cloud Native Community Groups](https://community.cncf.io/)ì— ì°¸ì—¬í•˜ì„¸ìš”
