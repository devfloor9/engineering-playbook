---
title: "MoE ëª¨ë¸ ì„œë¹™ ê°€ì´ë“œ"
sidebar_label: "6. MoE ëª¨ë¸ ì„œë¹™"
description: "Mixture of Experts ëª¨ë¸ì˜ EKS ê¸°ë°˜ ë°°í¬ ë° ìµœì í™” ì „ëµ"
tags: [eks, moe, vllm, tgi, model-serving, gpu, mixtral]
category: "genai-aiml"
last_update:
  date: 2026-02-14
  author: devfloor9
sidebar_position: 6
---

import { RoutingMechanisms, MoeVsDense, GpuMemoryRequirements, ParallelizationStrategies, TensorParallelismConfig, VllmVsTgi, KvCacheConfig, BatchOptimization, MonitoringMetrics, GpuVsTrainium2 } from '@site/src/components/MoeModelTables';

# MoE ëª¨ë¸ ì„œë¹™ ê°€ì´ë“œ

> **ğŸ“Œ í˜„ì¬ ë²„ì „**: vLLM v0.6.3 / v0.7.x (2025-02 ì•ˆì • ë²„ì „), TGI 3.3.5 (ìœ ì§€ë³´ìˆ˜ ëª¨ë“œ). ë³¸ ë¬¸ì„œì˜ ë°°í¬ ì˜ˆì‹œëŠ” ìµœì‹  ì•ˆì • ë²„ì „ ê¸°ì¤€ì…ë‹ˆë‹¤.

> ğŸ“… **ì‘ì„±ì¼**: 2025-02-09 | **ìˆ˜ì •ì¼**: 2026-02-14 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 9ë¶„

## ê°œìš”

Mixture of Experts(MoE) ëª¨ë¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” í˜ì‹ ì ì¸ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” Amazon EKS í™˜ê²½ì—ì„œ Mixtral, DeepSeek-MoE, Qwen-MoE ë“±ì˜ MoE ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë°°í¬í•˜ê³  ìš´ì˜í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

### ì£¼ìš” ëª©í‘œ

- **MoE ì•„í‚¤í…ì²˜ ì´í•´**: Expert ë„¤íŠ¸ì›Œí¬ì™€ ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì˜ ë™ì‘ ì›ë¦¬
- **íš¨ìœ¨ì ì¸ ë°°í¬**: vLLM ë° TGIë¥¼ í™œìš©í•œ ìµœì í™”ëœ MoE ëª¨ë¸ ì„œë¹™
- **ë¦¬ì†ŒìŠ¤ ìµœì í™”**: GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ë¶„ì‚° ë°°í¬ ì „ëµ
- **ì„±ëŠ¥ íŠœë‹**: KV Cache, Speculative Decoding ë“± ê³ ê¸‰ ìµœì í™” ê¸°ë²•

---

## MoE ì•„í‚¤í…ì²˜ ì´í•´

### Expert ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°

MoE ëª¨ë¸ì€ ì—¬ëŸ¬ ê°œì˜ "Expert" ë„¤íŠ¸ì›Œí¬ì™€ ì´ë¥¼ ì„ íƒí•˜ëŠ” "Router(Gate)" ë„¤íŠ¸ì›Œí¬ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph "MoE Layer"
        INPUT[Input Token<br/>Hidden State]
        
        subgraph "Router Network"
            GATE[Gating Network<br/>Softmax Router]
        end
        
        subgraph "Expert Networks"
            E1[Expert 1<br/>FFN]
            E2[Expert 2<br/>FFN]
            E3[Expert 3<br/>FFN]
            E4[Expert 4<br/>FFN]
            EN[Expert N<br/>FFN]
        end
        
        COMBINE[Weighted<br/>Combination]
        OUTPUT[Output<br/>Hidden State]
    end
    
    INPUT --> GATE
    GATE -->|"Top-K Selection<br/>(K=2)"| E1 & E2
    GATE -.->|"Not Selected"| E3 & E4 & EN
    E1 & E2 --> COMBINE
    COMBINE --> OUTPUT
```

### ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜

MoE ëª¨ë¸ì˜ í•µì‹¬ì€ ì…ë ¥ í† í°ì— ë”°ë¼ ì ì ˆí•œ Expertë¥¼ ì„ íƒí•˜ëŠ” ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.

<RoutingMechanisms />

:::info ë¼ìš°íŒ… ë™ì‘ ì›ë¦¬

1. **Gate ê³„ì‚°**: ì…ë ¥ í† í°ì˜ hidden stateë¥¼ Gate ë„¤íŠ¸ì›Œí¬ì— í†µê³¼
2. **Expert ì„ íƒ**: Softmax ì¶œë ¥ì—ì„œ Top-K Expert ì„ íƒ
3. **ë³‘ë ¬ ì²˜ë¦¬**: ì„ íƒëœ Expertë“¤ì´ ë³‘ë ¬ë¡œ ì…ë ¥ ì²˜ë¦¬
4. **ê°€ì¤‘ í•©ì‚°**: Expert ì¶œë ¥ì„ Gate ê°€ì¤‘ì¹˜ë¡œ ê²°í•©

:::

### MoE vs Dense ëª¨ë¸ ë¹„êµ

<MoeVsDense />

```mermaid
flowchart LR
    subgraph "Dense Model (70B)"
        D_IN[Input] --> D_ALL[All 70B<br/>Parameters<br/>Active]
        D_ALL --> D_OUT[Output]
    end
    
    subgraph "MoE Model (47B Total, ~13B Active)"
        M_IN[Input] --> M_GATE[Router]
        M_GATE --> M_E1[Expert 1<br/>7B Active]
        M_GATE --> M_E2[Expert 2<br/>7B Active]
        M_E1 & M_E2 --> M_OUT[Output]
        M_GATE -.-> M_E3[Expert 3-8<br/>Inactive]
    end
```

:::tip MoE ëª¨ë¸ì˜ ì¥ì 

- **ì—°ì‚° íš¨ìœ¨ì„±**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ ì¼ë¶€ë§Œ í™œì„±í™”í•˜ì—¬ ì¶”ë¡  ì†ë„ í–¥ìƒ
- **í™•ì¥ì„±**: Expert ì¶”ê°€ë¡œ ëª¨ë¸ ìš©ëŸ‰ í™•ì¥ ê°€ëŠ¥
- **ì „ë¬¸í™”**: ê° Expertê°€ íŠ¹ì • ë„ë©”ì¸/íƒœìŠ¤í¬ì— íŠ¹í™”

:::

---

## MoE ëª¨ë¸ ì„œë¹™ ê³ ë ¤ì‚¬í•­

### GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­

MoE ëª¨ë¸ì€ í™œì„±í™”ë˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” ì ì§€ë§Œ, ì „ì²´ Expertë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.

<GpuMemoryRequirements />

:::info DeepSeek-V3 ë©”ëª¨ë¦¬ ìµœì í™”
DeepSeek-V3ëŠ” Multi-head Latent Attention (MLA) ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ KV ìºì‹œ ë©”ëª¨ë¦¬ë¥¼ í¬ê²Œ ì ˆê°í•©ë‹ˆë‹¤. ì „í†µì ì¸ MHA(Multi-Head Attention) ëŒ€ë¹„ ì•½ 40% ë©”ëª¨ë¦¬ ì ˆê° íš¨ê³¼ê°€ ìˆì–´, ì‹¤ì œ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì€ í‘œê¸°ëœ ê°’ë³´ë‹¤ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì •í™•í•œ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì€ ë°°ì¹˜ í¬ê¸°ì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¼ ë‹¬ë¼ì§€ë¯€ë¡œ í”„ë¡œíŒŒì¼ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
:::

:::warning ë©”ëª¨ë¦¬ ê³„ì‚° ì‹œ ì£¼ì˜ì‚¬í•­

- **KV Cache**: ë°°ì¹˜ í¬ê¸°ì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¼ ì¶”ê°€ ë©”ëª¨ë¦¬ í•„ìš”
- **Activation Memory**: ì¶”ë¡  ì¤‘ ì¤‘ê°„ í™œì„±í™” ê°’ ì €ì¥ ê³µê°„
- **CUDA Context**: GPUë‹¹ ì•½ 1-2GBì˜ CUDA ì˜¤ë²„í—¤ë“œ
- **Safety Margin**: ì‹¤ì œ ìš´ì˜ ì‹œ 10-20% ì—¬ìœ  ê³µê°„ í™•ë³´ ê¶Œì¥

:::

### ë¶„ì‚° ë°°í¬ ì „ëµ

ëŒ€ê·œëª¨ MoE ëª¨ë¸ì€ ë‹¨ì¼ GPUì— ë¡œë“œí•  ìˆ˜ ì—†ì–´ ë¶„ì‚° ë°°í¬ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph "Tensor Parallelism (TP=4)"
        direction LR
        TP1[GPU 0<br/>Layer 1-N<br/>Shard 1/4]
        TP2[GPU 1<br/>Layer 1-N<br/>Shard 2/4]
        TP3[GPU 2<br/>Layer 1-N<br/>Shard 3/4]
        TP4[GPU 3<br/>Layer 1-N<br/>Shard 4/4]
        TP1 <--> TP2 <--> TP3 <--> TP4
    end
    
    subgraph "Expert Parallelism (EP=2)"
        direction LR
        EP1[GPU 0-1<br/>Expert 1-4]
        EP2[GPU 2-3<br/>Expert 5-8]
        EP1 <-.-> EP2
    end
    
    subgraph "Pipeline Parallelism (PP=2)"
        direction TB
        PP1[GPU 0-3<br/>Layer 1-16]
        PP2[GPU 4-7<br/>Layer 17-32]
        PP1 --> PP2
    end
```

<ParallelizationStrategies />

### Expert í™œì„±í™” íŒ¨í„´

MoE ëª¨ë¸ì˜ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ Expert í™œì„±í™” íŒ¨í„´ì„ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph "Token Distribution"
        T1[Token 1] --> E1[Expert 1]
        T2[Token 2] --> E3[Expert 3]
        T3[Token 3] --> E1[Expert 1]
        T4[Token 4] --> E2[Expert 2]
        T5[Token 5] --> E4[Expert 4]
    end
    
    subgraph "Load Imbalance"
        E1_LOAD[Expert 1<br/>40% Load]
        E2_LOAD[Expert 2<br/>20% Load]
        E3_LOAD[Expert 3<br/>25% Load]
        E4_LOAD[Expert 4<br/>15% Load]
    end
```

:::info Expert ë¡œë“œ ë°¸ëŸ°ì‹±

- **Auxiliary Loss**: í•™ìŠµ ì‹œ Expert ê°„ ê· ë“± ë¶„ë°°ë¥¼ ìœ ë„í•˜ëŠ” ë³´ì¡° ì†ì‹¤
- **Capacity Factor**: Expertë‹¹ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìµœëŒ€ í† í° ìˆ˜ ì œí•œ
- **Token Dropping**: ìš©ëŸ‰ ì´ˆê³¼ ì‹œ í† í° ë“œë¡­ (ì¶”ë¡  ì‹œ ë¹„í™œì„±í™” ê¶Œì¥)

:::

---

## vLLM ê¸°ë°˜ MoE ë°°í¬

### vLLM MoE ì§€ì› ê¸°ëŠ¥

vLLM v0.6+ ë²„ì „ì€ MoE ëª¨ë¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ìµœì í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

- **Expert Parallelism**: ë‹¤ì¤‘ GPUì— Expert ë¶„ì‚°
- **Tensor Parallelism**: ë ˆì´ì–´ ë‚´ í…ì„œ ë¶„í• 
- **PagedAttention**: íš¨ìœ¨ì ì¸ KV Cache ê´€ë¦¬
- **Continuous Batching**: ë™ì  ë°°ì¹˜ ì²˜ë¦¬
- **FP8 KV Cache**: 2ë°° ë©”ëª¨ë¦¬ ì ˆê° (v0.6+)
- **Improved Prefix Caching**: 400%+ ì²˜ë¦¬ëŸ‰ í–¥ìƒ (v0.6+)
- **Multi-LoRA Serving**: ë‹¨ì¼ ê¸°ë³¸ ëª¨ë¸ì—ì„œ ì—¬ëŸ¬ LoRA ì–´ëŒ‘í„° ë™ì‹œ ì„œë¹™ (v0.6+)
- **GGUF Quantization**: GGUF í˜•ì‹ ì–‘ìí™” ëª¨ë¸ ì§€ì› (v0.6+)

### Mixtral 8x7B Deployment YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x7b-vllm
  namespace: inference
  labels:
    app: mixtral-8x7b
    serving-engine: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x7b
  template:
    metadata:
      labels:
        app: mixtral-8x7b
        serving-engine: vllm
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: p4d.24xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.3
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: VLLM_ATTENTION_BACKEND
              value: "FLASH_ATTN"
          args:
            - "--model"
            - "mistralai/Mixtral-8x7B-Instruct-v0.1"
            - "--tensor-parallel-size"
            - "2"
            - "--max-model-len"
            - "32768"
            - "--gpu-memory-utilization"
            - "0.90"
            - "--enable-chunked-prefill"
            - "--max-num-batched-tokens"
            - "32768"
            - "--enable-prefix-caching"
            - "--kv-cache-dtype"
            - "fp8"
            - "--trust-remote-code"
            - "--dtype"
            - "bfloat16"
          resources:
            requests:
              nvidia.com/gpu: 2
              memory: "180Gi"
              cpu: "24"
            limits:
              nvidia.com/gpu: 2
              memory: "200Gi"
              cpu: "32"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 300
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 5
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      terminationGracePeriodSeconds: 120
```

### Mixtral 8x22B ëŒ€ê·œëª¨ ë°°í¬ (4 GPU)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x22b-vllm
  namespace: inference
  labels:
    app: mixtral-8x22b
    serving-engine: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x22b
  template:
    metadata:
      labels:
        app: mixtral-8x22b
        serving-engine: vllm
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: p5.48xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.3
          ports:
            - name: http
              containerPort: 8000
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: NCCL_DEBUG
              value: "INFO"
            - name: NCCL_IB_DISABLE
              value: "0"
          args:
            - "--model"
            - "mistralai/Mixtral-8x22B-Instruct-v0.1"
            - "--tensor-parallel-size"
            - "4"
            - "--max-model-len"
            - "65536"
            - "--gpu-memory-utilization"
            - "0.92"
            - "--enable-chunked-prefill"
            - "--max-num-batched-tokens"
            - "65536"
            - "--enable-prefix-caching"
            - "--kv-cache-dtype"
            - "fp8"
            - "--dtype"
            - "bfloat16"
            - "--enforce-eager"
          resources:
            requests:
              nvidia.com/gpu: 4
              memory: "400Gi"
              cpu: "48"
            limits:
              nvidia.com/gpu: 4
              memory: "500Gi"
              cpu: "64"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
```

### vLLM í…ì„œ ë³‘ë ¬í™” ì„¤ì •

í…ì„œ ë³‘ë ¬í™”(Tensor Parallelism)ëŠ” ëª¨ë¸ì˜ ê° ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ GPUì— ë¶„í• í•©ë‹ˆë‹¤.

<TensorParallelismConfig />

:::tip í…ì„œ ë³‘ë ¬í™” ìµœì í™”

- **NVLink í™œìš©**: GPU ê°„ ê³ ì† í†µì‹ ì„ ìœ„í•´ NVLink ì§€ì› ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
- **TP í¬ê¸° ì„ íƒ**: ëª¨ë¸ í¬ê¸°ì™€ GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ìµœì†Œ TP í¬ê¸° ì„ íƒ
- **í†µì‹  ì˜¤ë²„í—¤ë“œ**: TP í¬ê¸°ê°€ í´ìˆ˜ë¡ All-Reduce í†µì‹  ì¦ê°€

:::

### vLLM Expert ë³‘ë ¬í™” ì„¤ì •

Expert ë³‘ë ¬í™”(Expert Parallelism)ëŠ” MoE ëª¨ë¸ì˜ Expertë¥¼ ì—¬ëŸ¬ GPUì— ë¶„ì‚°í•©ë‹ˆë‹¤.

```yaml
# Expert Parallelism í™œì„±í™” ì˜ˆì œ
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  - "--tensor-parallel-size"
  - "2"
  # Expert Parallelismì€ vLLMì—ì„œ ìë™ìœ¼ë¡œ ìµœì í™”ë¨
  # TP ë‚´ì—ì„œ Expertê°€ ë¶„ì‚° ë°°ì¹˜ë¨
  - "--distributed-executor-backend"
  - "ray"  # ë˜ëŠ” "mp" (multiprocessing)
```

---

## TGI ê¸°ë°˜ MoE ë°°í¬ (Legacy)

:::danger TGI ìœ ì§€ë³´ìˆ˜ ëª¨ë“œ - ì‹ ê·œ ë°°í¬ ë¹„ê¶Œì¥
Text Generation Inference(TGI)ëŠ” 2025ë…„ë¶€í„° ìœ ì§€ë³´ìˆ˜ ëª¨ë“œì— ì§„ì…í–ˆìŠµë‹ˆë‹¤. Hugging FaceëŠ” í–¥í›„ vLLM, SGLang ë“± ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì¶”ë¡  ì—”ì§„ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤. 

**ì‹ ê·œ ë°°í¬ì—ëŠ” vLLMì„ ì‚¬ìš©í•˜ì„¸ìš”.** ê¸°ì¡´ TGI ë°°í¬ëŠ” ê³„ì† ë™ì‘í•˜ì§€ë§Œ, ìƒˆë¡œìš´ ê¸°ëŠ¥ ì—…ë°ì´íŠ¸ë‚˜ ìµœì í™”ëŠ” ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
:::

### TGIì—ì„œ vLLMìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜

TGIë¥¼ ì‚¬ìš© ì¤‘ì´ë¼ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ vLLMìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **API í˜¸í™˜ì„±**: vLLMì€ OpenAI í˜¸í™˜ APIë¥¼ ì œê³µí•˜ë¯€ë¡œ í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ ë³€ê²½ ìµœì†Œí™”
2. **í™˜ê²½ ë³€ìˆ˜ ë§¤í•‘**:
   - TGI `MODEL_ID` â†’ vLLM `--model`
   - TGI `NUM_SHARD` â†’ vLLM `--tensor-parallel-size`
   - TGI `MAX_TOTAL_TOKENS` â†’ vLLM `--max-model-len`
3. **ì„±ëŠ¥ í–¥ìƒ**: vLLMì˜ PagedAttentionê³¼ Continuous Batchingìœ¼ë¡œ 2-3ë°° ì²˜ë¦¬ëŸ‰ í–¥ìƒ
4. **ìµœì‹  ê¸°ëŠ¥**: FP8 KV Cache, Prefix Caching, Multi-LoRA ë“± ìµœì‹  ìµœì í™” ê¸°ë²• í™œìš©

### TGI MoE ì§€ì› ê¸°ëŠ¥ (Legacy)

Text Generation Inference(TGI)ëŠ” Hugging Faceì—ì„œ ê°œë°œí•œ ê³ ì„±ëŠ¥ ì¶”ë¡  ì„œë²„ì…ë‹ˆë‹¤.

- **Flash Attention 2**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ì—°ì‚°
- **Paged Attention**: ë™ì  KV Cache ê´€ë¦¬
- **Tensor Parallelism**: ë‹¤ì¤‘ GPU ë¶„ì‚° ì¶”ë¡ 
- **Quantization**: AWQ, GPTQ, EETQ ì§€ì›

### TGI Mixtral 8x7B Deployment YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x7b-tgi
  namespace: inference
  labels:
    app: mixtral-8x7b
    serving-engine: tgi
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x7b-tgi
  template:
    metadata:
      labels:
        app: mixtral-8x7b-tgi
        serving-engine: tgi
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: p4d.24xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: tgi
          image: ghcr.io/huggingface/text-generation-inference:3.3.5
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: MODEL_ID
              value: "mistralai/Mixtral-8x7B-Instruct-v0.1"
            - name: NUM_SHARD
              value: "2"
            - name: MAX_INPUT_LENGTH
              value: "8192"
            - name: MAX_TOTAL_TOKENS
              value: "32768"
            - name: MAX_BATCH_PREFILL_TOKENS
              value: "32768"
            - name: DTYPE
              value: "bfloat16"
            - name: QUANTIZE
              value: ""  # ë˜ëŠ” "awq", "gptq"
            - name: TRUST_REMOTE_CODE
              value: "true"
          resources:
            requests:
              nvidia.com/gpu: 2
              memory: "180Gi"
              cpu: "24"
            limits:
              nvidia.com/gpu: 2
              memory: "200Gi"
              cpu: "32"
          volumeMounts:
            - name: model-cache
              mountPath: /data
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 300
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 120
            periodSeconds: 10
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
```

### TGI ì–‘ìí™” ë°°í¬ (AWQ)

ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ AWQ ì–‘ìí™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x7b-tgi-awq
  namespace: inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x7b-tgi-awq
  template:
    metadata:
      labels:
        app: mixtral-8x7b-tgi-awq
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: g5.48xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: tgi
          image: ghcr.io/huggingface/text-generation-inference:3.3.5
          ports:
            - name: http
              containerPort: 8080
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: MODEL_ID
              value: "TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ"
            - name: NUM_SHARD
              value: "2"
            - name: MAX_INPUT_LENGTH
              value: "8192"
            - name: MAX_TOTAL_TOKENS
              value: "16384"
            - name: QUANTIZE
              value: "awq"
          resources:
            requests:
              nvidia.com/gpu: 2
              memory: "90Gi"
              cpu: "16"
            limits:
              nvidia.com/gpu: 2
              memory: "120Gi"
              cpu: "24"
```

### vLLM vs TGI ì„±ëŠ¥ ë¹„êµ

<VllmVsTgi />

:::tip ì¶”ë¡  ì—”ì§„ ì„ íƒ ê°€ì´ë“œ

- **vLLM**: ìµœê³  ì²˜ë¦¬ëŸ‰ì´ í•„ìš”í•œ ê²½ìš°, ëŒ€ê·œëª¨ ë°°ì¹˜ ì²˜ë¦¬
- **TGI**: Hugging Face ìƒíƒœê³„ í†µí•©, ê°„í¸í•œ ë°°í¬
- **ê³µí†µ**: ë‘ ì—”ì§„ ëª¨ë‘ MoE ëª¨ë¸ì„ ì˜ ì§€ì›í•˜ë©°, ì›Œí¬ë¡œë“œì— ë”°ë¼ ì„ íƒ

:::

---

## AWS Trainium2 ê¸°ë°˜ MoE ë°°í¬

### Trainium2 ê°œìš”

AWS Trainium2ëŠ” AWSê°€ ì„¤ê³„í•œ 2ì„¸ëŒ€ ML ê°€ì†ê¸°ë¡œ, ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ì¶”ë¡ ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. GPU ëŒ€ë¹„ ë¹„ìš© íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ì œê³µí•˜ë©°, NeuronX SDKë¥¼ í†µí•´ PyTorch ëª¨ë¸ì„ ì‰½ê²Œ ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- **ê³ ì„±ëŠ¥**: ë‹¨ì¼ trn2.48xlarge ì¸ìŠ¤í„´ìŠ¤ì—ì„œ Llama 3.1 405B ì¶”ë¡  ê°€ëŠ¥
- **ë¹„ìš© íš¨ìœ¨**: GPU ëŒ€ë¹„ ìµœëŒ€ 50% ë¹„ìš© ì ˆê°
- **NeuronX SDK**: PyTorch 2.5+ ì§€ì›, ìµœì†Œ ì½”ë“œ ë³€ê²½ìœ¼ë¡œ ëª¨ë¸ ì˜¨ë³´ë”©
- **NxD Inference**: ëŒ€ê·œëª¨ LLM ë°°í¬ë¥¼ ë‹¨ìˆœí™”í•˜ëŠ” PyTorch ê¸°ë°˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **FP8 ì–‘ìí™”**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
- **Flash Decoding**: Speculative Decoding ì§€ì›

### Trainium2 ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë° ë¹„ìš© ë¹„êµ

<GpuVsTrainium2 />

### NeuronX SDK ì„¤ì¹˜

```bash
# Neuron SDK 2.21+ ì„¤ì¹˜
pip install neuronx-cc==2.* torch-neuronx torchvision

# NxD Inference ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install neuronx-distributed-inference

# Transformers NeuronX ì„¤ì¹˜
pip install transformers-neuronx
```

### Mixtral 8x7B Trainium2 ë°°í¬

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x7b-trainium2
  namespace: inference
  labels:
    app: mixtral-8x7b
    accelerator: trainium2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x7b-trainium2
  template:
    metadata:
      labels:
        app: mixtral-8x7b-trainium2
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: trn2.48xlarge
      tolerations:
        - key: aws.amazon.com/neuron
          operator: Exists
          effect: NoSchedule
      containers:
        - name: neuron-inference
          image: public.ecr.aws/neuron/pytorch-inference-neuronx:2.1.2-neuronx-py310-sdk2.21.0-ubuntu20.04
          ports:
            - name: http
              containerPort: 8000
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: NEURON_RT_NUM_CORES
              value: "16"
            - name: NEURON_CC_FLAGS
              value: "--model-type=transformer"
          command:
            - python
            - -m
            - neuronx_distributed_inference.serve
          args:
            - --model_id
            - mistralai/Mixtral-8x7B-Instruct-v0.1
            - --batch_size
            - "4"
            - --sequence_length
            - "2048"
            - --tp_degree
            - "8"
            - --amp
            - "fp16"
          resources:
            requests:
              aws.amazon.com/neuron: 16
              memory: "400Gi"
              cpu: "96"
            limits:
              aws.amazon.com/neuron: 16
              memory: "480Gi"
              cpu: "128"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: neuron-cache
              mountPath: /var/tmp/neuron-compile-cache
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: neuron-cache
          emptyDir:
            sizeLimit: 50Gi
```

### NxD Inference Python ì˜ˆì œ

```python
# neuron_inference.py
import torch
from transformers import AutoTokenizer
from neuronx_distributed_inference import NxDInference

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# NxD Inference ì´ˆê¸°í™”
nxd_model = NxDInference(
    model_id=model_id,
    tp_degree=8,  # Tensor Parallelism
    batch_size=4,
    sequence_length=2048,
    amp="fp16",
    neuron_config={
        "enable_bucketing": True,
        "enable_saturate_infinity": True,
    }
)

# ì¶”ë¡  ìˆ˜í–‰
prompt = "Explain Mixture of Experts architecture"
inputs = tokenizer(prompt, return_tensors="pt")

with torch.inference_mode():
    outputs = nxd_model.generate(
        input_ids=inputs.input_ids,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```


:::tip Trainium2 ì‚¬ìš© ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤

- **ë¹„ìš© ìµœì í™”**: GPU ëŒ€ë¹„ 50% ì´ìƒ ë¹„ìš© ì ˆê°ì´ í•„ìš”í•œ ê²½ìš°
- **ëŒ€ê·œëª¨ ë°°í¬**: ìˆ˜ì‹­~ìˆ˜ë°± ê°œì˜ ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ ìš´ì˜
- **ì•ˆì •ì ì¸ ì›Œí¬ë¡œë“œ**: ì‹¤í—˜ì  ê¸°ëŠ¥ë³´ë‹¤ ì•ˆì •ì„±ê³¼ ë¹„ìš©ì´ ì¤‘ìš”í•œ í”„ë¡œë•ì…˜ í™˜ê²½
- **AWS ë„¤ì´í‹°ë¸Œ**: AWS ìƒíƒœê³„ ë‚´ì—ì„œ ì™„ì „ ê´€ë¦¬í˜• ì†”ë£¨ì…˜ ì„ í˜¸

:::

:::warning Trainium2 ì œì•½ì‚¬í•­

- **ëª¨ë¸ ì§€ì›**: ëª¨ë“  ëª¨ë¸ì´ ì§€ì›ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, NeuronX SDK í˜¸í™˜ì„± í™•ì¸ í•„ìš”
- **ì»¤ìŠ¤í…€ ì»¤ë„**: ì¼ë¶€ ì»¤ìŠ¤í…€ CUDA ì»¤ë„ì€ Neuronìœ¼ë¡œ í¬íŒ… í•„ìš”
- **ë””ë²„ê¹…**: GPU ëŒ€ë¹„ ë””ë²„ê¹… ë„êµ¬ê°€ ì œí•œì 
- **ë¦¬ì „ ê°€ìš©ì„±**: ì¼ë¶€ AWS ë¦¬ì „ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥

:::

---

## Service ë° Ingress ì„¤ì •

### MoE ëª¨ë¸ Service YAML

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mixtral-8x7b-service
  namespace: inference
  labels:
    app: mixtral-8x7b
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8000
      targetPort: 8000
      protocol: TCP
  selector:
    app: mixtral-8x7b
---
apiVersion: v1
kind: Service
metadata:
  name: mixtral-8x7b-tgi-service
  namespace: inference
  labels:
    app: mixtral-8x7b-tgi
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: 8080
      protocol: TCP
  selector:
    app: mixtral-8x7b-tgi
```

### Gateway API HTTPRoute ì„¤ì •

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: moe-model-route
  namespace: inference
spec:
  parentRefs:
    - name: inference-gateway
      namespace: kgateway-system
  hostnames:
    - "inference.example.com"
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /v1/mixtral
      backendRefs:
        - name: mixtral-8x7b-service
          port: 8000
      filters:
        - type: URLRewrite
          urlRewrite:
            path:
              type: ReplacePrefixMatch
              replacePrefixMatch: /v1
    - matches:
        - path:
            type: PathPrefix
            value: /v1/mixtral-tgi
      backendRefs:
        - name: mixtral-8x7b-tgi-service
          port: 8080
```

---

## ì„±ëŠ¥ ìµœì í™”

### KV Cache ìµœì í™”

KV CacheëŠ” ì¶”ë¡  ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph "Traditional KV Cache"
        T1[Token 1 KV] --> T2[Token 2 KV] --> T3[Token 3 KV]
        T3 --> WASTE[Wasted<br/>Memory]
    end
    
    subgraph "PagedAttention (vLLM)"
        P1[Page 1<br/>Token 1-4]
        P2[Page 2<br/>Token 5-8]
        P3[Page 3<br/>Token 9-12]
        POOL[Memory Pool<br/>Dynamic Allocation]
        P1 & P2 & P3 --> POOL
    end
```

#### vLLM KV Cache ì„¤ì •

```yaml
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  # GPU ë©”ëª¨ë¦¬ ì¤‘ KV Cacheì— í• ë‹¹í•  ë¹„ìœ¨
  - "--gpu-memory-utilization"
  - "0.90"
  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (KV Cache í¬ê¸°ì— ì˜í–¥)
  - "--max-model-len"
  - "32768"
  # Chunked Prefillë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ í–¥ìƒ
  - "--enable-chunked-prefill"
  # ë°°ì¹˜ë‹¹ ìµœëŒ€ í† í° ìˆ˜
  - "--max-num-batched-tokens"
  - "32768"
```

<KvCacheConfig />

### Speculative Decoding

Speculative Decodingì€ ì‘ì€ ë“œë˜í”„íŠ¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant Draft as Draft Model<br/>(Small)
    participant Target as Target Model<br/>(Mixtral 8x7B)
    participant Output as Output
    
    Note over Draft,Output: Speculative Decoding ê³¼ì •
    
    Draft->>Draft: Kê°œ í† í° ë¹ ë¥´ê²Œ ìƒì„±
    Draft->>Target: Kê°œ í† í° ê²€ì¦ ìš”ì²­
    Target->>Target: ë³‘ë ¬ë¡œ Kê°œ í† í° ê²€ì¦
    
    alt ëª¨ë“  í† í° ìŠ¹ì¸
        Target->>Output: Kê°œ í† í° ì¶œë ¥
    else ì¼ë¶€ í† í° ê±°ë¶€
        Target->>Output: ìŠ¹ì¸ëœ í† í°ë§Œ ì¶œë ¥
        Target->>Draft: ê±°ë¶€ ì§€ì ë¶€í„° ì¬ìƒì„±
    end
```

#### vLLM Speculative Decoding ì„¤ì •

```yaml
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  - "--tensor-parallel-size"
  - "2"
  # Speculative Decoding í™œì„±í™”
  - "--speculative-model"
  - "mistralai/Mistral-7B-Instruct-v0.2"
  # ë“œë˜í”„íŠ¸ ëª¨ë¸ì´ ìƒì„±í•  í† í° ìˆ˜
  - "--num-speculative-tokens"
  - "5"
  # ë“œë˜í”„íŠ¸ ëª¨ë¸ í…ì„œ ë³‘ë ¬ í¬ê¸°
  - "--speculative-draft-tensor-parallel-size"
  - "1"
```

:::info Speculative Decoding íš¨ê³¼

- **ì†ë„ í–¥ìƒ**: 1.5x - 2.5x ì²˜ë¦¬ëŸ‰ ì¦ê°€ (ì›Œí¬ë¡œë“œì— ë”°ë¼ ë‹¤ë¦„)
- **í’ˆì§ˆ ìœ ì§€**: ì¶œë ¥ í’ˆì§ˆì€ ë™ì¼ (ê²€ì¦ ê³¼ì •ìœ¼ë¡œ ë³´ì¥)
- **ì¶”ê°€ ë©”ëª¨ë¦¬**: ë“œë˜í”„íŠ¸ ëª¨ë¸ì„ ìœ„í•œ ì¶”ê°€ GPU ë©”ëª¨ë¦¬ í•„ìš”

:::

### ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ëŠ” GPU í™œìš©ë¥ ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

```yaml
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  # Continuous Batching ê´€ë ¨ ì„¤ì •
  - "--max-num-seqs"
  - "256"  # ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìµœëŒ€ ì‹œí€€ìŠ¤ ìˆ˜
  - "--max-num-batched-tokens"
  - "32768"  # ë°°ì¹˜ë‹¹ ìµœëŒ€ í† í° ìˆ˜
  # Prefillê³¼ Decode ë¶„ë¦¬
  - "--enable-chunked-prefill"
  - "--max-num-batched-tokens"
  - "32768"
```

<BatchOptimization />

---

## ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### MoE ëª¨ë¸ ì „ìš© ë©”íŠ¸ë¦­

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: moe-model-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: mixtral-8x7b
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
  namespaceSelector:
    matchNames:
      - inference
```

### ì£¼ìš” ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

<MonitoringMetrics />

### Prometheus ì•Œë¦¼ ê·œì¹™

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: moe-model-alerts
  namespace: monitoring
spec:
  groups:
    - name: moe-model-alerts
      rules:
        - alert: MoEModelHighLatency
          expr: |
            histogram_quantile(0.95, 
              rate(vllm:e2e_request_latency_seconds_bucket[5m])
            ) > 30
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "MoE ëª¨ë¸ ì‘ë‹µ ì§€ì—° (P95 > 30ì´ˆ)"
            description: "{{ $labels.model_name }} ëª¨ë¸ì˜ P95 ì§€ì—°ì‹œê°„ì´ 30ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."
            
        - alert: MoEModelKVCacheFull
          expr: vllm:gpu_cache_usage_perc > 0.95
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "KV Cache ìš©ëŸ‰ ë¶€ì¡±"
            description: "KV Cache ì‚¬ìš©ë¥ ì´ 95%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìƒˆ ìš”ì²­ì´ ê±°ë¶€ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            
        - alert: MoEModelQueueBacklog
          expr: vllm:num_requests_waiting > 100
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "ìš”ì²­ ëŒ€ê¸°ì—´ ì¦ê°€"
            description: "ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ì´ 100ê°œë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìŠ¤ì¼€ì¼ ì•„ì›ƒì„ ê³ ë ¤í•˜ì„¸ìš”."
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²° ë°©ë²•

#### OOM (Out of Memory) ì˜¤ë¥˜

```bash
# ì¦ìƒ: CUDA out of memory ì˜¤ë¥˜
# í•´ê²° ë°©ë²•:
# 1. gpu-memory-utilization ê°’ ë‚®ì¶”ê¸°
--gpu-memory-utilization 0.85

# 2. max-model-len ì¤„ì´ê¸°
--max-model-len 16384

# 3. í…ì„œ ë³‘ë ¬ í¬ê¸° ëŠ˜ë¦¬ê¸° (ë” ë§ì€ GPU ì‚¬ìš©)
--tensor-parallel-size 4
```

#### ëŠë¦° ëª¨ë¸ ë¡œë”©

```bash
# ì¦ìƒ: ëª¨ë¸ ë¡œë”©ì— 10ë¶„ ì´ìƒ ì†Œìš”
# í•´ê²° ë°©ë²•:
# 1. ëª¨ë¸ ìºì‹œ PVC ì‚¬ìš©
# 2. FSx for Lustre ì‚¬ìš©ìœ¼ë¡œ ë¹ ë¥¸ ëª¨ë¸ ë¡œë”©
# 3. ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ
```

#### Expert ë¡œë“œ ë¶ˆê· í˜•

```bash
# ì¦ìƒ: íŠ¹ì • GPUë§Œ ë†’ì€ ì‚¬ìš©ë¥ 
# í•´ê²° ë°©ë²•:
# 1. ë°°ì¹˜ í¬ê¸° ì¦ê°€ë¡œ í† í° ë¶„ì‚° ê°œì„ 
--max-num-seqs 256

# 2. ë‹¤ì–‘í•œ ì…ë ¥ìœ¼ë¡œ Expert í™œì„±í™” ë¶„ì‚°
```

:::warning ë””ë²„ê¹… íŒ

- **ë¡œê·¸ ë ˆë²¨ ì¡°ì •**: `VLLM_LOGGING_LEVEL=DEBUG` í™˜ê²½ ë³€ìˆ˜ë¡œ ìƒì„¸ ë¡œê·¸ í™•ì¸
- **NCCL ë””ë²„ê·¸**: `NCCL_DEBUG=INFO`ë¡œ GPU ê°„ í†µì‹  ë¬¸ì œ ì§„ë‹¨
- **ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§**: `nvidia-smi dmon`ìœ¼ë¡œ ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§

:::

---

## ìš”ì•½

MoE ëª¨ë¸ ì„œë¹™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### í•µì‹¬ í¬ì¸íŠ¸

1. **ì•„í‚¤í…ì²˜ ì´í•´**: Expert ë„¤íŠ¸ì›Œí¬ì™€ ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì˜ ë™ì‘ ì›ë¦¬ íŒŒì•…
2. **ë©”ëª¨ë¦¬ ê³„íš**: ì „ì²´ Expertë¥¼ ë¡œë“œí•´ì•¼ í•˜ë¯€ë¡œ ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ í™•ë³´
3. **ë¶„ì‚° ë°°í¬**: í…ì„œ ë³‘ë ¬í™”ì™€ Expert ë³‘ë ¬í™”ë¥¼ ì ì ˆíˆ ì¡°í•©
4. **ì¶”ë¡  ì—”ì§„ ì„ íƒ**: vLLM(ê³ ì²˜ë¦¬ëŸ‰) ë˜ëŠ” TGI(ê°„í¸í•œ ë°°í¬) ì„ íƒ
5. **ì„±ëŠ¥ ìµœì í™”**: KV Cache, Speculative Decoding, ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ì ìš©

### ë‹¤ìŒ ë‹¨ê³„

- [GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬](./gpu-resource-management.md) - GPU í´ëŸ¬ìŠ¤í„° ë™ì  ë¦¬ì†ŒìŠ¤ í• ë‹¹
- [Inference Gateway ë¼ìš°íŒ…](./inference-gateway-routing.md) - ë‹¤ì¤‘ ëª¨ë¸ ë¼ìš°íŒ… ì „ëµ
- [Agentic AI í”Œë«í¼ ì•„í‚¤í…ì²˜](./agentic-platform-architecture.md) - ì „ì²´ í”Œë«í¼ êµ¬ì„±

---

## ì°¸ê³  ìë£Œ

- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)
- [TGI ê³µì‹ ë¬¸ì„œ](https://huggingface.co/docs/text-generation-inference)
- [Mixtral ëª¨ë¸ ì¹´ë“œ](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
- [MoE ì•„í‚¤í…ì²˜ ë…¼ë¬¸](https://arxiv.org/abs/2101.03961)
- [PagedAttention ë…¼ë¬¸](https://arxiv.org/abs/2309.06180)
