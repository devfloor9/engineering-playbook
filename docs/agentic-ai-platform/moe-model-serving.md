---
title: "MoE ëª¨ë¸ ì„œë¹™ ê°€ì´ë“œ"
sidebar_label: "MoE ëª¨ë¸ ì„œë¹™"
description: "Mixture of Experts ëª¨ë¸ì˜ EKS ê¸°ë°˜ ë°°í¬ ë° ìµœì í™” ì „ëµ"
tags: [eks, moe, vllm, tgi, model-serving, gpu, mixtral]
category: "genai-aiml"
date: 2025-02-05
authors: [devfloor9]
sidebar_position: 7
---

# MoE ëª¨ë¸ ì„œë¹™ ê°€ì´ë“œ

> ğŸ“… **ì‘ì„±ì¼**: 2025-02-05 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 12ë¶„

## ê°œìš”

Mixture of Experts(MoE) ëª¨ë¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” í˜ì‹ ì ì¸ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” Amazon EKS í™˜ê²½ì—ì„œ Mixtral, DeepSeek-MoE, Qwen-MoE ë“±ì˜ MoE ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë°°í¬í•˜ê³  ìš´ì˜í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

### ì£¼ìš” ëª©í‘œ

- **MoE ì•„í‚¤í…ì²˜ ì´í•´**: Expert ë„¤íŠ¸ì›Œí¬ì™€ ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì˜ ë™ì‘ ì›ë¦¬
- **íš¨ìœ¨ì ì¸ ë°°í¬**: vLLM ë° TGIë¥¼ í™œìš©í•œ ìµœì í™”ëœ MoE ëª¨ë¸ ì„œë¹™
- **ë¦¬ì†ŒìŠ¤ ìµœì í™”**: GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ë¶„ì‚° ë°°í¬ ì „ëµ
- **ì„±ëŠ¥ íŠœë‹**: KV Cache, Speculative Decoding ë“± ê³ ê¸‰ ìµœì í™” ê¸°ë²•

---

## MoE ì•„í‚¤í…ì²˜ ì´í•´

### Expert ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°

MoE ëª¨ë¸ì€ ì—¬ëŸ¬ ê°œì˜ "Expert" ë„¤íŠ¸ì›Œí¬ì™€ ì´ë¥¼ ì„ íƒí•˜ëŠ” "Router(Gate)" ë„¤íŠ¸ì›Œí¬ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph "MoE Layer"
        INPUT[Input Token<br/>Hidden State]
        
        subgraph "Router Network"
            GATE[Gating Network<br/>Softmax Router]
        end
        
        subgraph "Expert Networks"
            E1[Expert 1<br/>FFN]
            E2[Expert 2<br/>FFN]
            E3[Expert 3<br/>FFN]
            E4[Expert 4<br/>FFN]
            EN[Expert N<br/>FFN]
        end
        
        COMBINE[Weighted<br/>Combination]
        OUTPUT[Output<br/>Hidden State]
    end
    
    INPUT --> GATE
    GATE -->|"Top-K Selection<br/>(K=2)"| E1 & E2
    GATE -.->|"Not Selected"| E3 & E4 & EN
    E1 & E2 --> COMBINE
    COMBINE --> OUTPUT
```

### ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜

MoE ëª¨ë¸ì˜ í•µì‹¬ì€ ì…ë ¥ í† í°ì— ë”°ë¼ ì ì ˆí•œ Expertë¥¼ ì„ íƒí•˜ëŠ” ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.

| ë¼ìš°íŒ… ë°©ì‹ | ì„¤ëª… | ëŒ€í‘œ ëª¨ë¸ |
| --- | --- | --- |
| Top-K Routing | ìƒìœ„ Kê°œì˜ Expertë§Œ í™œì„±í™” | Mixtral (K=2) |
| Expert Choice | Expertê°€ ì²˜ë¦¬í•  í† í°ì„ ì„ íƒ | Switch Transformer |
| Soft MoE | ëª¨ë“  Expertì— ê°€ì¤‘ì¹˜ ë¶„ë°° | Soft MoE |
| Hash Routing | í•´ì‹œ ê¸°ë°˜ ê²°ì •ì  ë¼ìš°íŒ… | Hash Layers |

:::info ë¼ìš°íŒ… ë™ì‘ ì›ë¦¬

1. **Gate ê³„ì‚°**: ì…ë ¥ í† í°ì˜ hidden stateë¥¼ Gate ë„¤íŠ¸ì›Œí¬ì— í†µê³¼
2. **Expert ì„ íƒ**: Softmax ì¶œë ¥ì—ì„œ Top-K Expert ì„ íƒ
3. **ë³‘ë ¬ ì²˜ë¦¬**: ì„ íƒëœ Expertë“¤ì´ ë³‘ë ¬ë¡œ ì…ë ¥ ì²˜ë¦¬
4. **ê°€ì¤‘ í•©ì‚°**: Expert ì¶œë ¥ì„ Gate ê°€ì¤‘ì¹˜ë¡œ ê²°í•©

:::

### MoE vs Dense ëª¨ë¸ ë¹„êµ

| íŠ¹ì„± | Dense ëª¨ë¸ | MoE ëª¨ë¸ |
| --- | --- | --- |
| íŒŒë¼ë¯¸í„° í™œì„±í™” | 100% (ì „ì²´) | 10-25% (ì¼ë¶€ Expert) |
| ì¶”ë¡  ì—°ì‚°ëŸ‰ | ë†’ìŒ | ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ |
| ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ | íŒŒë¼ë¯¸í„° ìˆ˜ì— ë¹„ë¡€ | ì „ì²´ íŒŒë¼ë¯¸í„° ë¡œë“œ í•„ìš” |
| í•™ìŠµ íš¨ìœ¨ì„± | í‘œì¤€ | ë” ë§ì€ ë°ì´í„°ë¡œ íš¨ìœ¨ì  í•™ìŠµ |
| í™•ì¥ì„± | ì„ í˜• ì¦ê°€ | Expert ì¶”ê°€ë¡œ íš¨ìœ¨ì  í™•ì¥ |

```mermaid
flowchart LR
    subgraph "Dense Model (70B)"
        D_IN[Input] --> D_ALL[All 70B<br/>Parameters<br/>Active]
        D_ALL --> D_OUT[Output]
    end
    
    subgraph "MoE Model (8x7B = 47B Active)"
        M_IN[Input] --> M_GATE[Router]
        M_GATE --> M_E1[Expert 1<br/>7B Active]
        M_GATE --> M_E2[Expert 2<br/>7B Active]
        M_E1 & M_E2 --> M_OUT[Output]
        M_GATE -.-> M_E3[Expert 3-8<br/>Inactive]
    end
```

:::tip MoE ëª¨ë¸ì˜ ì¥ì 

- **ì—°ì‚° íš¨ìœ¨ì„±**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ ì¼ë¶€ë§Œ í™œì„±í™”í•˜ì—¬ ì¶”ë¡  ì†ë„ í–¥ìƒ
- **í™•ì¥ì„±**: Expert ì¶”ê°€ë¡œ ëª¨ë¸ ìš©ëŸ‰ í™•ì¥ ê°€ëŠ¥
- **ì „ë¬¸í™”**: ê° Expertê°€ íŠ¹ì • ë„ë©”ì¸/íƒœìŠ¤í¬ì— íŠ¹í™”

:::

---

## MoE ëª¨ë¸ ì„œë¹™ ê³ ë ¤ì‚¬í•­

### GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­

MoE ëª¨ë¸ì€ í™œì„±í™”ë˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” ì ì§€ë§Œ, ì „ì²´ Expertë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.

| ëª¨ë¸ | ì´ íŒŒë¼ë¯¸í„° | í™œì„± íŒŒë¼ë¯¸í„° | FP16 ë©”ëª¨ë¦¬ | INT8 ë©”ëª¨ë¦¬ | ê¶Œì¥ GPU |
| --- | --- | --- | --- | --- | --- |
| Mixtral 8x7B | 46.7B | 12.9B | ~94GB | ~47GB | 2x A100 80GB |
| Mixtral 8x22B | 141B | 39B | ~282GB | ~141GB | 4x H100 80GB |
| DeepSeek-MoE 16B | 16.4B | 2.8B | ~33GB | ~17GB | 1x A100 40GB |
| Qwen1.5-MoE-A2.7B | 14.3B | 2.7B | ~29GB | ~15GB | 1x A100 40GB |
| DBRX | 132B | 36B | ~264GB | ~132GB | 4x H100 80GB |

:::warning ë©”ëª¨ë¦¬ ê³„ì‚° ì‹œ ì£¼ì˜ì‚¬í•­

- **KV Cache**: ë°°ì¹˜ í¬ê¸°ì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¼ ì¶”ê°€ ë©”ëª¨ë¦¬ í•„ìš”
- **Activation Memory**: ì¶”ë¡  ì¤‘ ì¤‘ê°„ í™œì„±í™” ê°’ ì €ì¥ ê³µê°„
- **CUDA Context**: GPUë‹¹ ì•½ 1-2GBì˜ CUDA ì˜¤ë²„í—¤ë“œ
- **Safety Margin**: ì‹¤ì œ ìš´ì˜ ì‹œ 10-20% ì—¬ìœ  ê³µê°„ í™•ë³´ ê¶Œì¥

:::

### ë¶„ì‚° ë°°í¬ ì „ëµ

ëŒ€ê·œëª¨ MoE ëª¨ë¸ì€ ë‹¨ì¼ GPUì— ë¡œë“œí•  ìˆ˜ ì—†ì–´ ë¶„ì‚° ë°°í¬ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph "Tensor Parallelism (TP=4)"
        direction LR
        TP1[GPU 0<br/>Layer 1-N<br/>Shard 1/4]
        TP2[GPU 1<br/>Layer 1-N<br/>Shard 2/4]
        TP3[GPU 2<br/>Layer 1-N<br/>Shard 3/4]
        TP4[GPU 3<br/>Layer 1-N<br/>Shard 4/4]
        TP1 <--> TP2 <--> TP3 <--> TP4
    end
    
    subgraph "Expert Parallelism (EP=2)"
        direction LR
        EP1[GPU 0-1<br/>Expert 1-4]
        EP2[GPU 2-3<br/>Expert 5-8]
        EP1 <-.-> EP2
    end
    
    subgraph "Pipeline Parallelism (PP=2)"
        direction TB
        PP1[GPU 0-3<br/>Layer 1-16]
        PP2[GPU 4-7<br/>Layer 17-32]
        PP1 --> PP2
    end
```

| ë³‘ë ¬í™” ì „ëµ | ì„¤ëª… | ì¥ì  | ë‹¨ì  |
| --- | --- | --- | --- |
| Tensor Parallelism (TP) | ë ˆì´ì–´ ë‚´ í…ì„œë¥¼ GPU ê°„ ë¶„í•  | ë‚®ì€ ì§€ì—°ì‹œê°„ | ë†’ì€ í†µì‹  ì˜¤ë²„í—¤ë“œ |
| Expert Parallelism (EP) | Expertë¥¼ GPU ê°„ ë¶„ì‚° | MoEì— ìµœì í™” | All-to-All í†µì‹  í•„ìš” |
| Pipeline Parallelism (PP) | ë ˆì´ì–´ë¥¼ GPU ê°„ ìˆœì°¨ ë¶„í•  | ë©”ëª¨ë¦¬ íš¨ìœ¨ì  | íŒŒì´í”„ë¼ì¸ ë²„ë¸” ë°œìƒ |

### Expert í™œì„±í™” íŒ¨í„´

MoE ëª¨ë¸ì˜ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ Expert í™œì„±í™” íŒ¨í„´ì„ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph "Token Distribution"
        T1[Token 1] --> E1[Expert 1]
        T2[Token 2] --> E3[Expert 3]
        T3[Token 3] --> E1[Expert 1]
        T4[Token 4] --> E2[Expert 2]
        T5[Token 5] --> E4[Expert 4]
    end
    
    subgraph "Load Imbalance"
        E1_LOAD[Expert 1<br/>40% Load]
        E2_LOAD[Expert 2<br/>20% Load]
        E3_LOAD[Expert 3<br/>25% Load]
        E4_LOAD[Expert 4<br/>15% Load]
    end
```

:::info Expert ë¡œë“œ ë°¸ëŸ°ì‹±

- **Auxiliary Loss**: í•™ìŠµ ì‹œ Expert ê°„ ê· ë“± ë¶„ë°°ë¥¼ ìœ ë„í•˜ëŠ” ë³´ì¡° ì†ì‹¤
- **Capacity Factor**: Expertë‹¹ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìµœëŒ€ í† í° ìˆ˜ ì œí•œ
- **Token Dropping**: ìš©ëŸ‰ ì´ˆê³¼ ì‹œ í† í° ë“œë¡­ (ì¶”ë¡  ì‹œ ë¹„í™œì„±í™” ê¶Œì¥)

:::

---

## vLLM ê¸°ë°˜ MoE ë°°í¬

### vLLM MoE ì§€ì› ê¸°ëŠ¥

vLLMì€ MoE ëª¨ë¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ìµœì í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

- **Expert Parallelism**: ë‹¤ì¤‘ GPUì— Expert ë¶„ì‚°
- **Tensor Parallelism**: ë ˆì´ì–´ ë‚´ í…ì„œ ë¶„í• 
- **PagedAttention**: íš¨ìœ¨ì ì¸ KV Cache ê´€ë¦¬
- **Continuous Batching**: ë™ì  ë°°ì¹˜ ì²˜ë¦¬

### Mixtral 8x7B Deployment YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x7b-vllm
  namespace: inference
  labels:
    app: mixtral-8x7b
    serving-engine: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x7b
  template:
    metadata:
      labels:
        app: mixtral-8x7b
        serving-engine: vllm
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: p4d.24xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.0
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: VLLM_ATTENTION_BACKEND
              value: "FLASH_ATTN"
          args:
            - "--model"
            - "mistralai/Mixtral-8x7B-Instruct-v0.1"
            - "--tensor-parallel-size"
            - "2"
            - "--max-model-len"
            - "32768"
            - "--gpu-memory-utilization"
            - "0.90"
            - "--enable-chunked-prefill"
            - "--max-num-batched-tokens"
            - "32768"
            - "--trust-remote-code"
            - "--dtype"
            - "bfloat16"
          resources:
            requests:
              nvidia.com/gpu: 2
              memory: "180Gi"
              cpu: "24"
            limits:
              nvidia.com/gpu: 2
              memory: "200Gi"
              cpu: "32"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 300
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 5
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      terminationGracePeriodSeconds: 120
```

### Mixtral 8x22B ëŒ€ê·œëª¨ ë°°í¬ (4 GPU)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x22b-vllm
  namespace: inference
  labels:
    app: mixtral-8x22b
    serving-engine: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x22b
  template:
    metadata:
      labels:
        app: mixtral-8x22b
        serving-engine: vllm
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: p5.48xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.0
          ports:
            - name: http
              containerPort: 8000
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: NCCL_DEBUG
              value: "INFO"
            - name: NCCL_IB_DISABLE
              value: "0"
          args:
            - "--model"
            - "mistralai/Mixtral-8x22B-Instruct-v0.1"
            - "--tensor-parallel-size"
            - "4"
            - "--max-model-len"
            - "65536"
            - "--gpu-memory-utilization"
            - "0.92"
            - "--enable-chunked-prefill"
            - "--max-num-batched-tokens"
            - "65536"
            - "--dtype"
            - "bfloat16"
            - "--enforce-eager"
          resources:
            requests:
              nvidia.com/gpu: 4
              memory: "400Gi"
              cpu: "48"
            limits:
              nvidia.com/gpu: 4
              memory: "500Gi"
              cpu: "64"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
```

### vLLM í…ì„œ ë³‘ë ¬í™” ì„¤ì •

í…ì„œ ë³‘ë ¬í™”(Tensor Parallelism)ëŠ” ëª¨ë¸ì˜ ê° ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ GPUì— ë¶„í• í•©ë‹ˆë‹¤.

| ëª¨ë¸ | ê¶Œì¥ TP í¬ê¸° | GPU êµ¬ì„± | ë©”ëª¨ë¦¬/GPU |
| --- | --- | --- | --- |
| Mixtral 8x7B | 2 | 2x A100 80GB | ~47GB |
| Mixtral 8x22B | 4 | 4x H100 80GB | ~70GB |
| DeepSeek-MoE 16B | 1 | 1x A100 40GB | ~33GB |
| DBRX | 4-8 | 4-8x H100 80GB | ~33-66GB |

:::tip í…ì„œ ë³‘ë ¬í™” ìµœì í™”

- **NVLink í™œìš©**: GPU ê°„ ê³ ì† í†µì‹ ì„ ìœ„í•´ NVLink ì§€ì› ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
- **TP í¬ê¸° ì„ íƒ**: ëª¨ë¸ í¬ê¸°ì™€ GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ìµœì†Œ TP í¬ê¸° ì„ íƒ
- **í†µì‹  ì˜¤ë²„í—¤ë“œ**: TP í¬ê¸°ê°€ í´ìˆ˜ë¡ All-Reduce í†µì‹  ì¦ê°€

:::

### vLLM Expert ë³‘ë ¬í™” ì„¤ì •

Expert ë³‘ë ¬í™”(Expert Parallelism)ëŠ” MoE ëª¨ë¸ì˜ Expertë¥¼ ì—¬ëŸ¬ GPUì— ë¶„ì‚°í•©ë‹ˆë‹¤.

```yaml
# Expert Parallelism í™œì„±í™” ì˜ˆì œ
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  - "--tensor-parallel-size"
  - "2"
  # Expert Parallelismì€ vLLMì—ì„œ ìë™ìœ¼ë¡œ ìµœì í™”ë¨
  # TP ë‚´ì—ì„œ Expertê°€ ë¶„ì‚° ë°°ì¹˜ë¨
  - "--distributed-executor-backend"
  - "ray"  # ë˜ëŠ” "mp" (multiprocessing)
```

---

## TGI ê¸°ë°˜ MoE ë°°í¬

### TGI MoE ì§€ì› ê¸°ëŠ¥

Text Generation Inference(TGI)ëŠ” Hugging Faceì—ì„œ ê°œë°œí•œ ê³ ì„±ëŠ¥ ì¶”ë¡  ì„œë²„ì…ë‹ˆë‹¤.

- **Flash Attention 2**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ì—°ì‚°
- **Paged Attention**: ë™ì  KV Cache ê´€ë¦¬
- **Tensor Parallelism**: ë‹¤ì¤‘ GPU ë¶„ì‚° ì¶”ë¡ 
- **Quantization**: AWQ, GPTQ, EETQ ì§€ì›

### TGI Mixtral 8x7B Deployment YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x7b-tgi
  namespace: inference
  labels:
    app: mixtral-8x7b
    serving-engine: tgi
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x7b-tgi
  template:
    metadata:
      labels:
        app: mixtral-8x7b-tgi
        serving-engine: tgi
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: p4d.24xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: tgi
          image: ghcr.io/huggingface/text-generation-inference:2.3.0
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: MODEL_ID
              value: "mistralai/Mixtral-8x7B-Instruct-v0.1"
            - name: NUM_SHARD
              value: "2"
            - name: MAX_INPUT_LENGTH
              value: "8192"
            - name: MAX_TOTAL_TOKENS
              value: "32768"
            - name: MAX_BATCH_PREFILL_TOKENS
              value: "32768"
            - name: DTYPE
              value: "bfloat16"
            - name: QUANTIZE
              value: ""  # ë˜ëŠ” "awq", "gptq"
            - name: TRUST_REMOTE_CODE
              value: "true"
          resources:
            requests:
              nvidia.com/gpu: 2
              memory: "180Gi"
              cpu: "24"
            limits:
              nvidia.com/gpu: 2
              memory: "200Gi"
              cpu: "32"
          volumeMounts:
            - name: model-cache
              mountPath: /data
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 300
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 120
            periodSeconds: 10
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
```

### TGI ì–‘ìí™” ë°°í¬ (AWQ)

ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ AWQ ì–‘ìí™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x7b-tgi-awq
  namespace: inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x7b-tgi-awq
  template:
    metadata:
      labels:
        app: mixtral-8x7b-tgi-awq
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: g5.48xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: tgi
          image: ghcr.io/huggingface/text-generation-inference:2.3.0
          ports:
            - name: http
              containerPort: 8080
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: MODEL_ID
              value: "TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ"
            - name: NUM_SHARD
              value: "2"
            - name: MAX_INPUT_LENGTH
              value: "8192"
            - name: MAX_TOTAL_TOKENS
              value: "16384"
            - name: QUANTIZE
              value: "awq"
          resources:
            requests:
              nvidia.com/gpu: 2
              memory: "90Gi"
              cpu: "16"
            limits:
              nvidia.com/gpu: 2
              memory: "120Gi"
              cpu: "24"
```

### vLLM vs TGI ì„±ëŠ¥ ë¹„êµ

| íŠ¹ì„± | vLLM | TGI |
| --- | --- | --- |
| ì²˜ë¦¬ëŸ‰ (tokens/s) | ë†’ìŒ | ì¤‘ìƒ |
| ì§€ì—°ì‹œê°„ (TTFT) | ë‚®ìŒ | ì¤‘ê°„ |
| ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± | ë§¤ìš° ë†’ìŒ (PagedAttention) | ë†’ìŒ |
| MoE ìµœì í™” | ìš°ìˆ˜ | ì–‘í˜¸ |
| ì–‘ìí™” ì§€ì› | AWQ, GPTQ, SqueezeLLM | AWQ, GPTQ, EETQ |
| API í˜¸í™˜ì„± | OpenAI í˜¸í™˜ | ìì²´ API + OpenAI í˜¸í™˜ |
| ì»¤ë®¤ë‹ˆí‹° | í™œë°œ | í™œë°œ |

:::tip ì¶”ë¡  ì—”ì§„ ì„ íƒ ê°€ì´ë“œ

- **vLLM**: ìµœê³  ì²˜ë¦¬ëŸ‰ì´ í•„ìš”í•œ ê²½ìš°, ëŒ€ê·œëª¨ ë°°ì¹˜ ì²˜ë¦¬
- **TGI**: Hugging Face ìƒíƒœê³„ í†µí•©, ê°„í¸í•œ ë°°í¬
- **ê³µí†µ**: ë‘ ì—”ì§„ ëª¨ë‘ MoE ëª¨ë¸ì„ ì˜ ì§€ì›í•˜ë©°, ì›Œí¬ë¡œë“œì— ë”°ë¼ ì„ íƒ

:::

---

## Service ë° Ingress ì„¤ì •

### MoE ëª¨ë¸ Service YAML

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mixtral-8x7b-service
  namespace: inference
  labels:
    app: mixtral-8x7b
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8000
      targetPort: 8000
      protocol: TCP
  selector:
    app: mixtral-8x7b
---
apiVersion: v1
kind: Service
metadata:
  name: mixtral-8x7b-tgi-service
  namespace: inference
  labels:
    app: mixtral-8x7b-tgi
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: 8080
      protocol: TCP
  selector:
    app: mixtral-8x7b-tgi
```

### Gateway API HTTPRoute ì„¤ì •

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: moe-model-route
  namespace: inference
spec:
  parentRefs:
    - name: inference-gateway
      namespace: gateway-system
  hostnames:
    - "inference.example.com"
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /v1/mixtral
      backendRefs:
        - name: mixtral-8x7b-service
          port: 8000
      filters:
        - type: URLRewrite
          urlRewrite:
            path:
              type: ReplacePrefixMatch
              replacePrefixMatch: /v1
    - matches:
        - path:
            type: PathPrefix
            value: /v1/mixtral-tgi
      backendRefs:
        - name: mixtral-8x7b-tgi-service
          port: 8080
```

---

## ì„±ëŠ¥ ìµœì í™”

### KV Cache ìµœì í™”

KV CacheëŠ” ì¶”ë¡  ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph "Traditional KV Cache"
        T1[Token 1 KV] --> T2[Token 2 KV] --> T3[Token 3 KV]
        T3 --> WASTE[Wasted<br/>Memory]
    end
    
    subgraph "PagedAttention (vLLM)"
        P1[Page 1<br/>Token 1-4]
        P2[Page 2<br/>Token 5-8]
        P3[Page 3<br/>Token 9-12]
        POOL[Memory Pool<br/>Dynamic Allocation]
        P1 & P2 & P3 --> POOL
    end
```

#### vLLM KV Cache ì„¤ì •

```yaml
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  # GPU ë©”ëª¨ë¦¬ ì¤‘ KV Cacheì— í• ë‹¹í•  ë¹„ìœ¨
  - "--gpu-memory-utilization"
  - "0.90"
  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (KV Cache í¬ê¸°ì— ì˜í–¥)
  - "--max-model-len"
  - "32768"
  # Chunked Prefillë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ í–¥ìƒ
  - "--enable-chunked-prefill"
  # ë°°ì¹˜ë‹¹ ìµœëŒ€ í† í° ìˆ˜
  - "--max-num-batched-tokens"
  - "32768"
```

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¶Œì¥ê°’ |
| --- | --- | --- |
| `gpu-memory-utilization` | GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨ | 0.85-0.92 |
| `max-model-len` | ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ | ëª¨ë¸ ì§€ì› ë²”ìœ„ ë‚´ |
| `max-num-batched-tokens` | ë°°ì¹˜ë‹¹ ìµœëŒ€ í† í° | ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • |
| `enable-chunked-prefill` | Chunked Prefill í™œì„±í™” | ê¶Œì¥ |

### Speculative Decoding

Speculative Decodingì€ ì‘ì€ ë“œë˜í”„íŠ¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant Draft as Draft Model<br/>(Small)
    participant Target as Target Model<br/>(Mixtral 8x7B)
    participant Output as Output
    
    Note over Draft,Output: Speculative Decoding ê³¼ì •
    
    Draft->>Draft: Kê°œ í† í° ë¹ ë¥´ê²Œ ìƒì„±
    Draft->>Target: Kê°œ í† í° ê²€ì¦ ìš”ì²­
    Target->>Target: ë³‘ë ¬ë¡œ Kê°œ í† í° ê²€ì¦
    
    alt ëª¨ë“  í† í° ìŠ¹ì¸
        Target->>Output: Kê°œ í† í° ì¶œë ¥
    else ì¼ë¶€ í† í° ê±°ë¶€
        Target->>Output: ìŠ¹ì¸ëœ í† í°ë§Œ ì¶œë ¥
        Target->>Draft: ê±°ë¶€ ì§€ì ë¶€í„° ì¬ìƒì„±
    end
```

#### vLLM Speculative Decoding ì„¤ì •

```yaml
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  - "--tensor-parallel-size"
  - "2"
  # Speculative Decoding í™œì„±í™”
  - "--speculative-model"
  - "mistralai/Mistral-7B-Instruct-v0.2"
  # ë“œë˜í”„íŠ¸ ëª¨ë¸ì´ ìƒì„±í•  í† í° ìˆ˜
  - "--num-speculative-tokens"
  - "5"
  # ë“œë˜í”„íŠ¸ ëª¨ë¸ í…ì„œ ë³‘ë ¬ í¬ê¸°
  - "--speculative-draft-tensor-parallel-size"
  - "1"
```

:::info Speculative Decoding íš¨ê³¼

- **ì†ë„ í–¥ìƒ**: 1.5x - 2.5x ì²˜ë¦¬ëŸ‰ ì¦ê°€ (ì›Œí¬ë¡œë“œì— ë”°ë¼ ë‹¤ë¦„)
- **í’ˆì§ˆ ìœ ì§€**: ì¶œë ¥ í’ˆì§ˆì€ ë™ì¼ (ê²€ì¦ ê³¼ì •ìœ¼ë¡œ ë³´ì¥)
- **ì¶”ê°€ ë©”ëª¨ë¦¬**: ë“œë˜í”„íŠ¸ ëª¨ë¸ì„ ìœ„í•œ ì¶”ê°€ GPU ë©”ëª¨ë¦¬ í•„ìš”

:::

### ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ëŠ” GPU í™œìš©ë¥ ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

```yaml
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  # Continuous Batching ê´€ë ¨ ì„¤ì •
  - "--max-num-seqs"
  - "256"  # ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìµœëŒ€ ì‹œí€€ìŠ¤ ìˆ˜
  - "--max-num-batched-tokens"
  - "32768"  # ë°°ì¹˜ë‹¹ ìµœëŒ€ í† í° ìˆ˜
  # Prefillê³¼ Decode ë¶„ë¦¬
  - "--enable-chunked-prefill"
  - "--max-num-batched-tokens"
  - "32768"
```

| ìµœì í™” ê¸°ë²• | ì„¤ëª… | íš¨ê³¼ |
| --- | --- | --- |
| Continuous Batching | ìš”ì²­ì„ ë™ì ìœ¼ë¡œ ë°°ì¹˜ì— ì¶”ê°€/ì œê±° | ì²˜ë¦¬ëŸ‰ 2-3x í–¥ìƒ |
| Chunked Prefill | Prefillì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ Decodeì™€ ë³‘í–‰ | ì§€ì—°ì‹œê°„ ê°ì†Œ |
| Dynamic SplitFuse | Prefill/Decode ë™ì  ë¶„ë¦¬ | GPU í™œìš©ë¥  í–¥ìƒ |

---

## ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### MoE ëª¨ë¸ ì „ìš© ë©”íŠ¸ë¦­

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: moe-model-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: mixtral-8x7b
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
  namespaceSelector:
    matchNames:
      - inference
```

### ì£¼ìš” ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ì„ê³„ê°’ |
| --- | --- | --- |
| `vllm:num_requests_running` | í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ìš”ì²­ ìˆ˜ | - |
| `vllm:num_requests_waiting` | ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ ìˆ˜ | > 100 ê²½ê³  |
| `vllm:gpu_cache_usage_perc` | KV Cache ì‚¬ìš©ë¥  | > 95% ê²½ê³  |
| `vllm:avg_prompt_throughput_toks_per_s` | í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ëŸ‰ | - |
| `vllm:avg_generation_throughput_toks_per_s` | ìƒì„± ì²˜ë¦¬ëŸ‰ | - |
| `DCGM_FI_DEV_GPU_UTIL` | GPU ì‚¬ìš©ë¥  | > 90% ê²½ê³  |
| `DCGM_FI_DEV_FB_USED` | GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | > 95% ìœ„í—˜ |

### Prometheus ì•Œë¦¼ ê·œì¹™

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: moe-model-alerts
  namespace: monitoring
spec:
  groups:
    - name: moe-model-alerts
      rules:
        - alert: MoEModelHighLatency
          expr: |
            histogram_quantile(0.95, 
              rate(vllm:e2e_request_latency_seconds_bucket[5m])
            ) > 30
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "MoE ëª¨ë¸ ì‘ë‹µ ì§€ì—° (P95 > 30ì´ˆ)"
            description: "{{ $labels.model_name }} ëª¨ë¸ì˜ P95 ì§€ì—°ì‹œê°„ì´ 30ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."
            
        - alert: MoEModelKVCacheFull
          expr: vllm:gpu_cache_usage_perc > 0.95
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "KV Cache ìš©ëŸ‰ ë¶€ì¡±"
            description: "KV Cache ì‚¬ìš©ë¥ ì´ 95%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìƒˆ ìš”ì²­ì´ ê±°ë¶€ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            
        - alert: MoEModelQueueBacklog
          expr: vllm:num_requests_waiting > 100
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "ìš”ì²­ ëŒ€ê¸°ì—´ ì¦ê°€"
            description: "ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ì´ 100ê°œë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìŠ¤ì¼€ì¼ ì•„ì›ƒì„ ê³ ë ¤í•˜ì„¸ìš”."
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²° ë°©ë²•

#### OOM (Out of Memory) ì˜¤ë¥˜

```bash
# ì¦ìƒ: CUDA out of memory ì˜¤ë¥˜
# í•´ê²° ë°©ë²•:
# 1. gpu-memory-utilization ê°’ ë‚®ì¶”ê¸°
--gpu-memory-utilization 0.85

# 2. max-model-len ì¤„ì´ê¸°
--max-model-len 16384

# 3. í…ì„œ ë³‘ë ¬ í¬ê¸° ëŠ˜ë¦¬ê¸° (ë” ë§ì€ GPU ì‚¬ìš©)
--tensor-parallel-size 4
```

#### ëŠë¦° ëª¨ë¸ ë¡œë”©

```bash
# ì¦ìƒ: ëª¨ë¸ ë¡œë”©ì— 10ë¶„ ì´ìƒ ì†Œìš”
# í•´ê²° ë°©ë²•:
# 1. ëª¨ë¸ ìºì‹œ PVC ì‚¬ìš©
# 2. FSx for Lustre ì‚¬ìš©ìœ¼ë¡œ ë¹ ë¥¸ ëª¨ë¸ ë¡œë”©
# 3. ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ
```

#### Expert ë¡œë“œ ë¶ˆê· í˜•

```bash
# ì¦ìƒ: íŠ¹ì • GPUë§Œ ë†’ì€ ì‚¬ìš©ë¥ 
# í•´ê²° ë°©ë²•:
# 1. ë°°ì¹˜ í¬ê¸° ì¦ê°€ë¡œ í† í° ë¶„ì‚° ê°œì„ 
--max-num-seqs 256

# 2. ë‹¤ì–‘í•œ ì…ë ¥ìœ¼ë¡œ Expert í™œì„±í™” ë¶„ì‚°
```

:::warning ë””ë²„ê¹… íŒ

- **ë¡œê·¸ ë ˆë²¨ ì¡°ì •**: `VLLM_LOGGING_LEVEL=DEBUG` í™˜ê²½ ë³€ìˆ˜ë¡œ ìƒì„¸ ë¡œê·¸ í™•ì¸
- **NCCL ë””ë²„ê·¸**: `NCCL_DEBUG=INFO`ë¡œ GPU ê°„ í†µì‹  ë¬¸ì œ ì§„ë‹¨
- **ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§**: `nvidia-smi dmon`ìœ¼ë¡œ ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§

:::

---

## ìš”ì•½

MoE ëª¨ë¸ ì„œë¹™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### í•µì‹¬ í¬ì¸íŠ¸

1. **ì•„í‚¤í…ì²˜ ì´í•´**: Expert ë„¤íŠ¸ì›Œí¬ì™€ ë¼ìš°íŒ… ë©”ì»¤ë‹ˆì¦˜ì˜ ë™ì‘ ì›ë¦¬ íŒŒì•…
2. **ë©”ëª¨ë¦¬ ê³„íš**: ì „ì²´ Expertë¥¼ ë¡œë“œí•´ì•¼ í•˜ë¯€ë¡œ ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ í™•ë³´
3. **ë¶„ì‚° ë°°í¬**: í…ì„œ ë³‘ë ¬í™”ì™€ Expert ë³‘ë ¬í™”ë¥¼ ì ì ˆíˆ ì¡°í•©
4. **ì¶”ë¡  ì—”ì§„ ì„ íƒ**: vLLM(ê³ ì²˜ë¦¬ëŸ‰) ë˜ëŠ” TGI(ê°„í¸í•œ ë°°í¬) ì„ íƒ
5. **ì„±ëŠ¥ ìµœì í™”**: KV Cache, Speculative Decoding, ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ì ìš©

### ë‹¤ìŒ ë‹¨ê³„

- [GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬](./gpu-resource-management.md) - GPU í´ëŸ¬ìŠ¤í„° ë™ì  ë¦¬ì†ŒìŠ¤ í• ë‹¹
- [Inference Gateway ë¼ìš°íŒ…](./inference-gateway-routing.md) - ë‹¤ì¤‘ ëª¨ë¸ ë¼ìš°íŒ… ì „ëµ
- [Agentic AI í”Œë«í¼ ì•„í‚¤í…ì²˜](./agentic-platform-architecture.md) - ì „ì²´ í”Œë«í¼ êµ¬ì„±

---

## ì°¸ê³  ìë£Œ

- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)
- [TGI ê³µì‹ ë¬¸ì„œ](https://huggingface.co/docs/text-generation-inference)
- [Mixtral ëª¨ë¸ ì¹´ë“œ](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
- [MoE ì•„í‚¤í…ì²˜ ë…¼ë¬¸](https://arxiv.org/abs/2101.03961)
- [PagedAttention ë…¼ë¬¸](https://arxiv.org/abs/2309.06180)
