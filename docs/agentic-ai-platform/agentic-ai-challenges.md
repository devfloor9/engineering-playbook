---
title: "Agentic AI Platform ê¸°ìˆ ì  ë„ì „ê³¼ì œì™€ í•´ê²°ë°©ì•ˆ"
sidebar_label: "ê¸°ìˆ ì  ë„ì „ê³¼ì œ"
description: "Agentic AI Platform êµ¬ì¶• ì‹œ ì§ë©´í•˜ëŠ” 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œì™€ í•´ê²° ë°©ì•ˆ"
tags: [eks, kubernetes, genai, agentic-ai, gpu, infrastructure, challenges, karpenter]
category: "genai-aiml"
date: 2025-02-05
update: 2026-02-04
authors: [devfloor9]
sidebar_position: 3
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

> ğŸ“… **ì‘ì„±ì¼**: 2025-02-05 | **ìˆ˜ì •ì¼**: 2026-02-04 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 25ë¶„

:::tip TL;DR (í•µì‹¬ ìš”ì•½)
**Agentic AI í”Œë«í¼ì˜ 4ê°€ì§€ ë„ì „ê³¼ì œì™€ í•´ê²°ì±…:**

| ë„ì „ê³¼ì œ | í•µì‹¬ ì†”ë£¨ì…˜ |
|----------|-------------|
| 1. GPU ëª¨ë‹ˆí„°ë§ ë° ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§ | **Karpenter + DCGM Exporter** |
| 2. ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§ | **Kgateway + KEDA + vLLM** |
| 3. í† í°/ì„¸ì…˜ ë¹„ìš© ëª¨ë‹ˆí„°ë§ | **LangFuse / LangSmith** |
| 4. FM íŒŒì¸íŠœë‹ ìë™í™” | **NeMo + Kubeflow** |

**ê¶Œì¥ ì‹œì‘ì :** EKS Auto Modeë¡œ í´ëŸ¬ìŠ¤í„° ìƒì„± â†’ Karpenter ìë™ êµ¬ì„± â†’ GPU NodePool ì¶”ê°€ â†’ AI ì›Œí¬ë¡œë“œ ë°°í¬

**í•µì‹¬ ë©”ì‹œì§€:** Kubernetes + EKS Auto Mode + Karpenter ì¡°í•©ìœ¼ë¡œ GPU ì¸í”„ë¼ ì™„ì „ ìë™í™” ë‹¬ì„±
:::

Agentic AI Platformì„ êµ¬ì¶•í•˜ê³  ìš´ì˜í•˜ëŠ” ê³¼ì •ì—ì„œ í”Œë«í¼ ì—”ì§€ë‹ˆì–´ì™€ ì•„í‚¤í…íŠ¸ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì  ë„ì „ê³¼ì œì— ì§ë©´í•©ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œë¥¼ ë¶„ì„í•˜ê³ , **í´ë¼ìš°ë“œ ì¸í”„ë¼ ìë™í™”ì™€ AI í”Œë«í¼ì˜ ìœ ê¸°ì  í†µí•©**ì´ ì™œ í•µì‹¬ í•´ê²°ì±…ì¸ì§€ ì„¤ëª…í•©ë‹ˆë‹¤.

## Agentic AI í”Œë«í¼ì˜ 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œ

Frontier Model(ìµœì‹  ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì„ í™œìš©í•œ Agentic AI ì‹œìŠ¤í…œì€ ê¸°ì¡´ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ëŠ” **ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì¸í”„ë¼ ìš”êµ¬ì‚¬í•­**ì„ ê°€ì§‘ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œ"
        C1["ğŸ–¥ï¸ ë„ì „ê³¼ì œ 1<br/>GPU ëª¨ë‹ˆí„°ë§ ë° ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§"]
        C2["ğŸ”€ ë„ì „ê³¼ì œ 2<br/>Agentic AI ìš”ì²­ ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§"]
        C3["ğŸ“Š ë„ì „ê³¼ì œ 3<br/>í† í°/ì„¸ì…˜ ìˆ˜ì¤€ ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© ì»¨íŠ¸ë¡¤"]
        C4["ğŸ”§ ë„ì „ê³¼ì œ 4<br/>FM íŒŒì¸íŠœë‹ê³¼ ìë™í™” íŒŒì´í”„ë¼ì¸"]
    end

    subgraph "ê³µí†µ íŠ¹ì„±"
        COMMON["GPU ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì <br/>ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ì›Œí¬ë¡œë“œ<br/>ë†’ì€ ì¸í”„ë¼ ë¹„ìš©<br/>ë³µì¡í•œ ë¶„ì‚° ì‹œìŠ¤í…œ"]
    end

    C1 --> COMMON
    C2 --> COMMON
    C3 --> COMMON
    C4 --> COMMON

    style C1 fill:#ff6b6b
    style C2 fill:#4ecdc4
    style C3 fill:#45b7d1
    style C4 fill:#96ceb4
    style COMMON fill:#f9f9f9
```

### ë„ì „ê³¼ì œ ìš”ì•½

| ë„ì „ê³¼ì œ | í•µì‹¬ ë¬¸ì œ | ê¸°ì¡´ ì¸í”„ë¼ì˜ í•œê³„ |
| --- | --- | --- |
| **GPU ëª¨ë‹ˆí„°ë§ ë° ìŠ¤ì¼€ì¤„ë§** | ë©€í‹° í´ëŸ¬ìŠ¤í„° GPU ê°€ì‹œì„± ë¶€ì¬, ì„¸ëŒ€ë³„ ì›Œí¬ë¡œë“œ ë§¤ì¹­ | ìˆ˜ë™ ëª¨ë‹ˆí„°ë§, ì •ì  í• ë‹¹ |
| **ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§** | ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ íŠ¸ë˜í”½, ë©€í‹° ëª¨ë¸ ì„œë¹™ ë³µì¡ì„± | ëŠë¦° í”„ë¡œë¹„ì €ë‹, ê³ ì • ìš©ëŸ‰ |
| **ë¹„ìš© ì»¨íŠ¸ë¡¤** | GPU ìœ íœ´ ë¹„ìš©, í† í° ë ˆë²¨ ì¶”ì  ì–´ë ¤ì›€ | ë¹„ìš© ê°€ì‹œì„± ë¶€ì¬, ìµœì í™” ë¶ˆê°€ |
| **FM íŒŒì¸íŠœë‹** | ë¶„ì‚° í•™ìŠµ ì¸í”„ë¼ ë³µì¡ì„±, ë¦¬ì†ŒìŠ¤ í”„ë¡œë¹„ì €ë‹ ì§€ì—° | ìˆ˜ë™ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬, ë‚®ì€ í™œìš©ë¥  |

:::warning ê¸°ì¡´ ì¸í”„ë¼ ì ‘ê·¼ ë°©ì‹ì˜ í•œê³„
ì „í†µì ì¸ VM ê¸°ë°˜ ì¸í”„ë¼ë‚˜ ìˆ˜ë™ ê´€ë¦¬ ë°©ì‹ìœ¼ë¡œëŠ” Agentic AIì˜ **ë™ì ì´ê³  ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ì›Œí¬ë¡œë“œ íŒ¨í„´**ì— íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU ë¦¬ì†ŒìŠ¤ì˜ ë†’ì€ ë¹„ìš©ê³¼ ë³µì¡í•œ ë¶„ì‚° ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ì€ **ìë™í™”ëœ ì¸í”„ë¼ ê´€ë¦¬**ë¥¼ í•„ìˆ˜ë¡œ ë§Œë“­ë‹ˆë‹¤.
:::

---

## í•´ê²°ì˜ í•µì‹¬: í´ë¼ìš°ë“œ ì¸í”„ë¼ ìë™í™”ì™€ AI í”Œë«í¼ì˜ í†µí•©

Agentic AI í”Œë«í¼ì˜ ë„ì „ê³¼ì œë¥¼ í•´ê²°í•˜ëŠ” í•µì‹¬ì€ **í´ë¼ìš°ë“œ ì¸í”„ë¼ ìë™í™”ì™€ AI ì›Œí¬ë¡œë“œì˜ ìœ ê¸°ì  í†µí•©**ì…ë‹ˆë‹¤. ì´ í†µí•©ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```mermaid
graph LR
    subgraph "AI ì›Œí¬ë¡œë“œ íŠ¹ì„±"
        AI1["ë™ì  ë¦¬ì†ŒìŠ¤ ìš”êµ¬"]
        AI2["ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ íŠ¸ë˜í”½"]
        AI3["ê³ ë¹„ìš© GPU ë¦¬ì†ŒìŠ¤"]
        AI4["ë³µì¡í•œ ë¶„ì‚° ì²˜ë¦¬"]
    end

    subgraph "ì¸í”„ë¼ ìë™í™” ìš”êµ¬ì‚¬í•­"
        INF1["ì‹¤ì‹œê°„ í”„ë¡œë¹„ì €ë‹"]
        INF2["ìë™ ìŠ¤ì¼€ì¼ë§"]
        INF3["ë¹„ìš© ìµœì í™”"]
        INF4["ì„ ì–¸ì  ê´€ë¦¬"]
    end

    subgraph "í†µí•© í”Œë«í¼"
        PLATFORM["Kubernetes<br/>ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"]
    end

    AI1 --> PLATFORM
    AI2 --> PLATFORM
    AI3 --> PLATFORM
    AI4 --> PLATFORM
    PLATFORM --> INF1
    PLATFORM --> INF2
    PLATFORM --> INF3
    PLATFORM --> INF4

    style PLATFORM fill:#326ce5
```

### ì™œ Kubernetesì¸ê°€?

KubernetesëŠ” Agentic AI í”Œë«í¼ì˜ ëª¨ë“  ë„ì „ê³¼ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” **ì´ìƒì ì¸ ê¸°ë°˜ í”Œë«í¼**ì…ë‹ˆë‹¤:

| Kubernetes í•µì‹¬ ê¸°ëŠ¥ | AI í”Œë«í¼ ì ìš© | í•´ê²°ë˜ëŠ” ë„ì „ê³¼ì œ |
| --- | --- | --- |
| **ì„ ì–¸ì  ë¦¬ì†ŒìŠ¤ ê´€ë¦¬** | GPU ë¦¬ì†ŒìŠ¤ë¥¼ ì½”ë“œë¡œ ì •ì˜í•˜ê³  ë²„ì „ ê´€ë¦¬ | ë„ì „ê³¼ì œ 1, 4 |
| **ìë™ ìŠ¤ì¼€ì¼ë§ (HPA/VPA)** | íŠ¸ë˜í”½ íŒ¨í„´ì— ë”°ë¥¸ Pod ìë™ í™•ì¥/ì¶•ì†Œ | ë„ì „ê³¼ì œ 2 |
| **ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ë°˜ ê²©ë¦¬** | íŒ€/í”„ë¡œì íŠ¸ë³„ ë¦¬ì†ŒìŠ¤ í• ë‹¹ëŸ‰ ê´€ë¦¬ | ë„ì „ê³¼ì œ 3 |
| **Operator íŒ¨í„´** | ë³µì¡í•œ ë¶„ì‚° í•™ìŠµ ì›Œí¬í”Œë¡œìš° ìë™í™” | ë„ì „ê³¼ì œ 4 |
| **ì„œë¹„ìŠ¤ ë©”ì‹œ í†µí•©** | ë©€í‹° ëª¨ë¸ ë¼ìš°íŒ… ë° íŠ¸ë˜í”½ ê´€ë¦¬ | ë„ì „ê³¼ì œ 2 |
| **ë©”íŠ¸ë¦­ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** | GPU ì‚¬ìš©ë¥  ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§ ê²°ì • | ë„ì „ê³¼ì œ 1, 3 |

```mermaid
graph TB
    subgraph "Kubernetes í•µì‹¬ ì»´í¬ë„ŒíŠ¸"
        API["API Server<br/>ì„ ì–¸ì  ë¦¬ì†ŒìŠ¤ ê´€ë¦¬"]
        SCHED["Scheduler<br/>GPU ì¸ì‹ ìŠ¤ì¼€ì¤„ë§"]
        CTRL["Controller Manager<br/>ìƒíƒœ ì¡°ì • ë£¨í”„"]
        ETCD["etcd<br/>í´ëŸ¬ìŠ¤í„° ìƒíƒœ ì €ì¥"]
    end

    subgraph "AI ì›Œí¬ë¡œë“œ ì§€ì›"
        GPU["GPU Device Plugin<br/>GPU ë¦¬ì†ŒìŠ¤ ì¶”ìƒí™”"]
        HPA["HPA/KEDA<br/>ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§"]
        OP["Operators<br/>ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ìë™í™”"]
    end

    subgraph "ë„ì „ê³¼ì œ í•´ê²°"
        S1["âœ… GPU ë¦¬ì†ŒìŠ¤ í†µí•© ê´€ë¦¬"]
        S2["âœ… ë™ì  ìŠ¤ì¼€ì¼ë§"]
        S3["âœ… ë¦¬ì†ŒìŠ¤ í• ë‹¹ëŸ‰ ê´€ë¦¬"]
        S4["âœ… ë¶„ì‚° í•™ìŠµ ìë™í™”"]
    end

    API --> GPU
    SCHED --> GPU
    CTRL --> HPA
    CTRL --> OP
    GPU --> S1
    HPA --> S2
    API --> S3
    OP --> S4

    style API fill:#326ce5
    style SCHED fill:#326ce5
    style CTRL fill:#326ce5
```

:::info Kubernetesì˜ AI ì›Œí¬ë¡œë“œ ì§€ì›
KubernetesëŠ” NVIDIA GPU Operator, Kubeflow, KEDA ë“± AI/ML ìƒíƒœê³„ì™€ì˜ í’ë¶€í•œ í†µí•©ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬, ë¶„ì‚° í•™ìŠµ, ëª¨ë¸ ì„œë¹™ì„ **ë‹¨ì¼ í”Œë«í¼ì—ì„œ í†µí•© ê´€ë¦¬**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

---

ì´ì œ Kubernetesê°€ AI ì›Œí¬ë¡œë“œì— ì í•©í•œ ì´ìœ ë¥¼ ì´í•´í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ, **ê° ë„ì „ê³¼ì œë¥¼ í•´ê²°í•˜ëŠ” êµ¬ì²´ì ì¸ ì˜¤í”ˆì†ŒìŠ¤ ì†”ë£¨ì…˜ë“¤**ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## Kubernetes ìƒíƒœê³„ì˜ Agentic AI ì†”ë£¨ì…˜ ë²„ë“œë·°

Kubernetes ìƒíƒœê³„ì—ëŠ” Agentic AI í”Œë«í¼ì˜ ê° ë„ì „ê³¼ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ **ì „ë¬¸í™”ëœ ì˜¤í”ˆì†ŒìŠ¤ ì†”ë£¨ì…˜**ë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ ì†”ë£¨ì…˜ë“¤ì€ Kubernetes ë„¤ì´í‹°ë¸Œë¡œ ì„¤ê³„ë˜ì–´ **ì„ ì–¸ì  ê´€ë¦¬, ìë™ ìŠ¤ì¼€ì¼ë§, ê³ ê°€ìš©ì„±**ì˜ ì´ì ì„ ê·¸ëŒ€ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì†”ë£¨ì…˜ ë§¤í•‘ ê°œìš”

```mermaid
graph TB
    subgraph "4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œ"
        C1["ğŸ–¥ï¸ GPU ëª¨ë‹ˆí„°ë§ ë°<br/>ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§"]
        C2["ğŸ”€ ë™ì  ë¼ìš°íŒ… ë°<br/>ìŠ¤ì¼€ì¼ë§"]
        C3["ğŸ“Š í† í°/ì„¸ì…˜ ëª¨ë‹ˆí„°ë§<br/>ë° ë¹„ìš© ì»¨íŠ¸ë¡¤"]
        C4["ğŸ”§ FM íŒŒì¸íŠœë‹ê³¼<br/>ìë™í™” íŒŒì´í”„ë¼ì¸"]
    end

    subgraph "Kubernetes ë„¤ì´í‹°ë¸Œ ì†”ë£¨ì…˜"
        S1["Karpenter<br/>GPU ë…¸ë“œ ìë™ í”„ë¡œë¹„ì €ë‹"]
        S2["Kgateway + LiteLLM<br/>Inference Gateway"]
        S3["LangFuse / LangSmith<br/>LLM Observability"]
        S4["NeMo + Kubeflow<br/>ë¶„ì‚° í•™ìŠµ íŒŒì´í”„ë¼ì¸"]
    end

    subgraph "ëª¨ë¸ ì„œë¹™ ê³„ì¸µ"
        VLLM["vLLM<br/>ê³ ì„±ëŠ¥ ì¶”ë¡  ì—”ì§„"]
        LLMD["llm-d<br/>ë¶„ì‚° ì¶”ë¡  ìŠ¤ì¼€ì¤„ëŸ¬"]
    end

    subgraph "Agent ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"
        KAGENT["KAgent<br/>Kubernetes Agent í”„ë ˆì„ì›Œí¬"]
    end

    C1 --> S1
    C2 --> S2
    C3 --> S3
    C4 --> S4
    
    S2 --> VLLM
    S2 --> LLMD
    KAGENT --> S2
    KAGENT --> S3

    style C1 fill:#ff6b6b
    style C2 fill:#4ecdc4
    style C3 fill:#45b7d1
    style C4 fill:#96ceb4
    style S1 fill:#ffd93d
    style S2 fill:#4286f4
    style S3 fill:#9b59b6
    style S4 fill:#76b900
    style VLLM fill:#e74c3c
    style LLMD fill:#e74c3c
    style KAGENT fill:#2ecc71
```

### ë„ì „ê³¼ì œë³„ ì†”ë£¨ì…˜ ìƒì„¸ ë§¤í•‘

| ë„ì „ê³¼ì œ | í•µì‹¬ ì†”ë£¨ì…˜ | ë³´ì¡° ì†”ë£¨ì…˜ | í•´ê²°í•˜ëŠ” ë¬¸ì œ |
| --- | --- | --- | --- |
| **GPU ëª¨ë‹ˆí„°ë§ ë° ìŠ¤ì¼€ì¤„ë§** | Karpenter | DCGM Exporter, NVIDIA GPU Operator | GPU ë…¸ë“œ ìë™ í”„ë¡œë¹„ì €ë‹, ì„¸ëŒ€ë³„ ì›Œí¬ë¡œë“œ ë§¤ì¹­ |
| **ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§** | Kgateway, LiteLLM | KEDA, vLLM, llm-d | ë©€í‹° ëª¨ë¸ ë¼ìš°íŒ…, íŠ¸ë˜í”½ ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§ |
| **í† í°/ë¹„ìš© ëª¨ë‹ˆí„°ë§** | LangFuse, LangSmith | OpenTelemetry, Prometheus | í† í° ë ˆë²¨ ì¶”ì , ë¹„ìš© ê°€ì‹œì„±, í’ˆì§ˆ í‰ê°€ |
| **FM íŒŒì¸íŠœë‹** | NeMo, Kubeflow | MLflow, Ray | ë¶„ì‚° í•™ìŠµ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜, íŒŒì´í”„ë¼ì¸ ìë™í™” |

### í•µì‹¬ ì†”ë£¨ì…˜ ì†Œê°œ

#### 1. ëª¨ë¸ ì„œë¹™: vLLM + llm-d

**vLLM**ì€ LLM ì¶”ë¡ ì„ ìœ„í•œ ê³ ì„±ëŠ¥ ì„œë¹™ ì—”ì§„ìœ¼ë¡œ, PagedAttentionì„ í†µí•´ **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”**í•©ë‹ˆë‹¤.

**llm-d**ëŠ” Kubernetes í™˜ê²½ì—ì„œ LLM ì¶”ë¡  ìš”ì²­ì„ **ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì‚°**í•˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ì…ë‹ˆë‹¤.

```mermaid
graph LR
    subgraph "ì¶”ë¡  ìš”ì²­ íë¦„"
        REQ["Client Request"]
        LLMD["llm-d<br/>Request Router"]
        
        subgraph "vLLM Instances"
            V1["vLLM Pod 1<br/>GPU: A100"]
            V2["vLLM Pod 2<br/>GPU: A100"]
            V3["vLLM Pod 3<br/>GPU: H100"]
        end
    end

    REQ --> LLMD
    LLMD --> V1
    LLMD --> V2
    LLMD --> V3

    style LLMD fill:#e74c3c
    style V1 fill:#3498db
    style V2 fill:#3498db
    style V3 fill:#3498db
```

| ì†”ë£¨ì…˜ | ì—­í•  | í•µì‹¬ ê¸°ëŠ¥ |
| --- | --- | --- |
| **vLLM** | ì¶”ë¡  ì—”ì§„ | PagedAttention, Continuous Batching, Speculative Decoding |
| **llm-d** | ë¶„ì‚° ìŠ¤ì¼€ì¤„ëŸ¬ | ë¡œë“œ ë°¸ëŸ°ì‹±, Prefix Caching ì¸ì‹ ë¼ìš°íŒ…, ì¥ì•  ë³µêµ¬ |

#### 2. Inference Gateway: Kgateway + LiteLLM

**Kgateway**ëŠ” Kubernetes Gateway API ê¸°ë°˜ì˜ AI ì¶”ë¡  ê²Œì´íŠ¸ì›¨ì´ë¡œ, **ë©€í‹° ëª¨ë¸ ë¼ìš°íŒ…ê³¼ íŠ¸ë˜í”½ ê´€ë¦¬**ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

**LiteLLM**ì€ ë‹¤ì–‘í•œ LLM í”„ë¡œë°”ì´ë”ë¥¼ **í†µí•© APIë¡œ ì¶”ìƒí™”**í•˜ì—¬ ëª¨ë¸ ì „í™˜ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "Gateway ê³„ì¸µ"
        CLIENT["Client Applications"]
        KGW["Kgateway<br/>Inference Gateway"]
        LITE["LiteLLM<br/>Provider Abstraction"]
    end

    subgraph "ëª¨ë¸ ë°±ì—”ë“œ"
        SELF["Self-hosted<br/>vLLM / TGI"]
        BEDROCK["Amazon Bedrock"]
        OPENAI["OpenAI API"]
    end

    CLIENT --> KGW
    KGW --> LITE
    LITE --> SELF
    LITE --> BEDROCK
    LITE --> OPENAI

    style KGW fill:#4286f4
    style LITE fill:#9b59b6
```

| ì†”ë£¨ì…˜ | ì—­í•  | í•µì‹¬ ê¸°ëŠ¥ |
| --- | --- | --- |
| **Kgateway** | íŠ¸ë˜í”½ ê´€ë¦¬ | í—¤ë” ê¸°ë°˜ ë¼ìš°íŒ…, ê°€ì¤‘ì¹˜ ë¶„ë°°, Rate Limiting, Canary ë°°í¬ |
| **LiteLLM** | API ì¶”ìƒí™” | 100+ LLM í”„ë¡œë°”ì´ë” ì§€ì›, í†µí•© API, í´ë°± ì„¤ì •, ë¹„ìš© ì¶”ì  |

#### 3. LLM Observability: LangFuse + LangSmith

**LangFuse**ì™€ **LangSmith**ëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ **ì „ì²´ ë¼ì´í”„ì‚¬ì´í´ì„ ì¶”ì **í•˜ëŠ” ê´€ì¸¡ì„± í”Œë«í¼ì…ë‹ˆë‹¤.

```mermaid
graph LR
    subgraph "LLM Application"
        APP["Agent Application"]
        CHAIN["LangChain / LlamaIndex"]
    end

    subgraph "Observability Platform"
        LF["LangFuse<br/>(Self-hosted)"]
        LS["LangSmith<br/>(Managed)"]
    end

    subgraph "ë¶„ì„ ê¸°ëŠ¥"
        TRACE["Trace ë¶„ì„"]
        COST["ë¹„ìš© ì¶”ì "]
        EVAL["í’ˆì§ˆ í‰ê°€"]
        DEBUG["ë””ë²„ê¹…"]
    end

    APP --> CHAIN
    CHAIN --> LF
    CHAIN --> LS
    LF --> TRACE & COST & EVAL & DEBUG
    LS --> TRACE & COST & EVAL & DEBUG

    style LF fill:#45b7d1
    style LS fill:#9b59b6
```

| ì†”ë£¨ì…˜ | ë°°í¬ ë°©ì‹ | í•µì‹¬ ê¸°ëŠ¥ |
| --- | --- | --- |
| **LangFuse** | Self-hosted (K8s) | í† í° ì¶”ì , ë¹„ìš© ë¶„ì„, í”„ë¡¬í”„íŠ¸ ê´€ë¦¬, A/B í…ŒìŠ¤íŠ¸ |
| **LangSmith** | Managed SaaS | íŠ¸ë ˆì´ì‹±, í‰ê°€, ë°ì´í„°ì…‹ ê´€ë¦¬, í˜‘ì—… ê¸°ëŠ¥ |

#### 4. Agent ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜: KAgent

**KAgent**ëŠ” Kubernetes ë„¤ì´í‹°ë¸Œ AI Agent í”„ë ˆì„ì›Œí¬ë¡œ, **Agent ì›Œí¬í”Œë¡œìš°ë¥¼ CRDë¡œ ì •ì˜**í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "KAgent Architecture"
        CRD["Agent CRD<br/>ì„ ì–¸ì  ì •ì˜"]
        CTRL["KAgent Controller<br/>ìƒíƒœ ê´€ë¦¬"]
        
        subgraph "Agent Components"
            TOOL["Tool Definitions"]
            MEM["Memory Store"]
            LLM["LLM Backend"]
        end
    end

    subgraph "Integration"
        KGW["Kgateway"]
        OBS["LangFuse"]
    end

    CRD --> CTRL
    CTRL --> TOOL & MEM & LLM
    CTRL --> KGW
    CTRL --> OBS

    style CRD fill:#2ecc71
    style CTRL fill:#2ecc71
```

| ê¸°ëŠ¥ | ì„¤ëª… |
| --- | --- |
| **ì„ ì–¸ì  Agent ì •ì˜** | YAMLë¡œ Agent êµ¬ì„±, ë„êµ¬, ë©”ëª¨ë¦¬ ì •ì˜ |
| **ìë™ ìŠ¤ì¼€ì¼ë§** | ìš”ì²­ëŸ‰ì— ë”°ë¥¸ Agent ì¸ìŠ¤í„´ìŠ¤ ìë™ í™•ì¥ |
| **í†µí•© ê´€ì¸¡ì„±** | LangFuse/LangSmithì™€ ìë™ ì—°ë™ |
| **ë„êµ¬ ê´€ë¦¬** | MCP(Model Context Protocol) ê¸°ë°˜ ë„êµ¬ í†µí•© |

### ì†”ë£¨ì…˜ ìŠ¤íƒ í†µí•© ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "Client Layer"
        WEB["Web Application"]
        API["API Clients"]
        AGENT["Agent Applications"]
    end

    subgraph "Gateway Layer"
        KGW["Kgateway<br/>Traffic Management"]
        LITE["LiteLLM<br/>Provider Abstraction"]
    end

    subgraph "Orchestration Layer"
        KAGENT["KAgent<br/>Agent Framework"]
        KEDA["KEDA<br/>Event-driven Scaling"]
    end

    subgraph "Serving Layer"
        LLMD["llm-d<br/>Request Scheduler"]
        VLLM1["vLLM Instance 1"]
        VLLM2["vLLM Instance 2"]
        VLLM3["vLLM Instance 3"]
    end

    subgraph "Infrastructure Layer"
        KARP["Karpenter<br/>Node Provisioning"]
        GPU1["GPU Node 1"]
        GPU2["GPU Node 2"]
        GPU3["GPU Node 3"]
    end

    subgraph "Observability Layer"
        LF["LangFuse<br/>LLM Tracing"]
        PROM["Prometheus<br/>Metrics"]
        GRAF["Grafana<br/>Dashboards"]
    end

    WEB & API & AGENT --> KGW
    KGW --> LITE
    LITE --> KAGENT
    KAGENT --> LLMD
    LLMD --> VLLM1 & VLLM2 & VLLM3
    VLLM1 --> GPU1
    VLLM2 --> GPU2
    VLLM3 --> GPU3
    KARP --> GPU1 & GPU2 & GPU3
    KEDA --> VLLM1 & VLLM2 & VLLM3
    
    KAGENT -.-> LF
    VLLM1 & VLLM2 & VLLM3 -.-> PROM
    PROM --> GRAF
    LF --> GRAF

    style KGW fill:#4286f4
    style KAGENT fill:#2ecc71
    style KARP fill:#ffd93d
    style LF fill:#45b7d1
    style LLMD fill:#e74c3c
```

---

ì§€ê¸ˆê¹Œì§€ Kubernetes ìƒíƒœê³„ì˜ ë‹¤ì–‘í•œ ì†”ë£¨ì…˜ë“¤ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ì œ ì´ ì†”ë£¨ì…˜ë“¤ì„ **ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ìš´ì˜í•˜ê¸° ìœ„í•œ ì¸í”„ë¼ ìë™í™” ì „ëµ**ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

## Amazon EKSì™€ Karpenter: Kubernetesì˜ ì¥ì  ê·¹ëŒ€í™”

Kubernetesê°€ AI í”Œë«í¼ì˜ ê¸°ë°˜ì´ë¼ë©´, **Amazon EKSì™€ Karpenterì˜ ì¡°í•©**ì€ Kubernetesì˜ ì¥ì ì„ ê·¹ëŒ€í™”í•˜ì—¬ **ì™„ì „ ìë™í™”ëœ ìµœì ì˜ ì¸í”„ë¼**ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### EKS + Karpenter + AWS ì¸í”„ë¼ í†µí•© ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "AWS ê´€ë¦¬í˜• ì„œë¹„ìŠ¤"
        EKS["Amazon EKS<br/>ê´€ë¦¬í˜• Kubernetes"]
        EC2["Amazon EC2<br/>GPU ì¸ìŠ¤í„´ìŠ¤"]
        S3["Amazon S3<br/>ëª¨ë¸ ìŠ¤í† ë¦¬ì§€"]
        CW["CloudWatch<br/>í†µí•© ëª¨ë‹ˆí„°ë§"]
    end

    subgraph "Karpenter ìë™í™” ê³„ì¸µ"
        KARP["Karpenter Controller"]
        NP1["GPU Inference NodePool"]
        NP2["GPU Training NodePool"]
        NP3["Spot NodePool"]
    end

    subgraph "AI ì›Œí¬ë¡œë“œ"
        INF["ì¶”ë¡  ì„œë¹„ìŠ¤"]
        TRAIN["í•™ìŠµ ì‘ì—…"]
        BATCH["ë°°ì¹˜ ì²˜ë¦¬"]
    end

    EKS --> KARP
    KARP --> NP1 & NP2 & NP3
    NP1 --> EC2
    NP2 --> EC2
    NP3 --> EC2
    NP1 --> INF
    NP2 --> TRAIN
    NP3 --> BATCH
    INF & TRAIN --> S3
    INF & TRAIN --> CW

    style EKS fill:#ff9900
    style KARP fill:#ffd93d
    style EC2 fill:#ff9900
```

### ì™œ EKS + Karpenterì¸ê°€?

| ê³„ì¸µ | ì—­í•  | ì œê³µ ê°€ì¹˜ |
| --- | --- | --- |
| **Amazon EKS** | ê´€ë¦¬í˜• Kubernetes Control Plane | ìš´ì˜ ë¶€ë‹´ ì œê±°, ê³ ê°€ìš©ì„±, ë³´ì•ˆ |
| **Karpenter** | ì§€ëŠ¥í˜• ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ | Just-in-Time GPU í”„ë¡œë¹„ì €ë‹, ë¹„ìš© ìµœì í™” |
| **AWS ì¸í”„ë¼** | GPU ì¸ìŠ¤í„´ìŠ¤, ìŠ¤í† ë¦¬ì§€, ë„¤íŠ¸ì›Œí¬ | ë‹¤ì–‘í•œ GPU ì˜µì…˜, EFA ê³ ì† ë„¤íŠ¸ì›Œí¬, Spot ì¸ìŠ¤í„´ìŠ¤ |

### Karpenter: AI ì¸í”„ë¼ ìë™í™”ì˜ í•µì‹¬

KarpenterëŠ” ê¸°ì¡´ Cluster Autoscalerì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , **AI ì›Œí¬ë¡œë“œì— ìµœì í™”ëœ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹**ì„ ì œê³µí•©ë‹ˆë‹¤:

```mermaid
flowchart LR
    subgraph "ê¸°ì¡´ ë°©ì‹ (Cluster Autoscaler)"
        CA1[Pod Pending] --> CA2[Node Group í™•ì¸]
        CA2 --> CA3[ASG ìŠ¤ì¼€ì¼ ì•„ì›ƒ]
        CA3 --> CA4[ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ]
        CA4 --> CA5[Pod ìŠ¤ì¼€ì¤„ë§]
        CA5 --> CA6["â±ï¸ 5-10ë¶„ ì†Œìš”"]
    end

    subgraph "Karpenter ë°©ì‹"
        K1[Pod Pending] --> K2[ì›Œí¬ë¡œë“œ ë¶„ì„]
        K2 --> K3[ìµœì  ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ]
        K3 --> K4[ì¦‰ì‹œ í”„ë¡œë¹„ì €ë‹]
        K4 --> K5["âš¡ 2-3ë¶„ ì†Œìš”"]
    end

    style CA6 fill:#ff6b6b
    style K5 fill:#4ecdc4
    style K2 fill:#ffd93d
    style K3 fill:#ffd93d
    style K4 fill:#ffd93d
```

### Karpenterê°€ ì œê³µí•˜ëŠ” í•µì‹¬ ê°€ì¹˜

| ê¸°ëŠ¥ | ì„¤ëª… | Agentic AI ì ìš© |
| --- | --- | --- |
| **Just-in-Time í”„ë¡œë¹„ì €ë‹** | ì›Œí¬ë¡œë“œ ìš”êµ¬ì— ë”°ë¼ ì¦‰ì‹œ ë…¸ë“œ ìƒì„± | GPU ë…¸ë“œ ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™” |
| **Spot ì¸ìŠ¤í„´ìŠ¤ ì§€ì›** | ìµœëŒ€ 90% ë¹„ìš© ì ˆê° | ì¶”ë¡  ì›Œí¬ë¡œë“œ ë¹„ìš© ìµœì í™” |
| **Consolidation** | ìœ íœ´ ë…¸ë“œ ìë™ ì •ë¦¬ | GPU ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± ê·¹ëŒ€í™” |
| **ë‹¤ì–‘í•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…** | ì›Œí¬ë¡œë“œì— ìµœì í™”ëœ ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ | ëª¨ë¸ í¬ê¸°ë³„ ìµœì  GPU ë§¤ì¹­ |
| **Disruption Budgets** | ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”í•˜ë©° ë…¸ë“œ ê´€ë¦¬ | ì•ˆì •ì ì¸ ìŠ¤ì¼€ì¼ ë‹¤ìš´ |

### EKS Auto Mode: ì™„ì „ ìë™í™”ì˜ ì™„ì„±

**EKS Auto Mode**ëŠ” Karpenterë¥¼ í¬í•¨í•œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ìë™ìœ¼ë¡œ êµ¬ì„±í•˜ê³  ê´€ë¦¬í•˜ì—¬, AI ì¸í”„ë¼ ìë™í™”ì˜ ë§ˆì§€ë§‰ í¼ì¦ì„ ì™„ì„±í•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "EKS Auto Modeê°€ ìë™ ê´€ë¦¬"
        AUTO["EKS Auto Mode"]
        KARP["Karpenter<br/>(ìë™ êµ¬ì„±)"]
        VPC_CNI["VPC CNI<br/>(ìë™ êµ¬ì„±)"]
        CSI["EBS CSI Driver<br/>(ìë™ êµ¬ì„±)"]
        COREDNS["CoreDNS<br/>(ìë™ êµ¬ì„±)"]
        POD_ID["Pod Identity Agent<br/>(ìë™ êµ¬ì„±)"]
    end

    subgraph "ì‚¬ìš©ì ì •ì˜ ì˜ì—­"
        NP["Custom NodePool<br/>(GPU ìµœì í™”)"]
        NC["Custom NodeClass<br/>(EFA, ìŠ¤í† ë¦¬ì§€)"]
        WL["AI ì›Œí¬ë¡œë“œ"]
    end

    AUTO --> KARP
    AUTO --> VPC_CNI
    AUTO --> CSI
    AUTO --> COREDNS
    AUTO --> POD_ID
    KARP --> NP
    NP --> NC
    NC --> WL

    style AUTO fill:#ff9900
    style KARP fill:#ffd93d
```

#### EKS Auto Mode vs ìˆ˜ë™ êµ¬ì„± ë¹„êµ

| êµ¬ì„± ìš”ì†Œ | ìˆ˜ë™ êµ¬ì„± (EKS Standard) | EKS Auto Mode |
| --- | --- | --- |
| **Karpenter ì„¤ì¹˜** | Helm ì°¨íŠ¸ ìˆ˜ë™ ì„¤ì¹˜, IAM ì—­í•  êµ¬ì„± | âœ… ìë™ ì„¤ì¹˜ ë° êµ¬ì„± |
| **NodePool ê´€ë¦¬** | ì§ì ‘ ì •ì˜ í•„ìš” | ê¸°ë³¸ ì œê³µ + ì»¤ìŠ¤í…€ ê°€ëŠ¥ |
| **VPC CNI** | ìˆ˜ë™ ì„¤ì¹˜ ë° ì—…ê·¸ë ˆì´ë“œ | âœ… ìë™ ê´€ë¦¬ |
| **EBS CSI Driver** | ìˆ˜ë™ ì„¤ì¹˜, IRSA êµ¬ì„± | âœ… ìë™ ê´€ë¦¬ |
| **CoreDNS** | ìˆ˜ë™ ìŠ¤ì¼€ì¼ë§ | âœ… ìë™ ìŠ¤ì¼€ì¼ë§ |
| **ë³´ì•ˆ íŒ¨ì¹˜** | ìˆ˜ë™ ì ìš© | âœ… ìë™ ì ìš© |
| **ë²„ì „ ì—…ê·¸ë ˆì´ë“œ** | ìˆ˜ë™ ê³„íš ë° ì‹¤í–‰ | âœ… ìë™ ì—…ê·¸ë ˆì´ë“œ |

#### EKS Auto Modeì˜ AI ì›Œí¬ë¡œë“œ ì´ì 

```mermaid
sequenceDiagram
    participant User as í”Œë«í¼ ì—”ì§€ë‹ˆì–´
    participant Auto as EKS Auto Mode
    participant Karp as Karpenter (ìë™ ê´€ë¦¬)
    participant EC2 as AWS EC2

    Note over User,EC2: EKS Auto Mode í´ëŸ¬ìŠ¤í„° ìƒì„±
    User->>Auto: í´ëŸ¬ìŠ¤í„° ìƒì„± ìš”ì²­
    Auto->>Auto: Karpenter ìë™ ì„¤ì¹˜
    Auto->>Auto: ê¸°ë³¸ NodePool êµ¬ì„±
    Auto-->>User: í´ëŸ¬ìŠ¤í„° ì¤€ë¹„ ì™„ë£Œ

    Note over User,EC2: GPU ì›Œí¬ë¡œë“œ ë°°í¬
    User->>Auto: GPU Pod ë°°í¬
    Auto->>Karp: Pending Pod ê°ì§€
    Karp->>EC2: GPU ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œë¹„ì €ë‹
    EC2-->>Karp: p4d.24xlarge ì¤€ë¹„
    Karp-->>User: Pod ì‹¤í–‰ ì¤‘

    Note over User,EC2: ìë™ ìµœì í™”
    Karp->>Karp: Consolidation ì‹¤í–‰
    Karp->>EC2: ìœ íœ´ ë…¸ë“œ ì •ë¦¬
```

#### GPU ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ EKS Auto Mode ì„¤ì •

EKS Auto Modeì—ì„œ GPU ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ NodePoolì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
# EKS Auto Modeì—ì„œ GPU NodePool ì¶”ê°€
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-inference
        eks-auto-mode: "true"
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.xlarge
            - g5.2xlarge
            - g5.4xlarge
            - g5.12xlarge
            - p4d.24xlarge
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default  # EKS Auto Mode ê¸°ë³¸ NodeClass í™œìš©
  limits:
    nvidia.com/gpu: 50
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
```

:::tip EKS Auto Mode ê¶Œì¥ ì‚¬í•­
EKS Auto ModeëŠ” **ìƒˆë¡œìš´ AI í”Œë«í¼ êµ¬ì¶• ì‹œ ê¶Œì¥ë˜ëŠ” ì˜µì…˜**ì…ë‹ˆë‹¤:
- Karpenter ì„¤ì¹˜ ë° êµ¬ì„± ìë™í™”ë¡œ **ì´ˆê¸° êµ¬ì¶• ì‹œê°„ 80% ë‹¨ì¶•**
- í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ìë™ ì—…ê·¸ë ˆì´ë“œë¡œ **ìš´ì˜ ë¶€ë‹´ ëŒ€í­ ê°ì†Œ**
- GPU NodePoolë§Œ ì»¤ìŠ¤í…€ ì •ì˜í•˜ë©´ **ì¦‰ì‹œ AI ì›Œí¬ë¡œë“œ ë°°í¬ ê°€ëŠ¥**
:::

:::info EKS Auto Modeì™€ GPU ì§€ì›
EKS Auto ModeëŠ” NVIDIA GPUë¥¼ í¬í•¨í•œ ê°€ì† ì»´í“¨íŒ… ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì™„ë²½íˆ ì§€ì›í•©ë‹ˆë‹¤. ê¸°ë³¸ NodeClassì— GPU ë“œë¼ì´ë²„ê°€ í¬í•¨ëœ AMIê°€ ìë™ìœ¼ë¡œ ì„ íƒë˜ë©°, í•„ìš”ì‹œ ì»¤ìŠ¤í…€ NodeClassë¡œ EFA ë„¤íŠ¸ì›Œí¬ ë“± ê³ ê¸‰ ì„¤ì •ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

### Karpenter vs Cluster Autoscaler ìƒì„¸ ë¹„êµ

:::tip Karpenter vs Cluster Autoscaler
KarpenterëŠ” Node Group ì—†ì´ ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­ì„ ì§ì ‘ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. GPU ì›Œí¬ë¡œë“œì˜ ê²½ìš° í”„ë¡œë¹„ì €ë‹ ì‹œê°„ì´ **50% ì´ìƒ ë‹¨ì¶•**ë˜ê³ , Consolidationì„ í†µí•´ **ë¹„ìš©ì´ 20-30% ì ˆê°**ë©ë‹ˆë‹¤.
:::

### ë„ì „ê³¼ì œë³„ Karpenter í•´ê²° ë°©ì•ˆ ë§¤í•‘

```mermaid
graph TB
    subgraph "4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œ"
        C1["ğŸ–¥ï¸ GPU ëª¨ë‹ˆí„°ë§ ë°<br/>ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§"]
        C2["ğŸ”€ Agentic AI ìš”ì²­<br/>ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§"]
        C3["ğŸ“Š í† í°/ì„¸ì…˜ ìˆ˜ì¤€<br/>ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© ì»¨íŠ¸ë¡¤"]
        C4["ğŸ”§ FM íŒŒì¸íŠœë‹ê³¼<br/>ìë™í™” íŒŒì´í”„ë¼ì¸"]
    end

    subgraph "Karpenter ì¤‘ì‹¬ í•´ê²° ë°©ì•ˆ"
        S1["â­ Karpenter NodePool<br/>GPU ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ"]
        S2["Karpenter + KEDA<br/>End-to-End ìë™ ìŠ¤ì¼€ì¼ë§"]
        S3["Spot + Consolidation<br/>ë¹„ìš© 50-70% ì ˆê°"]
        S4["Training NodePool<br/>EFA ë„¤íŠ¸ì›Œí¬ ìµœì í™”"]
    end

    subgraph "ë³´ì¡° ì†”ë£¨ì…˜"
        A1["DCGM Exporter<br/>GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘"]
        A2["Gateway API<br/>ë™ì  ë¼ìš°íŒ…"]
        A3["LangFuse<br/>í† í° ì¶”ì "]
        A4["NeMo + Kubeflow<br/>í•™ìŠµ íŒŒì´í”„ë¼ì¸"]
    end

    C1 --> S1
    C2 --> S2
    C3 --> S3
    C4 --> S4
    S1 --> A1
    S2 --> A2
    S3 --> A3
    S4 --> A4

    style C1 fill:#ff6b6b
    style C2 fill:#4ecdc4
    style C3 fill:#45b7d1
    style C4 fill:#96ceb4
    style S1 fill:#ffd93d
    style S2 fill:#ffd93d
    style S3 fill:#ffd93d
    style S4 fill:#ffd93d
```

:::info ëŒ€ìƒ ë…ì
ì´ ë¬¸ì„œëŠ” Agentic AI Platform ë„ì…ì„ ê²€í† í•˜ëŠ” **ê¸°ìˆ  ì˜ì‚¬ê²°ì •ì**ì™€ **ì†”ë£¨ì…˜ ì•„í‚¤í…íŠ¸**ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤. Kubernetes ê¸°ë°˜ AI ì¸í”„ë¼ì˜ í•„ìš”ì„±ê³¼ EKS + Karpenterë¥¼ í™œìš©í•œ êµ¬ì²´ì ì¸ êµ¬í˜„ ë°©ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.
:::

---

## 4ê°€ì§€ í•µì‹¬ ê¸°ìˆ ì  ë„ì „ê³¼ì œ ìƒì„¸ ë¶„ì„

### ë„ì „ê³¼ì œ 1: GPU ëª¨ë‹ˆí„°ë§ ë° ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¤„ë§

Agentic AI ì›Œí¬ë¡œë“œëŠ” GPU ë¦¬ì†ŒìŠ¤ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. ë³µìˆ˜ì˜ GPU í´ëŸ¬ìŠ¤í„°ë¥¼ ìš´ì˜í•  ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ì–´ë ¤ì›€ì— ì§ë©´í•©ë‹ˆë‹¤.

#### ê¸°ìˆ ì  ë¬¸ì œì  ìƒì„¸ ë¶„ì„

**1. ë©€í‹° í´ëŸ¬ìŠ¤í„° GPU ê°€ì‹œì„± ë¶€ì¬**

ëŒ€ê·œëª¨ AI í”Œë«í¼ì—ì„œëŠ” ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ì— ë¶„ì‚°ëœ GPU ë¦¬ì†ŒìŠ¤ë¥¼ í†µí•©ì ìœ¼ë¡œ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "ê°€ì‹œì„± ë¬¸ì œ"
        Q1["í´ëŸ¬ìŠ¤í„° Aì˜ GPU ì‚¬ìš©ë¥ ì€?"]
        Q2["ì „ì²´ ìœ íœ´ GPUëŠ” ëª‡ ê°œ?"]
        Q3["ì–´ëŠ í´ëŸ¬ìŠ¤í„°ì— ì›Œí¬ë¡œë“œ ë°°ì¹˜?"]
    end

    subgraph "ë¶„ì‚°ëœ GPU í´ëŸ¬ìŠ¤í„°"
        subgraph "Cluster A (US-East)"
            A1["A100 x 16<br/>ì‚¬ìš©ë¥ : ???"]
        end
        subgraph "Cluster B (US-West)"
            B1["H100 x 8<br/>ì‚¬ìš©ë¥ : ???"]
        end
        subgraph "Cluster C (EU)"
            C1["A100 x 24<br/>ì‚¬ìš©ë¥ : ???"]
        end
    end

    Q1 -.-> A1
    Q2 -.-> A1 & B1 & C1
    Q3 -.-> A1 & B1 & C1

    style Q1 fill:#ff6b6b
    style Q2 fill:#ff6b6b
    style Q3 fill:#ff6b6b
```

| ë¬¸ì œ ì˜ì—­ | êµ¬ì²´ì  ì–´ë ¤ì›€ | ì˜í–¥ |
| --- | --- | --- |
| ë©”íŠ¸ë¦­ ìˆ˜ì§‘ | í´ëŸ¬ìŠ¤í„°ë³„ ë‹¤ë¥¸ ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ | í†µí•© ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ì–´ë ¤ì›€ |
| ì‹¤ì‹œê°„ í˜„í™© | GPU í• ë‹¹ ìƒíƒœ íŒŒì•… ì§€ì—° | ë¦¬ì†ŒìŠ¤ ë‚­ë¹„, ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨ |
| ìš©ëŸ‰ ê³„íš | ì „ì²´ GPU ì¸ë²¤í† ë¦¬ íŒŒì•… ë¶ˆê°€ | ê³¼ì‰/ë¶€ì¡± í”„ë¡œë¹„ì €ë‹ |

**2. GPU ì„¸ëŒ€ë³„ ì›Œí¬ë¡œë“œ ë§¤ì¹­ ë³µì¡ì„±**

A100, H100, H200 ë“± ë‹¤ì–‘í•œ GPU ì„¸ëŒ€ê°€ í˜¼í•© ìš´ì˜ë  ë•Œ, ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë§ëŠ” ìµœì ì˜ GPUë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤:

| GPU ì„¸ëŒ€ | ë©”ëª¨ë¦¬ | FP16 ì„±ëŠ¥ | ì í•© ì›Œí¬ë¡œë“œ | ì‹œê°„ë‹¹ ë¹„ìš© |
| --- | --- | --- | --- | --- |
| A10G | 24GB | 125 TFLOPS | ì†Œê·œëª¨ ì¶”ë¡  (7B ì´í•˜) | ~$1.0 |
| A100 40GB | 40GB | 312 TFLOPS | ì¤‘ê·œëª¨ ì¶”ë¡ /í•™ìŠµ | ~$4.1 |
| A100 80GB | 80GB | 312 TFLOPS | ëŒ€ê·œëª¨ ëª¨ë¸ | ~$5.1 |
| H100 80GB | 80GB | 989 TFLOPS | ì´ˆëŒ€ê·œëª¨ í•™ìŠµ/ì¶”ë¡  | ~$12.3 |
| H200 | 141GB | 989 TFLOPS | ìµœëŒ€ ê·œëª¨ ëª¨ë¸ | ~$15.0+ |

**3. GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì˜ ê¸°ìˆ ì  í•œê³„**

- DCGM Exporterì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì£¼ê¸°ì™€ ì •í™•ë„
- MIG(Multi-Instance GPU) í™˜ê²½ì—ì„œì˜ ë©”íŠ¸ë¦­ ë¶„ë¦¬
- ì»¨í…Œì´ë„ˆ ë ˆë²¨ GPU ì‚¬ìš©ëŸ‰ ì¶”ì ì˜ ì–´ë ¤ì›€

```mermaid
graph LR
    subgraph "GPU í´ëŸ¬ìŠ¤í„° í™˜ê²½"
        subgraph "Cluster A"
            A100_1["A100 x 8"]
            A100_2["A100 x 8"]
        end
        subgraph "Cluster B"
            H100_1["H100 x 8"]
            H100_2["H100 x 8"]
        end
        subgraph "Cluster C"
            H200_1["H200 x 8"]
        end
    end

    subgraph "Karpenter + ëª¨ë‹ˆí„°ë§"
        KARP["Karpenter<br/>NodePool"]
        DCGM["DCGM Exporter"]
        PROM["Prometheus"]
    end

    A100_1 --> DCGM
    H100_1 --> DCGM
    H200_1 --> DCGM
    DCGM --> PROM
    PROM --> KARP

    style KARP fill:#ffd93d
```

#### Karpenter ê¸°ë°˜ í•´ê²° ë°©ì•ˆ (ê¶Œì¥)

**Karpenter NodePool**ì„ í™œìš©í•˜ë©´ GPU ì›Œí¬ë¡œë“œì— ìµœì í™”ëœ ë…¸ë“œë¥¼ ìë™ìœ¼ë¡œ í”„ë¡œë¹„ì €ë‹í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<Tabs>
<TabItem value="nodepool" label="GPU NodePool ì„¤ì •" default>

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-inference
        workload: genai
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge    # 8x A100 40GB
            - p5.48xlarge     # 8x H100 80GB
            - g5.48xlarge     # 8x A10G 24GB
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 100
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
  weight: 100
```

</TabItem>
<TabItem value="nodeclass" label="EC2NodeClass ì„¤ì •">

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu-nodeclass
spec:
  role: KarpenterNodeRole-${CLUSTER_NAME}
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 500Gi
        volumeType: gp3
        iops: 10000
        throughput: 500
        encrypted: true
  instanceStorePolicy: RAID0
  userData: |
    #!/bin/bash
    nvidia-smi -pm 1
    modprobe efa
```

</TabItem>
</Tabs>

#### Karpenterì˜ GPU ì›Œí¬ë¡œë“œ ìµœì í™” ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì„¤ëª… | íš¨ê³¼ |
| --- | --- | --- |
| ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìë™ ì„ íƒ | ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” GPU ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ | ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ë°©ì§€ |
| Spot ì¸ìŠ¤í„´ìŠ¤ í´ë°± | Spot ë¶ˆê°€ ì‹œ On-Demandë¡œ ìë™ ì „í™˜ | ê°€ìš©ì„± ë³´ì¥ |
| Consolidation | ìœ íœ´ GPU ë…¸ë“œ ìë™ ì •ë¦¬ | ë¹„ìš© 30% ì ˆê° |
| ë¹ ë¥¸ í”„ë¡œë¹„ì €ë‹ | Node Group ì—†ì´ ì§ì ‘ EC2 API í˜¸ì¶œ | í”„ë¡œë¹„ì €ë‹ ì‹œê°„ 50% ë‹¨ì¶• |

#### ë³´ì¡° ì†”ë£¨ì…˜: NVIDIA GPU Operator

Karpenterì™€ í•¨ê»˜ NVIDIA GPU Operatorë¥¼ ì‚¬ìš©í•˜ì—¬ GPU ë“œë¼ì´ë²„ ë° ëª¨ë‹ˆí„°ë§ ìŠ¤íƒì„ ìë™í™”í•©ë‹ˆë‹¤.

```yaml
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: cluster-policy
spec:
  operator:
    defaultRuntime: containerd
  driver:
    enabled: true
    version: "535.104.05"
  toolkit:
    enabled: true
  devicePlugin:
    enabled: true
  dcgmExporter:
    enabled: true
  migManager:
    enabled: true
```

### ë„ì „ê³¼ì œ 2: Agentic AI ìš”ì²­ ë™ì  ë¼ìš°íŒ… ë° ìŠ¤ì¼€ì¼ë§

Agentic AI ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ FM(Foundation Model)ì„ ë™ì‹œì— ì„œë¹™í•˜ë©°, íŠ¸ë˜í”½ íŒ¨í„´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ëŒ€ì‘í•´ì•¼ í•©ë‹ˆë‹¤.

#### ê¸°ìˆ ì  ë¬¸ì œì  ìƒì„¸ ë¶„ì„

**1. ë©€í‹° ëª¨ë¸ ì„œë¹™ì˜ ë³µì¡ì„±**

Agentic AI ì‹œìŠ¤í…œì€ ë‹¨ì¼ ëª¨ë¸ì´ ì•„ë‹Œ ì—¬ëŸ¬ ëª¨ë¸ì„ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "Agent ìš”ì²­ ì²˜ë¦¬ íë¦„"
        REQ["ì‚¬ìš©ì ìš”ì²­"]
        ROUTER["ìš”ì²­ ë¼ìš°í„°"]
        
        subgraph "ëª¨ë¸ ì„ íƒ ë¡œì§"
            M1["GPT-4<br/>ë³µì¡í•œ ì¶”ë¡ "]
            M2["Claude-3<br/>ê¸´ ì»¨í…ìŠ¤íŠ¸"]
            M3["Llama-70B<br/>ë¹„ìš© íš¨ìœ¨"]
            M4["Embedding<br/>ë²¡í„° ê²€ìƒ‰"]
        end
        
        RESP["ì‘ë‹µ ì¡°í•©"]
    end

    REQ --> ROUTER
    ROUTER --> M1
    ROUTER --> M2
    ROUTER --> M3
    ROUTER --> M4
    M1 & M2 & M3 & M4 --> RESP

    style ROUTER fill:#4ecdc4
```

| ë¼ìš°íŒ… ê¸°ì¤€ | ì„¤ëª… | êµ¬í˜„ ë³µì¡ë„ |
| --- | --- | --- |
| ìš”ì²­ ìœ í˜• | ì½”ë“œ ìƒì„±, ëŒ€í™”, ìš”ì•½ ë“± | ì¤‘ê°„ |
| ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ | í† í° ìˆ˜ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ | ë‚®ìŒ |
| ë¹„ìš© ì œì•½ | ì˜ˆì‚° ë‚´ ìµœì  ëª¨ë¸ ì„ íƒ | ë†’ìŒ |
| ì§€ì—° ì‹œê°„ ìš”êµ¬ | SLA ê¸°ë°˜ ëª¨ë¸ ì„ íƒ | ë†’ìŒ |
| ëª¨ë¸ ê°€ìš©ì„± | ì¥ì•  ì‹œ í´ë°± ëª¨ë¸ ì„ íƒ | ì¤‘ê°„ |

**2. ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ íŠ¸ë˜í”½ íŒ¨í„´**

Agentic AI ì›Œí¬ë¡œë“œëŠ” ê¸°ì¡´ ì›¹ ì„œë¹„ìŠ¤ì™€ ë‹¤ë¥¸ íŠ¸ë˜í”½ íŠ¹ì„±ì„ ë³´ì…ë‹ˆë‹¤:

```mermaid
graph LR
    subgraph "íŠ¸ë˜í”½ íŠ¹ì„± ë¹„êµ"
        subgraph "ì¼ë°˜ ì›¹ ì„œë¹„ìŠ¤"
            W1["ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´"]
            W2["ì§§ì€ ìš”ì²­ ì‹œê°„"]
            W3["ê· ì¼í•œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©"]
        end
        
        subgraph "Agentic AI ì„œë¹„ìŠ¤"
            A1["ë²„ìŠ¤íŠ¸ íŠ¸ë˜í”½"]
            A2["ê¸´ ìš”ì²­ ì‹œê°„ (ìˆ˜ì´ˆ~ìˆ˜ë¶„)"]
            A3["ìš”ì²­ë³„ ë¦¬ì†ŒìŠ¤ í¸ì°¨ í¼"]
        end
    end

    style A1 fill:#ff6b6b
    style A2 fill:#ff6b6b
    style A3 fill:#ff6b6b
```

**3. GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì§€ì—°**

íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œ GPU ë…¸ë“œ í™•ë³´ê¹Œì§€ì˜ ì‹œê°„ì´ ì„œë¹„ìŠ¤ í’ˆì§ˆì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤:

| ë‹¨ê³„ | ê¸°ì¡´ ë°©ì‹ (Cluster Autoscaler) | Karpenter |
| --- | --- | --- |
| Pending Pod ê°ì§€ | 30-60ì´ˆ | ì¦‰ì‹œ |
| ìŠ¤ì¼€ì¼ë§ ê²°ì • | Node Group ê¸°ë°˜ | ì›Œí¬ë¡œë“œ ì§ì ‘ ë¶„ì„ |
| ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ | ê³ ì •ëœ íƒ€ì… | ìµœì  íƒ€ì… ìë™ ì„ íƒ |
| í”„ë¡œë¹„ì €ë‹ | ASG ê²½ìœ  (2-5ë¶„) | ì§ì ‘ EC2 API (1-3ë¶„) |
| **ì´ ì†Œìš” ì‹œê°„** | **5-10ë¶„** | **2-4ë¶„** |

**4. ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì‹œ ì„œë¹„ìŠ¤ ì˜í–¥**

GPU ë…¸ë“œ ì¶•ì†Œ ì‹œ ì§„í–‰ ì¤‘ì¸ ìš”ì²­ ì²˜ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤:

- LLM ì¶”ë¡ ì€ ìˆ˜ì´ˆ~ìˆ˜ë¶„ ì†Œìš”
- ê°‘ì‘ìŠ¤ëŸ¬ìš´ ë…¸ë“œ ì¢…ë£Œ ì‹œ ìš”ì²­ ì‹¤íŒ¨
- Graceful shutdown êµ¬í˜„ í•„ìš”

```mermaid
graph TB
    subgraph "Client Requests"
        REQ1["Chat Request"]
        REQ2["Code Generation"]
        REQ3["RAG Query"]
    end

    subgraph "Gateway Layer"
        GW["Kgateway<br/>Inference Gateway"]
        ROUTE["Dynamic Router"]
    end

    subgraph "Karpenter ê´€ë¦¬ ë…¸ë“œ"
        subgraph "Model Serving"
            M1["vLLM - GPT-4"]
            M2["vLLM - Claude"]
            M3["TGI - Llama"]
        end
        KARP["Karpenter<br/>Auto Provisioning"]
    end

    REQ1 --> GW
    REQ2 --> GW
    REQ3 --> GW
    GW --> ROUTE
    ROUTE --> M1
    ROUTE --> M2
    ROUTE --> M3
    M1 & M2 & M3 -.-> KARP

    style KARP fill:#ffd93d
    style GW fill:#4286f4
```

#### Karpenter + KEDA ì—°ë™ í•´ê²° ë°©ì•ˆ (ê¶Œì¥)

Karpenterì™€ KEDAë¥¼ ì—°ë™í•˜ë©´ **ì›Œí¬ë¡œë“œ ìŠ¤ì¼€ì¼ë§ê³¼ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ì´ ìë™ìœ¼ë¡œ ì—°ê³„**ë©ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì íŠ¸ë˜í”½
    participant KEDA as KEDA Controller
    participant HPA as HPA
    participant Karpenter as Karpenter
    participant AWS as AWS EC2

    User->>KEDA: íŠ¸ë˜í”½ ê¸‰ì¦ ê°ì§€
    KEDA->>HPA: Pod ìŠ¤ì¼€ì¼ ì•„ì›ƒ íŠ¸ë¦¬ê±°
    HPA->>Karpenter: Pending Pod ê°ì§€
    Karpenter->>AWS: ìµœì  GPU ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œë¹„ì €ë‹
    AWS-->>Karpenter: p4d.24xlarge ì¤€ë¹„ ì™„ë£Œ
    Karpenter-->>HPA: ìƒˆ ë…¸ë“œì— Pod ìŠ¤ì¼€ì¤„ë§
    HPA-->>User: ì‘ë‹µ ì§€ì—° ì‹œê°„ ì •ìƒí™”
```

<Tabs>
<TabItem value="keda" label="KEDA ScaledObject" default>

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: vllm-gpu-scaler
  namespace: ai-inference
spec:
  scaleTargetRef:
    name: vllm-deployment
  minReplicaCount: 2
  maxReplicaCount: 20
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.observability:9090
        metricName: vllm_pending_requests
        threshold: "50"
        query: |
          sum(vllm_pending_requests{namespace="ai-inference"})
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.observability:9090
        metricName: gpu_utilization
        threshold: "70"
        query: |
          avg(DCGM_FI_DEV_GPU_UTIL{namespace="ai-inference"})
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
        scaleDown:
          stabilizationWindowSeconds: 300
```

</TabItem>
<TabItem value="httproute" label="Gateway API HTTPRoute">

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: ai-model-routing
  namespace: ai-inference
spec:
  parentRefs:
    - name: ai-gateway
      namespace: ai-gateway
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /v1/chat/completions
          headers:
            - name: x-model-id
              value: "gpt-4"
      backendRefs:
        - name: vllm-gpt4
          port: 8000
          weight: 80
        - name: vllm-gpt4-canary
          port: 8000
          weight: 20
    - matches:
        - path:
            type: PathPrefix
            value: /v1/chat/completions
          headers:
            - name: x-model-id
              value: "claude-3"
      backendRefs:
        - name: vllm-claude
          port: 8000
```

</TabItem>
</Tabs>

#### Karpenter Disruption ì •ì±…ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´

íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œì—ë„ ì„œë¹„ìŠ¤ ì•ˆì •ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ Karpenter ì„¤ì •ì…ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-inference-stable
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
    budgets:
      # ë™ì‹œì— ì¤‘ë‹¨ ê°€ëŠ¥í•œ ë…¸ë“œ ìˆ˜ ì œí•œ
      - nodes: "20%"
      # ì—…ë¬´ ì‹œê°„ì—ëŠ” ì¤‘ë‹¨ ë°©ì§€
      - nodes: "0"
        schedule: "0 9 * * 1-5"
        duration: 10h
```

:::warning ìŠ¤ì¼€ì¼ë§ ì£¼ì˜ì‚¬í•­
GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ì€ ì¼ë°˜ CPU ë…¸ë“œë³´ë‹¤ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. Karpenterì˜ `consolidationPolicy`ë¥¼ ì ì ˆíˆ ì„¤ì •í•˜ì—¬ ë¶ˆí•„ìš”í•œ ìŠ¤ì¼€ì¼ ë‹¤ìš´ì„ ë°©ì§€í•˜ì„¸ìš”.
:::

### ë„ì „ê³¼ì œ 3: í† í°/ì„¸ì…˜ ìˆ˜ì¤€ ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© ì»¨íŠ¸ë¡¤

LLM ê¸°ë°˜ ì‹œìŠ¤í…œì—ì„œëŠ” í† í° ë‹¨ìœ„ì˜ ì„¸ë°€í•œ ëª¨ë‹ˆí„°ë§ê³¼ ë¹„ìš© ê´€ë¦¬ê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤. íŠ¹íˆ GPU ì¸í”„ë¼ ë¹„ìš©ì´ ì „ì²´ ìš´ì˜ ë¹„ìš©ì˜ 70-80%ë¥¼ ì°¨ì§€í•˜ë¯€ë¡œ, **ì¸í”„ë¼ ë ˆë²¨ì˜ ë¹„ìš© ìµœì í™”**ê°€ í•µì‹¬ì…ë‹ˆë‹¤.

#### ê¸°ìˆ ì  ë¬¸ì œì  ìƒì„¸ ë¶„ì„

**1. í† í° ë ˆë²¨ ë¹„ìš© ì¶”ì ì˜ ë³µì¡ì„±**

LLM ì„œë¹„ìŠ¤ì˜ ë¹„ìš© êµ¬ì¡°ëŠ” ë‹¤ì¸µì ì…ë‹ˆë‹¤:

```
ì´ ë¹„ìš© = GPU ì¸í”„ë¼ ë¹„ìš© + API í˜¸ì¶œ ë¹„ìš© + ìŠ¤í† ë¦¬ì§€ ë¹„ìš© + ë„¤íŠ¸ì›Œí¬ ë¹„ìš©
```

| ë¹„ìš© ìš”ì†Œ | ì¸¡ì • ë‚œì´ë„ | ë¹„ì¤‘ | ë¬¸ì œì  |
| --- | --- | --- | --- |
| GPU ì¸í”„ë¼ | ì¤‘ê°„ | 70-80% | ìœ íœ´ ì‹œê°„ ë¹„ìš© ë°œìƒ, ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ë³„ ë‹¨ê°€ ì°¨ì´ |
| í† í° ì‚¬ìš©ëŸ‰ | ë†’ìŒ | 10-15% | ì…ë ¥/ì¶œë ¥ í† í° ë¹„ìœ¨ ì˜ˆì¸¡ ì–´ë ¤ì›€ |
| ìŠ¤í† ë¦¬ì§€ | ë‚®ìŒ | 5-10% | ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ í¬ê¸° ì¦ê°€ |
| ë„¤íŠ¸ì›Œí¬ | ë‚®ìŒ | 3-5% | Cross-AZ íŠ¸ë˜í”½ ë¹„ìš© |

**2. GPU ìœ íœ´ ë¹„ìš© ë¬¸ì œ**

```mermaid
graph LR
    subgraph "ì¼ë°˜ì ì¸ GPU ì‚¬ìš© íŒ¨í„´"
        direction TB
        T1["09:00-12:00<br/>ì‚¬ìš©ë¥  80%"]
        T2["12:00-14:00<br/>ì‚¬ìš©ë¥  30%"]
        T3["14:00-18:00<br/>ì‚¬ìš©ë¥  70%"]
        T4["18:00-09:00<br/>ì‚¬ìš©ë¥  10%"]
    end

    subgraph "ë¹„ìš© ë‚­ë¹„ ì˜ì—­"
        W1["ì ì‹¬ ì‹œê°„<br/>ìœ íœ´ GPU ë¹„ìš©"]
        W2["ì•¼ê°„/ì£¼ë§<br/>ìœ íœ´ GPU ë¹„ìš©"]
    end

    T2 --> W1
    T4 --> W2

    style W1 fill:#ff6b6b
    style W2 fill:#ff6b6b
```

**3. ë©€í‹° í…Œë„ŒíŠ¸ ë¹„ìš© ë¶„ë¦¬ì˜ ì–´ë ¤ì›€**

- íŒ€/í”„ë¡œì íŠ¸ë³„ GPU ì‚¬ìš©ëŸ‰ ì •í™•í•œ ì¸¡ì • í•„ìš”
- ê³µìœ  GPU ë…¸ë“œì—ì„œì˜ ë¹„ìš© í• ë‹¹ ë¡œì§ ë³µì¡
- ì‹¤ì‹œê°„ í• ë‹¹ëŸ‰(Quota) ê´€ë¦¬ ë° ì´ˆê³¼ ë°©ì§€

**4. ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ë¹„ìš© ê¸‰ì¦**

- íŠ¸ë˜í”½ ìŠ¤íŒŒì´í¬ ì‹œ ìë™ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì¸í•œ ë¹„ìš© ê¸‰ì¦
- Spot ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì‹œ On-Demand í´ë°±ìœ¼ë¡œ ë¹„ìš© ì¦ê°€
- ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ ì¼ì‹œì  ë¦¬ì†ŒìŠ¤ ì¤‘ë³µ ì‚¬ìš©

```mermaid
graph TB
    subgraph "AI Application"
        APP["Agent Application"]
        SDK["LangFuse SDK"]
    end

    subgraph "Observability Stack"
        LF["LangFuse"]
        OTEL["OpenTelemetry<br/>Collector"]
    end

    subgraph "Metrics & Cost"
        PROM["Prometheus"]
        GRAF["Grafana"]
        COST["Cost Dashboard"]
    end

    subgraph "Karpenter ë¹„ìš© ìµœì í™”"
        KARP["Karpenter"]
        SPOT["Spot ì¸ìŠ¤í„´ìŠ¤"]
        CONSOL["Consolidation"]
        BUDGET["Budget ì •ì±…"]
    end

    APP --> SDK
    SDK --> LF
    LF --> OTEL
    OTEL --> PROM
    PROM --> GRAF
    PROM --> COST
    KARP --> SPOT
    KARP --> CONSOL
    KARP --> BUDGET
    SPOT --> COST
    CONSOL --> COST

    style LF fill:#45b7d1
    style KARP fill:#ffd93d
```

#### Karpenter ê¸°ë°˜ ë¹„ìš© ìµœì í™” ì „ëµ (ê¶Œì¥)

KarpenterëŠ” GPU ì¸í”„ë¼ ë¹„ìš© ìµœì í™”ì˜ **í•µì‹¬ ë ˆë²„**ì…ë‹ˆë‹¤. ë‹¤ìŒ 4ê°€ì§€ ì „ëµì„ ì¡°í•©í•˜ì—¬ ìµœëŒ€ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì „ëµ 1: Spot ì¸ìŠ¤í„´ìŠ¤ ìš°ì„  í™œìš©**

Karpenterì˜ Spot ì¸ìŠ¤í„´ìŠ¤ ì§€ì›ì„ í™œìš©í•˜ë©´ GPU ë¹„ìš©ì„ **ìµœëŒ€ 90%ê¹Œì§€ ì ˆê°**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-spot-inference
spec:
  template:
    metadata:
      labels:
        cost-tier: spot
        workload: inference
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.12xlarge
            - g5.24xlarge
            - g5.48xlarge
            - p4d.24xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-spot-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: karpenter.sh/capacity-type
          value: "spot"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 32
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 30s
  weight: 50  # On-Demandë³´ë‹¤ ìš°ì„  ì„ íƒ
```

**ì „ëµ 2: ì‹œê°„ëŒ€ë³„ ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ë¹„ìš© ê´€ë¦¬**

ì—…ë¬´ ì‹œê°„ê³¼ ë¹„ì—…ë¬´ ì‹œê°„ì— ë”°ë¥¸ ì°¨ë³„í™”ëœ ë¦¬ì†ŒìŠ¤ ì •ì±…ì„ ì ìš©í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-scheduled-pool
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.12xlarge
            - g5.24xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodeclass
  limits:
    nvidia.com/gpu: 16
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
    budgets:
      # ì—…ë¬´ ì‹œê°„: ì•ˆì •ì„± ìš°ì„  (ë…¸ë“œ ì¤‘ë‹¨ ìµœì†Œí™”)
      - nodes: "10%"
        schedule: "0 9 * * 1-5"
        duration: 9h
      # ë¹„ì—…ë¬´ ì‹œê°„: ë¹„ìš© ìš°ì„  (ì ê·¹ì  í†µí•©)
      - nodes: "50%"
        schedule: "0 18 * * 1-5"
        duration: 15h
      # ì£¼ë§: ìµœì†Œ ë¦¬ì†ŒìŠ¤ ìœ ì§€
      - nodes: "80%"
        schedule: "0 0 * * 0,6"
        duration: 24h
```

**ì „ëµ 3: Consolidationì„ í†µí•œ ìœ íœ´ ë¦¬ì†ŒìŠ¤ ì œê±°**

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-consolidation-pool
spec:
  disruption:
    # ë…¸ë“œê°€ ë¹„ì–´ìˆê±°ë‚˜ í™œìš©ë„ê°€ ë‚®ì„ ë•Œ í†µí•©
    consolidationPolicy: WhenEmptyOrUnderutilized
    # ë¹ ë¥¸ í†µí•©ìœ¼ë¡œ ë¹„ìš© ì ˆê° (30ì´ˆ ëŒ€ê¸° í›„ í†µí•©)
    consolidateAfter: 30s
```

**ì „ëµ 4: ì›Œí¬ë¡œë“œë³„ ì¸ìŠ¤í„´ìŠ¤ ìµœì í™”**

```yaml
# ì†Œê·œëª¨ ëª¨ë¸ìš© (7B ì´í•˜) - ë¹„ìš© íš¨ìœ¨ì 
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-small-models
spec:
  template:
    spec:
      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - g5.xlarge      # 1x A10G - $1.01/hr
            - g5.2xlarge     # 1x A10G - $1.21/hr
  weight: 100  # ìµœìš°ì„  ì„ íƒ

---
# ëŒ€ê·œëª¨ ëª¨ë¸ìš© (70B+) - ì„±ëŠ¥ ìš°ì„ 
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-large-models
spec:
  template:
    spec:
      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge   # 8x A100 - $32.77/hr
            - p5.48xlarge    # 8x H100 - $98.32/hr
  weight: 10   # í•„ìš”ì‹œì—ë§Œ ì„ íƒ
```

#### ë¹„ìš© ìµœì í™” ì „ëµ ë¹„êµ

| ì „ëµ | êµ¬í˜„ ë°©ë²• | ì˜ˆìƒ ì ˆê°ë¥  | ì ìš© ì›Œí¬ë¡œë“œ | ìœ„í—˜ë„ |
| --- | --- | --- | --- | --- |
| Spot ì¸ìŠ¤í„´ìŠ¤ | Karpenter NodePool | 60-90% | ì¶”ë¡ , ë°°ì¹˜ ì²˜ë¦¬ | ì¤‘ê°„ (ì¤‘ë‹¨ ê°€ëŠ¥) |
| Consolidation | Karpenter disruption | 20-30% | ëª¨ë“  ì›Œí¬ë¡œë“œ | ë‚®ìŒ |
| Right-sizing | Karpenter ì¸ìŠ¤í„´ìŠ¤ ìë™ ì„ íƒ | 15-25% | ëª¨ë“  ì›Œí¬ë¡œë“œ | ë‚®ìŒ |
| ìŠ¤ì¼€ì¤„ ê¸°ë°˜ | Karpenter budgets | 30-40% | ë¹„ì—…ë¬´ ì‹œê°„ | ë‚®ìŒ |
| ë³µí•© ì ìš© | ìœ„ ì „ëµ ì¡°í•© | 50-70% | ì „ì²´ | ì¤‘ê°„ |

#### ë³´ì¡° ì†”ë£¨ì…˜: LangFuse ê¸°ë°˜ í† í° ì¶”ì 

ì¸í”„ë¼ ë¹„ìš©ê³¼ í•¨ê»˜ í† í° ë ˆë²¨ ë¹„ìš©ë„ ì¶”ì í•´ì•¼ ì™„ì „í•œ ë¹„ìš© ê°€ì‹œì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langfuse
  namespace: observability
spec:
  replicas: 2
  selector:
    matchLabels:
      app: langfuse
  template:
    metadata:
      labels:
        app: langfuse
    spec:
      containers:
        - name: langfuse
          image: langfuse/langfuse:latest
          ports:
            - containerPort: 3000
          env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: database-url
            - name: NEXTAUTH_SECRET
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: nextauth-secret
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
```

#### ë¹„ìš© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„±

```yaml
# Prometheus ë¹„ìš© ê´€ë ¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê·œì¹™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: gpu-cost-rules
  namespace: monitoring
spec:
  groups:
    - name: gpu-cost
      rules:
        - record: gpu:hourly_cost:sum
          expr: |
            sum(
              karpenter_nodes_total_pod_requests{resource_type="nvidia.com/gpu"} 
              * on(instance_type) group_left() 
              aws_ec2_instance_hourly_cost
            )
        - alert: HighGPUCostAlert
          expr: gpu:hourly_cost:sum > 100
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "ì‹œê°„ë‹¹ GPU ë¹„ìš©ì´ $100ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤"
```

:::tip ë¹„ìš© ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸
1. **Spot ì¸ìŠ¤í„´ìŠ¤ ë¹„ìœ¨**: ì¶”ë¡  ì›Œí¬ë¡œë“œì˜ 70% ì´ìƒì„ Spotìœ¼ë¡œ ìš´ì˜
2. **Consolidation í™œì„±í™”**: 30ì´ˆ ì´ë‚´ ìœ íœ´ ë…¸ë“œ ì •ë¦¬
3. **ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ì •ì±…**: ë¹„ì—…ë¬´ ì‹œê°„ ë¦¬ì†ŒìŠ¤ 50% ì´ìƒ ì¶•ì†Œ
4. **Right-sizing**: ëª¨ë¸ í¬ê¸°ì— ë§ëŠ” ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìë™ ì„ íƒ
:::

:::warning ë¹„ìš© ìµœì í™” ì£¼ì˜ì‚¬í•­
- Spot ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì‹œ ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”ë¥¼ ìœ„í•œ graceful shutdown êµ¬í˜„ í•„ìˆ˜
- ê³¼ë„í•œ Consolidationì€ ìŠ¤ì¼€ì¼ ì•„ì›ƒ ì§€ì—°ì„ ìœ ë°œí•  ìˆ˜ ìˆìŒ
- ë¹„ìš© ì ˆê°ê³¼ SLA ì¤€ìˆ˜ ì‚¬ì´ì˜ ê· í˜•ì  ì„¤ì • í•„ìš”
:::

### ë„ì „ê³¼ì œ 4: FM íŒŒì¸íŠœë‹ê³¼ ìë™í™” íŒŒì´í”„ë¼ì¸

Foundation Modelì„ íŠ¹ì • ë„ë©”ì¸ì— ë§ê²Œ íŒŒì¸íŠœë‹í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ê²ƒì€ ë³µì¡í•œ ê³¼ì •ì…ë‹ˆë‹¤. íŠ¹íˆ **ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµ í™˜ê²½ì—ì„œì˜ GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**ê°€ í•µì‹¬ ê³¼ì œì…ë‹ˆë‹¤.

#### ê¸°ìˆ ì  ë¬¸ì œì  ìƒì„¸ ë¶„ì„

**1. ë¶„ì‚° í•™ìŠµ í™˜ê²½ì˜ ë³µì¡ì„±**

ëŒ€ê·œëª¨ LLM íŒŒì¸íŠœë‹ì€ ë‹¨ì¼ GPUë¡œëŠ” ë¶ˆê°€ëŠ¥í•˜ë©°, ë©€í‹° ë…¸ë“œ ë¶„ì‚° í•™ìŠµì´ í•„ìˆ˜ì…ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "ë¶„ì‚° í•™ìŠµ í† í´ë¡œì§€"
        direction LR
        subgraph "Node 1"
            N1G1["GPU 0-3"]
            N1G2["GPU 4-7"]
        end
        subgraph "Node 2"
            N2G1["GPU 0-3"]
            N2G2["GPU 4-7"]
        end
        subgraph "Node 3"
            N3G1["GPU 0-3"]
            N3G2["GPU 4-7"]
        end
        subgraph "Node 4"
            N4G1["GPU 0-3"]
            N4G2["GPU 4-7"]
        end
    end

    subgraph "í†µì‹  íŒ¨í„´"
        NCCL["NCCL All-Reduce"]
        EFA["EFA ë„¤íŠ¸ì›Œí¬"]
    end

    N1G1 <--> NCCL
    N2G1 <--> NCCL
    N3G1 <--> NCCL
    N4G1 <--> NCCL
    NCCL <--> EFA

    style EFA fill:#ff9900
```

| ë³‘ë ¬í™” ì „ëµ | ì„¤ëª… | ì ìš© ì‹œë‚˜ë¦¬ì˜¤ | ë³µì¡ë„ |
| --- | --- | --- | --- |
| Data Parallelism | ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ ê° GPUì—ì„œ ë™ì¼ ëª¨ë¸ í•™ìŠµ | ì‘ì€ ëª¨ë¸, ëŒ€ìš©ëŸ‰ ë°ì´í„° | ë‚®ìŒ |
| Tensor Parallelism | ëª¨ë¸ì˜ í…ì„œë¥¼ GPU ê°„ ë¶„í•  | ë‹¨ì¼ ë ˆì´ì–´ê°€ GPU ë©”ëª¨ë¦¬ ì´ˆê³¼ ì‹œ | ë†’ìŒ |
| Pipeline Parallelism | ëª¨ë¸ ë ˆì´ì–´ë¥¼ GPU ê°„ ë¶„í•  | ë§¤ìš° ê¹Šì€ ëª¨ë¸ | ì¤‘ê°„ |
| FSDP | ëª¨ë¸ íŒŒë¼ë¯¸í„°, ê·¸ë˜ë””ì–¸íŠ¸, ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¶„í•  | ëŒ€ê·œëª¨ ëª¨ë¸ íš¨ìœ¨ì  í•™ìŠµ | ì¤‘ê°„ |

**2. GPU ë¦¬ì†ŒìŠ¤ í”„ë¡œë¹„ì €ë‹ ì§€ì—°**

í•™ìŠµ ì‘ì—…ì€ ì¼ë°˜ì ìœ¼ë¡œ **ë°°ì¹˜ í˜•íƒœ**ë¡œ ì‹¤í–‰ë˜ë©°, ë¦¬ì†ŒìŠ¤ í™•ë³´ ì‹œê°„ì´ ì „ì²´ íŒŒì´í”„ë¼ì¸ íš¨ìœ¨ì„±ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤:

```mermaid
sequenceDiagram
    participant User as ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸
    participant Pipeline as ML Pipeline
    participant Scheduler as K8s Scheduler
    participant Karpenter as Karpenter
    participant AWS as AWS EC2

    User->>Pipeline: í•™ìŠµ Job ì œì¶œ
    Pipeline->>Scheduler: Pod ìƒì„± ìš”ì²­ (32 GPU)
    
    Note over Scheduler: ê¸°ì¡´ ë°©ì‹: Node Group ëŒ€ê¸°
    Scheduler->>Karpenter: Pending Pod ê°ì§€
    
    Note over Karpenter: ì›Œí¬ë¡œë“œ ë¶„ì„
    Karpenter->>Karpenter: ìµœì  ì¸ìŠ¤í„´ìŠ¤ ê³„ì‚°<br/>(4x p4d.24xlarge)
    
    Karpenter->>AWS: ë³‘ë ¬ ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œë¹„ì €ë‹
    AWS-->>Karpenter: ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ (2-3ë¶„)
    
    Karpenter-->>Scheduler: ë…¸ë“œ ë“±ë¡
    Scheduler-->>Pipeline: Pod ìŠ¤ì¼€ì¤„ë§ ì™„ë£Œ
    Pipeline-->>User: í•™ìŠµ ì‹œì‘
```

**3. í•™ìŠµ ì¤‘ ì¥ì•  ë³µêµ¬ì˜ ì–´ë ¤ì›€**

- ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë³µêµ¬ ì „ëµ í•„ìš”
- ë…¸ë“œ ì¥ì•  ì‹œ ì „ì²´ í•™ìŠµ ì¬ì‹œì‘ ë°©ì§€
- Spot ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© ì‹œ ì¤‘ë‹¨ ì²˜ë¦¬

**4. ë¦¬ì†ŒìŠ¤ í™œìš© íš¨ìœ¨ì„±**

- í•™ìŠµ ì™„ë£Œ í›„ GPU ë…¸ë“œ ìœ íœ´ ìƒíƒœ ì§€ì†
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„
- ì‹¤í—˜ê³¼ í”„ë¡œë•ì…˜ í•™ìŠµ ê°„ ë¦¬ì†ŒìŠ¤ ê²½í•©

```mermaid
graph LR
    subgraph "Data Pipeline"
        DATA["Training Data"]
        PREP["Data Preprocessing"]
    end

    subgraph "Karpenter ê´€ë¦¬ í•™ìŠµ í´ëŸ¬ìŠ¤í„°"
        KARP["Karpenter<br/>Training NodePool"]
        NEMO["NeMo Framework"]
        DIST["Distributed Training"]
    end

    subgraph "Model Registry"
        CKPT["Checkpoint Storage"]
        MLFLOW["MLflow Registry"]
    end

    subgraph "Deployment"
        SERVE["Model Serving"]
        CANARY["Canary Deployment"]
    end

    DATA --> PREP
    PREP --> NEMO
    KARP --> NEMO
    NEMO --> DIST
    DIST --> CKPT
    CKPT --> MLFLOW
    MLFLOW --> SERVE
    SERVE --> CANARY

    style KARP fill:#ffd93d
    style NEMO fill:#76b900
```

#### Karpenter ê¸°ë°˜ í•™ìŠµ ì¸í”„ë¼ êµ¬ì„± (ê¶Œì¥)

**ì „ëµ 1: í•™ìŠµ ì „ìš© NodePool ë¶„ë¦¬**

í•™ìŠµ ì›Œí¬ë¡œë“œëŠ” ì¶”ë¡ ê³¼ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§€ë¯€ë¡œ ë³„ë„ì˜ NodePoolë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-training-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-training
        workload: ml-training
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]  # í•™ìŠµì€ On-Demand ê¶Œì¥ (ì•ˆì •ì„±)
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p5.48xlarge     # 8x H100 80GB - ëŒ€ê·œëª¨ í•™ìŠµ
            - p4d.24xlarge    # 8x A100 40GB - ì¤‘ê·œëª¨ í•™ìŠµ
            - p4de.24xlarge   # 8x A100 80GB - ë©”ëª¨ë¦¬ ì§‘ì•½ì  í•™ìŠµ
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-training-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: workload-type
          value: "training"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 64
  disruption:
    # í•™ìŠµ ì¤‘ì—ëŠ” ë…¸ë“œ ì¤‘ë‹¨ ë°©ì§€
    consolidationPolicy: WhenEmpty
    consolidateAfter: 1h  # í•™ìŠµ ì™„ë£Œ í›„ 1ì‹œê°„ ëŒ€ê¸°
    budgets:
      # í•™ìŠµ ì¤‘ì—ëŠ” ë…¸ë“œ ì¤‘ë‹¨ ì™„ì „ ë°©ì§€
      - nodes: "0"
```

**ì „ëµ 2: EFA ë„¤íŠ¸ì›Œí¬ ìµœì í™” NodeClass**

ë¶„ì‚° í•™ìŠµì˜ ì„±ëŠ¥ì€ GPU ê°„ í†µì‹  ì†ë„ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. EFA(Elastic Fabric Adapter)ë¥¼ í™œìš©í•˜ì—¬ ìµœëŒ€ ì„±ëŠ¥ì„ í™•ë³´í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu-training-nodeclass
spec:
  role: KarpenterNodeRole-${CLUSTER_NAME}
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
        network-type: efa-enabled  # EFA ì§€ì› ì„œë¸Œë„·
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 1000Gi  # ëŒ€ìš©ëŸ‰ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        volumeType: gp3
        iops: 16000
        throughput: 1000
        encrypted: true
        deleteOnTermination: true
  instanceStorePolicy: RAID0  # NVMe ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ í™œìš©
  userData: |
    #!/bin/bash
    set -e
    
    # NVIDIA ë“œë¼ì´ë²„ ì„¤ì •
    nvidia-smi -pm 1
    nvidia-smi -ac 1593,1410  # H100 ìµœì  í´ëŸ­ ì„¤ì •
    
    # EFA ë“œë¼ì´ë²„ ë¡œë“œ
    modprobe efa
    
    # NCCL í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    echo 'export NCCL_DEBUG=INFO' >> /etc/profile.d/nccl.sh
    echo 'export NCCL_SOCKET_IFNAME=eth0' >> /etc/profile.d/nccl.sh
    echo 'export FI_EFA_USE_DEVICE_RDMA=1' >> /etc/profile.d/nccl.sh
    echo 'export FI_PROVIDER=efa' >> /etc/profile.d/nccl.sh
    
    # ëŒ€ìš©ëŸ‰ í˜ì´ì§€ ì„¤ì • (í•™ìŠµ ì„±ëŠ¥ í–¥ìƒ)
    echo 'vm.nr_hugepages=5120' >> /etc/sysctl.conf
    sysctl -p
  tags:
    Environment: production
    Workload: ml-training
    CostCenter: ml-platform
```

**ì „ëµ 3: ì‹¤í—˜ìš© Spot ê¸°ë°˜ NodePool**

í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ë‚˜ ì‹¤í—˜ì  í•™ìŠµì—ëŠ” Spot ì¸ìŠ¤í„´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ë¹„ìš©ì„ ì ˆê°í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-experiment-pool
spec:
  template:
    metadata:
      labels:
        node-type: gpu-experiment
        workload: ml-experiment
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - p4d.24xlarge
            - g5.48xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-experiment-nodeclass
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: workload-type
          value: "experiment"
          effect: NoSchedule
  limits:
    nvidia.com/gpu: 32
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 10m  # ì‹¤í—˜ ì™„ë£Œ í›„ ë¹ ë¥¸ ì •ë¦¬
  weight: 30  # í”„ë¡œë•ì…˜ í•™ìŠµë³´ë‹¤ ë‚®ì€ ìš°ì„ ìˆœìœ„
```

#### NeMo ë¶„ì‚° í•™ìŠµ Job ì˜ˆì œ

Karpenterê°€ í”„ë¡œë¹„ì €ë‹í•œ ë…¸ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” NeMo ë¶„ì‚° í•™ìŠµ Jobì…ë‹ˆë‹¤.

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: nemo-finetune-llama-70b
  namespace: ai-training
spec:
  parallelism: 4  # 4ê°œ ë…¸ë“œ ë³‘ë ¬ ì‹¤í–‰
  completions: 4
  completionMode: Indexed
  template:
    metadata:
      labels:
        app: nemo-training
        model: llama-70b
    spec:
      restartPolicy: OnFailure
      containers:
        - name: nemo
          image: nvcr.io/nvidia/nemo:24.01
          command:
            - /bin/bash
            - -c
            - |
              # ë¶„ì‚° í•™ìŠµ í™˜ê²½ ì„¤ì •
              export MASTER_ADDR=$(hostname -i)
              export MASTER_PORT=29500
              export WORLD_SIZE=32  # 4 nodes x 8 GPUs
              export RANK=$JOB_COMPLETION_INDEX
              
              python -m torch.distributed.launch \
                --nproc_per_node=8 \
                --nnodes=4 \
                --node_rank=$RANK \
                --master_addr=$MASTER_ADDR \
                --master_port=$MASTER_PORT \
                /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_finetuning.py \
                --config-path=/config \
                --config-name=llama_70b_finetune
          args:
            - model.data.train_ds.file_path=/data/train.jsonl
            - model.data.validation_ds.file_path=/data/val.jsonl
            - trainer.devices=8
            - trainer.num_nodes=4
            - trainer.max_epochs=3
            - trainer.precision=bf16-mixed
            - model.tensor_model_parallel_size=4
            - model.pipeline_model_parallel_size=2
            - exp_manager.checkpoint_callback_params.save_top_k=3
          resources:
            requests:
              nvidia.com/gpu: 8
              memory: "900Gi"
              cpu: "90"
            limits:
              nvidia.com/gpu: 8
              memory: "1100Gi"
              cpu: "96"
          volumeMounts:
            - name: training-data
              mountPath: /data
            - name: checkpoints
              mountPath: /checkpoints
            - name: config
              mountPath: /config
            - name: shm
              mountPath: /dev/shm
      nodeSelector:
        node-type: gpu-training
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - key: workload-type
          operator: Equal
          value: "training"
          effect: NoSchedule
      volumes:
        - name: training-data
          persistentVolumeClaim:
            claimName: training-data-pvc
        - name: checkpoints
          persistentVolumeClaim:
            claimName: checkpoints-pvc
        - name: config
          configMap:
            name: nemo-training-config
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 256Gi  # ëŒ€ìš©ëŸ‰ ê³µìœ  ë©”ëª¨ë¦¬
```

#### í•™ìŠµ íŒŒì´í”„ë¼ì¸ ìë™í™”

Kubeflow Pipelinesì™€ Karpenterë¥¼ ì—°ë™í•˜ì—¬ End-to-End í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ìë™í™”í•©ë‹ˆë‹¤.

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: llm-finetune-pipeline
  namespace: ai-training
spec:
  entrypoint: finetune-pipeline
  templates:
    - name: finetune-pipeline
      dag:
        tasks:
          - name: data-preparation
            template: prepare-data
          - name: training
            template: distributed-training
            dependencies: [data-preparation]
          - name: evaluation
            template: evaluate-model
            dependencies: [training]
          - name: deployment
            template: deploy-model
            dependencies: [evaluation]

    - name: distributed-training
      resource:
        action: create
        manifest: |
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: nemo-finetune-{{workflow.uid}}
          spec:
            # ... (ìœ„ì˜ Job ìŠ¤í™)
      # Karpenterê°€ ìë™ìœ¼ë¡œ í•„ìš”í•œ GPU ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
```

#### í•™ìŠµ ì¸í”„ë¼ ë¹„ìš© ìµœì í™” ì „ëµ

| ì „ëµ | ì ìš© ëŒ€ìƒ | ì˜ˆìƒ ì ˆê°ë¥  | êµ¬í˜„ ë°©ë²• |
| --- | --- | --- | --- |
| Spot ì‹¤í—˜ í´ëŸ¬ìŠ¤í„° | í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | 60-80% | ë³„ë„ NodePool |
| ìë™ ë…¸ë“œ ì •ë¦¬ | í•™ìŠµ ì™„ë£Œ í›„ | 20-30% | Consolidation |
| ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì¬ì‹œì‘ | Spot ì¤‘ë‹¨ ëŒ€ì‘ | 10-20% | NeMo ì²´í¬í¬ì¸íŠ¸ |
| ì‹œê°„ëŒ€ë³„ ìŠ¤ì¼€ì¤„ë§ | ë¹„ì—…ë¬´ ì‹œê°„ í•™ìŠµ | 15-25% | CronJob + Karpenter |

:::tip í•™ìŠµ ì¸í”„ë¼ ëª¨ë²” ì‚¬ë¡€
1. **í”„ë¡œë•ì…˜ í•™ìŠµ**: On-Demand ì¸ìŠ¤í„´ìŠ¤ë¡œ ì•ˆì •ì„± í™•ë³´
2. **ì‹¤í—˜/íŠœë‹**: Spot ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¹„ìš© ì ˆê°
3. **ì²´í¬í¬ì¸íŠ¸**: FSx for Lustreì— ì£¼ê¸°ì  ì €ì¥
4. **ëª¨ë‹ˆí„°ë§**: TensorBoard + Prometheusë¡œ í•™ìŠµ ì§„í–‰ ì¶”ì 
:::

:::warning ë¶„ì‚° í•™ìŠµ ì£¼ì˜ì‚¬í•­
- EFA ë„¤íŠ¸ì›Œí¬ê°€ ì§€ì›ë˜ëŠ” ì„œë¸Œë„·ì—ì„œë§Œ ìµœì  ì„±ëŠ¥ ë°œíœ˜
- NCCL í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì´ ì„±ëŠ¥ì— í° ì˜í–¥
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°ì™€ ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ê°„ ê· í˜• í•„ìš”
:::

:::info ì¤‘ê°„ ìš”ì•½: 4ê°€ì§€ ë„ì „ê³¼ì œì™€ Karpenter ê¸°ë°˜ í•´ê²° ë°©ì•ˆ
ì§€ê¸ˆê¹Œì§€ Agentic AI í”Œë«í¼ì˜ 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œ(GPU ëª¨ë‹ˆí„°ë§, ë™ì  ìŠ¤ì¼€ì¼ë§, ë¹„ìš© ì»¨íŠ¸ë¡¤, FM íŒŒì¸íŠœë‹)ì™€ **Karpenter + EKS Auto Mode** ê¸°ë°˜ í•´ê²° ë°©ì•ˆì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì„¹ì…˜ì—ì„œëŠ” ì´ ì†”ë£¨ì…˜ë“¤ì´ **ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ì—ì„œ ì–´ë–»ê²Œ í†µí•©ë˜ëŠ”ì§€**ì™€ **EKSì—ì„œì˜ ì‹¤ì œ êµ¬ì¶• ë°©ë²•**ì„ ë‹¤ë£¹ë‹ˆë‹¤.
:::

---

## ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ì™€ Kubernetes í†µí•© ì•„í‚¤í…ì²˜

Agentic AI í”Œë«í¼ì€ ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë“¤ì´ Kubernetesë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìœ ê¸°ì ìœ¼ë¡œ í†µí•©ë˜ì–´ êµ¬ì„±ë©ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” **LLM Observability, ëª¨ë¸ ì„œë¹™, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤, GPU ì¸í”„ë¼** ì˜ì—­ì˜ í•µì‹¬ ì˜¤í”ˆì†ŒìŠ¤ë“¤ì´ ì–´ë–»ê²Œ í˜‘ë ¥í•˜ì—¬ ì™„ì „í•œ Agentic AI í”Œë«í¼ì„ í˜•ì„±í•˜ëŠ”ì§€ ì„¤ëª…í•©ë‹ˆë‹¤.

### ì˜¤í”ˆì†ŒìŠ¤ í†µí•© ì „ì²´ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "Application Layer"
        AGENT["Agentic AI Application"]
        RAG["RAG Pipeline"]
    end

    subgraph "LLM Observability Layer"
        LF["LangFuse<br/>(Self-hosted)"]
        LS["LangSmith<br/>(Managed)"]
        RAGAS["RAGAS<br/>(RAG í’ˆì§ˆ í‰ê°€)"]
    end

    subgraph "Inference Gateway Layer"
        LITE["LiteLLM<br/>(Provider Abstraction)"]
        KGW["Kgateway<br/>(Traffic Management)"]
    end

    subgraph "Model Serving Layer"
        LLMD["llm-d<br/>(Distributed Scheduler)"]
        VLLM["vLLM<br/>(Inference Engine)"]
    end

    subgraph "Vector Database Layer"
        MILVUS["Milvus<br/>(Vector Store)"]
    end

    subgraph "GPU Infrastructure Layer"
        DRA["DRA<br/>(Dynamic Resource Allocation)"]
        DCGM["DCGM<br/>(GPU Monitoring)"]
        NCCL["NCCL<br/>(GPU Communication)"]
        KARP["Karpenter<br/>(Node Provisioning)"]
    end

    AGENT --> LF & LS
    AGENT --> LITE
    RAG --> MILVUS
    RAG --> RAGAS
    LITE --> KGW
    KGW --> LLMD
    LLMD --> VLLM
    VLLM --> DRA
    DRA --> DCGM
    VLLM --> NCCL
    KARP --> DRA

    style LF fill:#45b7d1
    style LS fill:#9b59b6
    style RAGAS fill:#e67e22
    style LITE fill:#9b59b6
    style LLMD fill:#e74c3c
    style MILVUS fill:#00d4aa
    style DRA fill:#326ce5
    style DCGM fill:#76b900
    style NCCL fill:#76b900
    style KARP fill:#ffd93d
```

### ê³„ì¸µë³„ ì˜¤í”ˆì†ŒìŠ¤ ì—­í• ê³¼ í†µí•©

#### 1. LLM Observability ê³„ì¸µ: LangFuse, LangSmith, RAGAS

LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ **ì „ì²´ ë¼ì´í”„ì‚¬ì´í´ì„ ì¶”ì í•˜ê³  í’ˆì§ˆì„ í‰ê°€**í•˜ëŠ” í•µì‹¬ ë„êµ¬ë“¤ì…ë‹ˆë‹¤.

| ì†”ë£¨ì…˜ | ì—­í•  | Kubernetes í†µí•© ë°©ì‹ | í•µì‹¬ ê¸°ëŠ¥ |
| --- | --- | --- | --- |
| **LangFuse** | LLM íŠ¸ë ˆì´ì‹± (Self-hosted) | Helm Chart, StatefulSet | í† í° ì¶”ì , ë¹„ìš© ë¶„ì„, í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬ |
| **LangSmith** | LLM íŠ¸ë ˆì´ì‹± (Managed) | SDK ì—°ë™ | íŠ¸ë ˆì´ì‹±, í‰ê°€, ë°ì´í„°ì…‹ ê´€ë¦¬, í˜‘ì—… |
| **RAGAS** | RAG í’ˆì§ˆ í‰ê°€ | Job/CronJob | Faithfulness, Relevancy, Context Precision í‰ê°€ |

```mermaid
graph LR
    subgraph "LLM Application"
        APP["Agent App"]
        SDK1["LangFuse SDK"]
        SDK2["LangSmith SDK"]
    end

    subgraph "Kubernetes Cluster"
        subgraph "LangFuse Stack"
            LF_WEB["LangFuse Web<br/>(Deployment)"]
            LF_WORKER["LangFuse Worker<br/>(Deployment)"]
            LF_DB["PostgreSQL<br/>(StatefulSet)"]
            LF_REDIS["Redis<br/>(StatefulSet)"]
        end
        
        subgraph "RAGAS Evaluation"
            RAGAS_JOB["RAGAS Job<br/>(CronJob)"]
        end
    end

    APP --> SDK1 --> LF_WEB
    APP --> SDK2
    LF_WEB --> LF_WORKER --> LF_DB
    LF_WORKER --> LF_REDIS
    RAGAS_JOB --> LF_DB

    style LF_WEB fill:#45b7d1
    style RAGAS_JOB fill:#e67e22
```

**LangFuse Kubernetes ë°°í¬ ì˜ˆì‹œ:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langfuse-web
  namespace: observability
spec:
  replicas: 2
  selector:
    matchLabels:
      app: langfuse-web
  template:
    spec:
      containers:
        - name: langfuse
          image: langfuse/langfuse:latest
          env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: database-url
            - name: NEXTAUTH_SECRET
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: nextauth-secret
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: ragas-evaluation
  namespace: observability
spec:
  schedule: "0 */6 * * *"  # 6ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: ragas
              image: ragas/ragas:latest
              command: ["python", "-m", "ragas.evaluate"]
              env:
                - name: LANGFUSE_HOST
                  value: "http://langfuse-web:3000"
          restartPolicy: OnFailure
```

#### 2. Inference Gateway ê³„ì¸µ: LiteLLM

**LiteLLM**ì€ 100ê°œ ì´ìƒì˜ LLM í”„ë¡œë°”ì´ë”ë¥¼ **í†µí•© OpenAI í˜¸í™˜ APIë¡œ ì¶”ìƒí™”**í•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "LiteLLM Gateway"
        PROXY["LiteLLM Proxy<br/>(Deployment)"]
        CONFIG["Config<br/>(ConfigMap)"]
        CACHE["Redis Cache<br/>(StatefulSet)"]
    end

    subgraph "LLM Backends"
        SELF["Self-hosted<br/>vLLM / TGI"]
        BEDROCK["Amazon Bedrock"]
        OPENAI["OpenAI API"]
        ANTHROPIC["Anthropic API"]
    end

    subgraph "Features"
        LB["Load Balancing"]
        FALLBACK["Fallback Logic"]
        COST["Cost Tracking"]
        RATE["Rate Limiting"]
    end

    PROXY --> SELF & BEDROCK & OPENAI & ANTHROPIC
    CONFIG --> PROXY
    CACHE --> PROXY
    PROXY --> LB & FALLBACK & COST & RATE

    style PROXY fill:#9b59b6
```

**LiteLLM Kubernetes ë°°í¬ ì˜ˆì‹œ:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: litellm-proxy
  namespace: ai-gateway
spec:
  replicas: 3
  selector:
    matchLabels:
      app: litellm
  template:
    spec:
      containers:
        - name: litellm
          image: ghcr.io/berriai/litellm:main-latest
          ports:
            - containerPort: 4000
          env:
            - name: LITELLM_MASTER_KEY
              valueFrom:
                secretKeyRef:
                  name: litellm-secrets
                  key: master-key
            - name: REDIS_HOST
              value: "redis-cache"
          volumeMounts:
            - name: config
              mountPath: /app/config.yaml
              subPath: config.yaml
      volumes:
        - name: config
          configMap:
            name: litellm-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ai-gateway
data:
  config.yaml: |
    model_list:
      - model_name: gpt-4
        litellm_params:
          model: openai/gpt-4
          api_key: os.environ/OPENAI_API_KEY
      - model_name: claude-3
        litellm_params:
          model: anthropic/claude-3-opus
          api_key: os.environ/ANTHROPIC_API_KEY
      - model_name: llama-70b
        litellm_params:
          model: openai/llama-70b
          api_base: http://vllm-llama:8000/v1
    
    router_settings:
      routing_strategy: least-busy
      enable_fallbacks: true
      
    general_settings:
      master_key: os.environ/LITELLM_MASTER_KEY
```

#### 3. ë¶„ì‚° ì¶”ë¡  ê³„ì¸µ: llm-d

**llm-d**ëŠ” Kubernetes í™˜ê²½ì—ì„œ LLM ì¶”ë¡  ìš”ì²­ì„ **ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì‚°**í•˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ì…ë‹ˆë‹¤.

| ê¸°ëŠ¥ | ì„¤ëª… | Kubernetes í†µí•© |
| --- | --- | --- |
| **Prefix Caching ì¸ì‹** | ë™ì¼ í”„ë¡¬í”„íŠ¸ í”„ë¦¬í”½ìŠ¤ë¥¼ ê°€ì§„ ìš”ì²­ì„ ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¼ìš°íŒ… | Service Discovery í™œìš© |
| **ë¡œë“œ ë°¸ëŸ°ì‹±** | GPU ì‚¬ìš©ë¥  ê¸°ë°˜ ì§€ëŠ¥í˜• ë¶„ë°° | Prometheus ë©”íŠ¸ë¦­ ì—°ë™ |
| **ì¥ì•  ë³µêµ¬** | ì¸ìŠ¤í„´ìŠ¤ ì¥ì•  ì‹œ ìë™ ì¬ë¼ìš°íŒ… | Health Check + Endpoint Slice |
| **ë™ì  ìŠ¤ì¼€ì¼ë§** | ìš”ì²­ëŸ‰ì— ë”°ë¥¸ ë°±ì—”ë“œ í™•ì¥ | KEDA ì—°ë™ |

```mermaid
graph LR
    subgraph "llm-d Architecture"
        ROUTER["llm-d Router<br/>(Deployment)"]
        SCHED["Scheduler Logic"]
        CACHE["Prefix Cache Index"]
    end

    subgraph "vLLM Backends"
        V1["vLLM-1<br/>GPU: A100"]
        V2["vLLM-2<br/>GPU: A100"]
        V3["vLLM-3<br/>GPU: H100"]
    end

    subgraph "Kubernetes Resources"
        SVC["Service"]
        EP["EndpointSlice"]
        HPA["HPA/KEDA"]
    end

    ROUTER --> SCHED --> CACHE
    SCHED --> V1 & V2 & V3
    SVC --> ROUTER
    EP --> V1 & V2 & V3
    HPA --> V1 & V2 & V3

    style ROUTER fill:#e74c3c
    style SCHED fill:#e74c3c
```

**llm-d Kubernetes ë°°í¬ ì˜ˆì‹œ:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-d-router
  namespace: ai-inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-d
  template:
    spec:
      containers:
        - name: llm-d
          image: ghcr.io/llm-d/llm-d:latest
          ports:
            - containerPort: 8080
          env:
            - name: BACKENDS
              value: "vllm-0.vllm:8000,vllm-1.vllm:8000,vllm-2.vllm:8000"
            - name: ROUTING_STRATEGY
              value: "prefix-aware"
            - name: PROMETHEUS_ENDPOINT
              value: "http://prometheus:9090"
          resources:
            requests:
              memory: "256Mi"
              cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: llm-d
  namespace: ai-inference
spec:
  selector:
    app: llm-d
  ports:
    - port: 8080
      targetPort: 8080
```

#### 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê³„ì¸µ: Milvus

**Milvus**ëŠ” ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ **í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤**ì…ë‹ˆë‹¤. RAG íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¡œ, Kubernetesì—ì„œ **ë¶„ì‚° ì•„í‚¤í…ì²˜ë¡œ ìš´ì˜**ë©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "Milvus Distributed Architecture"
        subgraph "Access Layer"
            PROXY["Milvus Proxy<br/>(Deployment)"]
        end
        
        subgraph "Coordinator Layer"
            ROOT["Root Coord"]
            QUERY["Query Coord"]
            DATA["Data Coord"]
            INDEX["Index Coord"]
        end
        
        subgraph "Worker Layer"
            QN["Query Nodes<br/>(StatefulSet)"]
            DN["Data Nodes<br/>(StatefulSet)"]
            IN["Index Nodes<br/>(StatefulSet)"]
        end
        
        subgraph "Storage Layer"
            ETCD["etcd<br/>(Metadata)"]
            MINIO["MinIO/S3<br/>(Object Storage)"]
            PULSAR["Pulsar<br/>(Message Queue)"]
        end
    end

    PROXY --> ROOT & QUERY & DATA & INDEX
    QUERY --> QN
    DATA --> DN
    INDEX --> IN
    QN & DN & IN --> ETCD & MINIO & PULSAR

    style PROXY fill:#00d4aa
    style QN fill:#00d4aa
    style DN fill:#00d4aa
    style IN fill:#00d4aa
```

| ì»´í¬ë„ŒíŠ¸ | Kubernetes ë¦¬ì†ŒìŠ¤ | ì—­í•  |
| --- | --- | --- |
| **Proxy** | Deployment | í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ ì²˜ë¦¬, ë¼ìš°íŒ… |
| **Coordinators** | Deployment | ë©”íƒ€ë°ì´í„° ê´€ë¦¬, ì‘ì—… ì¡°ì • |
| **Query Nodes** | StatefulSet | ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰ |
| **Data Nodes** | StatefulSet | ë°ì´í„° ì‚½ì…/ì‚­ì œ ì²˜ë¦¬ |
| **Index Nodes** | StatefulSet | ì¸ë±ìŠ¤ ë¹Œë“œ |

**Milvus Helm ë°°í¬:**

```bash
# Milvus Operator ì„¤ì¹˜
helm repo add milvus https://milvus-io.github.io/milvus-helm/
helm install milvus-operator milvus/milvus-operator -n milvus-operator --create-namespace

# Milvus í´ëŸ¬ìŠ¤í„° ë°°í¬
kubectl apply -f - <<EOF
apiVersion: milvus.io/v1beta1
kind: Milvus
metadata:
  name: milvus-cluster
  namespace: ai-vectordb
spec:
  mode: cluster
  dependencies:
    etcd:
      inCluster:
        values:
          replicaCount: 3
    storage:
      inCluster:
        values:
          mode: distributed
    pulsar:
      inCluster:
        values:
          components:
            autorecovery: false
  components:
    proxy:
      replicas: 2
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"
    queryNode:
      replicas: 3
      resources:
        requests:
          cpu: "2"
          memory: "8Gi"
    dataNode:
      replicas: 2
    indexNode:
      replicas: 2
      resources:
        requests:
          nvidia.com/gpu: 1  # GPU ê°€ì† ì¸ë±ì‹±
EOF
```

#### 5. GPU ì¸í”„ë¼ ê³„ì¸µ: DRA, DCGM, NCCL

GPU ë¦¬ì†ŒìŠ¤ì˜ **ë™ì  í• ë‹¹, ëª¨ë‹ˆí„°ë§, ê³ ì† í†µì‹ **ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ì¸í”„ë¼ ì»´í¬ë„ŒíŠ¸ë“¤ì…ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "GPU Infrastructure Stack"
        subgraph "Resource Allocation"
            DRA["DRA<br/>(Dynamic Resource Allocation)"]
            DRIVER["NVIDIA Device Plugin"]
        end
        
        subgraph "Monitoring"
            DCGM["DCGM Exporter"]
            PROM["Prometheus"]
            GRAF["Grafana"]
        end
        
        subgraph "Communication"
            NCCL["NCCL<br/>(GPU Collective Comm)"]
            EFA["EFA Driver"]
        end
        
        subgraph "Node Management"
            KARP["Karpenter"]
            GPU_OP["GPU Operator"]
        end
    end

    subgraph "GPU Nodes"
        N1["Node 1<br/>8x A100"]
        N2["Node 2<br/>8x A100"]
        N3["Node 3<br/>8x H100"]
    end

    DRA --> DRIVER --> N1 & N2 & N3
    DCGM --> N1 & N2 & N3
    DCGM --> PROM --> GRAF
    NCCL --> EFA --> N1 & N2 & N3
    KARP --> N1 & N2 & N3
    GPU_OP --> DRIVER & DCGM

    style DRA fill:#326ce5
    style DCGM fill:#76b900
    style NCCL fill:#76b900
    style KARP fill:#ffd93d
```

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | Kubernetes í†µí•© |
| --- | --- | --- |
| **DRA (Dynamic Resource Allocation)** | GPU ë¦¬ì†ŒìŠ¤ ë™ì  í• ë‹¹ | ResourceClaim, ResourceClass CRD |
| **DCGM (Data Center GPU Manager)** | GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘ | DaemonSet, ServiceMonitor |
| **NCCL (NVIDIA Collective Communication Library)** | ë©€í‹° GPU í†µì‹  ìµœì í™” | Pod í™˜ê²½ë³€ìˆ˜, EFA ì—°ë™ |

**DRA ê¸°ë°˜ GPU í• ë‹¹ ì˜ˆì‹œ:**

```yaml
# ResourceClass ì •ì˜
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClass
metadata:
  name: gpu.nvidia.com
driverName: gpu.nvidia.com
---
# ResourceClaimTemplate ì •ì˜
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaimTemplate
metadata:
  name: gpu-claim-template
  namespace: ai-inference
spec:
  spec:
    resourceClassName: gpu.nvidia.com
    parametersRef:
      apiGroup: gpu.nvidia.com
      kind: GpuClaimParameters
      name: a100-params
---
# GPU íŒŒë¼ë¯¸í„° ì •ì˜
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: a100-params
  namespace: ai-inference
spec:
  count: 1
  selector:
    gpu.nvidia.com/product: "NVIDIA-A100-SXM4-80GB"
---
# Podì—ì„œ DRA ì‚¬ìš©
apiVersion: v1
kind: Pod
metadata:
  name: vllm-inference
  namespace: ai-inference
spec:
  containers:
    - name: vllm
      image: vllm/vllm-openai:latest
      resources:
        claims:
          - name: gpu
  resourceClaims:
    - name: gpu
      source:
        resourceClaimTemplateName: gpu-claim-template
```

**DCGM Exporter ë°°í¬:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: dcgm-exporter
  namespace: gpu-monitoring
spec:
  selector:
    matchLabels:
      app: dcgm-exporter
  template:
    metadata:
      labels:
        app: dcgm-exporter
    spec:
      nodeSelector:
        nvidia.com/gpu.present: "true"
      containers:
        - name: dcgm-exporter
          image: nvcr.io/nvidia/k8s/dcgm-exporter:3.3.0-3.2.0-ubuntu22.04
          ports:
            - containerPort: 9400
              name: metrics
          env:
            - name: DCGM_EXPORTER_LISTEN
              value: ":9400"
            - name: DCGM_EXPORTER_KUBERNETES
              value: "true"
          securityContext:
            privileged: true
          volumeMounts:
            - name: pod-resources
              mountPath: /var/lib/kubelet/pod-resources
      volumes:
        - name: pod-resources
          hostPath:
            path: /var/lib/kubelet/pod-resources
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: dcgm-exporter
  namespace: gpu-monitoring
spec:
  selector:
    matchLabels:
      app: dcgm-exporter
  endpoints:
    - port: metrics
      interval: 15s
```

**NCCL ìµœì í™” ì„¤ì •:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nccl-config
  namespace: ai-training
data:
  nccl-env.sh: |
    # NCCL í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    export NCCL_DEBUG=INFO
    export NCCL_SOCKET_IFNAME=eth0
    export NCCL_IB_DISABLE=0
    
    # EFA ì‚¬ìš© ì‹œ ì„¤ì •
    export FI_PROVIDER=efa
    export FI_EFA_USE_DEVICE_RDMA=1
    export FI_EFA_FORK_SAFE=1
    
    # ì„±ëŠ¥ ìµœì í™”
    export NCCL_ALGO=Ring
    export NCCL_PROTO=Simple
    export NCCL_MIN_NCHANNELS=4
    export NCCL_MAX_NCHANNELS=8
```

### DRA ì‹¬ì¸µ ë¶„ì„: Dynamic Resource Allocation

#### DRAì˜ ë“±ì¥ ë°°ê²½ê³¼ í•„ìš”ì„±

Kubernetes ì´ˆê¸° ë‹¨ê³„ì—ì„œ GPU ë¦¬ì†ŒìŠ¤ í• ë‹¹ì€ **Device Plugin** ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ê·¼ë³¸ì ì¸ í•œê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

| í•œê³„ì  | ì„¤ëª… | ì˜í–¥ |
| --- | --- | --- |
| **ì •ì  í• ë‹¹** | ë…¸ë“œ ì‹œì‘ ì‹œ ë¦¬ì†ŒìŠ¤ ìˆ˜ëŸ‰ ê³ ì • | GPU ë¶€ë¶„ í• ë‹¹ ë¶ˆê°€ëŠ¥, ë‚®ì€ í™œìš©ë¥  |
| **ì„¸ë¶„í™” ë¶ˆê°€** | GPU ì „ì²´ë¥¼ Podì—ë§Œ í• ë‹¹ ê°€ëŠ¥ | GPU íŒŒí‹°ì…”ë‹ ë¯¸ì§€ì› (MIG ì‚¬ìš© ë¶ˆê°€) |
| **ìš°ì„ ìˆœìœ„ ë¯¸ì§€ì›** | ì„ ì°©ìˆœ í• ë‹¹ë§Œ ê°€ëŠ¥ | QoS í´ë˜ìŠ¤ ë¯¸ì ìš©, ê³µì •í•œ ë¦¬ì†ŒìŠ¤ ë°°ë¶„ ì–´ë ¤ì›€ |
| **ë‹¤ì´ë‚˜ë¯¹ ìš”êµ¬ì‚¬í•­ ë¯¸ëŒ€ì‘** | ëŸ°íƒ€ì„ ë¦¬ì†ŒìŠ¤ ë³€ê²½ ë¶ˆê°€ | ì´ˆê¸° ìš”ì²­ ê°’ ê³ ì •, ìŠ¤ì¼€ì¼ë§ ì–´ë ¤ì›€ |
| **ë©€í‹° ë¦¬ì†ŒìŠ¤ ì¡°ì • ë¶ˆê°€** | ì—¬ëŸ¬ ë¦¬ì†ŒìŠ¤ íƒ€ì… ì¡°ìœ¨ ë¶ˆê°€ | Podì´ GPU 1ê°œë§Œ ë°›ì•˜ëŠ”ë° ë©”ëª¨ë¦¬ ë¶€ì¡± ìƒí™© |

**DRA (Dynamic Resource Allocation)**ëŠ” Kubernetes 1.26+ë¶€í„° ë„ì…ë˜ì–´ ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤.

#### DRAì˜ í•µì‹¬ ê°œë…

DRAëŠ” **ì„ ì–¸ì  ë¦¬ì†ŒìŠ¤ ìš”ì²­ê³¼ ì¦‰ì‹œ í• ë‹¹**ì„ ë¶„ë¦¬í•˜ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì…ë‹ˆë‹¤:

```mermaid
graph LR
    A["Pod ìƒì„±<br/>(ResourceClaim ìš”ì²­)"] -->|Pending| B["Karpenter<br/>(ë…¸ë“œ ë¶„ì„)"]
    B -->|ë¦¬ì†ŒìŠ¤ ë¶€ì¡±| C["ìƒˆ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹"]
    C -->|í• ë‹¹ ì¤€ë¹„| D["DRA Controller<br/>(ë¦¬ì†ŒìŠ¤ ì˜ˆì•½)"]
    D -->|Allocated| E["Pod Binding"]
    E -->|Reserved| F["Pod ìŠ¤ì¼€ì¤„ë§"]
    F -->|InUse| G["Pod ì‹¤í–‰"]

    H["Resource Quota<br/>í™•ì¸"] -->|ì ìš©| D
    I["GPU íŒŒí‹°ì…”ë‹<br/>ì •ì±…"] -->|ì ìš©| D

    style A fill:#e8f4f8
    style D fill:#326ce5
    style E fill:#76b900
    style G fill:#ffd93d
```

#### ResourceClaim ë¼ì´í”„ì‚¬ì´í´

DRAì˜ í•µì‹¬ì€ **ResourceClaim**ì´ë¼ëŠ” ìƒˆë¡œìš´ Kubernetes ë¦¬ì†ŒìŠ¤ì…ë‹ˆë‹¤:

```yaml
# 1. ë¼ì´í”„ì‚¬ì´í´ ìƒíƒœ ì„¤ëª…

# PENDING ìƒíƒœ: ë¦¬ì†ŒìŠ¤ í• ë‹¹ ëŒ€ê¸° ì¤‘
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: gpu-claim-vllm
  namespace: ai-inference
spec:
  resourceClassName: gpu.nvidia.com
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClaimParameters
    name: h100-params
status:
  phase: Pending  # ì•„ì§ í• ë‹¹ë˜ì§€ ì•ŠìŒ

---

# ALLOCATED ìƒíƒœ: DRA ì»¨íŠ¸ë¡¤ëŸ¬ê°€ ë¦¬ì†ŒìŠ¤ ì˜ˆì•½ ì™„ë£Œ
status:
  phase: Allocated
  allocation:
    resourceHandle: "gpu-handle-12345"
    shareable: false

---

# RESERVED ìƒíƒœ: Podì´ ë°”ì¸ë”©ë  ì¤€ë¹„ ì™„ë£Œ
status:
  phase: Reserved
  allocation:
    resourceHandle: "gpu-handle-12345"
    nodeName: "gpu-node-01"

---

# INUSE ìƒíƒœ: Podì´ í™œì„± ì‹¤í–‰ ì¤‘
status:
  phase: InUse
  allocation:
    resourceHandle: "gpu-handle-12345"
    nodeName: "gpu-node-01"
  reservedFor:
    - kind: Pod
      name: vllm-inference
      namespace: ai-inference
      uid: "abc123"
```

ê° ìƒíƒœì—ì„œ ë‹¤ìŒ ìƒíƒœë¡œ ì „í™˜ë˜ë ¤ë©´ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤:

- **Pending â†’ Allocated**: DRA ë“œë¼ì´ë²„ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ë° ì˜ˆì•½
- **Allocated â†’ Reserved**: Podì´ ResourceClaimì„ ì§€ì •í•˜ê³  ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ë…¸ë“œ ê²°ì •
- **Reserved â†’ InUse**: Podì´ ì‹¤ì œë¡œ ë…¸ë“œì—ì„œ ì‹¤í–‰ ì‹œì‘

#### DRA vs Device Plugin ìƒì„¸ ë¹„êµ

| í•­ëª© | Device Plugin | DRA |
| --- | --- | --- |
| **ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì‹œì ** | ë…¸ë“œ ì‹œì‘ ì‹œ (ì •ì ) | Pod ìŠ¤ì¼€ì¤„ë§ ì‹œ (ë™ì ) |
| **í• ë‹¹ ë‹¨ìœ„** | ì „ì²´ GPUë§Œ ê°€ëŠ¥ | GPU ë¶„í•  ê°€ëŠ¥ (MIG, time-slicing) |
| **ìš°ì„ ìˆœìœ„ ì§€ì›** | ì—†ìŒ (ì„ ì°©ìˆœ) | ResourceClaimì˜ ìš°ì„ ìˆœìœ„ ì§€ì› |
| **ë©€í‹° ë¦¬ì†ŒìŠ¤ ì¡°ìœ¨** | ë¶ˆê°€ëŠ¥ | Pod ìˆ˜ì¤€ì—ì„œ ì—¬ëŸ¬ ë¦¬ì†ŒìŠ¤ ì¡°ìœ¨ |
| **ì„±ëŠ¥ ì œì•½ ì •ì±…** | ì—†ìŒ | ResourceClassë¡œ ì„±ëŠ¥ ì •ì±… ì •ì˜ ê°€ëŠ¥ |
| **í• ë‹¹ ë³µì›ë ¥** | ë…¸ë“œ ì¥ì•  ì‹œ ìˆ˜ë™ ì •ë¦¬ | ìë™ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ |
| **Kubernetes ë²„ì „** | 1.8+ | 1.26+ (Alpha), 1.29+ (Beta) |
| **ì„±ìˆ™ë„** | í”„ë¡œë•ì…˜ | ì ì§„ì  ì ìš© ê¶Œì¥ |

:::tip DRA ì„ íƒ ê°€ì´ë“œ
**DRAë¥¼ ì‚¬ìš©í•´ì•¼ í•  ë•Œ:**
- GPU íŒŒí‹°ì…”ë‹ì´ í•„ìš”í•œ ê²½ìš° (MIG, time-slicing)
- ë©€í‹° í…Œë„ŒíŠ¸ í™˜ê²½ì—ì„œ ê³µì •í•œ ë¦¬ì†ŒìŠ¤ ë°°ë¶„ í•„ìš”
- ë¦¬ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ë¥¼ ì ìš©í•´ì•¼ í•˜ëŠ” ê²½ìš°
- ë™ì  ìŠ¤ì¼€ì¼ë§ì´ ì¤‘ìš”í•œ ê²½ìš°

**Device Pluginì´ ì¶©ë¶„í•œ ê²½ìš°:**
- ë‹¨ìˆœíˆ GPUë¥¼ ì „ì²´ ë‹¨ìœ„ë¡œë§Œ í• ë‹¹
- ë ˆê±°ì‹œ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„± ì¤‘ìš”
- Kubernetes ë²„ì „ì´ 1.25 ì´í•˜
:::

#### ê³ ê¸‰ GPU íŒŒí‹°ì…”ë‹ ì „ëµ

##### 1. MIG (Multi-Instance GPU) ê¸°ë°˜ íŒŒí‹°ì…”ë‹

MIGëŠ” H100, A100 ê°™ì€ ìµœì‹  GPUë¥¼ ìµœëŒ€ 7ê°œì˜ ë…ë¦½ì ì¸ GPUë¡œ ë¶„í• í•©ë‹ˆë‹¤:

```yaml
# MIG í”„ë¡œí•„ ì •ì˜
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: a100-mig-1g.5gb
  namespace: ai-inference
spec:
  # MIG í”„ë¡œí•„ ì„ íƒ: 1g.5gb, 2g.10gb, 3g.20gb, 7g.40gb
  mig:
    profile: "1g.5gb"  # 5GB ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ MIG ì¸ìŠ¤í„´ìŠ¤
    count: 1

---

# MIG ê¸°ë°˜ ResourceClass
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClass
metadata:
  name: gpu.nvidia.com/mig
driverName: nvidia.com/gpu
structuredParameters: true
parametersSchema:
  openAPIV3Schema:
    type: object
    properties:
      gpuProfile:
        type: string
        enum: ["1g.5gb", "2g.10gb", "3g.20gb", "7g.40gb"]
        default: "1g.5gb"

---

# MIG ResourceClaim ì‚¬ìš© ì˜ˆì‹œ
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: inference-gpu-mig
  namespace: ai-inference
spec:
  resourceClassName: gpu.nvidia.com/mig
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClaimParameters
    name: a100-mig-1g.5gb

---

# Podì—ì„œ MIG ResourceClaim ì‚¬ìš©
apiVersion: v1
kind: Pod
metadata:
  name: vllm-mig-inference
  namespace: ai-inference
spec:
  containers:
    - name: vllm
      image: vllm/vllm-openai:latest
      command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
      args:
        - "--model"
        - "meta-llama/Llama-2-7b-hf"
        - "--gpu-memory-utilization"
        - "0.9"
      resources:
        requests:
          memory: "4Gi"
          cpu: "4"
        claims:
          - name: mig-gpu
  resourceClaims:
    - name: mig-gpu
      source:
        resourceClaimTemplateName: mig-template
```

**MIG í”„ë¡œí•„ ì„±ëŠ¥ ì§€í‘œ:**

| í”„ë¡œí•„ | ë©”ëª¨ë¦¬ | SM ìˆ˜ | ìš©ë„ | ì˜ˆìƒ ì²˜ë¦¬ëŸ‰ |
| --- | --- | --- | --- | --- |
| 1g.5gb | 5GB | 14 | ì†Œí˜• ëª¨ë¸ (3B-7B) | ~20 tok/s |
| 2g.10gb | 10GB | 28 | ì¤‘í˜• ëª¨ë¸ (7B-13B) | ~50 tok/s |
| 3g.20gb | 20GB | 42 | ëŒ€í˜• ëª¨ë¸ (13B-70B) | ~100 tok/s |
| 7g.40gb | 40GB | 84 | ì´ˆëŒ€í˜• ëª¨ë¸ (70B+) | ~200 tok/s |

##### 2. Time-Slicing ê¸°ë°˜ íŒŒí‹°ì…”ë‹

Time-Slicingì€ ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ GPU ì‹œê°„ì„ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ Podì´ ë™ì¼ GPUë¥¼ ê³µìœ í•©ë‹ˆë‹¤:

```yaml
# Time-Slicing ResourceSlice ì •ì˜
apiVersion: gpu.nvidia.com/v1alpha1
kind: ResourceSlice
metadata:
  name: gpu-node-timeslice
  namespace: ai-inference
spec:
  nodeName: gpu-node-01
  devices:
    - id: 0  # GPU 0
      vendor: nvidia
      model: "A100-SXM4-80GB"
      # Time-slicing ì„¤ì •: ìµœëŒ€ 4ê°œ Podì´ ë™ì¼ GPU ì‚¬ìš© ê°€ëŠ¥
      timeSlicing:
        replicas: 4
        # GPU ìŠ¤ì¼€ì¤„ë§ ì •ì±…: "aggressive", "default", "conservative"
        schedulingPolicy: "default"
        # ì»¨í…ìŠ¤íŠ¸ ìŠ¤ìœ„ì¹­ ì˜¤ë²„í—¤ë“œ ì„¤ì • (ms)
        contextSwitchInterval: 100

---

# Time-Slicing ResourceClass
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClass
metadata:
  name: gpu.nvidia.com/timeslice
driverName: nvidia.com/gpu
structuredParameters: true

---

# Time-Slicing ResourceClaim ì‚¬ìš©
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: inference-gpu-slice
  namespace: ai-inference
spec:
  resourceClassName: gpu.nvidia.com/timeslice

---

# ì—¬ëŸ¬ Podì´ ë™ì¼ GPUë¥¼ time-sliceë¡œ ê³µìœ 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-timeslice-replicas
  namespace: ai-inference
spec:
  replicas: 3  # 3ê°œ Podì´ ë™ì¼ GPU ê³µìœ 
  selector:
    matchLabels:
      app: vllm-slice
  template:
    metadata:
      labels:
        app: vllm-slice
    spec:
      containers:
        - name: vllm
          image: vllm/vllm-openai:latest
          resources:
            requests:
              memory: "8Gi"
              cpu: "2"
            claims:
              - name: gpu-slice
      resourceClaims:
        - name: gpu-slice
          source:
            resourceClaimTemplateName: timeslice-template
```

**Time-Slicing ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­:**

```mermaid
graph TB
    subgraph "Time-Slicing ì˜¤ë²„í—¤ë“œ"
        A["GPU ì»¨í…ìŠ¤íŠ¸ ìŠ¤ìœ„ì¹­"] -->|~100-500ms| B["L2 ìºì‹œ í”ŒëŸ¬ì‹œ"]
        B --> C["ìƒˆ ì»¤ë„ ë¡œë“œ"]
        C --> D["ë©”ëª¨ë¦¬ ì¬êµ¬ì„±"]
        D --> E["ì„±ëŠ¥ ì €í•˜ 5-15%"]
    end

    F["ì¶”ì²œ ì‚¬ìš© ì‚¬ë¡€"] -->|ë°°ì¹˜ ì¶”ë¡ | G["ì²˜ë¦¬ëŸ‰ ì¤‘ì‹¬"]
    F -->|ê°œë°œ/í…ŒìŠ¤íŠ¸| H["ë¹„ìš© ìµœì í™”"]
    F -->|ë‚®ì€ QoS ìš”êµ¬| I["ë¹„ê¸´ê¸‰ ì‘ì—…"]

    J["í”¼í•´ì•¼ í•  ì‚¬ìš© ì‚¬ë¡€"] -->|ì‹¤ì‹œê°„ ì¶”ë¡ | K["ë‚®ì€ ì§€ì—° ìš”êµ¬"]
    J -->|ê³ ì„±ëŠ¥ í•™ìŠµ| L["ë†’ì€ ì²˜ë¦¬ëŸ‰ í•„ìš”"]
    J -->|ë¯¼ê°í•œ ì• í”Œë¦¬ì¼€ì´ì…˜| M["ì„±ëŠ¥ ë³´ì¥ í•„ìš”"]

    style E fill:#ff6b6b
    style G fill:#76b900
    style K fill:#ff6b6b
```

#### Karpenterì™€ DRAì˜ í†µí•©

KarpenterëŠ” DRAì™€ í•¨ê»˜ ì‘ë™í•˜ì—¬ **ìë™ìœ¼ë¡œ í•„ìš”í•œ ë…¸ë“œë¥¼ í”„ë¡œë¹„ì €ë‹**í•©ë‹ˆë‹¤:

```yaml
# Karpenter NodePool: DRA ìµœì í™”
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: gpu-dra-pool
spec:
  template:
    metadata:
      labels:
        workload: ai-training
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: node.kubernetes.io/instance-type
          operator: In
          # H100 ì„ í˜¸, A100ìœ¼ë¡œ í´ë°±
          values: ["p5.48xlarge", "p4d.24xlarge"]
        - key: nvidia.com/gpu
          operator: In
          values: ["8", "16"]  # GPU ìˆ˜ ëª…ì‹œ

      nodeClassRef:
        name: gpu-dra-class

  limits:
    resources:
      cpu: "1000"
      memory: "1000Gi"
      nvidia.com/gpu: "1000"  # ìµœëŒ€ 1000ê°œ GPU

  disruption:
    consolidateAfter: 30s
    expireAfter: 720h
    budgets:
      - nodes: "10%"
        duration: 5m
        schedule: "0 9 * * mon-fri"  # í‰ì¼ ì—…ë¬´ì‹œê°„ ì œì™¸

---

# Karpenter EC2NodeClass: EFA + NVLink ìµœì í™”
apiVersion: karpenter.k8s.aws/v1beta1
kind: EC2NodeClass
metadata:
  name: gpu-dra-class
spec:
  amiFamily: GPU
  role: "KarpenterNodeRole"
  subnetSelector:
    karpenter.sh/discovery: "true"
  securityGroupSelector:
    karpenter.sh/discovery: "true"

  userData: |
    #!/bin/bash
    # EFA ë° NVLink ìµœì í™”
    echo "vm.max_map_count=262144" >> /etc/sysctl.conf
    sysctl -p

    # NVLink ë“œë¼ì´ë²„ ë¡œë“œ
    modprobe nvidia-uvm

    # GPU Operatorê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
    # - NVIDIA Driver ì„¤ì¹˜
    # - CUDA Toolkit ì„¤ì¹˜
    # - DCGM ë°°í¬
    # - DRA ë“œë¼ì´ë²„ ë°°í¬

  instanceProfile: "KarpenterNodeInstanceProfile"
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 200Gi
        volumeType: gp3
        encrypted: true
        deleteOnTermination: true

  metadataOptions:
    httpEndpoint: enabled
    httpProtocolIPv6: disabled
    httpPutResponseHopLimit: 2
```

#### EKSì—ì„œ DRA í™œì„±í™” ë°©ë²•

```bash
#!/bin/bash

# Step 1: EKS í´ëŸ¬ìŠ¤í„° ìƒì„± (DRA ì§€ì› ë²„ì „)
eksctl create cluster \
  --name ai-gpu-cluster \
  --version 1.29 \
  --region us-east-1 \
  --nodegroup-name cpu-nodes \
  --node-type t3.medium \
  --nodes 2 \
  --enable-ssm \
  --managed

# Step 2: DRA ê¸°ëŠ¥ í™œì„±í™” (í•„ìš”ì‹œ íŒŒë¼ë¯¸í„° ì¶”ê°€)
# Kubernetes 1.29+ëŠ” ê¸°ë³¸ í™œì„±í™”ë¨

# Step 3: GPU Operator ì„¤ì¹˜ (NVIDIA Driver + DRA ë“œë¼ì´ë²„ í¬í•¨)
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update

helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator \
  --create-namespace \
  --set driver.enabled=true \
  --set dcgm.enabled=true \
  --set driver.rdma.enabled=true \
  --set driver.rdma.useHostMofed=true

# Step 4: NVIDIA ë¦¬ì†ŒìŠ¤ ë“œë¼ì´ë²„ ë°°í¬ (DRA í†µí•©)
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.15.0/\
nvidia-device-plugin.yml

# Step 5: Karpenter ì„¤ì¹˜ (NodePool ìë™ ê´€ë¦¬)
helm repo add karpenter https://charts.karpenter.sh
helm repo update

helm install karpenter karpenter/karpenter \
  --namespace karpenter \
  --create-namespace \
  --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=arn:aws:iam::ACCOUNT_ID:role/KarpenterControllerRole

# Step 6: DRA ResourceClass ìƒì„±
cat <<EOF | kubectl apply -f -
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClass
metadata:
  name: gpu.nvidia.com
driverName: nvidia.com/gpu
EOF

# Step 7: GPU NodePool ìƒì„± (Karpenter)
cat <<EOF | kubectl apply -f -
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: gpu-pool
spec:
  template:
    spec:
      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["p4d.24xlarge", "p5.48xlarge"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]
      nodeClassRef:
        name: gpu-class
  limits:
    resources:
      nvidia.com/gpu: "1000"
---
apiVersion: karpenter.k8s.aws/v1beta1
kind: EC2NodeClass
metadata:
  name: gpu-class
spec:
  amiFamily: GPU
  role: KarpenterNodeRole
EOF

# Step 8: DRA ìƒíƒœ í™•ì¸
kubectl get resourceclaims -A
kubectl get resourceclasses
kubectl describe resourceclaim <claim-name> -n <namespace>
```

#### DRA ì‹¤ì œ YAML ì˜ˆì‹œ: ë©€í‹° GPU ìš”ì²­

```yaml
# ê³ ê¸‰ ì˜ˆì‹œ: ë¶„ì‚° í•™ìŠµìš© ë©€í‹° GPU ResourceClaim
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: distributed-training-gpus
  namespace: ai-training
spec:
  resourceClassName: gpu.nvidia.com
  # ì—¬ëŸ¬ ê°œì˜ GPUë¥¼ í•œ ë²ˆì— ìš”ì²­
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClaimParameters
    name: distributed-params

---

# ë¶„ì‚° í•™ìŠµ íŒŒë¼ë¯¸í„° ì •ì˜
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: distributed-params
  namespace: ai-training
spec:
  # 8ê°œì˜ A100 GPU ìš”ì²­ (ë™ì¼ ë…¸ë“œ ë˜ëŠ” ë…¸ë“œ í˜ì–´ ì„ í˜¸)
  count: 8
  selector:
    matchLabels:
      gpu.nvidia.com/product: "NVIDIA-A100-SXM4-80GB"
  affinity:
    # ê°™ì€ ë…¸ë“œì— ë°°ì¹˜ëœ GPU ì„ í˜¸
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
              - key: karpenter.sh/instance-family
                operator: In
                values: ["p4d"]  # p4d ì¸ìŠ¤í„´ìŠ¤ ì„ í˜¸ (8ê°œ A100)
  # ë†’ì€ ì„±ëŠ¥ì„ ìœ„í•œ NVLink/NVSwitch í† í´ë¡œì§€ ì„ í˜¸
  performanceTier: "high"

---

# ë¶„ì‚° í•™ìŠµ Job ì •ì˜
apiVersion: batch/v1
kind: Job
metadata:
  name: llama-distributed-training
  namespace: ai-training
spec:
  parallelism: 2  # 2ê°œ Pod (ê°ê° 4ê°œ GPU)
  completions: 2
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: training
          image: nvcr.io/nvidia/pytorch:24.02-py3
          command:
            - "python"
            - "-m"
            - "torch.distributed.launch"
            - "--nproc_per_node=4"  # Podë‹¹ 4ê°œ GPU ì‚¬ìš©
            - "/app/train.py"
          env:
            # ë¶„ì‚° í•™ìŠµ í™˜ê²½ ë³€ìˆ˜
            - name: RANK
              valueFrom:
                fieldRef:
                  fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
            - name: WORLD_SIZE
              value: "2"  # ì´ 2ê°œ Pod
            - name: MASTER_ADDR
              value: "llama-distributed-training-0.training"
            - name: MASTER_PORT
              value: "29500"
            # NCCL ìµœì í™” ì„¤ì •
            - name: NCCL_DEBUG
              value: "INFO"
            - name: NCCL_ALGO
              value: "Ring"
            - name: NCCL_MIN_NCHANNELS
              value: "4"
          resources:
            requests:
              memory: "200Gi"
              cpu: "32"
            claims:
              - name: training-gpus
          volumeMounts:
            - name: training-data
              mountPath: /data
            - name: model-cache
              mountPath: /root/.cache/huggingface

      resourceClaims:
        - name: training-gpus
          source:
            resourceClaimTemplateName: multi-gpu-template

      volumes:
        - name: training-data
          persistentVolumeClaim:
            claimName: training-data-pvc
        - name: model-cache
          emptyDir: {}

---

# ResourceClaimTemplate: ë©€í‹° GPU í• ë‹¹ í…œí”Œë¦¿
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaimTemplate
metadata:
  name: multi-gpu-template
  namespace: ai-training
spec:
  spec:
    resourceClassName: gpu.nvidia.com
    parametersRef:
      apiGroup: gpu.nvidia.com
      kind: GpuClaimParameters
      name: distributed-params
```

#### DRA íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

| ë¬¸ì œ | ì¦ìƒ | ì§„ë‹¨ ë°©ë²• | í•´ê²° ë°©ë²• |
| --- | --- | --- | --- |
| **ResourceClaim Pending** | Podì´ Pending ìƒíƒœë¡œ ê³ ì°© | `kubectl describe claim <name>` í™•ì¸ | DRA ì»¨íŠ¸ë¡¤ëŸ¬ ë¡œê·¸ í™•ì¸, ResourceClass ì¡´ì¬ ì—¬ë¶€ í™•ì¸ |
| **GPU í• ë‹¹ ì‹¤íŒ¨** | `error: no resource available` | `kubectl get resourceclaims -o wide` | GPU ë…¸ë“œ ê°€ìš©ì„± í™•ì¸, Karpenter NodePool ì„¤ì • ê²€í†  |
| **MIG í”„ë¡œí•„ ë¶ˆì¼ì¹˜** | Podì´ Evictedë¨ | ë…¸ë“œì˜ ì‹¤ì œ MIG í”„ë¡œí•„ í™•ì¸ | `nvidia-smi -L` ì‹¤í–‰ í›„ ResourceClaim íŒŒë¼ë¯¸í„° ì •ì • |
| **Performance ì €í•˜** | ì˜ˆìƒë³´ë‹¤ ëŠë¦° ì²˜ë¦¬ ì†ë„ | NCCL ë¡œê·¸ ë° GPU ë©”íŠ¸ë¦­ í™•ì¸ | Time-slicing ì‚¬ìš© ì—¬ë¶€ í™•ì¸, ì „ìš© GPU ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½ |
| **DRA Driver ì¶©ëŒ** | Podì´ CrashLoopBackOff | `kubectl logs <pod>` í™•ì¸ | Device Pluginê³¼ DRA ë“œë¼ì´ë²„ ë²„ì „ í˜¸í™˜ì„± ê²€í†  |

```bash
# DRA íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ëª…ë ¹ì–´ ëª¨ìŒ

# 1. DRA ìƒíƒœ í™•ì¸
kubectl get resourceclaims -A --show-kind
kubectl get resourceclasses
kubectl get resourceslices

# 2. ResourceClaim ìƒì„¸ ì •ë³´
kubectl describe resourceclaim <claim-name> -n <namespace>

# 3. DRA ì»¨íŠ¸ë¡¤ëŸ¬ ë¡œê·¸ í™•ì¸
kubectl logs -n karpenter $(kubectl get pod -n karpenter -l app.kubernetes.io/name=karpenter -o jsonpath='{.items[0].metadata.name}')

# 4. GPU ë…¸ë“œì˜ ë¦¬ì†ŒìŠ¤ ìƒíƒœ
kubectl describe node <gpu-node-name>

# 5. Podì˜ ë¦¬ì†ŒìŠ¤ í• ë‹¹ ìƒíƒœ
kubectl get pod <pod-name> -o jsonpath='{.status.allocatedResources}'

# 6. NVIDIA ë“œë¼ì´ë²„ ìƒíƒœ í™•ì¸ (ë…¸ë“œì—ì„œ)
ssh <gpu-node>
nvidia-smi
nvidia-smi -L  # MIG í”„ë¡œí•„ í™•ì¸

# 7. MIG í”„ë¡œí•„ ì„¤ì • (í•„ìš”ì‹œ)
nvidia-smi -mig 1  # MIG í™œì„±í™”
nvidia-smi -mig 1 -i 0 -pm ENABLED  # GPU 0ì—ì„œ MIG í™œì„±í™”
nvidia-smi mig -cgi 9,14,14,14 -C  # MIG í”„ë¡œí•„ ì„¤ì • (1g.5gb 4ê°œ)
```

---

### NCCL ì‹¬ì¸µ ë¶„ì„: Collective Communication ìµœì í™”

#### NCCLì˜ ì—­í• ê³¼ ì¤‘ìš”ì„±

NCCL (**NVIDIA Collective Communication Library**)ëŠ” ë¶„ì‚° GPU í•™ìŠµì—ì„œ **multi-GPU ê°„ ê³ ì† í†µì‹ **ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ NCCLì˜ ìµœì í™” ì •ë„ì— ì§ì ‘ì ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "ë¶„ì‚° í•™ìŠµ ì„±ëŠ¥ ë¶„ì„"
        A["ì „ì²´ í•™ìŠµ ì‹œê°„"] --> B["ê³„ì‚° ì‹œê°„ 60%"]
        A --> C["í†µì‹  ì‹œê°„ 40%"]

        C --> D["NCCLì´ ìµœì í™”í•˜ëŠ” ì˜ì—­"]
        D --> E["Collective ì—°ì‚° ì‹œê°„"]
        E --> F["ë™ê¸°í™” ì˜¤ë²„í—¤ë“œ"]

        B --> G["GPU ê³„ì‚° (ì»¤ë„)"]

        style D fill:#326ce5
        style E fill:#76b900
        style F fill:#ff6b6b
    end

    subgraph "NCCLì´ í•´ê²°í•˜ëŠ” ë¬¸ì œ"
        H["Raw ë„¤íŠ¸ì›Œí¬ ëŒ€ë¹„<br/>3-10ë°° ê°œì„ "]
        I["CPU ì˜¤ë²„í—¤ë“œ ì œê±°"]
        J["GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±"]
        K["NVLink/EFA ìë™ í™œìš©"]
    end
```

**ë¶„ì‚° í•™ìŠµì—ì„œ NCCLì´ ì¤‘ìš”í•œ ì´ìœ :**

| í•­ëª© | ì˜í–¥ë„ | NCCLì˜ ìµœì í™” |
| --- | --- | --- |
| **ëª¨ë¸ ë³‘ë ¬í™” (Model Parallelism)** | ë†’ìŒ | ê° GPU ê°„ í™œì„±í™”/ê·¸ë˜ë””ì–¸íŠ¸ ì „ì†¡ ìµœì í™” |
| **ë°ì´í„° ë³‘ë ¬í™” (Data Parallelism)** | ë§¤ìš° ë†’ìŒ | AllReduceë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ë™ê¸°í™” ë¹ ë¦„ |
| **íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” (Pipeline Parallelism)** | ë†’ìŒ | ìŠ¤í…Œì´ì§€ ê°„ í™œì„±í™” ì „ì†¡ ìµœì í™” |
| **í˜¼í•© ì •ë°€ë„ í•™ìŠµ (Mixed Precision)** | ì¤‘ê°„ | ì••ì¶•ëœ ê·¸ë˜ë””ì–¸íŠ¸ í†µì‹  ìµœì í™” |

#### í•µì‹¬ ì§‘í•© ì—°ì‚° (Collective Operations)

##### 1. AllReduce - ê°€ì¥ ì¤‘ìš”í•œ ì—°ì‚°

AllReduceëŠ” ëª¨ë“  GPUì˜ ë°ì´í„°ë¥¼ í•©ì‚°í•˜ê³  ê²°ê³¼ë¥¼ ëª¨ë“  GPUì— ë°°ë¶„í•©ë‹ˆë‹¤:

```
ì´ˆê¸° ìƒíƒœ:
GPU 0: [1, 2, 3]
GPU 1: [4, 5, 6]
GPU 2: [7, 8, 9]
GPU 3: [10, 11, 12]

AllReduce í›„:
GPU 0: [22, 26, 30]  # 1+4+7+10, 2+5+8+11, 3+6+9+12
GPU 1: [22, 26, 30]
GPU 2: [22, 26, 30]
GPU 3: [22, 26, 30]
```

**AllReduce ì‚¬ìš© ì˜ˆì‹œ (ë¶„ì‚° í•™ìŠµì—ì„œ):**

```python
import torch
import torch.distributed as dist

# ë¶„ì‚° í•™ìŠµ ì´ˆê¸°í™”
dist.init_process_group("nccl")
rank = dist.get_rank()
world_size = dist.get_world_size()

# ê° GPUì˜ ê·¸ë˜ë””ì–¸íŠ¸ (ì„œë¡œ ë‹¤ë¦„)
gradients = torch.randn(1024, device=f"cuda:{rank}")

# AllReduce: ëª¨ë“  GPUì˜ ê·¸ë˜ë””ì–¸íŠ¸ í•©ì‚° ë° í‰ê· í™”
dist.all_reduce(gradients, op=dist.ReduceOp.SUM)
gradients /= world_size

# ì´ì œ ëª¨ë“  GPUê°€ ë™ì¼í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê°€ì§
# ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹œ ë™ê¸°í™”ë¨
```

##### 2. AllGather - ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘

AllGatherëŠ” ëª¨ë“  GPUì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ê° GPUì— ì „ì²´ ë°ì´í„°ë¥¼ ë°°ë¶„í•©ë‹ˆë‹¤:

```
ì´ˆê¸° ìƒíƒœ:
GPU 0: [1, 2]
GPU 1: [3, 4]
GPU 2: [5, 6]
GPU 3: [7, 8]

AllGather í›„:
GPU 0: [1, 2, 3, 4, 5, 6, 7, 8]
GPU 1: [1, 2, 3, 4, 5, 6, 7, 8]
GPU 2: [1, 2, 3, 4, 5, 6, 7, 8]
GPU 3: [1, 2, 3, 4, 5, 6, 7, 8]
```

**AllGather ì‚¬ìš© ì‚¬ë¡€:**

```python
# ì˜ˆì‹œ: ë°°ì¹˜ ì •ê·œí™”ì—ì„œ ëª¨ë“  GPUì˜ í†µê³„ ìˆ˜ì§‘
local_batch_stats = compute_batch_stats(local_batch)

# AllGatherë¡œ ëª¨ë“  GPUì˜ í†µê³„ ìˆ˜ì§‘
all_batch_stats = [torch.empty_like(local_batch_stats) for _ in range(world_size)]
dist.all_gather(all_batch_stats, local_batch_stats)

# ì „ì—­ í†µê³„ ê³„ì‚°
global_mean = torch.stack(all_batch_stats).mean(dim=0)
global_std = torch.stack(all_batch_stats).std(dim=0)
```

##### 3. ReduceScatter - AllGatherì˜ ì—­ì—°ì‚°

ReduceScatterëŠ” ë°ì´í„°ë¥¼ ë¨¼ì € í•©ì‚°í•œ í›„ ê° GPUì— ë¶„í• í•˜ì—¬ ë°°ë¶„í•©ë‹ˆë‹¤:

```
ì´ˆê¸° ìƒíƒœ:
GPU 0: [1, 2, 3, 4, 5, 6, 7, 8]
GPU 1: [9, 10, 11, 12, 13, 14, 15, 16]
GPU 2: [17, 18, 19, 20, 21, 22, 23, 24]
GPU 3: [25, 26, 27, 28, 29, 30, 31, 32]

ReduceScatter í•©ì‚° í›„ ë¶„í• :
GPU 0: [52, 56]      # (1+9+17+25), (2+10+18+26)
GPU 1: [60, 64]      # (3+11+19+27), (4+12+20+28)
GPU 2: [68, 72]      # (5+13+21+29), (6+14+22+30)
GPU 3: [76, 80]      # (7+15+23+31), (8+16+24+32)
```

**ReduceScatter ì‚¬ìš© ì‚¬ë¡€ (Model Parallelism):**

```python
# ëª¨ë¸ ë³‘ë ¬í™”ì—ì„œ ê³„ì‚° ê²°ê³¼ë¥¼ í•©ì‚°í•˜ê³  ë¶„í• 
local_output = model_fragment(input_data)

# ReduceScatter: ëª¨ë“  í”„ë˜ê·¸ë¨¼íŠ¸ í•©ì‚° í›„ ê° GPUì— ë¶„í• 
reduced_output = torch.empty(output_size // world_size, device=local_output.device)
dist.reduce_scatter(reduced_output, [local_output] * world_size)
```

##### 4. Broadcast - ë°ì´í„° ë°°í¬

BroadcastëŠ” í•œ GPUì˜ ë°ì´í„°ë¥¼ ëª¨ë“  GPUì— ë³µì‚¬í•©ë‹ˆë‹¤:

```
ì´ˆê¸° ìƒíƒœ:
GPU 0: [1, 2, 3, 4]
GPU 1: [0, 0, 0, 0]
GPU 2: [0, 0, 0, 0]
GPU 3: [0, 0, 0, 0]

Broadcast í›„:
GPU 0: [1, 2, 3, 4]
GPU 1: [1, 2, 3, 4]
GPU 2: [1, 2, 3, 4]
GPU 3: [1, 2, 3, 4]
```

**Broadcast ì‚¬ìš© ì‚¬ë¡€:**

```python
# ë§ˆìŠ¤í„° GPUì—ì„œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¸Œë¡œë“œìºìŠ¤íŠ¸
model_state = load_checkpoint() if rank == 0 else None

# Broadcast: ë§ˆìŠ¤í„° GPUì˜ ëª¨ë¸ ìƒíƒœë¥¼ ëª¨ë“  GPUì— ë°°í¬
dist.broadcast_object_list([model_state], src=0)
model.load_state_dict(model_state)
```

#### ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ ì¸ì‹

NCCLì€ GPU ê°„ ë¬¼ë¦¬ì  ì—°ê²° í† í´ë¡œì§€ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ìµœì ì˜ ê²½ë¡œë¥¼ ì„ íƒí•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "í† í´ë¡œì§€ ê³„ì¸µ (ìœ„ì—ì„œ ì•„ë˜ë¡œ ë¹ ë¦„)"
        L1["1. NVSwitch (ê°™ì€ ë…¸ë“œ ë‚´)<br/>ìµœëŒ€ 600GB/s"]
        L2["2. NVLink (ê°™ì€ ë…¸ë“œ ë‚´)<br/>ìµœëŒ€ 200GB/s"]
        L3["3. EFA/InfiniBand (ë…¸ë“œ ê°„)<br/>ìµœëŒ€ 100GB/s"]
        L4["4. Ethernet (ë…¸ë“œ ê°„)<br/>ìµœëŒ€ 10-100GB/s"]
    end

    L1 --> L2 --> L3 --> L4

    subgraph "NCCL ìë™ ê²½ë¡œ ì„ íƒ"
        A["í† í´ë¡œì§€ ë¶„ì„"] --> B["ìµœì  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ"]
        B --> C["ì±„ë„ êµ¬ì„±"]
    end

    style L1 fill:#76b900
    style L2 fill:#76b900
    style L3 fill:#4ecdc4
    style L4 fill:#ff6b6b
```

##### 1. NVSwitch - H100/A100 ê³ ì† ì¸í„°ì»¤ë„¥íŠ¸

```yaml
# H100 8ê°œ with NVSwitch êµ¬ì„± (p5.48xlarge)
# ëª¨ë“  GPU ê°„ 600GB/s ì–‘ë°©í–¥ ëŒ€ì—­í­

# NCCLì€ ìë™ìœ¼ë¡œ NVSwitch ê°ì§€
# ìµœì  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ (Ring â†’ Treeë¡œ ë³€ê²½ ê°€ëŠ¥)

# NVSwitch ìƒíƒœ í™•ì¸ (ë…¸ë“œì—ì„œ)
$ nvidia-smi nvlink -sc 0
# Links between GPUs (P2P and Host):
# GPU0 <-> GPU1: both ways active (NVSwitch)
# GPU0 <-> GPU2: both ways active (NVSwitch)
# ... (ëª¨ë“  ìŒì´ í™œì„±)
```

##### 2. NVLink - A100 GPU ì¸í„°ì»¤ë„¥íŠ¸

```yaml
# A100 8ê°œ with NVLink êµ¬ì„± (p4d.24xlarge)
# GPU ê°„ 200GB/s, ì œí•œëœ ì—°ê²°

# NVLink í† í´ë¡œì§€
# GPU0 -- GPU1 -- GPU2 -- GPU3
#  |       |       |       |
# GPU4 -- GPU5 -- GPU6 -- GPU7

# NCCLì´ Ring/Tree ì„ íƒ
# ëŒ€ë¶€ë¶„ Ring ì•Œê³ ë¦¬ì¦˜ ì„ íƒ (ì´ í† í´ë¡œì§€ì— ìµœì )
```

##### 3. EFA (Elastic Fabric Adapter) - AWS ë„¤íŠ¸ì›Œí¬

```yaml
# EFAë¥¼ í†µí•œ ë…¸ë“œ ê°„ í†µì‹  (ìµœëŒ€ 100GB/s)
# Ethernet ëŒ€ë¹„ 10ë°° ì´ìƒ ë¹ ë¦„

# EFA í™œì„±í™” ì„¤ì •
export FI_PROVIDER=efa
export FI_EFA_USE_DEVICE_RDMA=1
export FI_EFA_FORK_SAFE=1

# NCCLì´ EFA ìë™ ê°ì§€ ë° ì‚¬ìš©
export NCCL_DEBUG=INFO  # ë¡œê·¸ì—ì„œ EFA ì‚¬ìš© ì—¬ë¶€ í™•ì¸
```

#### NCCL ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ê°€ì´ë“œ

NCCLì€ ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ì™€ ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ìµœì ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ìë™ ì„ íƒí•©ë‹ˆë‹¤:

```mermaid
graph TB
    A["NCCL ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ê²°ì • íŠ¸ë¦¬"] --> B{"í† í´ë¡œì§€"}

    B -->|ëª¨ë“  GPU í’€ ì—°ê²°| C["Tree ì•Œê³ ë¦¬ì¦˜"]
    B -->|ì„ í˜•/ë§ í† í´ë¡œì§€| D["Ring ì•Œê³ ë¦¬ì¦˜"]
    B -->|CollNet ì§€ì›| E["CollNet ì•Œê³ ë¦¬ì¦˜"]

    C --> F["ë³‘ë ¬ë„ ë†’ìŒ<br/>ëŒ€ì—­í­ íš¨ìœ¨ ë‚®ìŒ"]
    D --> G["ê· í˜•ì¡íŒ ì„ íƒ<br/>ëŒ€ì—­í­ íš¨ìœ¨ ë†’ìŒ"]
    E --> H["ê·¹ë„ë¡œ íš¨ìœ¨ì <br/>ê³ ê¸‰ í•˜ë“œì›¨ì–´ í•„ìš”"]

    I{"ë°ì´í„° í¬ê¸°"} --> J["ì‘ìŒ < 1MB"]
    I --> K["ì¤‘ê°„ 1MB - 100MB"]
    I --> L["í¼ > 100MB"]

    J --> M["Tree ì„ í˜¸<br/>ë ˆì´í„´ì‹œ ì¤‘ìš”"]
    K --> N["Ring ë˜ëŠ” Tree"]
    L --> O["Ring ì„ í˜¸<br/>ëŒ€ì—­í­ ì¤‘ìš”"]

    style C fill:#326ce5
    style D fill:#4ecdc4
    style E fill:#76b900
    style M fill:#ff6b6b
    style O fill:#76b900
```

##### 1. Ring ì•Œê³ ë¦¬ì¦˜

Ring ì•Œê³ ë¦¬ì¦˜ì€ GPUë¥¼ ë§ êµ¬ì¡°ë¡œ ì—°ê²°í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤:

```
ë¼ìš´ë“œ 1:
GPU0 -> GPU1 (ë°ì´í„° ì „ì†¡)
GPU1 -> GPU2
GPU2 -> GPU3
GPU3 -> GPU0

ë¼ìš´ë“œ 2:
GPU0 -> GPU1 (ë‹¤ìŒ ì²­í¬ ì „ì†¡)
...

Nê°œ GPUì¼ ë•Œ: 2(N-1) ë¼ìš´ë“œ í•„ìš”
ê° ë¼ìš´ë“œë§ˆë‹¤ ìˆœì°¨ ì§€ì—° ìˆìŒ
```

**Ring ì•Œê³ ë¦¬ì¦˜ ì¥ì :**
- ëŒ€ì—­í­ í™œìš© ìµœì  (ê° ë§í¬ 100% ì‚¬ìš©)
- ëŠë¦° ë„¤íŠ¸ì›Œí¬ì—ì„œë„ ì•ˆì •ì 
- í™•ì¥ì„± ìš°ìˆ˜ (GPU ìˆ˜ ì¦ê°€í•´ë„ íš¨ìœ¨ ìœ ì§€)

**Ring ì•Œê³ ë¦¬ì¦˜ ë‹¨ì :**
- ë ˆì´í„´ì‹œ ë†’ìŒ (ìˆœì°¨ ì²˜ë¦¬)
- ì‘ì€ ë°ì´í„° ì „ì†¡ì— ë¹„íš¨ìœ¨ì 

```yaml
# Ring ì•Œê³ ë¦¬ì¦˜ ê°•ì œ ì„¤ì •
export NCCL_ALGO=Ring
export NCCL_PROTO=Simple
export NCCL_MIN_NCHANNELS=4
export NCCL_MAX_NCHANNELS=8
```

##### 2. Tree ì•Œê³ ë¦¬ì¦˜

Tree ì•Œê³ ë¦¬ì¦˜ì€ GPUë¥¼ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ë°°ì¹˜í•˜ì—¬ ë³‘ë ¬ ì „ì†¡í•©ë‹ˆë‹¤:

```
          Root (GPU0)
          /      \
       GPU1      GPU2
       / \       / \
     GPU3 GPU4 GPU5 GPU6

ë‹¨ê³„ 1: GPU1, GPU2ê°€ GPU0ìœ¼ë¡œë¶€í„° ë™ì‹œ ìˆ˜ì‹ 
ë‹¨ê³„ 2: GPU3, GPU4ê°€ GPU1ìœ¼ë¡œë¶€í„° ë™ì‹œ ìˆ˜ì‹ 
        GPU5, GPU6ì´ GPU2ë¡œë¶€í„° ë™ì‹œ ìˆ˜ì‹ 
```

**Tree ì•Œê³ ë¦¬ì¦˜ ì¥ì :**
- ë ˆì´í„´ì‹œ ë‚®ìŒ (ë³‘ë ¬ ì²˜ë¦¬)
- ì‘ì€ ë°ì´í„° ì „ì†¡ì— íš¨ìœ¨ì 
- í˜„ëŒ€ ì»´í“¨í„° í•˜ë“œì›¨ì–´ì— ë§ìŒ

**Tree ì•Œê³ ë¦¬ì¦˜ ë‹¨ì :**
- ëŒ€ì—­í­ í™œìš© ë‚®ìŒ
- ë£¨íŠ¸ ë…¸ë“œ ë³‘ëª© ê°€ëŠ¥ì„±

```yaml
# Tree ì•Œê³ ë¦¬ì¦˜ ê°•ì œ ì„¤ì •
export NCCL_ALGO=Tree
export NCCL_PROTO=Simple
```

##### 3. CollNet ì•Œê³ ë¦¬ì¦˜ (ìµœì‹ )

CollNetì€ ë§ì¶¤í˜• NVLink/EFA í•˜ë“œì›¨ì–´ë¥¼ í™œìš©í•œ ìµœì²¨ë‹¨ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤:

```
CollNet Switch (ì „ìš© í•˜ë“œì›¨ì–´)

GPU0, GPU1, GPU2, ... GPU7
  |    |     |           |
  \    |     |          /
   \ _|_ __|_  ________/
     [CollNet Switch]
      (ë™ì‹œ í†µì‹ )

ëª¨ë“  GPUê°€ ë™ì‹œì— í†µì‹  ê°€ëŠ¥
ìµœëŒ€ ëŒ€ì—­í­ í™œìš©
```

**CollNet ì¥ì :**
- ìµœê³  ì²˜ë¦¬ëŸ‰ (ëª¨ë“  ë§í¬ ë™ì‹œ ì‚¬ìš©)
- Ring/Tree ëŒ€ë¹„ 3-5ë°° ë¹ ë¦„
- ë°ì´í„° í¬ê¸°ì— ê´€ê³„ì—†ì´ ìµœì 

**CollNet ìš”êµ¬ì‚¬í•­:**
- AWS Trainium/Inferentia ë˜ëŠ” ê³ ê¸‰ í•˜ë“œì›¨ì–´
- H100 NVSwitch í™˜ê²½ ì¶”ì²œ

```yaml
# CollNet ì•Œê³ ë¦¬ì¦˜ ì„¤ì •
export NCCL_ALGO=CollNet
export NCCL_PROTO=LL  # LL(Low Latency) í”„ë¡œí† ì½œ
```

#### AWS EFA í†µí•© ìƒì„¸ ì„¤ì •

EFA (Elastic Fabric Adapter)ëŠ” EC2 ì¸ìŠ¤í„´ìŠ¤ ê°„ RDMAë¥¼ ì§€ì›í•˜ì—¬ ë…¸ë“œ ê°„ í†µì‹ ì„ ê·¹ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤:

```yaml
# Step 1: EFAê°€ ì§€ì›ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…
# p4d.24xlarge (8x A100), p5.48xlarge (8x H100)ëŠ” EFA ë‚´ì¥

# Step 2: EKS ë³´ì•ˆ ê·¸ë£¹ì— EFA í¬íŠ¸ ì—´ê¸°
apiVersion: ec2.amazonaws.com/v1
kind: SecurityGroup
metadata:
  name: efa-sg
spec:
  groupDescription: "EFA í†µì‹  í—ˆìš©"
  ingress:
    - IpProtocol: "-1"  # ëª¨ë“  í”„ë¡œí† ì½œ
      CidrIp: "10.0.0.0/16"  # VPC CIDR
      Description: "EFA ë‚´ë¶€ í†µì‹ "

---

# Step 3: Karpenter EC2NodeClassì—ì„œ EFA í™œì„±í™”
apiVersion: karpenter.k8s.aws/v1beta1
kind: EC2NodeClass
metadata:
  name: gpu-efa-class
spec:
  amiFamily: GPU
  securityGroupSelector:
    Name: efa-sg
  subnetSelector:
    Name: "private-*"

  userData: |
    #!/bin/bash
    # EFA ë“œë¼ì´ë²„ ì„¤ì¹˜
    cd /opt/aws-ofi-nccl
    ./install.sh

    # ì»¤ë„ íŒŒë¼ë¯¸í„° ìµœì í™”
    echo "net.core.rmem_max=268435456" >> /etc/sysctl.conf
    echo "net.core.wmem_max=268435456" >> /etc/sysctl.conf
    echo "net.ipv4.tcp_rmem=4096 87380 268435456" >> /etc/sysctl.conf
    echo "net.ipv4.tcp_wmem=4096 65536 268435456" >> /etc/sysctl.conf
    sysctl -p

---

# Step 4: Pod ë°°í¬ ì‹œ EFA í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
apiVersion: v1
kind: Pod
metadata:
  name: distributed-training-with-efa
  namespace: ai-training
spec:
  containers:
    - name: training
      image: nvcr.io/nvidia/pytorch:24.02-py3
      env:
        # EFA ì„¤ì •
        - name: FI_PROVIDER
          value: "efa"
        - name: FI_EFA_USE_DEVICE_RDMA
          value: "1"
        - name: FI_EFA_FORK_SAFE
          value: "1"
        # NCCL ìµœì í™”
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_PROTO
          value: "simple"
        - name: NCCL_ALGO
          value: "Ring"
        # ëŒ€ì—­í­ ìµœì í™”
        - name: NCCL_MIN_NCHANNELS
          value: "16"
        - name: NCCL_MAX_NCHANNELS
          value: "16"
        # AWS EFA ê²½ë¡œ ì„¤ì •
        - name: LD_LIBRARY_PATH
          value: "/opt/aws-ofi-nccl/lib:$LD_LIBRARY_PATH"
        - name: PATH
          value: "/opt/aws-ofi-nccl/bin:$PATH"

      resources:
        requests:
          memory: "200Gi"
          cpu: "32"
        claims:
          - name: gpus

  resourceClaims:
    - name: gpus
      source:
        resourceClaimTemplateName: efa-gpu-template

  affinity:
    # ê°™ì€ AZì˜ ë…¸ë“œì— ë°°ì¹˜ (EFA ëŒ€ì—­í­ ìµœëŒ€í™”)
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: topology.kubernetes.io/zone
                  operator: In
                  values: ["us-east-1a"]
            topologyKey: topology.kubernetes.io/zone
```

#### NCCL ì„±ëŠ¥ íŠœë‹ íŒŒë¼ë¯¸í„° ìƒì„¸ ì„¤ëª…

```yaml
# NCCL í™˜ê²½ ë³€ìˆ˜ ì™„ë²½ ê°€ì´ë“œ

# 1. ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
export NCCL_ALGO=Ring           # Ring (ê¸°ë³¸), Tree, CollNet
export NCCL_ALGO_ALL=Ring       # AllReduce ì•Œê³ ë¦¬ì¦˜ ì§€ì •
export NCCL_ALGO_TREE=Tree      # Tree ì•Œê³ ë¦¬ì¦˜ ê°•ì œ

# 2. í”„ë¡œí† ì½œ ì„ íƒ
export NCCL_PROTO=Simple        # Simple (ê¸°ë³¸) ë˜ëŠ” LL (Low Latency)
# LL: ì§§ì€ ë©”ì‹œì§€ ìµœì í™”, ê¸´ ë©”ì‹œì§€ëŠ” Simple ì‚¬ìš© ê°€ëŠ¥

# 3. ì±„ë„ ì„¤ì • (ë§¤ìš° ì¤‘ìš”)
export NCCL_MIN_NCHANNELS=4     # ìµœì†Œ ì±„ë„ ìˆ˜ (ê¸°ë³¸ 4)
export NCCL_MAX_NCHANNELS=8     # ìµœëŒ€ ì±„ë„ ìˆ˜ (ê¸°ë³¸ 32)
# ì±„ë„ ìˆ˜ = ë³‘ë ¬ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ìˆ˜
# ë” ë§ì€ ì±„ë„ = ë” ë†’ì€ ëŒ€ì—­í­ í™œìš©
# 8GPU ì´ìƒì—ì„œëŠ” 8-16 ê¶Œì¥

# 4. ë²„í¼ í¬ê¸°
export NCCL_BUFFSIZE=2097152    # ê¸°ë³¸ 2MB, 1MB-4MB ê¶Œì¥
# í° ë²„í¼ = ë” ë†’ì€ ì²˜ë¦¬ëŸ‰ (ë©”ëª¨ë¦¬ ì¦ê°€)
# ì‘ì€ ë²„í¼ = ë‚®ì€ ë ˆì´í„´ì‹œ (ì²˜ë¦¬ëŸ‰ ê°ì†Œ)

# 5. ë””ë²„ê·¸ ì„¤ì •
export NCCL_DEBUG=INFO          # TRACE, DEBUG, INFO, WARN
export NCCL_DEBUG_FILE=/var/log/nccl-debug.txt
export NCCL_DEBUG_SUBSYS=ALL    # ëª¨ë“  ì„œë¸Œì‹œìŠ¤í…œ ì¶”ì 

# 6. ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤
export NCCL_SOCKET_IFNAME=eth0  # ì‚¬ìš©í•  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤
export NCCL_IB_DISABLE=0        # InfiniBand ì‚¬ìš©
export NCCL_IB_GID_INDEX=3      # GID ì¸ë±ìŠ¤ (InfiniBand)

# 7. EFA ì„¤ì • (AWS)
export FI_PROVIDER=efa
export FI_EFA_USE_DEVICE_RDMA=1
export FI_EFA_FORK_SAFE=1
export NCCL_LIBNAME=libnccl.so.2

# 8. ì»¤ë„ ìµœì í™”
export NCCL_CHECKS_DISABLE=0    # ì•ˆì „ ê²€ì‚¬ í™œì„±í™” (í”„ë¡œë•ì…˜)
export NCCL_COMM_BLOCKING_WAIT=0
export NCCL_ASYNC_ERROR_HANDLING=1

# 9. P2P ì„¤ì •
export NCCL_P2P_DISABLE=0       # GPU P2P í†µì‹  í™œì„±í™”
export NCCL_P2P_LEVEL=SYS       # P2P ë ˆë²¨: LOC (ë¡œì»¬), SYS (ì‹œìŠ¤í…œ), PHB, PIU

# 10. íƒ€ì„ì•„ì›ƒ ì„¤ì •
export NCCL_COMM_WAIT_TIMEOUT=0 # 0 = ë¬´í•œ ëŒ€ê¸°
```

**NCCL ì±„ë„ ì„¤ì • ê²°ì • ê°€ì´ë“œ:**

```mermaid
graph TB
    A["NCCL_MIN_NCHANNELS ê²°ì •"] --> B{"GPU ìˆ˜"}

    B -->|4ê°œ ì´í•˜| C["ê¸°ë³¸ê°’ 4 ì‚¬ìš©<br/>ì¶©ë¶„í•œ ëŒ€ì—­í­"]
    B -->|8ê°œ| D["8ë¡œ ì„¤ì •<br/>ëŒ€ì—­í­ ì¦ê°€"]
    B -->|16ê°œ ì´ìƒ| E["16-32ë¡œ ì„¤ì •<br/>ìµœëŒ€ ë³‘ë ¬í™”"]

    F["NCCL_MAX_NCHANNELS ê²°ì •"] --> G{"ë°ì´í„° í¬ê¸°"}

    G -->|ì‘ìŒ < 10MB| H["4-8 ì„¤ì •<br/>ë©”ëª¨ë¦¬ ì ˆì•½"]
    G -->|ì¤‘ê°„ 10-100MB| I["8-16 ì„¤ì •<br/>ê· í˜•"]
    G -->|í¼ > 100MB| J["16-32 ì„¤ì •<br/>ìµœëŒ€ ì²˜ë¦¬ëŸ‰"]

    style C fill:#76b900
    style D fill:#4ecdc4
    style E fill:#326ce5
    style J fill:#326ce5
```

#### NCCL í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí‚¹

NCCL ê³µì‹ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤:

```bash
#!/bin/bash

# 1. nccl-tests ì„¤ì¹˜
git clone https://github.com/NVIDIA/nccl-tests.git
cd nccl-tests
make CUDA_HOME=/usr/local/cuda

# 2. AllReduce ë²¤ì¹˜ë§ˆí¬ (ë¶„ì‚° í•™ìŠµì—ì„œ ê°€ì¥ ì¤‘ìš”)
./build/all_reduce_perf -b 8 -e 1G -f 2 -g 8

# ì¶œë ¥ í•´ì„:
# -b 8: ì‹œì‘ ë°”ì´íŠ¸ (8bytes)
# -e 1G: ì¢…ë£Œ ë°”ì´íŠ¸ (1GB)
# -f 2: ì›Œë°ì—… ë°˜ë³µ ìˆ˜
# -g 8: ì‚¬ìš©í•  GPU ìˆ˜

# 3. AllGather ë²¤ì¹˜ë§ˆí¬
./build/all_gather_perf -b 8 -e 1G -f 2 -g 8

# 4. ReduceScatter ë²¤ì¹˜ë§ˆí¬
./build/reduce_scatter_perf -b 8 -e 1G -f 2 -g 8

# 5. Broadcast ë²¤ì¹˜ë§ˆí¬
./build/broadcast_perf -b 8 -e 1G -f 2 -g 8

# 6. ë¶„ì‚° í…ŒìŠ¤íŠ¸ (ë©€í‹° ë…¸ë“œ)
# ë…¸ë“œ 1ì—ì„œ:
./build/all_reduce_perf -b 8 -e 1G -f 2 -g 8

# ë…¸ë“œ 2ì—ì„œ:
./build/all_reduce_perf -b 8 -e 1G -f 2 -g 8

# 7. ì„±ëŠ¥ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
cat > analyze_nccl.py << 'EOF'
import subprocess
import re

operations = ['all_reduce', 'all_gather', 'reduce_scatter', 'broadcast']

for op in operations:
    result = subprocess.run(
        [f'./build/{op}_perf', '-b', '1M', '-e', '1G', '-f', '2', '-g', '8'],
        capture_output=True,
        text=True
    )

    # ìµœëŒ€ ì²˜ë¦¬ëŸ‰ ì¶”ì¶œ
    for line in result.stdout.split('\n'):
        if 'Avg bus bandwidth' in line:
            print(f"{op}: {line.strip()}")
EOF

python analyze_nccl.py
```

**ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ í•´ì„:**

```
# ì¢‹ì€ ê²°ê³¼ (8x A100 with NVLink):
# AllReduce Avg bus bandwidth: 1584.78 Gbps

# ë‚˜ìœ ê²°ê³¼ (Ethernetë§Œ ì‚¬ìš©):
# AllReduce Avg bus bandwidth: 15.24 Gbps

# ì„±ëŠ¥ ë¹„ìœ¨: 1584 / 15 = ì•½ 100ë°° ì°¨ì´!
```

#### NCCL ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

##### 1. íƒ€ì„ì•„ì›ƒ ì´ìŠˆ

```
ì—ëŸ¬: NCCL timeout
ì›ì¸:
- GPU ê°„ ì—°ê²° ë¬¸ì œ
- ë„¤íŠ¸ì›Œí¬ ì§€ì—°
- ë¶ˆì¼ì¹˜í•œ GPU ë©”ëª¨ë¦¬

í•´ê²° ë°©ë²•:
```

```yaml
# íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¦ê°€
apiVersion: v1
kind: ConfigMap
metadata:
  name: nccl-timeout-config
  namespace: ai-training
data:
  nccl-env.sh: |
    # ê¸°ë³¸ 20ì´ˆ íƒ€ì„ì•„ì›ƒì„ 300ì´ˆë¡œ ì¦ê°€
    export NCCL_COMM_WAIT_TIMEOUT=300

    # ë””ë²„ê·¸ í™œì„±í™”
    export NCCL_DEBUG=INFO
    export NCCL_DEBUG_FILE=/var/log/nccl-debug-%p.txt

    # ë™ê¸° ì—ëŸ¬ ì²˜ë¦¬
    export NCCL_ASYNC_ERROR_HANDLING=1

---

# Pod ì ìš©
apiVersion: batch/v1
kind: Job
metadata:
  name: nccl-timeout-test
spec:
  template:
    spec:
      containers:
        - name: training
          image: nccl-training:latest
          env:
            - name: NCCL_COMM_WAIT_TIMEOUT
              value: "300"
            - name: NCCL_DEBUG
              value: "INFO"
          volumeMounts:
            - name: logs
              mountPath: /var/log
      volumes:
        - name: logs
          emptyDir: {}
```

##### 2. ì„±ëŠ¥ ì €í•˜ ì›ì¸ ì§„ë‹¨

```bash
#!/bin/bash

# NCCL ì„±ëŠ¥ ì €í•˜ ì§„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸

echo "=== NCCL ì„±ëŠ¥ ì§„ë‹¨ ==="

# 1. GPU ìƒíƒœ í™•ì¸
echo "1. GPU ìƒíƒœ:"
nvidia-smi

# 2. NVLink ìƒíƒœ í™•ì¸
echo "2. NVLink/NVSwitch ìƒíƒœ:"
nvidia-smi nvlink -st

# 3. GPU P2P ì§€ì› í™•ì¸
echo "3. GPU P2P ì§€ì›:"
nvidia-smi topo -m

# 4. ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ í™•ì¸
echo "4. ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤:"
ip link show
ethtool -i eth0

# 5. EFA ìƒíƒœ í™•ì¸ (AWS)
echo "5. EFA ìƒíƒœ:"
fi_info -p efa | head -20

# 6. NCCL ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
echo "6. NCCL ì„±ëŠ¥ ì¸¡ì •:"
cd /path/to/nccl-tests
./build/all_reduce_perf -b 1G -e 1G -f 2 -g 8

# ê²°ê³¼:
# - NVSwitch ì‚¬ìš©: > 500 Gbps
# - NVLink ì‚¬ìš©: > 150 Gbps
# - EFA ì‚¬ìš©: > 50 Gbps
# - Ethernet: < 10 Gbps
```

**ì„±ëŠ¥ ì €í•˜ ì›ì¸ ë° í•´ê²°ì±…:**

| ì¦ìƒ | ê°€ëŠ¥í•œ ì›ì¸ | í•´ê²°ì±… |
| --- | --- | --- |
| ì˜ˆìƒë³´ë‹¤ ëŠë¦° ì „ì†¡ ì†ë„ | NVLink ë¯¸ì‚¬ìš© | `nvidia-smi nvlink -st` í™•ì¸, GPU ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸ |
| ë¶ˆê· í˜• ì²˜ë¦¬ ì†ë„ | ì¼ë¶€ GPUë§Œ ëŠë¦¼ | GPU êµì²´, ì—´ ë¬¸ì œ í™•ì¸ |
| ì£¼ê¸°ì  ì†ë„ ì €í•˜ | ë„¤íŠ¸ì›Œí¬ í˜¼ì¡ | ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì¦ê°€, EFA í™œì„±í™” |
| ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬ | ë²„í¼ í¬ê¸° ì´ˆê³¼ | `NCCL_BUFFSIZE` ê°ì†Œ |

#### NCCL í†µì‹  íŒ¨í„´ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph "AllReduce íŒ¨í„´ (Data Parallelism)"
        AR1["GPU0: [1,2,3]<br/>GPU1: [4,5,6]<br/>GPU2: [7,8,9]"]
        AR2["Reduce:<br/>í•©ì‚°"]
        AR3["AllReduce:<br/>ê²°ê³¼ ë°°í¬"]
        AR4["GPU0: [12,15,18]<br/>GPU1: [12,15,18]<br/>GPU2: [12,15,18]"]

        AR1 --> AR2 --> AR3 --> AR4
    end

    subgraph "AllGather íŒ¨í„´ (Batch Normalization)"
        AG1["GPU0: [a]<br/>GPU1: [b]<br/>GPU2: [c]"]
        AG2["Gather"]
        AG3["GPU0: [a,b,c]<br/>GPU1: [a,b,c]<br/>GPU2: [a,b,c]"]

        AG1 --> AG2 --> AG3
    end

    subgraph "Broadcast íŒ¨í„´ (Model Initialization)"
        BC1["GPU0: model_v1<br/>GPU1: old<br/>GPU2: old"]
        BC2["Broadcast from 0"]
        BC3["GPU0: model_v1<br/>GPU1: model_v1<br/>GPU2: model_v1"]

        BC1 --> BC2 --> BC3
    end

    style AR4 fill:#76b900
    style AG3 fill:#76b900
    style BC3 fill:#76b900
```

### í†µí•© ë°ì´í„° íë¦„

ì „ì²´ ì˜¤í”ˆì†ŒìŠ¤ ìŠ¤íƒì´ Kubernetesì—ì„œ ì–´ë–»ê²Œ í˜‘ë ¥í•˜ëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ë°ì´í„° íë¦„ì…ë‹ˆë‹¤:

```mermaid
sequenceDiagram
    participant User as User
    participant LiteLLM as LiteLLM Proxy
    participant llmd as llm-d Router
    participant vLLM as vLLM Instance
    participant Milvus as Milvus
    participant LangFuse as LangFuse
    participant DCGM as DCGM Exporter
    participant Karpenter as Karpenter

    User->>LiteLLM: RAG Query
    LiteLLM->>LangFuse: Start Trace
    LiteLLM->>Milvus: Vector Search
    Milvus-->>LiteLLM: Relevant Documents
    LiteLLM->>llmd: LLM Request + Context
    llmd->>llmd: Prefix Cache Check
    llmd->>vLLM: Route to Optimal Instance
    
    Note over vLLM,DCGM: GPU Metrics Collection
    vLLM->>DCGM: GPU Utilization
    DCGM->>Karpenter: Metrics for Scaling
    
    vLLM-->>llmd: Generated Response
    llmd-->>LiteLLM: Response
    LiteLLM->>LangFuse: End Trace + Tokens
    LiteLLM-->>User: Final Response

    Note over Karpenter: Auto-scale if needed
    Karpenter->>Karpenter: Provision/Consolidate Nodes
```

### ì˜¤í”ˆì†ŒìŠ¤ í†µí•©ì˜ í•µì‹¬ ì´ì 

:::tip Kubernetes ì¤‘ì‹¬ í†µí•©ì˜ ì´ì 
1. **ì„ ì–¸ì  ê´€ë¦¬**: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ YAMLë¡œ ì •ì˜í•˜ê³  GitOpsë¡œ ê´€ë¦¬
2. **ìë™ ë³µêµ¬**: Kubernetesì˜ ìê°€ ì¹˜ìœ  ê¸°ëŠ¥ìœ¼ë¡œ ì¥ì•  ìë™ ë³µêµ¬
3. **ìˆ˜í‰ í™•ì¥**: HPA/KEDA + Karpenterë¡œ End-to-End ìë™ ìŠ¤ì¼€ì¼ë§
4. **í†µí•© ê´€ì¸¡ì„±**: Prometheus + Grafanaë¡œ ì „ì²´ ìŠ¤íƒ ëª¨ë‹ˆí„°ë§
5. **ë„¤íŠ¸ì›Œí¬ í†µí•©**: Service Mesh, Gateway APIë¡œ íŠ¸ë˜í”½ ê´€ë¦¬ í†µí•©
:::

| í†µí•© ì˜ì—­ | ê´€ë ¨ ì˜¤í”ˆì†ŒìŠ¤ | Kubernetes ë¦¬ì†ŒìŠ¤ | ìë™í™” ìˆ˜ì¤€ |
| --- | --- | --- | --- |
| **LLM Observability** | LangFuse, LangSmith, RAGAS | Deployment, CronJob | ë†’ìŒ |
| **Inference Gateway** | LiteLLM, Kgateway | Deployment, Service, HTTPRoute | ë†’ìŒ |
| **ë¶„ì‚° ì¶”ë¡ ** | llm-d, vLLM | Deployment, StatefulSet | ë†’ìŒ |
| **ë²¡í„° ê²€ìƒ‰** | Milvus | Operator, StatefulSet | ì¤‘ê°„ |
| **GPU ì¸í”„ë¼** | DRA, DCGM, NCCL | DaemonSet, ResourceClaim | ë†’ìŒ |
| **ë…¸ë“œ ê´€ë¦¬** | Karpenter | NodePool, EC2NodeClass | ë§¤ìš° ë†’ìŒ |

---

## EKS ê¸°ë°˜ Agentic AI í”Œë«í¼ ê°„í¸ êµ¬ì¶•

ì•ì„œ ì†Œê°œí•œ Kubernetes ë„¤ì´í‹°ë¸Œ ì†”ë£¨ì…˜ë“¤ì€ **Amazon EKS í™˜ê²½ì—ì„œ ì†ì‰½ê²Œ ë°°í¬**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. EKS Auto Modeì™€ AWS ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ì˜ í†µí•©ì„ í†µí•´ **ë³µì¡í•œ ì¸í”„ë¼ êµ¬ì„± ì—†ì´** ì™„ì „í•œ Agentic AI í”Œë«í¼ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### EKSì˜ ê°„í¸ ë°°í¬ ì´ì 

```mermaid
graph LR
    subgraph "ì „í†µì  êµ¬ì¶• ë°©ì‹"
        T1["ì¸í”„ë¼ ì„¤ê³„<br/>2-4ì£¼"]
        T2["ì»´í¬ë„ŒíŠ¸ ì„¤ì¹˜<br/>2-3ì£¼"]
        T3["í†µí•© í…ŒìŠ¤íŠ¸<br/>1-2ì£¼"]
        T4["ìš´ì˜ ì¤€ë¹„<br/>1-2ì£¼"]
        T1 --> T2 --> T3 --> T4
    end

    subgraph "EKS ê¸°ë°˜ êµ¬ì¶•"
        E1["EKS Auto Mode<br/>í´ëŸ¬ìŠ¤í„° ìƒì„±<br/>1ì¼"]
        E2["Helm/Addon<br/>ì†”ë£¨ì…˜ ë°°í¬<br/>2-3ì¼"]
        E3["ì›Œí¬ë¡œë“œ ë°°í¬<br/>1-2ì¼"]
        E1 --> E2 --> E3
    end

    style T4 fill:#ff6b6b
    style E3 fill:#4ecdc4
```

| êµ¬ì¶• ë°©ì‹ | ì†Œìš” ì‹œê°„ | ìš´ì˜ ë³µì¡ë„ | ë¹„ìš© íš¨ìœ¨ì„± |
| --- | --- | --- | --- |
| **ì „í†µì  ë°©ì‹** | 6-11ì£¼ | ë†’ìŒ | ë‚®ìŒ |
| **EKS ê¸°ë°˜** | 1-2ì£¼ | ë‚®ìŒ | ë†’ìŒ |

### ì†”ë£¨ì…˜ë³„ EKS ë°°í¬ ë°©ë²•

| ì†”ë£¨ì…˜ | ë°°í¬ ë°©ë²• | EKS í†µí•© ì´ì  |
| --- | --- | --- |
| **Karpenter** | EKS Auto Mode (ìë™) | ì„¤ì¹˜/êµ¬ì„± ë¶ˆí•„ìš”, ìë™ ì—…ê·¸ë ˆì´ë“œ |
| **Kgateway** | Helm Chart | ALB Controller ì—°ë™, ACM ì¸ì¦ì„œ ìë™ ê´€ë¦¬ |
| **LiteLLM** | Helm Chart | Secrets Manager ì—°ë™, IAM ê¸°ë°˜ ì¸ì¦ |
| **vLLM** | Helm Chart | GPU NodePool ìë™ í”„ë¡œë¹„ì €ë‹ |
| **llm-d** | Helm Chart | Karpenter ì—°ë™ ìë™ ìŠ¤ì¼€ì¼ë§ |
| **LangFuse** | Helm Chart | RDS/Aurora ì—°ë™, S3 ìŠ¤í† ë¦¬ì§€ |
| **KAgent** | Helm Chart | Pod Identity ê¸°ë°˜ AWS ì„œë¹„ìŠ¤ ì ‘ê·¼ |
| **KEDA** | EKS Addon | ê´€ë¦¬í˜• ì„¤ì¹˜, CloudWatch ë©”íŠ¸ë¦­ ì—°ë™ |

### EKS í†µí•© ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "AWS Managed Services"
        EKS["Amazon EKS<br/>Auto Mode"]
        ALB["Application<br/>Load Balancer"]
        RDS["Amazon RDS<br/>(LangFuse DB)"]
        S3["Amazon S3<br/>(Model Storage)"]
        SM["Secrets Manager"]
        CW["CloudWatch"]
    end

    subgraph "EKS Cluster"
        subgraph "Karpenter ê´€ë¦¬ ë…¸ë“œ"
            GPU["GPU NodePool"]
            CPU["CPU NodePool"]
        end
        
        subgraph "AI Platform Stack"
            KGW["Kgateway"]
            LITE["LiteLLM"]
            VLLM["vLLM"]
            LLMD["llm-d"]
            KAGENT["KAgent"]
            LF["LangFuse"]
        end
    end

    EKS --> GPU & CPU
    ALB --> KGW
    KGW --> LITE --> VLLM
    VLLM --> GPU
    LF --> RDS
    VLLM --> S3
    LITE --> SM
    VLLM --> CW

    style EKS fill:#ff9900
    style ALB fill:#ff9900
    style RDS fill:#ff9900
    style S3 fill:#ff9900
```

### ê°„í¸ ë°°í¬ ì˜ˆì‹œ

EKS Auto Mode í´ëŸ¬ìŠ¤í„°ì—ì„œ ì „ì²´ Agentic AI ìŠ¤íƒì„ ë°°í¬í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤:

```bash
# 1. EKS Auto Mode í´ëŸ¬ìŠ¤í„° ìƒì„± (Karpenter ìë™ í¬í•¨)
eksctl create cluster --name ai-platform --region us-west-2 --auto-mode

# 2. GPU NodePool ì¶”ê°€
kubectl apply -f gpu-nodepool.yaml

# 3. AI Platform ì†”ë£¨ì…˜ ìŠ¤íƒ ë°°í¬
helm repo add kgateway https://kgateway.io/charts
helm repo add litellm https://litellm.github.io/helm
helm repo add vllm https://vllm-project.github.io/helm
helm repo add langfuse https://langfuse.github.io/helm

helm install kgateway kgateway/kgateway -n ai-gateway --create-namespace
helm install litellm litellm/litellm -n ai-inference --create-namespace
helm install vllm vllm/vllm -n ai-inference
helm install langfuse langfuse/langfuse -n observability --create-namespace

# 4. KEDA ì„¤ì¹˜ (EKS Addon)
aws eks create-addon --cluster-name ai-platform --addon-name keda
```

### EKS ê¸°ë°˜ êµ¬ì¶•ì˜ í•µì‹¬ ì´ì 

:::tip EKSë¡œ Agentic AI í”Œë«í¼ì„ êµ¬ì¶•í•˜ë©´
1. **ì¸í”„ë¼ ìë™í™”**: EKS Auto Mode + Karpenterë¡œ GPU ë…¸ë“œ ìë™ ê´€ë¦¬
2. **ê°„í¸í•œ ë°°í¬**: Helm Chartì™€ EKS Addonìœ¼ë¡œ ì†”ë£¨ì…˜ ìŠ¤íƒ ì›í´ë¦­ ë°°í¬
3. **AWS ì„œë¹„ìŠ¤ í†µí•©**: RDS, S3, Secrets Manager, CloudWatchì™€ ë„¤ì´í‹°ë¸Œ ì—°ë™
4. **ë³´ì•ˆ ê°•í™”**: Pod Identity, Security Groups for Pods, ì•”í˜¸í™” ìë™ ì ìš©
5. **ë¹„ìš© ìµœì í™”**: Spot ì¸ìŠ¤í„´ìŠ¤, Savings Plans, Consolidation ìë™ í™œìš©
:::

:::tip EKS Auto Mode ì‹œì‘í•˜ê¸°
EKS Auto ModeëŠ” AWS ì½˜ì†”, eksctl, ë˜ëŠ” Terraformì—ì„œ ê°„ë‹¨íˆ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
```bash
# eksctlë¡œ EKS Auto Mode í´ëŸ¬ìŠ¤í„° ìƒì„±
eksctl create cluster --name ai-platform --region us-west-2 --auto-mode
```
í´ëŸ¬ìŠ¤í„° ìƒì„± í›„ GPU NodePoolë§Œ ì¶”ê°€í•˜ë©´ ì¦‰ì‹œ AI ì›Œí¬ë¡œë“œë¥¼ ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

:::info ë‹¤ìŒ ë‹¨ê³„
ì´ ë¬¸ì„œì—ì„œ ì†Œê°œí•œ ê° ë„ì „ê³¼ì œì— ëŒ€í•œ ìƒì„¸í•œ êµ¬í˜„ ê°€ì´ë“œëŠ” ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ì¡°í•˜ì„¸ìš”:

- [GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬](./gpu-resource-management.md) - Karpenter ê¸°ë°˜ GPU í´ëŸ¬ìŠ¤í„° ë™ì  ë¦¬ì†ŒìŠ¤ í• ë‹¹
- [Inference Gateway](./inference-gateway-routing.md) - Kgateway ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…
- [Agent ëª¨ë‹ˆí„°ë§](./agent-monitoring.md) - LangFuse, LangSmith í†µí•©
- [NeMo í”„ë ˆì„ì›Œí¬](./nemo-framework.md) - FM íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸

:::

---

## ê²°ë¡ : Kubernetes + EKS Auto Modeë¡œ ì™„ì„±í•˜ëŠ” AI ì¸í”„ë¼ ìë™í™”

Agentic AI Platform êµ¬ì¶•ì˜ 4ê°€ì§€ í•µì‹¬ ë„ì „ê³¼ì œëŠ” **í´ë¼ìš°ë“œ ì¸í”„ë¼ ìë™í™”ì™€ AI í”Œë«í¼ì˜ ìœ ê¸°ì  í†µí•©**ì„ í†µí•´ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ **EKS Auto Mode**ëŠ” Karpenterë¥¼ í¬í•¨í•œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ìë™ìœ¼ë¡œ ê´€ë¦¬í•˜ì—¬ **ì™„ì „ ìë™í™”ì˜ ë§ˆì§€ë§‰ í¼ì¦**ì„ ì™„ì„±í•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "ë¬¸ì œ ì¸ì‹"
        P["Agentic AI í”Œë«í¼<br/>4ê°€ì§€ ë„ì „ê³¼ì œ"]
    end

    subgraph "í•´ê²° í”„ë ˆì„ì›Œí¬"
        K8S["Kubernetes<br/>ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"]
        AUTO["EKS Auto Mode<br/>ì™„ì „ ê´€ë¦¬í˜• + Karpenter ìë™í™”"]
        AWS["AWS ì¸í”„ë¼<br/>GPU, ë„¤íŠ¸ì›Œí¬, ìŠ¤í† ë¦¬ì§€"]
    end

    subgraph "ë‹¬ì„± ëª©í‘œ"
        G1["âœ… ì™„ì „ ìë™í™”ëœ GPU ê´€ë¦¬"]
        G2["âœ… ë¹„ìš© 50-70% ì ˆê°"]
        G3["âœ… í”„ë¡œë¹„ì €ë‹ ì‹œê°„ 50% ë‹¨ì¶•"]
        G4["âœ… ìš´ì˜ ë¶€ë‹´ 80% ê°ì†Œ"]
    end

    P --> K8S
    K8S --> AUTO
    AUTO --> AWS
    AWS --> G1 & G2 & G3 & G4

    style P fill:#ff6b6b
    style K8S fill:#326ce5
    style AUTO fill:#ff9900
    style G1 fill:#4ecdc4
    style G2 fill:#4ecdc4
    style G3 fill:#4ecdc4
    style G4 fill:#4ecdc4
```

### í•µì‹¬ ë©”ì‹œì§€

1. **KubernetesëŠ” AI ì¸í”„ë¼ì˜ í•„ìˆ˜ ê¸°ë°˜**: ì„ ì–¸ì  ë¦¬ì†ŒìŠ¤ ê´€ë¦¬, ìë™ ìŠ¤ì¼€ì¼ë§, Operator íŒ¨í„´ì„ í†µí•´ ë³µì¡í•œ AI ì›Œí¬ë¡œë“œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬
2. **EKS Auto Modeê°€ ì™„ì „ ìë™í™” ì‹¤í˜„**: Karpenter, VPC CNI, EBS CSI Driver ë“± í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ìë™ ê´€ë¦¬ë¡œ ìš´ì˜ ë¶€ë‹´ ëŒ€í­ ê°ì†Œ
3. **KarpenterëŠ” GPU ì¸í”„ë¼ ìë™í™”ì˜ í•µì‹¬**: Just-in-Time í”„ë¡œë¹„ì €ë‹, Spot ì¸ìŠ¤í„´ìŠ¤, Consolidationìœ¼ë¡œ ë¹„ìš©ê³¼ ì„±ëŠ¥ ìµœì í™”
4. **AWS ì¸í”„ë¼ í†µí•©ì´ ì‹œë„ˆì§€ ê·¹ëŒ€í™”**: EFA ë„¤íŠ¸ì›Œí¬, ë‹¤ì–‘í•œ GPU ì¸ìŠ¤í„´ìŠ¤, FSx ìŠ¤í† ë¦¬ì§€ì™€ì˜ ê¸´ë°€í•œ í†µí•©

### EKS Auto Mode: ê¶Œì¥ ì‹œì‘ì 

ìƒˆë¡œìš´ Agentic AI í”Œë«í¼ì„ êµ¬ì¶•í•œë‹¤ë©´ **EKS Auto Mode**ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤:

| ì´ì  | ì„¤ëª… |
| --- | --- |
| **ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥** | Karpenter ì„¤ì¹˜/êµ¬ì„± ì—†ì´ í´ëŸ¬ìŠ¤í„° ìƒì„± ì¦‰ì‹œ GPU ì›Œí¬ë¡œë“œ ë°°í¬ |
| **ìë™ ì—…ê·¸ë ˆì´ë“œ** | Karpenter, CNI, CSI ë“± í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ìë™ ì—…ë°ì´íŠ¸ |
| **ë³´ì•ˆ íŒ¨ì¹˜ ìë™í™”** | ë³´ì•ˆ ì·¨ì•½ì  íŒ¨ì¹˜ ìë™ ì ìš© |
| **ì»¤ìŠ¤í…€ í™•ì¥ ê°€ëŠ¥** | GPU NodePool, EFA NodeClass ë“± í•„ìš”ì‹œ ì»¤ìŠ¤í…€ ì„¤ì • ì¶”ê°€ |

### ë„ì „ê³¼ì œë³„ í•´ê²° ë°©ì•ˆ ìµœì¢… ìš”ì•½

| ë„ì „ê³¼ì œ | Kubernetes ê¸°ë°˜ | EKS Auto Mode + Karpenter | ê¸°ëŒ€ íš¨ê³¼ |
| --- | --- | --- | --- |
| **GPU ëª¨ë‹ˆí„°ë§** | DCGM + Prometheus | NodePool ê¸°ë°˜ í†µí•© ê´€ë¦¬ | ë¦¬ì†ŒìŠ¤ í™œìš©ë¥  40% í–¥ìƒ |
| **ë™ì  ìŠ¤ì¼€ì¼ë§** | HPA + KEDA | Just-in-Time í”„ë¡œë¹„ì €ë‹ (ìë™ êµ¬ì„±) | í”„ë¡œë¹„ì €ë‹ ì‹œê°„ 50% ë‹¨ì¶• |
| **ë¹„ìš© ì»¨íŠ¸ë¡¤** | ë„¤ì„ìŠ¤í˜ì´ìŠ¤ Quota | Spot + Consolidation (ìë™ í™œì„±í™”) | ë¹„ìš© 50-70% ì ˆê° |
| **FM íŒŒì¸íŠœë‹** | Kubeflow Operator | Training NodePool + EFA | í•™ìŠµ íš¨ìœ¨ì„± 30% í–¥ìƒ |

### í•µì‹¬ ê¶Œì¥ì‚¬í•­

1. **EKS Auto Modeë¡œ ì‹œì‘**: ìƒˆ í´ëŸ¬ìŠ¤í„°ëŠ” Auto Modeë¡œ ìƒì„±í•˜ì—¬ Karpenter ìë™ êµ¬ì„± í™œìš©
2. **GPU NodePool ì»¤ìŠ¤í…€ ì •ì˜**: ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë§ëŠ” GPU NodePool ì¶”ê°€ (ì¶”ë¡ /í•™ìŠµ/ì‹¤í—˜ ë¶„ë¦¬)
3. **Spot ì¸ìŠ¤í„´ìŠ¤ ì ê·¹ í™œìš©**: ì¶”ë¡  ì›Œí¬ë¡œë“œì˜ 70% ì´ìƒì„ Spotìœ¼ë¡œ ìš´ì˜
4. **Consolidation ê¸°ë³¸ í™œì„±í™”**: EKS Auto Modeì—ì„œ ìë™ í™œì„±í™”ëœ Consolidation í™œìš©
5. **KEDA ì—°ë™**: ë©”íŠ¸ë¦­ ê¸°ë°˜ Pod ìŠ¤ì¼€ì¼ë§ê³¼ Karpenter ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì—°ê³„
6. **EFA NodeClass ì¶”ê°€**: ë¶„ì‚° í•™ìŠµ ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ ê³ ì„±ëŠ¥ ë„¤íŠ¸ì›Œí¬ ì„¤ì •

---

## ì°¸ê³  ìë£Œ

### Kubernetes ë° ì¸í”„ë¼
- [Kubernetes ê³µì‹ ë¬¸ì„œ](https://kubernetes.io/docs/)
- [Karpenter ê³µì‹ ë¬¸ì„œ](https://karpenter.sh/docs/)
- [Amazon EKS Best Practices Guide](https://aws.github.io/aws-eks-best-practices/)
- [NVIDIA GPU Operator Documentation](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html)
- [KEDA - Kubernetes Event-driven Autoscaling](https://keda.sh/)

### ëª¨ë¸ ì„œë¹™ ë° ì¶”ë¡ 
- [vLLM Documentation](https://docs.vllm.ai/)
- [llm-d Project](https://github.com/llm-d/llm-d)
- [Kgateway Documentation](https://kgateway.io/docs/)
- [LiteLLM Documentation](https://docs.litellm.ai/)

### LLM Observability
- [LangFuse Documentation](https://langfuse.com/docs)
- [LangSmith Documentation](https://docs.smith.langchain.com/)

### Agent í”„ë ˆì„ì›Œí¬ ë° í•™ìŠµ
- [KAgent - Kubernetes Agent Framework](https://github.com/kagent-dev/kagent)
- [NVIDIA NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)
- [Kubeflow Documentation](https://www.kubeflow.org/docs/)
