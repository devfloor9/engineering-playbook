---
title: "AI Agent ëª¨ë‹ˆí„°ë§ ë° ìš´ì˜"
sidebar_label: "12. Agent ëª¨ë‹ˆí„°ë§ & ìš´ì˜"
description: "LangFuse, LangSmithë¥¼ í™œìš©í•œ Agentic AI ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë‹ˆí„°ë§, ì•Œë¦¼ ì„¤ì •, íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ"
tags: [eks, langfuse, langsmith, monitoring, observability, tracing, opentelemetry, operations, troubleshooting, alerting]
category: "genai-aiml"
last_update:
  date: 2026-02-14
  author: devfloor9
sidebar_position: 12
---

import {
  LangFuseVsLangSmithTable,
  LatencyMetricsTable,
  TokenUsageMetricsTable,
  ErrorRateMetricsTable,
  DailyChecksTable,
  WeeklyChecksTable,
  MaturityModelTable
} from '@site/src/components/AgentMonitoringTables';

# AI Agent ëª¨ë‹ˆí„°ë§ ë° ìš´ì˜

ì´ ë¬¸ì„œì—ì„œëŠ” LangFuseì™€ LangSmithë¥¼ í™œìš©í•˜ì—¬ Agentic AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ê³¼ ë™ì‘ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì í•˜ê³  ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. Kubernetes í™˜ê²½ì—ì„œì˜ ë°°í¬ë¶€í„° Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±, ì•Œë¦¼ ì„¤ì •, ê·¸ë¦¬ê³  íŠ¸ëŸ¬ë¸”ìŠˆíŒ…ê¹Œì§€ ì‹¤ë¬´ì— í•„ìš”í•œ ì „ì²´ ìš´ì˜ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ê°œìš”

Agentic AI ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ë³µì¡í•œ ì¶”ë¡  ì²´ì¸ê³¼ ë‹¤ì–‘í•œ ë„êµ¬ í˜¸ì¶œì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì—, ì „í†µì ì¸ APM(Application Performance Monitoring) ë„êµ¬ë§Œìœ¼ë¡œëŠ” ì¶©ë¶„í•œ ê°€ì‹œì„±ì„ í™•ë³´í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. LLM íŠ¹í™” ê´€ì¸¡ì„± ë„êµ¬ì¸ LangFuseì™€ LangSmithëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- **íŠ¸ë ˆì´ìŠ¤ ì¶”ì **: LLM í˜¸ì¶œ, ë„êµ¬ ì‹¤í–‰, ì—ì´ì „íŠ¸ ì¶”ë¡  ê³¼ì •ì˜ ì „ì²´ íë¦„ ì¶”ì 
- **í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„**: ì…ë ¥/ì¶œë ¥ í† í° ìˆ˜ ë° ë¹„ìš© ê³„ì‚°
- **í’ˆì§ˆ í‰ê°€**: ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜í™” ë° í”¼ë“œë°± ìˆ˜ì§‘
- **ë””ë²„ê¹…**: í”„ë¡¬í”„íŠ¸ ë° ì‘ë‹µ ë‚´ìš© ê²€í† ë¥¼ í†µí•œ ë¬¸ì œ ì§„ë‹¨

:::info ëŒ€ìƒ ë…ì
ì´ ë¬¸ì„œëŠ” í”Œë«í¼ ìš´ì˜ì, MLOps ì—”ì§€ë‹ˆì–´, AI ê°œë°œìë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤. Kubernetesì™€ Pythonì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤.
:::

## LangFuse vs LangSmith ë¹„êµ

<LangFuseVsLangSmithTable />

:::tip ì„ íƒ ê°€ì´ë“œ

- **LangFuse**: ë°ì´í„° ì£¼ê¶Œì´ ì¤‘ìš”í•˜ê±°ë‚˜, ë¹„ìš© ìµœì í™”ê°€ í•„ìš”í•œ ê²½ìš°
- **LangSmith**: LangChain ê¸°ë°˜ ê°œë°œì´ ì£¼ë ¥ì´ê³ , ë¹ ë¥¸ ì‹œì‘ì´ í•„ìš”í•œ ê²½ìš°
:::


## LangFuse Kubernetes ë°°í¬

### ì•„í‚¤í…ì²˜ ê°œìš”

LangFuse v2.75.0 ì´ìƒì€ ë‹¤ìŒ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph LangFuseStack["LangFuse Stack"]
        WEB["LangFuse Web<br/>(Next.js)"]
        API["LangFuse API<br/>(tRPC)"]
        WORKER["Background Worker<br/>(Queue Processing)"]
    end

    subgraph Storage["Storage Layer"]
        PG["PostgreSQL<br/>(Metadata)"]
        REDIS["Redis<br/>(Cache/Queue)"]
        S3["S3<br/>(Blob Storage)"]
    end

    subgraph Clients["AI Applications"]
        AGENT1["Agent 1"]
        AGENT2["Agent 2"]
        AGENTN["Agent N"]
    end

    AGENT1 --> API
    AGENT2 --> API
    AGENTN --> API
    WEB --> API
    API --> PG
    API --> REDIS
    WORKER --> PG
    WORKER --> REDIS
    WORKER --> S3

    style LangFuseStack fill:#e8f5e9
    style Storage fill:#e3f2fd
    style Clients fill:#fff3e0
```

### PostgreSQL ë°°í¬

LangFuseì˜ ë©”íƒ€ë°ì´í„° ì €ì¥ì„ ìœ„í•œ PostgreSQLì„ ë°°í¬í•©ë‹ˆë‹¤.

```yaml
# langfuse-postgres.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: observability
  labels:
    app.kubernetes.io/part-of: langfuse
---
apiVersion: v1
kind: Secret
metadata:
  name: langfuse-postgres-secret
  namespace: observability
type: Opaque
stringData:
  POSTGRES_USER: langfuse
  POSTGRES_PASSWORD: "your-secure-password-here"  # í”„ë¡œë•ì…˜ì—ì„œëŠ” Secrets Manager ì‚¬ìš©
  POSTGRES_DB: langfuse
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: langfuse-postgres-pvc
  namespace: observability
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: gp3
  resources:
    requests:
      storage: 100Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: langfuse-postgres
  namespace: observability
spec:
  serviceName: langfuse-postgres
  replicas: 1
  selector:
    matchLabels:
      app: langfuse-postgres
  template:
    metadata:
      labels:
        app: langfuse-postgres
    spec:
      containers:
        - name: postgres
          image: postgres:15-alpine
          ports:
            - containerPort: 5432
          envFrom:
            - secretRef:
                name: langfuse-postgres-secret
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
          livenessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - langfuse
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - langfuse
            initialDelaySeconds: 5
            periodSeconds: 5
      volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: langfuse-postgres-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: langfuse-postgres
  namespace: observability
spec:
  selector:
    app: langfuse-postgres
  ports:
    - port: 5432
      targetPort: 5432
  clusterIP: None
```


### LangFuse Deployment

LangFuse ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°°í¬í•©ë‹ˆë‹¤.

```yaml
# langfuse-deployment.yaml
apiVersion: v1
kind: Secret
metadata:
  name: langfuse-secret
  namespace: observability
type: Opaque
stringData:
  # í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜
  DATABASE_URL: "postgresql://langfuse:your-secure-password-here@langfuse-postgres:5432/langfuse"
  NEXTAUTH_SECRET: "your-nextauth-secret-32-chars-min"  # openssl rand -base64 32
  SALT: "your-salt-value-here"  # openssl rand -base64 32
  ENCRYPTION_KEY: "0000000000000000000000000000000000000000000000000000000000000000"  # 64 hex chars

  # ì„ íƒì  í™˜ê²½ ë³€ìˆ˜
  NEXTAUTH_URL: "https://langfuse.your-domain.com"
  LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: "true"

  # S3 ì„¤ì • (ì„ íƒì )
  S3_ENDPOINT: "https://s3.ap-northeast-2.amazonaws.com"
  S3_ACCESS_KEY_ID: "your-access-key"
  S3_SECRET_ACCESS_KEY: "your-secret-key"
  S3_BUCKET_NAME: "langfuse-traces"
  S3_REGION: "ap-northeast-2"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langfuse
  namespace: observability
  labels:
    app: langfuse
spec:
  replicas: 2
  selector:
    matchLabels:
      app: langfuse
  template:
    metadata:
      labels:
        app: langfuse
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "3000"
        prometheus.io/path: "/api/public/metrics"
    spec:
      containers:
        - name: langfuse
          image: langfuse/langfuse:2.75.0
          ports:
            - containerPort: 3000
              name: http
          envFrom:
            - secretRef:
                name: langfuse-secret
          env:
            - name: NODE_ENV
              value: "production"
            - name: PORT
              value: "3000"
            - name: HOSTNAME
              value: "0.0.0.0"
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /api/public/health
              port: 3000
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /api/public/health
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: langfuse
                topologyKey: kubernetes.io/hostname
---
apiVersion: v1
kind: Service
metadata:
  name: langfuse
  namespace: observability
spec:
  selector:
    app: langfuse
  ports:
    - port: 80
      targetPort: 3000
      name: http
  type: ClusterIP
```


### Ingress ì„¤ì •

ì™¸ë¶€ ì ‘ê·¼ì„ ìœ„í•œ Ingressë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

```yaml
# langfuse-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: langfuse-ingress
  namespace: observability
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:XXXXXXXXXXXX:certificate/xxx
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
    alb.ingress.kubernetes.io/ssl-redirect: "443"
    alb.ingress.kubernetes.io/healthcheck-path: /api/public/health
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: "15"
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: "5"
    alb.ingress.kubernetes.io/healthy-threshold-count: "2"
    alb.ingress.kubernetes.io/unhealthy-threshold-count: "2"
spec:
  ingressClassName: alb
  rules:
    - host: langfuse.your-domain.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: langfuse
                port:
                  number: 80
```

### HPA ì„¤ì •

íŠ¸ë˜í”½ì— ë”°ë¥¸ ìë™ ìŠ¤ì¼€ì¼ë§ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

```yaml
# langfuse-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langfuse-hpa
  namespace: observability
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langfuse
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 4
          periodSeconds: 15
      selectPolicy: Max
```

:::warning í”„ë¡œë•ì…˜ ë°°í¬ ì‹œ ì£¼ì˜ì‚¬í•­

- `NEXTAUTH_SECRET`, `SALT`, `ENCRYPTION_KEY`ëŠ” ë°˜ë“œì‹œ ì•ˆì „í•œ ëœë¤ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”
- í”„ë¡œë•ì…˜ì—ì„œëŠ” AWS Secrets Manager ë˜ëŠ” HashiCorp Vaultë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œí¬ë¦¿ì„ ê´€ë¦¬í•˜ì„¸ìš”
- PostgreSQLì€ Amazon RDS PostgreSQLì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤ (ê³ ê°€ìš©ì„±, ìë™ ë°±ì—…, ê´€ë¦¬í˜• ì—…ë°ì´íŠ¸)
- StatefulSet PostgreSQLì€ ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
:::

### AWS Secrets Manager í†µí•© (ê¶Œì¥)

í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” Kubernetes Secret ëŒ€ì‹  AWS Secrets Managerì™€ External Secrets Operatorë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```yaml
# external-secrets-operator ì„¤ì¹˜
helm repo add external-secrets https://charts.external-secrets.io
helm install external-secrets external-secrets/external-secrets -n external-secrets-system --create-namespace

# SecretStore ì„¤ì •
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: aws-secrets-manager
  namespace: observability
spec:
  provider:
    aws:
      service: SecretsManager
      region: ap-northeast-2
      auth:
        jwt:
          serviceAccountRef:
            name: langfuse

# ExternalSecret ì„¤ì •
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: langfuse-secret
  namespace: observability
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secrets-manager
    kind: SecretStore
  target:
    name: langfuse-secret
    creationPolicy: Owner
  data:
    - secretKey: DATABASE_URL
      remoteRef:
        key: langfuse/database-url
    - secretKey: NEXTAUTH_SECRET
      remoteRef:
        key: langfuse/nextauth-secret
    - secretKey: SALT
      remoteRef:
        key: langfuse/salt
    - secretKey: ENCRYPTION_KEY
      remoteRef:
        key: langfuse/encryption-key
```

### Amazon RDS PostgreSQL ì‚¬ìš© (ê¶Œì¥)

í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” StatefulSet PostgreSQL ëŒ€ì‹  Amazon RDSë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```yaml
# RDS PostgreSQL ì—°ê²° ì„¤ì •
apiVersion: v1
kind: Secret
metadata:
  name: langfuse-postgres-secret
  namespace: observability
type: Opaque
stringData:
  DATABASE_URL: "postgresql://langfuse:password@langfuse-db.xxxxxxxxxxxx.ap-northeast-2.rds.amazonaws.com:5432/langfuse?sslmode=require"
```

**RDS ì¥ì :**
- ìë™ ë°±ì—… ë° í¬ì¸íŠ¸ì¸íƒ€ì„ ë³µêµ¬
- Multi-AZ ê³ ê°€ìš©ì„±
- ìë™ íŒ¨ì¹˜ ë° ì—…ë°ì´íŠ¸
- ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ë° ëª¨ë‹ˆí„°ë§
- ì½ê¸° ì „ìš© ë³µì œë³¸ ì§€ì›


## LangSmith í†µí•©

LangSmithëŠ” LangChainì—ì„œ ì œê³µí•˜ëŠ” ê´€ë¦¬í˜• ê´€ì¸¡ì„± í”Œë«í¼ì…ë‹ˆë‹¤. Self-hosted ì˜µì…˜ì´ ì—†ì§€ë§Œ, LangChain ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ì˜ í†µí•©ì´ ë§¤ìš° ê°„í¸í•©ë‹ˆë‹¤.

### í™˜ê²½ ì„¤ì •

LangSmithë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
# langsmith-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: langsmith-config
  namespace: ai-agents
type: Opaque
stringData:
  LANGCHAIN_TRACING_V2: "true"
  LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
  LANGCHAIN_API_KEY: "ls__your-api-key-here"
  LANGCHAIN_PROJECT: "agentic-ai-production"
```

### LangChain ì—ì´ì „íŠ¸ ì—°ë™

LangSmithì™€ LangChain ì—ì´ì „íŠ¸ë¥¼ ì—°ë™í•˜ëŠ” Python ì½”ë“œ ì˜ˆì œì…ë‹ˆë‹¤.

```python
# agent_with_langsmith.py
import os
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import tool
from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (Kubernetes Secretì—ì„œ ì£¼ì…)
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
# LANGCHAIN_API_KEY=ls__xxx
# LANGCHAIN_PROJECT=agentic-ai-production

# ì»¤ìŠ¤í…€ ë„êµ¬ ì •ì˜
@tool
def search_knowledge_base(query: str) -> str:
    """ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # Milvus ê²€ìƒ‰ ë¡œì§
    return f"ê²€ìƒ‰ ê²°ê³¼: {query}ì— ëŒ€í•œ ì •ë³´..."

@tool
def create_support_ticket(title: str, description: str, priority: str = "medium") -> str:
    """ê³ ê° ì§€ì› í‹°ì¼“ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # í‹°ì¼“ ìƒì„± ë¡œì§
    return f"í‹°ì¼“ ìƒì„± ì™„ë£Œ: {title} (ìš°ì„ ìˆœìœ„: {priority})"

# ì—ì´ì „íŠ¸ ì„¤ì •
llm = ChatOpenAI(
    model="gpt-4-turbo",
    temperature=0.7,
    max_tokens=4096,
)

prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ê³ ê° ì§€ì› ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
    í•­ìƒ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ì¸ì •í•˜ì„¸ìš”.
    í•„ìš”í•œ ê²½ìš° ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê±°ë‚˜ í‹°ì¼“ì„ ìƒì„±í•˜ì„¸ìš”."""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

tools = [search_knowledge_base, create_support_ticket]
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    max_iterations=10,
    return_intermediate_steps=True,
)

# íŠ¸ë ˆì´ìŠ¤ ê°€ëŠ¥í•œ í•¨ìˆ˜ë¡œ ë˜í•‘
@traceable(
    name="customer_support_agent",
    run_type="chain",
    tags=["production", "customer-support"],
)
def run_agent(user_input: str, chat_history: list = None, metadata: dict = None):
    """ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  LangSmithì— íŠ¸ë ˆì´ìŠ¤ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
    if chat_history is None:
        chat_history = []

    # í˜„ì¬ ì‹¤í–‰ íŠ¸ë¦¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
    run_tree = get_current_run_tree()
    if run_tree and metadata:
        run_tree.extra["metadata"] = metadata

    result = agent_executor.invoke({
        "input": user_input,
        "chat_history": chat_history,
    })

    return result

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    response = run_agent(
        user_input="ì£¼ë¬¸ #12345ì˜ ë°°ì†¡ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
        metadata={
            "user_id": "user_123",
            "session_id": "session_456",
            "tenant_id": "tenant_abc",
        }
    )
    print(response)
```


### LangFuse Python í†µí•©

LangFuseë¥¼ Python ì• í”Œë¦¬ì¼€ì´ì…˜ì— í†µí•©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```python
# agent_with_langfuse.py
import os
from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context
from langfuse.openai import openai  # OpenAI ë˜í¼
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.callbacks import LangfuseCallbackHandler

# LangFuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
langfuse = Langfuse(
    public_key=os.environ.get("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.environ.get("LANGFUSE_SECRET_KEY"),
    host=os.environ.get("LANGFUSE_HOST", "https://langfuse.your-domain.com"),
)

# LangChain ì½œë°± í•¸ë“¤ëŸ¬
langfuse_handler = LangfuseCallbackHandler(
    public_key=os.environ.get("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.environ.get("LANGFUSE_SECRET_KEY"),
    host=os.environ.get("LANGFUSE_HOST"),
)

# ì—ì´ì „íŠ¸ ì„¤ì •
llm = ChatOpenAI(
    model="gpt-4-turbo",
    temperature=0.7,
    callbacks=[langfuse_handler],
)

@observe(name="customer_support_agent")
def run_agent_with_langfuse(
    user_input: str,
    user_id: str = None,
    session_id: str = None,
    tenant_id: str = None,
):
    """LangFuse íŠ¸ë ˆì´ì‹±ì´ ì ìš©ëœ ì—ì´ì „íŠ¸ ì‹¤í–‰"""

    # íŠ¸ë ˆì´ìŠ¤ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
    langfuse_context.update_current_trace(
        user_id=user_id,
        session_id=session_id,
        metadata={
            "tenant_id": tenant_id,
            "environment": os.environ.get("ENVIRONMENT", "production"),
        },
        tags=["customer-support", "production"],
    )

    # ì—ì´ì „íŠ¸ ì‹¤í–‰
    result = agent_executor.invoke(
        {"input": user_input, "chat_history": []},
        config={"callbacks": [langfuse_handler]},
    )

    # ì¶œë ¥ í† í° ë° ë¹„ìš© ê¸°ë¡
    langfuse_context.update_current_observation(
        output=result["output"],
        metadata={
            "intermediate_steps": len(result.get("intermediate_steps", [])),
        },
    )

    return result

@observe(name="vector_search", as_type="span")
def search_with_tracing(query: str, collection: str, top_k: int = 5):
    """ë²¡í„° ê²€ìƒ‰ì„ íŠ¸ë ˆì´ì‹±ê³¼ í•¨ê»˜ ìˆ˜í–‰"""
    from pymilvus import Collection

    langfuse_context.update_current_observation(
        input={"query": query, "collection": collection, "top_k": top_k},
    )

    # Milvus ê²€ìƒ‰ ìˆ˜í–‰
    collection = Collection(collection)
    results = collection.search(
        data=[get_embedding(query)],
        anns_field="embedding",
        param={"metric_type": "COSINE", "params": {"ef": 64}},
        limit=top_k,
        output_fields=["content", "metadata"],
    )

    langfuse_context.update_current_observation(
        output={"num_results": len(results[0])},
    )

    return results

# ì ìˆ˜ ë° í”¼ë“œë°± ê¸°ë¡
def record_feedback(trace_id: str, score: float, comment: str = None):
    """ì‚¬ìš©ì í”¼ë“œë°±ì„ LangFuseì— ê¸°ë¡"""
    langfuse.score(
        trace_id=trace_id,
        name="user_feedback",
        value=score,
        comment=comment,
    )

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    response = run_agent_with_langfuse(
        user_input="ì œí’ˆ ë°˜í’ˆ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
        user_id="user_123",
        session_id="session_456",
        tenant_id="tenant_abc",
    )

    # í”¼ë“œë°± ê¸°ë¡ (ì˜ˆ: ì‚¬ìš©ìê°€ ì‘ë‹µì— ë§Œì¡±)
    trace_id = langfuse_context.get_current_trace_id()
    record_feedback(trace_id, score=1.0, comment="ì •í™•í•œ ë‹µë³€")

    # í”ŒëŸ¬ì‹œí•˜ì—¬ ëª¨ë“  ì´ë²¤íŠ¸ ì „ì†¡
    langfuse.flush()
```


## í•µì‹¬ ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

Agentic AI ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì¶”ì í•´ì•¼ í•  í•µì‹¬ ë©”íŠ¸ë¦­ì„ ì •ì˜í•©ë‹ˆë‹¤.

### ë©”íŠ¸ë¦­ ì¹´í…Œê³ ë¦¬

```mermaid
graph TB
    subgraph Metrics["í•µì‹¬ ë©”íŠ¸ë¦­"]
        subgraph Latency["â±ï¸ Latency"]
            E2E["End-to-End ì§€ì—°"]
            LLM_LAT["LLM ì¶”ë¡  ì‹œê°„"]
            TOOL_LAT["ë„êµ¬ ì‹¤í–‰ ì‹œê°„"]
            RETRIEVAL_LAT["ê²€ìƒ‰ ì§€ì—°"]
        end

        subgraph Tokens["ğŸ”¢ Token Usage"]
            INPUT_TOK["ì…ë ¥ í† í°"]
            OUTPUT_TOK["ì¶œë ¥ í† í°"]
            TOTAL_TOK["ì´ í† í°"]
            COST["ë¹„ìš©"]
        end

        subgraph Errors["âŒ Error Rate"]
            LLM_ERR["LLM ì˜¤ë¥˜"]
            TOOL_ERR["ë„êµ¬ ì˜¤ë¥˜"]
            TIMEOUT["íƒ€ì„ì•„ì›ƒ"]
            RATE_LIMIT["Rate Limit"]
        end

        subgraph Traces["ğŸ” Trace Analysis"]
            CHAIN_LEN["ì²´ì¸ ê¸¸ì´"]
            TOOL_CALLS["ë„êµ¬ í˜¸ì¶œ ìˆ˜"]
            ITERATIONS["ë°˜ë³µ íšŸìˆ˜"]
            SUCCESS["ì„±ê³µë¥ "]
        end
    end

    style Latency fill:#e3f2fd
    style Tokens fill:#e8f5e9
    style Errors fill:#ffcdd2
    style Traces fill:#fff3e0
```

### Latency ë©”íŠ¸ë¦­

<LatencyMetricsTable />

### Token Usage ë©”íŠ¸ë¦­

<TokenUsageMetricsTable />

### Error Rate ë©”íŠ¸ë¦­

<ErrorRateMetricsTable />

### Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„¤ì •

```yaml
# prometheus-scrape-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-agent-scrape
  namespace: observability
data:
  agent-scrape.yaml: |
    scrape_configs:
      - job_name: 'langfuse'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - observability
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            regex: langfuse
            action: keep
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            regex: "3000"
            action: keep
        metrics_path: /api/public/metrics

      - job_name: 'ai-agents'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - ai-agents
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            regex: "true"
            action: keep
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
```


### Python ë©”íŠ¸ë¦­ ìµìŠ¤í¬í„°

ì—ì´ì „íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ Prometheus ë©”íŠ¸ë¦­ì„ ë…¸ì¶œí•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

```python
# metrics_exporter.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# ë©”íŠ¸ë¦­ ì •ì˜
AGENT_REQUEST_DURATION = Histogram(
    'agent_request_duration_seconds',
    'Agent request duration in seconds',
    ['agent_name', 'model', 'tenant_id'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

LLM_INFERENCE_DURATION = Histogram(
    'llm_inference_duration_seconds',
    'LLM inference duration in seconds',
    ['model', 'provider'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

LLM_TOKENS = Counter(
    'llm_tokens_total',
    'Total LLM tokens used',
    ['model', 'token_type', 'tenant_id']  # token_type: input, output
)

LLM_COST = Counter(
    'llm_cost_dollars_total',
    'Total LLM cost in USD',
    ['model', 'tenant_id']
)

AGENT_ERRORS = Counter(
    'agent_errors_total',
    'Total agent errors',
    ['agent_name', 'error_type', 'tenant_id']
)

TOOL_EXECUTION_DURATION = Histogram(
    'tool_execution_duration_seconds',
    'Tool execution duration in seconds',
    ['tool_name', 'agent_name'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]
)

ACTIVE_SESSIONS = Gauge(
    'agent_active_sessions',
    'Number of active agent sessions',
    ['agent_name', 'tenant_id']
)

# ëª¨ë¸ë³„ ë¹„ìš© (USD per 1K tokens)
MODEL_COSTS = {
    "gpt-4o": {"input": 0.0025, "output": 0.01},
    "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
    "gpt-4-turbo": {"input": 0.01, "output": 0.03},
    "claude-sonnet-4": {"input": 0.003, "output": 0.015},
    "claude-3.5-haiku": {"input": 0.0008, "output": 0.004},
    "claude-3-opus": {"input": 0.015, "output": 0.075},
}

def record_llm_usage(
    model: str,
    input_tokens: int,
    output_tokens: int,
    tenant_id: str,
    duration: float,
):
    """LLM ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­ ê¸°ë¡"""
    # í† í° ìˆ˜ ê¸°ë¡
    LLM_TOKENS.labels(model=model, token_type="input", tenant_id=tenant_id).inc(input_tokens)
    LLM_TOKENS.labels(model=model, token_type="output", tenant_id=tenant_id).inc(output_tokens)

    # ë¹„ìš© ê³„ì‚° ë° ê¸°ë¡
    if model in MODEL_COSTS:
        cost = (
            (input_tokens / 1000) * MODEL_COSTS[model]["input"] +
            (output_tokens / 1000) * MODEL_COSTS[model]["output"]
        )
        LLM_COST.labels(model=model, tenant_id=tenant_id).inc(cost)

    # ì¶”ë¡  ì‹œê°„ ê¸°ë¡
    LLM_INFERENCE_DURATION.labels(model=model, provider="openai").observe(duration)

def record_agent_request(
    agent_name: str,
    model: str,
    tenant_id: str,
    duration: float,
    success: bool,
    error_type: str = None,
):
    """ì—ì´ì „íŠ¸ ìš”ì²­ ë©”íŠ¸ë¦­ ê¸°ë¡"""
    AGENT_REQUEST_DURATION.labels(
        agent_name=agent_name,
        model=model,
        tenant_id=tenant_id
    ).observe(duration)

    if not success and error_type:
        AGENT_ERRORS.labels(
            agent_name=agent_name,
            error_type=error_type,
            tenant_id=tenant_id
        ).inc()

# ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘
def start_metrics_server(port: int = 8000):
    """Prometheus ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘"""
    start_http_server(port)
    print(f"Metrics server started on port {port}")
```


## Grafana ëŒ€ì‹œë³´ë“œ ë° ì•Œë¦¼

### ëŒ€ì‹œë³´ë“œ ê°œìš”

AI Agent ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ Grafana ëŒ€ì‹œë³´ë“œë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph Dashboard["Grafana Dashboard"]
        subgraph Row1["Overview"]
            TOTAL_REQ["ì´ ìš”ì²­ ìˆ˜"]
            SUCCESS_RATE["ì„±ê³µë¥ "]
            AVG_LATENCY["í‰ê·  ì§€ì—°"]
            ACTIVE_USERS["í™œì„± ì‚¬ìš©ì"]
        end

        subgraph Row2["Performance"]
            LATENCY_CHART["ì§€ì—° ì‹œê°„ ë¶„í¬"]
            THROUGHPUT["ì²˜ë¦¬ëŸ‰ ì¶”ì´"]
            ERROR_RATE["ì˜¤ë¥˜ìœ¨ ì¶”ì´"]
        end

        subgraph Row3["Cost & Usage"]
            TOKEN_USAGE["í† í° ì‚¬ìš©ëŸ‰"]
            COST_BY_MODEL["ëª¨ë¸ë³„ ë¹„ìš©"]
            COST_BY_TENANT["í…Œë„ŒíŠ¸ë³„ ë¹„ìš©"]
        end

        subgraph Row4["Traces"]
            TRACE_TABLE["ìµœê·¼ íŠ¸ë ˆì´ìŠ¤"]
            SLOW_TRACES["ëŠë¦° ìš”ì²­"]
            ERROR_TRACES["ì˜¤ë¥˜ íŠ¸ë ˆì´ìŠ¤"]
        end
    end

    style Row1 fill:#e3f2fd
    style Row2 fill:#e8f5e9
    style Row3 fill:#fff3e0
    style Row4 fill:#fce4ec
```

### Grafana ì•Œë¦¼ ê·œì¹™

```yaml
# grafana-alerts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-alert-rules
  namespace: observability
data:
  ai-agent-alerts.yaml: |
    apiVersion: 1
    groups:
      - orgId: 1
        name: AI Agent Alerts
        folder: AI Monitoring
        interval: 1m
        rules:
          - uid: agent-high-latency
            title: Agent High Latency
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: histogram_quantile(0.99, sum(rate(agent_request_duration_seconds_bucket[5m])) by (le, agent_name))
                  intervalMs: 1000
                  maxDataPoints: 43200
              - refId: B
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [10]
                        type: gt
                      operator:
                        type: and
                      query:
                        params: [A]
                      reducer:
                        type: last
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            noDataState: NoData
            execErrState: Error
            for: 5m
            annotations:
              summary: "Agent {{ $labels.agent_name }} P99 latency is above 10s"
              description: "Current P99 latency: {{ $values.A }}s"
            labels:
              severity: warning

          - uid: agent-high-error-rate
            title: Agent High Error Rate
            condition: C
            data:
              - refId: A
                datasourceUid: prometheus
                model:
                  expr: |
                    sum(rate(agent_errors_total[5m])) by (agent_name) /
                    sum(rate(agent_request_duration_seconds_count[5m])) by (agent_name)
              - refId: B
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [0.05]
                        type: gt
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            for: 5m
            annotations:
              summary: "Agent {{ $labels.agent_name }} error rate is above 5%"
              description: "Current error rate: {{ printf \"%.2f\" $values.A }}%"
            labels:
              severity: critical

          - uid: llm-rate-limit
            title: LLM Rate Limit Errors
            condition: C
            data:
              - refId: A
                datasourceUid: prometheus
                model:
                  expr: sum(increase(llm_rate_limit_errors_total[5m])) by (model)
              - refId: B
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [10]
                        type: gt
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            for: 2m
            annotations:
              summary: "LLM {{ $labels.model }} rate limit errors detected"
              description: "{{ $values.A }} rate limit errors in last 5 minutes"
            labels:
              severity: warning

          - uid: cost-budget-alert
            title: Daily Cost Budget Exceeded
            condition: C
            data:
              - refId: A
                datasourceUid: prometheus
                model:
                  expr: sum(increase(llm_cost_dollars_total[24h])) by (tenant_id)
              - refId: B
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [100]  # $100 daily budget
                        type: gt
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            for: 0s
            annotations:
              summary: "Tenant {{ $labels.tenant_id }} exceeded daily cost budget"
              description: "Current daily cost: ${{ printf \"%.2f\" $values.A }}"
            labels:
              severity: warning
```


## ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¼ì¼ ì ê²€ í•­ëª©

<DailyChecksTable />

### ì£¼ê°„ ì ê²€ í•­ëª©

<WeeklyChecksTable />


## íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

### GPU OOM (Out of Memory) ë¬¸ì œ

#### ì¦ìƒ

```
CUDA out of memory. Tried to allocate X GiB
RuntimeError: CUDA error: out of memory
```

#### ì§„ë‹¨

```bash
# GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
kubectl exec -it <pod-name> -n inference -- nvidia-smi

# DCGM ë©”íŠ¸ë¦­ í™•ì¸
kubectl exec -it <dcgm-exporter-pod> -n monitoring -- dcgmi dmon -e 155,156
```

#### í•´ê²° ë°©ì•ˆ

```yaml
# 1. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
env:
- name: MAX_BATCH_SIZE
  value: "16"  # 32ì—ì„œ 16ìœ¼ë¡œ ê°ì†Œ

# 2. ëª¨ë¸ ì–‘ìí™” ì ìš©
env:
- name: QUANTIZATION
  value: "int8"  # ë˜ëŠ” "fp8"

# 3. KV ìºì‹œ í¬ê¸° ì œí•œ
env:
- name: MAX_NUM_SEQS
  value: "128"  # ë™ì‹œ ì‹œí€€ìŠ¤ ìˆ˜ ì œí•œ
```

### ë„¤íŠ¸ì›Œí¬ ì§€ì—° ë¬¸ì œ

#### ì¦ìƒ

- ì¶”ë¡  ìš”ì²­ íƒ€ì„ì•„ì›ƒ
- ëª¨ë¸ ê°„ í†µì‹  ì§€ì—°
- NCCL íƒ€ì„ì•„ì›ƒ (ë¶„ì‚° ì¶”ë¡  ì‹œ)

#### í•´ê²° ë°©ì•ˆ

```yaml
# 1. Pod Anti-Affinityë¡œ ë¶„ì‚° ë°°ì¹˜
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchLabels:
            app: inference
        topologyKey: "topology.kubernetes.io/zone"

# 2. íƒ€ì„ì•„ì›ƒ ì¦ê°€
env:
- name: NCCL_TIMEOUT
  value: "1800"  # 30ë¶„
- name: REQUEST_TIMEOUT
  value: "300"   # 5ë¶„
```

### LangFuse ì—°ê²° ì˜¤ë¥˜

```bash
# ì¦ìƒ: LangFuseì— íŠ¸ë ˆì´ìŠ¤ê°€ ê¸°ë¡ë˜ì§€ ì•ŠìŒ

# 1. LangFuse ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
kubectl get pods -n observability -l app=langfuse

# 2. LangFuse ë¡œê·¸ í™•ì¸
kubectl logs -n observability -l app=langfuse --tail=100

# 3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl -v http://langfuse.observability.svc/api/public/health

# 4. í™˜ê²½ ë³€ìˆ˜ í™•ì¸
kubectl exec -n ai-agents <pod-name> -- env | grep LANGFUSE
```


## ë¹„ìš© ì¶”ì 

### ëª¨ë¸ë³„ ë¹„ìš© ë¶„ì„

LLM ì‚¬ìš© ë¹„ìš©ì„ ëª¨ë¸ë³„ë¡œ ì¶”ì í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.

```python
# cost_tracker.py
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json

@dataclass
class ModelPricing:
    """ëª¨ë¸ë³„ ê°€ê²© ì •ë³´ (USD per 1K tokens)"""
    input_price: float
    output_price: float

# 2025ë…„ ê¸°ì¤€ ëª¨ë¸ ê°€ê²©
MODEL_PRICING: Dict[str, ModelPricing] = {
    # OpenAI
    "gpt-4o": ModelPricing(0.0025, 0.01),
    "gpt-4o-mini": ModelPricing(0.00015, 0.0006),
    "gpt-4-turbo": ModelPricing(0.01, 0.03),

    # Anthropic
    "claude-sonnet-4": ModelPricing(0.003, 0.015),
    "claude-3.5-haiku": ModelPricing(0.0008, 0.004),
    "claude-3-opus": ModelPricing(0.015, 0.075),
}

@dataclass
class UsageRecord:
    """ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
    timestamp: datetime
    model: str
    input_tokens: int
    output_tokens: int
    tenant_id: str
    agent_name: str
    trace_id: str

    @property
    def total_tokens(self) -> int:
        return self.input_tokens + self.output_tokens

    @property
    def cost(self) -> float:
        if self.model not in MODEL_PRICING:
            return 0.0
        pricing = MODEL_PRICING[self.model]
        return (
            (self.input_tokens / 1000) * pricing.input_price +
            (self.output_tokens / 1000) * pricing.output_price
        )

class CostTracker:
    """ë¹„ìš© ì¶”ì ê¸°"""

    def __init__(self, langfuse_client=None):
        self.langfuse = langfuse_client
        self.records: List[UsageRecord] = []

    def record_usage(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int,
        tenant_id: str,
        agent_name: str,
        trace_id: str,
    ):
        """ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
        record = UsageRecord(
            timestamp=datetime.utcnow(),
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            tenant_id=tenant_id,
            agent_name=agent_name,
            trace_id=trace_id,
        )
        self.records.append(record)

        # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        from metrics_exporter import record_llm_usage
        record_llm_usage(
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            tenant_id=tenant_id,
            duration=0,  # ë³„ë„ ì¸¡ì • í•„ìš”
        )

        return record
```

### ë¹„ìš© ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬

```promql
# ì¼ë³„ ì´ ë¹„ìš©
sum(increase(llm_cost_dollars_total[24h]))

# í…Œë„ŒíŠ¸ë³„ ì¼ë³„ ë¹„ìš©
sum(increase(llm_cost_dollars_total[24h])) by (tenant_id)

# ëª¨ë¸ë³„ ë¹„ìš© ë¹„ìœ¨
sum(increase(llm_cost_dollars_total[24h])) by (model)
/ ignoring(model) group_left
sum(increase(llm_cost_dollars_total[24h]))

# ì˜ˆì‚° ëŒ€ë¹„ ì‚¬ìš©ë¥  (ì›”ê°„)
sum(increase(llm_cost_dollars_total[30d])) by (tenant_id)
/ on(tenant_id) group_left
tenant_monthly_budget_usd
```

:::tip ë¹„ìš© ìµœì í™” íŒ

1. **ëª¨ë¸ ì„ íƒ ìµœì í™”**: ê°„ë‹¨í•œ ì‘ì—…ì—ëŠ” ì €ë ´í•œ ëª¨ë¸(GPT-4o-mini, Claude 3.5 Haiku) ì‚¬ìš©
2. **í”„ë¡¬í”„íŠ¸ ìµœì í™”**: ë¶ˆí•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì œê±°ë¡œ ì…ë ¥ í† í° ì ˆê°
3. **ìºì‹± í™œìš©**: ë°˜ë³µì ì¸ ì¿¼ë¦¬ì— ëŒ€í•œ ì‘ë‹µ ìºì‹±
4. **ë°°ì¹˜ ì²˜ë¦¬**: ê°€ëŠ¥í•œ ê²½ìš° ìš”ì²­ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
:::


## ê²°ë¡ 

AI Agent ëª¨ë‹ˆí„°ë§ì€ Agentic AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì•ˆì •ì ì¸ ìš´ì˜ê³¼ ì§€ì†ì ì¸ ê°œì„ ì„ ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œ ë‹¤ë£¬ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ë©´:

### AWS ë„¤ì´í‹°ë¸Œ ê´€ì¸¡ì„±: CloudWatch Generative AI Observability

:::tip re:Invent 2025 ì‹ ê·œ ì„œë¹„ìŠ¤
Amazon CloudWatch Generative AI ObservabilityëŠ” 2025ë…„ 10ì›” GAë¡œ ì¶œì‹œëœ AI ì›Œí¬ë¡œë“œ ì „ìš© ê´€ì¸¡ì„± ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. LangFuse/LangSmithì™€ í•¨ê»˜ ë˜ëŠ” ëŒ€ì•ˆìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

CloudWatch Generative AI ObservabilityëŠ” LLM ë° AI ì—ì´ì „íŠ¸ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ AWS ë„¤ì´í‹°ë¸Œ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤:

- **ì¸í”„ë¼ ë¬´ê´€ ëª¨ë‹ˆí„°ë§**: Bedrock, EKS, ECS, ì˜¨í”„ë ˆë¯¸ìŠ¤ ë“± ëª¨ë“  í™˜ê²½ì˜ AI ì›Œí¬ë¡œë“œ ì§€ì›
- **ì—ì´ì „íŠ¸/ë„êµ¬ ì¶”ì **: ì—ì´ì „íŠ¸, ì§€ì‹ ë² ì´ìŠ¤, ë„êµ¬ í˜¸ì¶œì— ëŒ€í•œ ê¸°ë³¸ ì œê³µ ë·°
- **ì—”ë“œíˆ¬ì—”ë“œ íŠ¸ë ˆì´ì‹±**: ì „ì²´ AI ìŠ¤íƒì— ê±¸ì¹œ ì¶”ì 
- **í† í° ë° ì§€ì—° ë©”íŠ¸ë¦­**: ë„¤ì´í‹°ë¸Œ LLM íŠ¸ë ˆì´ì‹±
- **í”„ë ˆì„ì›Œí¬ í˜¸í™˜**: LangChain, LangGraph, CrewAI ë“± ì™¸ë¶€ í”„ë ˆì„ì›Œí¬ ì§€ì›

LangFuse v2.75.0 (Self-hosted ë°ì´í„° ì£¼ê¶Œ)ì™€ CloudWatch Gen AI Observability(AWS ë„¤ì´í‹°ë¸Œ í†µí•©)ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ê°€ì¥ í¬ê´„ì ì¸ ê´€ì¸¡ì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë˜í•œ **CloudWatch Application Signals**ë¥¼ í™œìš©í•˜ë©´ EKS ì›Œí¬ë¡œë“œì˜ ìë™ í…”ë ˆë©”íŠ¸ë¦¬ ìˆ˜ì§‘, ì„œë¹„ìŠ¤ ê´€ê³„ ì‹œê°í™”, SLI/SLO ëª¨ë‹ˆí„°ë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì½”ë“œ ë³€ê²½ ì—†ì´ ADOT(AWS Distro for OpenTelemetry) ê¸°ë°˜ ìë™ ê³„ì¸¡ì„ ì§€ì›í•©ë‹ˆë‹¤.

### í•µì‹¬ ìš”ì•½

1. **LangFuse ë°°í¬**: Kubernetes í™˜ê²½ì—ì„œ Self-hosted LangFuseë¥¼ ë°°í¬í•˜ì—¬ ë°ì´í„° ì£¼ê¶Œì„ í™•ë³´í•˜ê³  ë¹„ìš©ì„ ìµœì í™”
2. **LangSmith í†µí•©**: LangChain ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ê°„í¸í•˜ê²Œ íŠ¸ë ˆì´ì‹± í™œì„±í™”
3. **í•µì‹¬ ë©”íŠ¸ë¦­**: Latency, Token Usage, Error Rate, Trace ë¶„ì„ì„ í†µí•œ ì¢…í•©ì ì¸ ëª¨ë‹ˆí„°ë§
4. **Grafana ëŒ€ì‹œë³´ë“œ**: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ì„ í†µí•œ proactive ìš´ì˜
5. **ë¹„ìš© ì¶”ì **: ëª¨ë¸ë³„, í…Œë„ŒíŠ¸ë³„ ë¹„ìš© ë¶„ì„ìœ¼ë¡œ ì˜ˆì‚° ê´€ë¦¬ ë° ìµœì í™”
6. **íŠ¸ëŸ¬ë¸”ìŠˆíŒ…**: GPU OOM, ë„¤íŠ¸ì›Œí¬ ì§€ì—°, ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ë“± ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

### ëª¨ë‹ˆí„°ë§ ì„±ìˆ™ë„ ëª¨ë¸

<MaturityModelTable />

:::tip ë‹¤ìŒ ë‹¨ê³„

- [Agentic AI Platform ì•„í‚¤í…ì²˜](./agentic-platform-architecture.md) - ì „ì²´ í”Œë«í¼ ì„¤ê³„
- [Kagent Kubernetes Agent ê´€ë¦¬](./kagent-kubernetes-agents.md) - ì—ì´ì „íŠ¸ ë°°í¬ ë° ìš´ì˜
- [RAG í‰ê°€ í”„ë ˆì„ì›Œí¬](./ragas-evaluation.md) - Ragasë¥¼ í™œìš©í•œ í’ˆì§ˆ í‰ê°€
:::

## ì°¸ê³  ìë£Œ

- [LangFuse Documentation](https://langfuse.com/docs)
- [LangFuse GitHub Repository](https://github.com/langfuse/langfuse)
- [LangSmith Documentation](https://docs.smith.langchain.com/)
- [CloudWatch Generative AI Observability](https://aws.amazon.com/blogs/mt/launching-amazon-cloudwatch-generative-ai-observability-preview/)
- [CloudWatch Application Signals](https://aws.amazon.com/blogs/mt/amazon-cloudwatch-application-signals-new-enhancements-for-application-monitoring/)
- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)
- [Prometheus Monitoring](https://prometheus.io/docs/)
- [Grafana Dashboards](https://grafana.com/docs/grafana/latest/dashboards/)
- [LangChain Callbacks](https://python.langchain.com/docs/modules/callbacks/)
