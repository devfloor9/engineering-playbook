---
title: "NeMo í”„ë ˆì„ì›Œí¬"
sidebar_label: "8. NeMo í”„ë ˆì„ì›Œí¬"
description: "NVIDIA NeMoë¥¼ í™œìš©í•œ LLM íŒŒì¸íŠœë‹ ë° ìµœì í™” íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"
sidebar_position: 8
category: "genai-aiml"
last_update:
  date: 2026-02-13
  author: devfloor9
tags: [nemo, nvidia, fine-tuning, llm, training, tensorrt, genai]
---

import { NemoComponents, GPURequirements, CheckpointSharding, MonitoringMetrics, NCCLImportance } from '@site/src/components/NemoTables';

# NeMo í”„ë ˆì„ì›Œí¬

> ğŸ“… **ì‘ì„±ì¼**: 2026-02-13 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 26ë¶„

NVIDIA NeMoëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•™ìŠµ, íŒŒì¸íŠœë‹, ìµœì í™”ë¥¼ ìœ„í•œ ì—”ë“œíˆ¬ì—”ë“œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Kubernetes í™˜ê²½ì—ì„œ ë¶„ì‚° í•™ìŠµê³¼ íš¨ìœ¨ì ì¸ ëª¨ë¸ ë°°í¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

## ê°œìš”

### NeMoê°€ í•„ìš”í•œ ì´ìœ 

Agentic AI í”Œë«í¼ì—ì„œ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ì´ í•„ìš”í•œ ê²½ìš°:

- **ë„ë©”ì¸ ì ì‘**: íŠ¹ì • ì‚°ì—…/ë¶„ì•¼ì— ë§ëŠ” ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•
- **ì„±ëŠ¥ ìµœì í™”**: TensorRT-LLMì„ í†µí•œ ì¶”ë¡  ê°€ì†
- **ë¹„ìš© íš¨ìœ¨**: ì‘ì€ íŒŒì¸íŠœë‹ ëª¨ë¸ë¡œ ëŒ€í˜• ëª¨ë¸ ëŒ€ì²´
- **ë°ì´í„° í”„ë¼ì´ë²„ì‹œ**: ë¯¼ê°í•œ ë°ì´í„°ë¡œ ì˜¨í”„ë ˆë¯¸ìŠ¤ í•™ìŠµ

```mermaid
graph LR
    subgraph "NeMo Pipeline"
        Data["ë°ì´í„° ì¤€ë¹„"]
        Pretrain["ì‚¬ì „í•™ìŠµ<br/>(ì„ íƒ)"]
        Finetune["íŒŒì¸íŠœë‹"]
        Eval["í‰ê°€"]
        Export["TensorRT ë³€í™˜"]
        Deploy["ë°°í¬"]
    end
    
    Data --> Pretrain
    Pretrain --> Finetune
    Data --> Finetune
    Finetune --> Eval
    Eval --> Export
    Export --> Deploy
    
    style Finetune fill:#76b900,stroke:#333,stroke-width:2px
    style Export fill:#76b900,stroke:#333,stroke-width:2px
```

### NeMo í”„ë ˆì„ì›Œí¬ êµ¬ì„±ìš”ì†Œ

<NemoComponents />

## EKS ë°°í¬ ì•„í‚¤í…ì²˜

### ë¶„ì‚° í•™ìŠµ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "Control Plane"
        Launcher["NeMo Launcher"]
        Scheduler["Kubernetes Scheduler"]
    end
    
    subgraph "Worker Nodes"
        subgraph "Node 1"
            W1["Worker Pod"]
            G1["GPU 0-7"]
        end
        subgraph "Node 2"
            W2["Worker Pod"]
            G2["GPU 0-7"]
        end
        subgraph "Node 3"
            W3["Worker Pod"]
            G3["GPU 0-7"]
        end
    end
    
    subgraph "Storage"
        S3["S3 / FSx"]
        Checkpoint["ì²´í¬í¬ì¸íŠ¸"]
    end
    
    subgraph "Communication"
        NCCL["NCCL / EFA"]
    end
    
    Launcher --> Scheduler
    Scheduler --> W1
    Scheduler --> W2
    Scheduler --> W3
    
    W1 <--> NCCL
    W2 <--> NCCL
    W3 <--> NCCL
    
    W1 --> S3
    W2 --> S3
    W3 --> S3
    
    W1 --> Checkpoint
    W2 --> Checkpoint
    W3 --> Checkpoint
    
    style Launcher fill:#76b900,stroke:#333
    style NCCL fill:#4285f4,stroke:#333
```

### GPU ë…¸ë“œ ìš”êµ¬ì‚¬í•­

<GPURequirements />

## NeMo ì»¨í…Œì´ë„ˆ ë°°í¬

### Helm ì°¨íŠ¸ ì„¤ì¹˜

```bash
# NVIDIA NGC ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì¸ì¦
kubectl create secret docker-registry ngc-secret \
  --docker-server=nvcr.io \
  --docker-username='$oauthtoken' \
  --docker-password=${NGC_API_KEY} \
  --namespace=nemo

# NeMo Operator ì„¤ì¹˜
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update

helm install nemo-operator nvidia/nemo-operator \
  --namespace nemo \
  --create-namespace \
  --set operator.image.repository=nvcr.io/nvidia/nemo-operator \
  --set operator.image.tag=24.07
```

### NeMo í•™ìŠµ Job ì •ì˜

:::info ì°¸ê³ 
ì•„ë˜ `NeMoTraining` CRDëŠ” NeMoì˜ ì„ ì–¸ì  í•™ìŠµ ì •ì˜ ê°œë…ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” Kubeflow Training Operatorì˜ PyTorchJobì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì‚° í•™ìŠµì„ êµ¬ì„±í•©ë‹ˆë‹¤.
:::

```yaml
# NeMo í•™ìŠµ ê°œë… ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” PyTorchJob ì‚¬ìš©)
apiVersion: nemo.nvidia.com/v1alpha1
kind: NeMoTraining
metadata:
  name: llama-finetune
  namespace: nemo
spec:
  # ëª¨ë¸ ì„¤ì •
  model:
    name: "meta-llama/Llama-3.1-8B-Instruct"
    source: "huggingface"
  
  # í•™ìŠµ ì„¤ì •
  training:
    type: "sft"  # supervised fine-tuning
    epochs: 3
    batchSize: 4
    gradientAccumulationSteps: 8
    learningRate: 2e-5
    
    # ë¶„ì‚° í•™ìŠµ ì„¤ì •
    distributed:
      tensorParallelism: 1
      pipelineParallelism: 1
      dataParallelism: 8
  
  # ë°ì´í„° ì„¤ì •
  data:
    trainDataset: "s3://nemo-data/train.jsonl"
    valDataset: "s3://nemo-data/val.jsonl"
    format: "jsonl"
  
  # ë¦¬ì†ŒìŠ¤ ì„¤ì •
  resources:
    nodes: 1
    gpusPerNode: 8
    gpuType: "nvidia.com/gpu"
    
  # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
  checkpoint:
    enabled: true
    path: "s3://nemo-checkpoints/llama-finetune"
    saveInterval: 500
    
  # ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€
  image:
    repository: "nvcr.io/nvidia/nemo"
    tag: "24.07"
    pullSecrets:
      - name: ngc-secret
```

### PyTorchJobì„ í†µí•œ ë¶„ì‚° í•™ìŠµ

```yaml
apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: nemo-distributed-training
  namespace: nemo
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          containers:
          - name: pytorch
            image: nvcr.io/nvidia/nemo:24.07
            command:
            - python
            - -m
            - nemo.collections.llm.recipes.finetune
            - --config-path=/config
            - --config-name=llama_finetune
            env:
            - name: NCCL_DEBUG
              value: "INFO"
            - name: NCCL_IB_DISABLE
              value: "0"
            - name: FI_PROVIDER
              value: "efa"
            - name: FI_EFA_USE_DEVICE_RDMA
              value: "1"
            - name: NCCL_PROTO
              value: "simple"
            resources:
              limits:
                nvidia.com/gpu: 8
                vpc.amazonaws.com/efa: 4
            volumeMounts:
            - name: config
              mountPath: /config
            - name: data
              mountPath: /data
            - name: shm
              mountPath: /dev/shm
          volumes:
          - name: config
            configMap:
              name: nemo-config
          - name: data
            persistentVolumeClaim:
              claimName: training-data-pvc
          - name: shm
            emptyDir:
              medium: Memory
              sizeLimit: 64Gi
    Worker:
      replicas: 3
      restartPolicy: OnFailure
      template:
        spec:
          containers:
          - name: pytorch
            image: nvcr.io/nvidia/nemo:24.07
            # Worker ì„¤ì •ì€ Masterì™€ ë™ì¼
```

## íŒŒì¸íŠœë‹ ê°€ì´ë“œ

### SFT (Supervised Fine-Tuning)

```python
# nemo_sft_config.yaml
trainer:
  devices: 8
  num_nodes: 1
  accelerator: gpu
  precision: bf16
  max_epochs: 3
  val_check_interval: 500
  
model:
  # ê¸°ë³¸ ëª¨ë¸
  restore_from_path: /models/llama-3.1-8b.nemo
  
  # LoRA ì„¤ì • (íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹)
  peft:
    peft_scheme: "lora"
    lora_tuning:
      adapter_dim: 32
      alpha: 32
      dropout: 0.1
      target_modules:
        - "q_proj"
        - "v_proj"
        - "k_proj"
        - "o_proj"
  
  # ë°ì´í„° ì„¤ì •
  data:
    train_ds:
      file_path: /data/train.jsonl
      micro_batch_size: 4
      global_batch_size: 32
    validation_ds:
      file_path: /data/val.jsonl
      micro_batch_size: 4
      
  # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
  optim:
    name: fused_adam
    lr: 2e-5
    weight_decay: 0.01
    betas:
      - 0.9
      - 0.98
```

### ë°ì´í„° í˜•ì‹

```json
{"input": "ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”: EKSë€ ë¬´ì—‡ì¸ê°€ìš”?", "output": "Amazon EKS(Elastic Kubernetes Service)ëŠ” AWSì—ì„œ ì œê³µí•˜ëŠ” ê´€ë¦¬í˜• Kubernetes ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."}
{"input": "Karpenterì˜ ì£¼ìš” ê¸°ëŠ¥ì„ ì„¤ëª…í•˜ì„¸ìš”.", "output": "KarpenterëŠ” ìë™ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹, í†µí•©(consolidation), ë“œë¦¬í”„íŠ¸ ê°ì§€ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” Kubernetes ë…¸ë“œ ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ì…ë‹ˆë‹¤."}
```

### PEFT/LoRA íŒŒì¸íŠœë‹

```python
from nemo.collections.llm import finetune
from nemo.collections.llm.peft import LoRA

# LoRA ì„¤ì •
lora_config = LoRA(
    r=32,
    alpha=32,
    dropout=0.1,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
)

# íŒŒì¸íŠœë‹ ì‹¤í–‰
model = finetune(
    model_path="/models/llama-3.1-8b.nemo",
    data_path="/data/train.jsonl",
    peft_config=lora_config,
    trainer_config={
        "devices": 8,
        "max_epochs": 3,
        "precision": "bf16",
    },
    output_path="/output/llama-2-7b-finetuned",
)
```

## ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

### ëŒ€ê·œëª¨ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ìƒ¤ë”© (>70B)

70B ì´ìƒì˜ ëŒ€ê·œëª¨ ëª¨ë¸ì€ ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ìˆ˜ë°± GBì— ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. NeMoëŠ” ì²´í¬í¬ì¸íŠ¸ ìƒ¤ë”©ì„ í†µí•´ ì´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤:

```yaml
# ëŒ€ê·œëª¨ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ìƒ¤ë”© ì„¤ì •
trainer:
  checkpoint:
    # ìƒ¤ë”© í™œì„±í™”
    save_sharded_checkpoint: true
    
    # ìƒ¤ë“œ í¬ê¸° (GB ë‹¨ìœ„)
    shard_size_gb: 10
    
    # ë³‘ë ¬ ì €ì¥ ì›Œì»¤ ìˆ˜
    num_workers: 8
    
    # ì²´í¬í¬ì¸íŠ¸ ì••ì¶•
    compression: "gzip"
```

**ìƒ¤ë”© ì „ëµ:**

<CheckpointSharding />

```python
# ìƒ¤ë”©ëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
from nemo.collections.nlp.models import MegatronGPTModel

# ìë™ìœ¼ë¡œ ëª¨ë“  ìƒ¤ë“œë¥¼ ë³‘ë ¬ ë¡œë“œ
model = MegatronGPTModel.restore_from(
    restore_path="s3://checkpoints/llama-405b/sharded",
    trainer=trainer,
)
```

### S3 ì²´í¬í¬ì¸íŠ¸ ì €ì¥

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nemo-checkpoint-config
  namespace: nemo
data:
  checkpoint.yaml: |
    checkpoint:
      save_dir: "s3://nemo-checkpoints/${JOB_NAME}"
      save_top_k: 3
      save_last: true
      save_interval: 500
      
      # ìë™ ë³µêµ¬ ì„¤ì •
      resume:
        enabled: true
        resume_from_checkpoint: "auto"  # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìë™ ë³µêµ¬
```

### ì²´í¬í¬ì¸íŠ¸ ë³€í™˜

```bash
# NeMo ì²´í¬í¬ì¸íŠ¸ë¥¼ HuggingFace í˜•ì‹ìœ¼ë¡œ ë³€í™˜
python -m nemo.collections.llm.scripts.convert_nemo_to_hf \
  --input_path /checkpoints/llama-finetuned.nemo \
  --output_path /models/llama-finetuned-hf \
  --model_type llama
```

## TensorRT-LLM ë³€í™˜ ë° ìµœì í™”

### ëª¨ë¸ ë³€í™˜ íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    NeMo["NeMo<br/>Checkpoint"]
    HF["HuggingFace<br/>Format"]
    TRT["TensorRT-LLM<br/>Engine"]
    Triton["Triton<br/>Server"]
    
    NeMo --> HF
    HF --> TRT
    TRT --> Triton
    
    style TRT fill:#76b900,stroke:#333,stroke-width:2px
```

### TensorRT-LLM ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸

```python
# convert_to_trt.py
# TensorRT-LLM 0.8+ API ì‚¬ìš©
from tensorrt_llm import LLM

# ëª¨ë¸ ë³€í™˜ (from_pretrained API ì‚¬ìš©)
llm = LLM(
    model="/models/llama-finetuned-hf",
    # ë¹Œë“œ ì„¤ì •
    max_input_len=4096,
    max_output_len=2048,
    max_batch_size=64,
    
    # ì–‘ìí™” ì„¤ì •
    dtype="fp8",  # FP8 ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    
    # ìµœì í™” ì„¤ì •
    enable_paged_kv_cache=True,
    enable_chunked_context=True,
)

# ì—”ì§„ ì €ì¥
llm.save("/engines/llama-finetuned-trt")
```

### Kubernetes Jobìœ¼ë¡œ ë³€í™˜ ì‹¤í–‰

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: trt-llm-conversion
  namespace: nemo
spec:
  template:
    spec:
      containers:
      - name: converter
        image: nvcr.io/nvidia/tritonserver:24.07-trtllm-python-py3
        command:
        - python
        - /scripts/convert_to_trt.py
        - --input=/models/llama-finetuned-hf
        - --output=/engines/llama-finetuned-trt
        - --quantization=fp8
        - --max-batch-size=64
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "80Gi"
        volumeMounts:
        - name: models
          mountPath: /models
        - name: engines
          mountPath: /engines
        - name: scripts
          mountPath: /scripts
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: models-pvc
      - name: engines
        persistentVolumeClaim:
          claimName: engines-pvc
      - name: scripts
        configMap:
          name: conversion-scripts
      restartPolicy: Never
```

## Triton Inference Server ë°°í¬

### TensorRT-LLM ë°±ì—”ë“œ ì„¤ì •

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: triton-trtllm
  namespace: inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: triton-trtllm
  template:
    metadata:
      labels:
        app: triton-trtllm
    spec:
      containers:
      - name: triton
        image: nvcr.io/nvidia/tritonserver:24.07-trtllm-python-py3
        args:
        - tritonserver
        - --model-repository=/models
        - --http-port=8000
        - --grpc-port=8001
        - --metrics-port=8002
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 8001
          name: grpc
        - containerPort: 8002
          name: metrics
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "80Gi"
        volumeMounts:
        - name: model-repository
          mountPath: /models
      volumes:
      - name: model-repository
        persistentVolumeClaim:
          claimName: triton-models-pvc
```

### ëª¨ë¸ ì €ì¥ì†Œ êµ¬ì¡°

```
/models/
â””â”€â”€ llama-finetuned/
    â”œâ”€â”€ config.pbtxt
    â”œâ”€â”€ 1/
    â”‚   â””â”€â”€ model.plan
    â””â”€â”€ tokenizer/
        â”œâ”€â”€ tokenizer.json
        â””â”€â”€ tokenizer_config.json
```

### config.pbtxt ì„¤ì •

```protobuf
name: "llama-finetuned"
backend: "tensorrtllm"
max_batch_size: 64

input [
  {
    name: "input_ids"
    data_type: TYPE_INT32
    dims: [-1]
  },
  {
    name: "input_lengths"
    data_type: TYPE_INT32
    dims: [1]
  }
]

output [
  {
    name: "output_ids"
    data_type: TYPE_INT32
    dims: [-1]
  }
]

instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [0]
  }
]

parameters {
  key: "max_tokens_in_paged_kv_cache"
  value: { string_value: "8192" }
}

parameters {
  key: "batch_scheduler_policy"
  value: { string_value: "inflight_fused_batching" }
}
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### í•™ìŠµ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nemo-training-monitor
  namespace: nemo
spec:
  selector:
    matchLabels:
      app: nemo-training
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

### ì£¼ìš” ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

<MonitoringMetrics />

---

## NCCL ì‹¬ì¸µ ë¶„ì„: ë¶„ì‚° í•™ìŠµ í†µì‹  ìµœì í™”

### NCCLì˜ ì—­í• ê³¼ ì¤‘ìš”ì„±

NCCL (**NVIDIA Collective Communication Library**)ëŠ” ë¶„ì‚° GPU í•™ìŠµì—ì„œ **multi-GPU ê°„ ê³ ì† í†µì‹ **ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ NCCLì˜ ìµœì í™” ì •ë„ì— ì§ì ‘ì ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.

```mermaid
graph TB
    subgraph "ë¶„ì‚° í•™ìŠµ ì„±ëŠ¥ ë¶„ì„"
        A["ì „ì²´ í•™ìŠµ ì‹œê°„"] --> B["ê³„ì‚° ì‹œê°„ 60%"]
        A --> C["í†µì‹  ì‹œê°„ 40%"]

        C --> D["NCCLì´ ìµœì í™”í•˜ëŠ” ì˜ì—­"]
        D --> E["Collective ì—°ì‚° ì‹œê°„"]
        E --> F["ë™ê¸°í™” ì˜¤ë²„í—¤ë“œ"]

        B --> G["GPU ê³„ì‚° (ì»¤ë„)"]

        style D fill:#326ce5
        style E fill:#76b900
        style F fill:#ff6b6b
    end

    subgraph "NCCLì´ í•´ê²°í•˜ëŠ” ë¬¸ì œ"
        H["Raw ë„¤íŠ¸ì›Œí¬ ëŒ€ë¹„<br/>3-10ë°° ê°œì„ "]
        I["CPU ì˜¤ë²„í—¤ë“œ ì œê±°"]
        J["GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±"]
        K["NVLink/EFA ìë™ í™œìš©"]
    end
```

**ë¶„ì‚° í•™ìŠµì—ì„œ NCCLì´ ì¤‘ìš”í•œ ì´ìœ :**

<NCCLImportance />

### í•µì‹¬ ì§‘í•© ì—°ì‚° (Collective Operations)

#### 1. AllReduce - ê°€ì¥ ì¤‘ìš”í•œ ì—°ì‚°

AllReduceëŠ” ëª¨ë“  GPUì˜ ë°ì´í„°ë¥¼ í•©ì‚°í•˜ê³  ê²°ê³¼ë¥¼ ëª¨ë“  GPUì— ë°°ë¶„í•©ë‹ˆë‹¤:

```
ì´ˆê¸° ìƒíƒœ:
GPU 0: [1, 2, 3]
GPU 1: [4, 5, 6]
GPU 2: [7, 8, 9]
GPU 3: [10, 11, 12]

AllReduce í›„:
GPU 0: [22, 26, 30]  # 1+4+7+10, 2+5+8+11, 3+6+9+12
GPU 1: [22, 26, 30]
GPU 2: [22, 26, 30]
GPU 3: [22, 26, 30]
```

**AllReduce ì‚¬ìš© ì˜ˆì‹œ (ë¶„ì‚° í•™ìŠµì—ì„œ):**

```python
import torch
import torch.distributed as dist

# ë¶„ì‚° í•™ìŠµ ì´ˆê¸°í™”
dist.init_process_group("nccl")
rank = dist.get_rank()
world_size = dist.get_world_size()

# ê° GPUì˜ ê·¸ë˜ë””ì–¸íŠ¸ (ì„œë¡œ ë‹¤ë¦„)
gradients = torch.randn(1024, device=f"cuda:{rank}")

# AllReduce: ëª¨ë“  GPUì˜ ê·¸ë˜ë””ì–¸íŠ¸ í•©ì‚° ë° í‰ê· í™”
dist.all_reduce(gradients, op=dist.ReduceOp.SUM)
gradients /= world_size

# ì´ì œ ëª¨ë“  GPUê°€ ë™ì¼í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê°€ì§
# ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹œ ë™ê¸°í™”ë¨
```

#### 2. AllGather - ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘

AllGatherëŠ” ëª¨ë“  GPUì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ê° GPUì— ì „ì²´ ë°ì´í„°ë¥¼ ë°°ë¶„í•©ë‹ˆë‹¤:

```
ì´ˆê¸° ìƒíƒœ:
GPU 0: [1, 2]
GPU 1: [3, 4]
GPU 2: [5, 6]
GPU 3: [7, 8]

AllGather í›„:
GPU 0: [1, 2, 3, 4, 5, 6, 7, 8]
GPU 1: [1, 2, 3, 4, 5, 6, 7, 8]
GPU 2: [1, 2, 3, 4, 5, 6, 7, 8]
GPU 3: [1, 2, 3, 4, 5, 6, 7, 8]
```

**AllGather ì‚¬ìš© ì‚¬ë¡€:**

```python
# ì˜ˆì‹œ: ë°°ì¹˜ ì •ê·œí™”ì—ì„œ ëª¨ë“  GPUì˜ í†µê³„ ìˆ˜ì§‘
local_batch_stats = compute_batch_stats(local_batch)

# AllGatherë¡œ ëª¨ë“  GPUì˜ í†µê³„ ìˆ˜ì§‘
all_batch_stats = [torch.empty_like(local_batch_stats) for _ in range(world_size)]
dist.all_gather(all_batch_stats, local_batch_stats)

# ì „ì—­ í†µê³„ ê³„ì‚°
global_mean = torch.stack(all_batch_stats).mean(dim=0)
global_std = torch.stack(all_batch_stats).std(dim=0)
```

#### 3. ReduceScatter - AllGatherì˜ ì—­ì—°ì‚°

ReduceScatterëŠ” ë°ì´í„°ë¥¼ ë¨¼ì € í•©ì‚°í•œ í›„ ê° GPUì— ë¶„í• í•˜ì—¬ ë°°ë¶„í•©ë‹ˆë‹¤:

```
ì´ˆê¸° ìƒíƒœ:
GPU 0: [1, 2, 3, 4, 5, 6, 7, 8]
GPU 1: [9, 10, 11, 12, 13, 14, 15, 16]
GPU 2: [17, 18, 19, 20, 21, 22, 23, 24]
GPU 3: [25, 26, 27, 28, 29, 30, 31, 32]

ReduceScatter í•©ì‚° í›„ ë¶„í• :
GPU 0: [52, 56]      # (1+9+17+25), (2+10+18+26)
GPU 1: [60, 64]      # (3+11+19+27), (4+12+20+28)
GPU 2: [68, 72]      # (5+13+21+29), (6+14+22+30)
GPU 3: [76, 80]      # (7+15+23+31), (8+16+24+32)
```

**ReduceScatter ì‚¬ìš© ì‚¬ë¡€ (Model Parallelism):**

```python
# ëª¨ë¸ ë³‘ë ¬í™”ì—ì„œ ê³„ì‚° ê²°ê³¼ë¥¼ í•©ì‚°í•˜ê³  ë¶„í• 
local_output = model_fragment(input_data)

# ReduceScatter: ëª¨ë“  í”„ë˜ê·¸ë¨¼íŠ¸ í•©ì‚° í›„ ê° GPUì— ë¶„í• 
reduced_output = torch.empty(output_size // world_size, device=local_output.device)
dist.reduce_scatter(reduced_output, [local_output] * world_size)
```

#### 4. Broadcast - ë°ì´í„° ë°°í¬

BroadcastëŠ” í•œ GPUì˜ ë°ì´í„°ë¥¼ ëª¨ë“  GPUì— ë³µì‚¬í•©ë‹ˆë‹¤:

```
ì´ˆê¸° ìƒíƒœ:
GPU 0: [1, 2, 3, 4]
GPU 1: [0, 0, 0, 0]
GPU 2: [0, 0, 0, 0]
GPU 3: [0, 0, 0, 0]

Broadcast í›„:
GPU 0: [1, 2, 3, 4]
GPU 1: [1, 2, 3, 4]
GPU 2: [1, 2, 3, 4]
GPU 3: [1, 2, 3, 4]
```

**Broadcast ì‚¬ìš© ì‚¬ë¡€:**

```python
# ë§ˆìŠ¤í„° GPUì—ì„œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¸Œë¡œë“œìºìŠ¤íŠ¸
model_state = load_checkpoint() if rank == 0 else None

# Broadcast: ë§ˆìŠ¤í„° GPUì˜ ëª¨ë¸ ìƒíƒœë¥¼ ëª¨ë“  GPUì— ë°°í¬
dist.broadcast_object_list([model_state], src=0)
model.load_state_dict(model_state)
```

### ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ ì¸ì‹

NCCLì€ GPU ê°„ ë¬¼ë¦¬ì  ì—°ê²° í† í´ë¡œì§€ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ìµœì ì˜ ê²½ë¡œë¥¼ ì„ íƒí•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "í† í´ë¡œì§€ ê³„ì¸µ (ìœ„ì—ì„œ ì•„ë˜ë¡œ ë¹ ë¦„)"
        L1["1. NVSwitch (ê°™ì€ ë…¸ë“œ ë‚´)<br/>ìµœëŒ€ 600GB/s"]
        L2["2. NVLink (ê°™ì€ ë…¸ë“œ ë‚´)<br/>ìµœëŒ€ 200GB/s"]
        L3["3. EFA/InfiniBand (ë…¸ë“œ ê°„)<br/>ìµœëŒ€ 100GB/s"]
        L4["4. Ethernet (ë…¸ë“œ ê°„)<br/>ìµœëŒ€ 10-100GB/s"]
    end

    L1 --> L2 --> L3 --> L4

    subgraph "NCCL ìë™ ê²½ë¡œ ì„ íƒ"
        A["í† í´ë¡œì§€ ë¶„ì„"] --> B["ìµœì  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ"]
        B --> C["ì±„ë„ êµ¬ì„±"]
    end

    style L1 fill:#76b900
    style L2 fill:#76b900
    style L3 fill:#4ecdc4
    style L4 fill:#ff6b6b
```

### NCCL ì„±ëŠ¥ íŠœë‹ íŒŒë¼ë¯¸í„°

```yaml
# NCCL í™˜ê²½ ë³€ìˆ˜ ì™„ë²½ ê°€ì´ë“œ

# 1. ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
export NCCL_ALGO=Ring           # Ring (ê¸°ë³¸), Tree, CollNet
export NCCL_ALGO_ALL=Ring       # AllReduce ì•Œê³ ë¦¬ì¦˜ ì§€ì •
export NCCL_ALGO_TREE=Tree      # Tree ì•Œê³ ë¦¬ì¦˜ ê°•ì œ

# 2. í”„ë¡œí† ì½œ ì„ íƒ
export NCCL_PROTO=Simple        # Simple (ê¸°ë³¸) ë˜ëŠ” LL (Low Latency)

# 3. ì±„ë„ ì„¤ì • (ë§¤ìš° ì¤‘ìš”)
export NCCL_MIN_NCHANNELS=4     # ìµœì†Œ ì±„ë„ ìˆ˜ (ê¸°ë³¸ 4)
export NCCL_MAX_NCHANNELS=8     # ìµœëŒ€ ì±„ë„ ìˆ˜ (ê¸°ë³¸ 32)

# 4. ë²„í¼ í¬ê¸°
export NCCL_BUFFSIZE=2097152    # ê¸°ë³¸ 2MB, 1MB-4MB ê¶Œì¥

# 5. ë””ë²„ê·¸ ì„¤ì •
export NCCL_DEBUG=INFO          # TRACE, DEBUG, INFO, WARN
export NCCL_DEBUG_FILE=/var/log/nccl-debug.txt
export NCCL_DEBUG_SUBSYS=ALL    # ëª¨ë“  ì„œë¸Œì‹œìŠ¤í…œ ì¶”ì 

# 6. ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤
export NCCL_SOCKET_IFNAME=eth0  # ì‚¬ìš©í•  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤
export NCCL_IB_DISABLE=0        # InfiniBand ì‚¬ìš©

# 7. EFA ì„¤ì • (AWS)
export FI_PROVIDER=efa
export FI_EFA_USE_DEVICE_RDMA=1
export FI_EFA_FORK_SAFE=1

# 8. ì»¤ë„ ìµœì í™”
export NCCL_CHECKS_DISABLE=0    # ì•ˆì „ ê²€ì‚¬ í™œì„±í™” (í”„ë¡œë•ì…˜)
export NCCL_COMM_BLOCKING_WAIT=0
export NCCL_ASYNC_ERROR_HANDLING=1

# 9. P2P ì„¤ì •
export NCCL_P2P_DISABLE=0       # GPU P2P í†µì‹  í™œì„±í™”
export NCCL_P2P_LEVEL=SYS       # P2P ë ˆë²¨: LOC (ë¡œì»¬), SYS (ì‹œìŠ¤í…œ)

# 10. íƒ€ì„ì•„ì›ƒ ì„¤ì •
export NCCL_COMM_WAIT_TIMEOUT=0 # 0 = ë¬´í•œ ëŒ€ê¸°
```

---

## ê´€ë ¨ ë¬¸ì„œ

- [GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬](./gpu-resource-management.md)
- [MoE ëª¨ë¸ ì„œë¹™](./moe-model-serving.md)
- [Inference Gateway](./inference-gateway-routing.md)

:::tip ê¶Œì¥ ì‚¬í•­

- íŒŒì¸íŠœë‹ ì „ ê¸°ë³¸ ëª¨ë¸ë¡œ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì„¸ìš”
- LoRA/QLoRAë¥¼ ì‚¬ìš©í•˜ë©´ ì ì€ GPUë¡œë„ ëŒ€í˜• ëª¨ë¸ íŒŒì¸íŠœë‹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤
- TensorRT-LLM ë³€í™˜ìœ¼ë¡œ ì¶”ë¡  ì„±ëŠ¥ì„ 2-4ë°° í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- NCCL í™˜ê²½ ë³€ìˆ˜ë¥¼ ì ì ˆíˆ ì„¤ì •í•˜ë©´ ë¶„ì‚° í•™ìŠµ ì„±ëŠ¥ì„ í¬ê²Œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
:::

:::warning ì£¼ì˜ì‚¬í•­

- ëŒ€ê·œëª¨ í•™ìŠµì€ ìƒë‹¹í•œ GPU ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤. Spot ì¸ìŠ¤í„´ìŠ¤ì™€ ì²´í¬í¬ì¸íŠ¸ë¥¼ í™œìš©í•˜ì„¸ìš”
- ë¶„ì‚° í•™ìŠµ ì‹œ NCCL í†µì‹  ì˜¤ë²„í—¤ë“œë¥¼ ê³ ë ¤í•˜ì—¬ ë…¸ë“œ ìˆ˜ë¥¼ ê²°ì •í•˜ì„¸ìš”
- ì²´í¬í¬ì¸íŠ¸ëŠ” ë°˜ë“œì‹œ S3 ë“± ì˜êµ¬ ìŠ¤í† ë¦¬ì§€ì— ì €ì¥í•˜ì„¸ìš”
- EFA ì‚¬ìš© ì‹œ ì ì ˆí•œ ë³´ì•ˆ ê·¸ë£¹ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤ (ëª¨ë“  íŠ¸ë˜í”½ í—ˆìš©)
:::
