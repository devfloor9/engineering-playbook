---
title: "ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§ ë° ìë™ ë³µêµ¬ íŒ¨í„´"
sidebar_label: "4. ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§ ë° ìë™ ë³µêµ¬"
description: "ML ê¸°ë°˜ ì˜ˆì¸¡ ì˜¤í† ìŠ¤ì¼€ì¼ë§, Karpenter+AI ì„ ì œ í”„ë¡œë¹„ì €ë‹, AI Agent ììœ¨ ì¸ì‹œë˜íŠ¸ ëŒ€ì‘, Kiro í”„ë¡œê·¸ë˜ë¨¸í‹± ë””ë²„ê¹… íŒ¨í„´"
sidebar_position: 4
category: "aiops-aidlc"
tags: [aiops, predictive-scaling, auto-remediation, karpenter, self-healing, eks, kiro, mcp, ai-agent, chaos-engineering]
last_update:
  date: 2026-02-12
  author: devfloor9
---

import { ScalingComparison, ResponsePatterns, MaturityTable, EvolutionStages, MLModelComparison, AnomalyMetrics, RightSizingResults, ChaosExperiments, DashboardPanels } from '@site/src/components/PredictiveOpsTables';

# ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§ ë° ìë™ ë³µêµ¬ íŒ¨í„´

> ğŸ“… **ì‘ì„±ì¼**: 2026-02-12 | â±ï¸ **ì½ëŠ” ì‹œê°„**: ì•½ 30ë¶„ | ğŸ“Œ **ê¸°ì¤€ í™˜ê²½**: EKS 1.35+, Karpenter v1.1+, CloudWatch, Kiro

---

## 1. ê°œìš”

### 1.1 ë°˜ì‘í˜•ì—ì„œ ììœ¨í˜•ìœ¼ë¡œ

EKS ìš´ì˜ì˜ ì§„í™”ëŠ” **ë°˜ì‘í˜• â†’ ì˜ˆì¸¡í˜• â†’ ììœ¨í˜•**ì˜ 3ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.

<EvolutionStages />

:::info ì´ ë¬¸ì„œì˜ ë²”ìœ„
ë°˜ì‘í˜• ìŠ¤ì¼€ì¼ë§ì˜ í•œê³„ë¥¼ ë„˜ì–´, ML ê¸°ë°˜ ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§ê³¼ AI Agentë¥¼ í†µí•œ ììœ¨ ë³µêµ¬ íŒ¨í„´ì„ ë‹¤ë£¹ë‹ˆë‹¤. íŠ¹íˆ Kiro+MCP ê¸°ë°˜ **í”„ë¡œê·¸ë˜ë¨¸í‹± ë””ë²„ê¹…**ê³¼ Kagent/Strands ê¸°ë°˜ **ìë™ ì¸ì‹œë˜íŠ¸ ëŒ€ì‘**ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
:::

### 1.2 ì™œ ì˜ˆì¸¡ ìš´ì˜ì´ í•„ìš”í•œê°€

- **HPAì˜ í•œê³„**: ë©”íŠ¸ë¦­ ì„ê³„ê°’ ì´ˆê³¼ í›„ ë°˜ì‘ â†’ ì´ë¯¸ ì‚¬ìš©ì ì˜í–¥ ë°œìƒ
- **Cold Start ë¬¸ì œ**: ìƒˆ Pod ì‹œì‘ê¹Œì§€ 30ì´ˆ-2ë¶„ â†’ íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œ ëŒ€ì‘ ë¶ˆê°€
- **ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ ì§€ì—°**: Karpenterë„ ë…¸ë“œ ì‹œì‘ì— 1-3ë¶„ ì†Œìš”
- **ë³µí•© ì¥ì• **: ë‹¨ì¼ ë©”íŠ¸ë¦­ìœ¼ë¡œëŠ” ê°ì§€ ë¶ˆê°€í•œ ë³µí•© ì›ì¸ ì¥ì•  ì¦ê°€
- **ë¹„ìš© ë¹„íš¨ìœ¨**: ê³¼ë„í•œ ì—¬ìœ  ë¦¬ì†ŒìŠ¤ í™•ë³´ â†’ ë¹„ìš© ë‚­ë¹„

---

## 2. ML ê¸°ë°˜ ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§

### 2.1 HPAì˜ í•œê³„

HPA(Horizontal Pod Autoscaler)ëŠ” **í˜„ì¬ ë©”íŠ¸ë¦­**ì— ë°˜ì‘í•˜ë¯€ë¡œ êµ¬ì¡°ì  í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.

<ScalingComparison />

```
[HPAì˜ ë°˜ì‘í˜• ìŠ¤ì¼€ì¼ë§]

íŠ¸ë˜í”½ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
                      â†‘ ì„ê³„ê°’ ì´ˆê³¼
                      |
Pod ìˆ˜  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                  â†‘ ìŠ¤ì¼€ì¼ì•„ì›ƒ ì‹œì‘
                  |  (ì§€ì—° ë°œìƒ)
ì‚¬ìš©ì   âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ—âœ—âœ—âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
ê²½í—˜              â†‘ ì„±ëŠ¥ ì €í•˜ êµ¬ê°„

[ML ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§]

íŠ¸ë˜í”½ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
             â†‘ ì˜ˆì¸¡ ì‹œì  (30ë¶„ ì „)
             |
Pod ìˆ˜  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
             â†‘ ì‚¬ì „ ìŠ¤ì¼€ì¼ì•„ì›ƒ
             |
ì‚¬ìš©ì   âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
ê²½í—˜     (ì„±ëŠ¥ ì €í•˜ ì—†ìŒ)
```

### 2.2 ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸

EKS ì›Œí¬ë¡œë“œì˜ íŠ¸ë˜í”½ íŒ¨í„´ì„ ì˜ˆì¸¡í•˜ëŠ” ëŒ€í‘œì  ML ëª¨ë¸:

<MLModelComparison />

### 2.3 Prophet ê¸°ë°˜ ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§ êµ¬í˜„

```python
# Prophet ê¸°ë°˜ EKS íŠ¸ë˜í”½ ì˜ˆì¸¡
import boto3
from prophet import Prophet
import pandas as pd
from datetime import datetime, timedelta

def fetch_metrics_from_amp(workspace_id, query, hours=168):
    """AMPì—ì„œ ì§€ë‚œ 7ì¼ê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    client = boto3.client('amp', region_name='ap-northeast-2')
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=hours)

    response = client.query_range(
        workspaceId=workspace_id,
        query=query,
        startTime=start_time,
        endTime=end_time,
        step='5m'
    )
    return response

def predict_scaling(metrics_df, forecast_hours=2):
    """Prophetìœ¼ë¡œ í–¥í›„ íŠ¸ë˜í”½ ì˜ˆì¸¡"""
    # Prophet í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    df = metrics_df.rename(columns={
        'timestamp': 'ds',
        'value': 'y'
    })

    model = Prophet(
        changepoint_prior_scale=0.05,
        seasonality_mode='multiplicative',
        daily_seasonality=True,
        weekly_seasonality=True,
    )
    model.fit(df)

    # í–¥í›„ forecast_hours ì˜ˆì¸¡
    future = model.make_future_dataframe(
        periods=forecast_hours * 12,  # 5ë¶„ ê°„ê²©
        freq='5min'
    )
    forecast = model.predict(future)

    return forecast[['ds', 'yhat', 'yhat_upper', 'yhat_lower']]

def calculate_required_pods(predicted_rps, pod_capacity_rps=100):
    """ì˜ˆì¸¡ RPS ê¸°ë°˜ í•„ìš” Pod ìˆ˜ ê³„ì‚°"""
    # ìƒí•œê°’(yhat_upper) ì‚¬ìš©ìœ¼ë¡œ ì•ˆì „ ë§ˆì§„ í™•ë³´
    required = int(predicted_rps / pod_capacity_rps) + 1
    return max(required, 2)  # ìµœì†Œ 2ê°œ ìœ ì§€

def apply_scaling(namespace, deployment, target_replicas):
    """kubectlì„ í†µí•´ ìŠ¤ì¼€ì¼ë§ ì ìš©"""
    import subprocess
    cmd = f"kubectl scale deployment/{deployment} -n {namespace} --replicas={target_replicas}"
    subprocess.run(cmd.split(), check=True)
    print(f"Scaled {deployment} to {target_replicas} replicas")
```

### 2.4 CronJob ê¸°ë°˜ ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§ ìë™í™”

```yaml
# ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§ì„ ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: predictive-scaler
  namespace: scaling
spec:
  schedule: "*/15 * * * *"  # 15ë¶„ë§ˆë‹¤ ì‹¤í–‰
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: predictive-scaler
          containers:
            - name: scaler
              image: my-registry/predictive-scaler:latest
              env:
                - name: AMP_WORKSPACE_ID
                  value: "ws-xxxxx"
                - name: TARGET_NAMESPACE
                  value: "payment"
                - name: TARGET_DEPLOYMENT
                  value: "payment-service"
                - name: FORECAST_HOURS
                  value: "2"
              resources:
                requests:
                  cpu: 500m
                  memory: 1Gi
                limits:
                  cpu: "1"
                  memory: 2Gi
          restartPolicy: OnFailure
```

### 2.5 ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ì˜ˆì¸¡ ë° ML ì¶”ë¡  ì›Œí¬ë¡œë“œ ìµœì í™”

EKSì˜ **Container Network Observability**ëŠ” Pod-to-Pod í†µì‹  íŒ¨í„´ì„ ì„¸ë°€í•˜ê²Œ ëª¨ë‹ˆí„°ë§í•˜ì—¬, ë„¤íŠ¸ì›Œí¬ ë³‘ëª©ì„ ì‚¬ì „ì— ì˜ˆì¸¡í•˜ê³  ML ì¶”ë¡  ì›Œí¬ë¡œë“œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Container Network Observability ë°ì´í„° í™œìš©

**1. Pod-to-Pod í†µì‹  íŒ¨í„´ â†’ ë„¤íŠ¸ì›Œí¬ ë³‘ëª© ì˜ˆì¸¡**

```python
# Container Network Observability ë©”íŠ¸ë¦­ ê¸°ë°˜ ë³‘ëª© ì˜ˆì¸¡
import boto3
from prophet import Prophet
import pandas as pd

def predict_network_bottleneck(cluster_name, namespace):
    """
    Pod-to-Pod ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì„ ì˜ˆì¸¡í•˜ì—¬ ë³‘ëª© ê°€ëŠ¥ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤.
    """
    cloudwatch = boto3.client('cloudwatch')

    # Container Network Observability ë©”íŠ¸ë¦­ ì¡°íšŒ
    metrics = cloudwatch.get_metric_data(
        MetricDataQueries=[
            {
                'Id': 'rx_latency',
                'MetricStat': {
                    'Metric': {
                        'Namespace': 'ContainerInsights',
                        'MetricName': 'pod_network_rx_latency_ms',
                        'Dimensions': [
                            {'Name': 'ClusterName', 'Value': cluster_name},
                            {'Name': 'Namespace', 'Value': namespace}
                        ]
                    },
                    'Period': 300,
                    'Stat': 'Average'
                }
            },
            {
                'Id': 'tx_bytes',
                'MetricStat': {
                    'Metric': {
                        'Namespace': 'ContainerInsights',
                        'MetricName': 'pod_network_tx_bytes',
                        'Dimensions': [
                            {'Name': 'ClusterName', 'Value': cluster_name},
                            {'Name': 'Namespace', 'Value': namespace}
                        ]
                    },
                    'Period': 300,
                    'Stat': 'Sum'
                }
            }
        ],
        StartTime=datetime.utcnow() - timedelta(days=7),
        EndTime=datetime.utcnow()
    )

    # Prophet ëª¨ë¸ë¡œ í–¥í›„ 2ì‹œê°„ ì˜ˆì¸¡
    df = pd.DataFrame({
        'ds': [d['Timestamp'] for d in metrics['MetricDataResults'][0]['Timestamps']],
        'y': [d for d in metrics['MetricDataResults'][0]['Values']]
    })

    model = Prophet(changepoint_prior_scale=0.05)
    model.fit(df)

    future = model.make_future_dataframe(periods=24, freq='5min')
    forecast = model.predict(future)

    # ë³‘ëª© ì˜ˆì¸¡: ë ˆì´í„´ì‹œê°€ í‰ì†Œ ëŒ€ë¹„ 2ë°° ì´ìƒ ì¦ê°€ ì˜ˆìƒ
    baseline = df['y'].mean()
    predicted_peak = forecast['yhat'].iloc[-1]

    if predicted_peak > baseline * 2:
        return {
            'bottleneck_risk': 'HIGH',
            'predicted_latency_ms': predicted_peak,
            'baseline_latency_ms': baseline,
            'action': 'consider_network_policy_optimization'
        }
    return {'bottleneck_risk': 'LOW'}
```

**2. Cross-AZ íŠ¸ë˜í”½ ì¶”ì´ â†’ ë¹„ìš© ìµœì í™” ì˜ˆì¸¡**

```promql
# Cross-AZ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ë¹„ìš© ì¶”ì 
sum(rate(pod_network_tx_bytes{
  source_az!="", dest_az!="",
  source_az!=dest_az
}[5m])) by (source_az, dest_az)
* 0.01 / 1024 / 1024 / 1024  # $0.01/GB
```

**ë¹„ìš© ìµœì í™” ì „ëµ**:

- **í† í´ë¡œì§€ ì¸ì‹ ìŠ¤ì¼€ì¤„ë§**: Kubernetes Topology Aware Hintsë¥¼ í™œìš©í•˜ì—¬ ë™ì¼ AZ ë‚´ í†µì‹  ì„ í˜¸
- **ì„œë¹„ìŠ¤ ë©”ì‹œ ìµœì í™”**: Istio locality load balancingìœ¼ë¡œ Cross-AZ íŠ¸ë˜í”½ ìµœì†Œí™”
- **ì˜ˆì¸¡ ê¸°ë°˜ ë°°ì¹˜**: ML ëª¨ë¸ì´ í†µì‹  íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ìµœì  AZ ë°°ì¹˜ ì œì•ˆ

```yaml
# Topology Aware Hints í™œì„±í™”
apiVersion: v1
kind: Service
metadata:
  name: ml-inference-service
  annotations:
    service.kubernetes.io/topology-mode: Auto
spec:
  selector:
    app: ml-inference
  ports:
    - port: 8080
  type: ClusterIP
```

#### ML ì¶”ë¡  ì›Œí¬ë¡œë“œ ì„±ëŠ¥ ì˜ˆì¸¡

**1. Ray, vLLM, Triton, PyTorch ì›Œí¬ë¡œë“œ ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**

```yaml
# vLLM ì¶”ë¡  ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-network-monitoring
data:
  metrics.yaml: |
    # Container Network Observability ë©”íŠ¸ë¦­
    metrics:
      - pod_network_rx_bytes
      - pod_network_tx_bytes
      - pod_network_rx_latency_ms
      - pod_network_rx_errors_total

    # ì¶”ê°€ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­
    custom_metrics:
      - name: vllm_inference_network_throughput_mbps
        query: |
          sum(rate(pod_network_rx_bytes{app="vllm-inference"}[1m]))
          / 1024 / 1024

      - name: vllm_model_load_network_time_ms
        query: |
          histogram_quantile(0.99,
            rate(pod_network_rx_latency_bucket{
              app="vllm-inference",
              operation="model_load"
            }[5m])
          )
```

**Ray ë¶„ì‚° ì¶”ë¡  ë„¤íŠ¸ì›Œí¬ íŒ¨í„´**:

```python
# Ray í´ëŸ¬ìŠ¤í„°ì˜ ë„¤íŠ¸ì›Œí¬ ë³‘ëª© ê°ì§€
import ray
from ray import serve

@serve.deployment
class LLMInferenceDeployment:
    def __init__(self):
        self.model = load_model()
        self.network_monitor = NetworkMonitor()

    async def __call__(self, request):
        # ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì¶”ì 
        start_time = time.time()

        # Rayì˜ ë¶„ì‚° ì¶”ë¡  í˜¸ì¶œ
        result = await self.model.generate(request.prompt)

        network_latency = time.time() - start_time

        # CloudWatchì— ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì „ì†¡
        self.network_monitor.record_latency(network_latency)

        # ë„¤íŠ¸ì›Œí¬ ë³‘ëª© ê°ì§€ ì‹œ ìŠ¤ì¼€ì¼ ì•„ì›ƒ íŠ¸ë¦¬ê±°
        if network_latency > 200:  # 200ms ì´ìƒ
            trigger_scale_out()

        return result
```

**2. ì¶”ë¡  ë ˆì´í„´ì‹œ â†’ ìŠ¤ì¼€ì¼ ì•„ì›ƒ íŠ¸ë¦¬ê±° ì˜ˆì¸¡**

```python
# ML ì¶”ë¡  ë ˆì´í„´ì‹œ ê¸°ë°˜ ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§
def predict_inference_scaling(service_name, forecast_hours=2):
    """
    ì¶”ë¡  ë ˆì´í„´ì‹œ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ìŠ¤ì¼€ì¼ ì•„ì›ƒ í•„ìš” ì‹œì ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """
    # ì§€ë‚œ 7ì¼ê°„ ì¶”ë¡  ë ˆì´í„´ì‹œ ë°ì´í„° ìˆ˜ì§‘
    latency_data = fetch_inference_latency_from_cloudwatch(
        service_name=service_name,
        days=7
    )

    # ì¶”ë¡  ìš”ì²­ ìˆ˜ ë°ì´í„° ìˆ˜ì§‘
    request_volume = fetch_request_volume(service_name, days=7)

    # ë ˆì´í„´ì‹œì™€ ìš”ì²­ ìˆ˜ì˜ ìƒê´€ê´€ê³„ ë¶„ì„
    df = pd.DataFrame({
        'timestamp': latency_data['timestamps'],
        'latency_p99': latency_data['p99'],
        'request_rate': request_volume['rate']
    })

    # ì„ê³„ê°’ ê³„ì‚°: P99 ë ˆì´í„´ì‹œ > 500ms ì‹œì ì˜ ìš”ì²­ ìˆ˜
    threshold_requests = df[df['latency_p99'] > 500]['request_rate'].min()

    # Prophetìœ¼ë¡œ í–¥í›„ ìš”ì²­ ìˆ˜ ì˜ˆì¸¡
    prophet_df = df[['timestamp', 'request_rate']].rename(
        columns={'timestamp': 'ds', 'request_rate': 'y'}
    )

    model = Prophet()
    model.fit(prophet_df)

    future = model.make_future_dataframe(
        periods=forecast_hours * 12,  # 5ë¶„ ê°„ê²©
        freq='5min'
    )
    forecast = model.predict(future)

    # ìŠ¤ì¼€ì¼ ì•„ì›ƒ í•„ìš” ì‹œì  ì˜ˆì¸¡
    scale_out_needed = forecast[
        forecast['yhat'] > threshold_requests
    ]['ds'].min()

    if pd.notna(scale_out_needed):
        # ì˜ˆì¸¡ëœ ì‹œê°„ 30ë¶„ ì „ì— ì„ ì œì  ìŠ¤ì¼€ì¼ ì•„ì›ƒ
        preemptive_time = scale_out_needed - timedelta(minutes=30)

        return {
            'scale_out_recommended': True,
            'recommended_time': preemptive_time,
            'predicted_request_rate': forecast.iloc[-1]['yhat'],
            'threshold': threshold_requests,
            'current_replicas': get_current_replicas(service_name),
            'recommended_replicas': calculate_required_replicas(
                forecast.iloc[-1]['yhat'],
                threshold_requests
            )
        }

    return {'scale_out_recommended': False}
```

**3. GPU ì‚¬ìš©ë¥  + ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ìƒê´€ê´€ê³„ ë¶„ì„**

```promql
# GPU ì‚¬ìš©ë¥ ê³¼ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ì˜ ìƒê´€ê´€ê³„
# (NVIDIA DCGM Exporter ë©”íŠ¸ë¦­ + Container Network Observability)

# GPU ì‚¬ìš©ë¥ 
DCGM_FI_DEV_GPU_UTIL{
  namespace="ml-inference",
  pod=~"vllm-.*"
}

# ë™ì‹œ ë„¤íŠ¸ì›Œí¬ ìˆ˜ì‹  ëŒ€ì—­í­
sum(rate(pod_network_rx_bytes{
  namespace="ml-inference",
  pod=~"vllm-.*"
}[1m])) by (pod)

# ìƒê´€ê´€ê³„ ë¶„ì„: GPU ì‚¬ìš©ë¥  < 50% && ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ > 100MB/s
# â†’ ë„¤íŠ¸ì›Œí¬ ë³‘ëª©ì´ GPU í™œìš©ë„ë¥¼ ì €í•´í•˜ê³  ìˆìŒ
```

**ìµœì í™” ì „ëµ**:

```yaml
# ë„¤íŠ¸ì›Œí¬ ë³‘ëª© í•´ì†Œ: Enhanced Networking ë° ENA Express í™œì„±í™”
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: ml-inference-pool
spec:
  template:
    spec:
      requirements:
        - key: karpenter.k8s.aws/instance-family
          operator: In
          values: ["p5", "p4d"]  # ìµœì‹  GPU ì¸ìŠ¤í„´ìŠ¤ (ENA Express ì§€ì›)
        - key: karpenter.k8s.aws/instance-size
          operator: In
          values: ["24xlarge", "48xlarge"]
      nodeClassRef:
        name: ml-inference-class
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: ml-inference-class
spec:
  amiSelectorTerms:
    - alias: al2023@latest
  userData: |
    #!/bin/bash
    # ENA Express í™œì„±í™” (100Gbps ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥)
    ethtool -K eth0 ena-express on

    # TCP BBR congestion control (ë†’ì€ ëŒ€ì—­í­ ìµœì í™”)
    echo "net.ipv4.tcp_congestion_control=bbr" >> /etc/sysctl.conf
    sysctl -p
```

#### EKS Auto Mode ìë™ ë³µêµ¬/ìê°€ ì¹˜ìœ 

**EKS Auto Mode**ëŠ” ë…¸ë“œ ì¥ì• ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ë³µêµ¬í•˜ì—¬, **MTTR(Mean Time To Recovery)**ì„ ëŒ€í­ ê°œì„ í•©ë‹ˆë‹¤.

**1. ìë™ ë…¸ë“œ ì¥ì•  ê°ì§€ ë° êµì²´**

```mermaid
graph TD
    A[ë…¸ë“œ í—¬ìŠ¤ì²´í¬] -->|ì‹¤íŒ¨| B[Auto Mode ê°ì§€]
    B --> C{ì¥ì•  ìœ í˜• ë¶„ì„}
    C -->|í•˜ë“œì›¨ì–´ ì¥ì• | D[ìƒˆ ë…¸ë“œ ì¦‰ì‹œ í”„ë¡œë¹„ì €ë‹]
    C -->|ë„¤íŠ¸ì›Œí¬ ì¥ì• | E[ë„¤íŠ¸ì›Œí¬ ì¬êµ¬ì„± ì‹œë„]
    C -->|ì†Œí”„íŠ¸ì›¨ì–´ ì¥ì• | F[ìë™ ì¬ë¶€íŒ…]
    D --> G[Pod ìë™ ì´ë™]
    E --> G
    F --> G
    G --> H[ì„œë¹„ìŠ¤ ë³µêµ¬ ê²€ì¦]
    H -->|ì‹¤íŒ¨| B
    H -->|ì„±ê³µ| I[ë³µêµ¬ ì™„ë£Œ]
```

**ìë™ ë³µêµ¬ íŠ¸ë¦¬ê±°**:

- **NodeNotReady**: ë…¸ë“œê°€ 5ë¶„ ì´ìƒ NotReady ìƒíƒœ
- **NetworkUnavailable**: ë„¤íŠ¸ì›Œí¬ í”ŒëŸ¬ê·¸ì¸ ì¥ì• 
- **MemoryPressure/DiskPressure**: ë¦¬ì†ŒìŠ¤ ë¶€ì¡±
- **Unschedulable**: ë…¸ë“œê°€ ìŠ¤ì¼€ì¤„ë§ ë¶ˆê°€ ìƒíƒœ

**2. OS íŒ¨ì¹­ ìë™í™”**

Auto ModeëŠ” **ì œë¡œ ë‹¤ìš´íƒ€ì„ OS íŒ¨ì¹­**ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤:

```yaml
# Auto Mode ë…¸ë“œ ìë™ ì—…ë°ì´íŠ¸ ì •ì±… (ì‚¬ìš©ì ì„¤ì • ë¶ˆí•„ìš”)
# AWSê°€ ìë™ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë‚´ë¶€ ì •ì±… ì˜ˆì‹œ
nodeMaintenance:
  autoUpdate: true
  maintenanceWindow:
    preferredDays: ["Sunday", "Wednesday"]
    preferredHours: ["02:00-06:00"]  # UTC
  strategy:
    type: RollingUpdate
    maxUnavailable: 1
    respectPodDisruptionBudget: true
```

**íŒ¨ì¹­ í”„ë¡œì„¸ìŠ¤**:

1. **ì‹ ê·œ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹**: ìµœì‹  AL2023 AMIë¡œ ìƒˆ ë…¸ë“œ ìƒì„±
2. **Pod ì•ˆì „ ì´ë™**: PDBë¥¼ ì¤€ìˆ˜í•˜ë©° ê¸°ì¡´ ë…¸ë“œì—ì„œ ìƒˆ ë…¸ë“œë¡œ Pod ì´ë™
3. **êµ¬ ë…¸ë“œ ì œê±°**: ëª¨ë“  Pod ì´ë™ ì™„ë£Œ í›„ êµ¬ ë…¸ë“œ ì¢…ë£Œ
4. **ê²€ì¦**: ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ í†µê³¼ í™•ì¸

**3. ë³´ì•ˆ ì„œë¹„ìŠ¤ í†µí•©**

Auto ModeëŠ” AWS ë³´ì•ˆ ì„œë¹„ìŠ¤ì™€ ìë™ í†µí•©ë˜ì–´ **ë³´ì•ˆ ì¸ì‹œë˜íŠ¸ ìë™ ëŒ€ì‘**ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:

```
GuardDuty Extended Threat Detection
  â†“ (ì•”í˜¸í™”í ì±„êµ´ ê°ì§€)
Auto Mode ìë™ ëŒ€ì‘
  â†“
1. ì˜í–¥ë°›ì€ ë…¸ë“œ ê²©ë¦¬ (Taint: NoSchedule)
2. ìƒˆ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
3. ê¹¨ë—í•œ ë…¸ë“œë¡œ Pod ì´ë™
4. ê°ì—¼ëœ ë…¸ë“œ ì¢…ë£Œ ë° í¬ë Œì‹ ë°ì´í„° ìˆ˜ì§‘
5. CloudWatch Logsì— ì¸ì‹œë˜íŠ¸ ê¸°ë¡
```

**4. ì˜ˆì¸¡ì  ê´€ì : Auto Modeì˜ MTTR ê°œì„ **

**ê¸°ì¡´ ìˆ˜ë™ ìš´ì˜ vs Auto Mode ë¹„êµ**:

| ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ | ìˆ˜ë™ ìš´ì˜ MTTR | Auto Mode MTTR | ê°œì„ ìœ¨ |
|--------------|----------------|----------------|--------|
| ë…¸ë“œ í•˜ë“œì›¨ì–´ ì¥ì•  | 15-30ë¶„ | 2-5ë¶„ | **83% ë‹¨ì¶•** |
| OS ë³´ì•ˆ íŒ¨ì¹˜ | ìˆ˜ ì‹œê°„ (ê³„íšëœ ë‹¤ìš´íƒ€ì„) | 0ë¶„ (ì œë¡œ ë‹¤ìš´íƒ€ì„) | **100% ê°œì„ ** |
| ë„¤íŠ¸ì›Œí¬ í”ŒëŸ¬ê·¸ì¸ ì¥ì•  | 10-20ë¶„ | 1-3ë¶„ | **85% ë‹¨ì¶•** |
| ì•…ì„±ì½”ë“œ ê°ì—¼ | 30ë¶„-1ì‹œê°„ | 5-10ë¶„ | **80% ë‹¨ì¶•** |

**ì˜ˆì¸¡ ìš´ì˜ ê´€ì ì˜ Auto Mode ê°€ì¹˜**:

- **ì„ ì œì  êµì²´**: ë…¸ë“œ ì„±ëŠ¥ ì €í•˜ë¥¼ ê°ì§€í•˜ì—¬ ì¥ì•  ì „ì— êµì²´
- **ìë™ ìš©ëŸ‰ ê´€ë¦¬**: ì›Œí¬ë¡œë“œ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ìµœì  ë…¸ë“œ íƒ€ì… ìë™ ì„ íƒ
- **ë¬´ì¤‘ë‹¨ ìœ ì§€ë³´ìˆ˜**: ì‚¬ìš©ì ê°œì… ì—†ì´ ë³´ì•ˆ íŒ¨ì¹˜ ë° ì—…ê·¸ë ˆì´ë“œ ìë™ ìˆ˜í–‰
- **ë¹„ìš© ìµœì í™”**: Spot ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì‹œ ìë™ìœ¼ë¡œ On-Demandë¡œ í˜ì¼ì˜¤ë²„

:::tip Auto Mode + ì˜ˆì¸¡ ìš´ì˜ì˜ ì‹œë„ˆì§€
Auto Modeì˜ ìë™ ë³µêµ¬ ê¸°ëŠ¥ì€ **ë°˜ì‘ì (Reactive)**ì´ì§€ë§Œ, Container Network Observability ë°ì´í„°ì™€ ê²°í•©í•˜ë©´ **ì˜ˆì¸¡ì (Predictive)** ìš´ì˜ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ì €í•˜ íŒ¨í„´ì„ ê°ì§€í•˜ì—¬ ì¥ì• ê°€ ë°œìƒí•˜ê¸° ì „ì— ë…¸ë“œë¥¼ êµì²´í•˜ê±°ë‚˜, ML ì¶”ë¡  ì›Œí¬ë¡œë“œì˜ ë„¤íŠ¸ì›Œí¬ ë³‘ëª©ì„ ì‚¬ì „ì— í•´ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

---

## 3. Karpenter + AI ì˜ˆì¸¡

### 3.1 Karpenter ê¸°ë³¸ ë™ì‘

KarpenterëŠ” Pending Podë¥¼ ê°ì§€í•˜ì—¬ **ì í•©í•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì„ ìë™ ì„ íƒ**í•˜ê³  í”„ë¡œë¹„ì €ë‹í•©ë‹ˆë‹¤.

```yaml
# Karpenter NodePool ì„¤ì •
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: default
spec:
  template:
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64", "arm64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: karpenter.k8s.aws/instance-family
          operator: In
          values: ["m7g", "m7i", "c7g", "c7i", "r7g"]
        - key: karpenter.k8s.aws/instance-size
          operator: In
          values: ["medium", "large", "xlarge", "2xlarge"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default
  limits:
    cpu: "100"
    memory: 400Gi
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: default
spec:
  role: KarpenterNodeRole
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: my-cluster
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: my-cluster
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 100Gi
        volumeType: gp3
        iops: 3000
        throughput: 125
```

### 3.2 AI ì˜ˆì¸¡ ê¸°ë°˜ ì„ ì œ í”„ë¡œë¹„ì €ë‹

Karpenter ìì²´ëŠ” Pending Podì— ë°˜ì‘í•˜ì§€ë§Œ, **AI ì˜ˆì¸¡ê³¼ ê²°í•©**í•˜ë©´ ì„ ì œì ìœ¼ë¡œ ë…¸ë“œë¥¼ í”„ë¡œë¹„ì €ë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```mermaid
graph TD
    subgraph Prediction["ğŸ§  ì˜ˆì¸¡ ë ˆì´ì–´"]
        CW_M["CloudWatch<br/>Metrics"]
        ML["ML ëª¨ë¸<br/>(Prophet/ARIMA)"]
        PRED["íŠ¸ë˜í”½<br/>ì˜ˆì¸¡ ê²°ê³¼"]
    end

    subgraph Preemptive["âš¡ ì„ ì œ ì¡°ì¹˜"]
        WARM["Warm Pool<br/>Pod ìƒì„±"]
        PAUSE["Pause<br/>Containers"]
        KARP["Karpenter<br/>ë…¸ë“œ í”„ë¡œë¹„ì €ë‹"]
    end

    subgraph Runtime["ğŸš€ ì‹¤ì œ íŠ¸ë˜í”½"]
        TRAFFIC["íŠ¸ë˜í”½<br/>ìœ ì…"]
        HPA2["HPA<br/>ì¦‰ì‹œ ìŠ¤ì¼€ì¼"]
        READY["Pod<br/>ì¦‰ì‹œ ì„œë¹„ìŠ¤"]
    end

    CW_M --> ML --> PRED
    PRED --> WARM --> KARP
    PRED --> PAUSE --> KARP
    TRAFFIC --> HPA2 --> READY
    KARP -.->|ë…¸ë“œ ì¤€ë¹„ ì™„ë£Œ| READY
```

**ì„ ì œ í”„ë¡œë¹„ì €ë‹ ì „ëµ**:

```yaml
# Placeholder Podë¡œ ë…¸ë“œ ì„ ì œ í™•ë³´
apiVersion: apps/v1
kind: Deployment
metadata:
  name: capacity-reservation
  namespace: scaling
spec:
  replicas: 0  # ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ëŸ¬ê°€ ë™ì ìœ¼ë¡œ ì¡°ì •
  selector:
    matchLabels:
      app: capacity-reservation
  template:
    metadata:
      labels:
        app: capacity-reservation
    spec:
      priorityClassName: capacity-reservation  # ë‚®ì€ ìš°ì„ ìˆœìœ„
      terminationGracePeriodSeconds: 0
      containers:
        - name: pause
          image: registry.k8s.io/pause:3.9
          resources:
            requests:
              cpu: "1"
              memory: 2Gi
---
# ë‚®ì€ ìš°ì„ ìˆœìœ„ í´ë˜ìŠ¤ (ì‹¤ì œ ì›Œí¬ë¡œë“œì— ì˜í•´ ì¶•ì¶œë¨)
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: capacity-reservation
value: -10
globalDefault: false
description: "Karpenter ë…¸ë“œ ì„ ì œ í”„ë¡œë¹„ì €ë‹ìš©"
```

:::tip ì„ ì œ í”„ë¡œë¹„ì €ë‹ì˜ ì›ë¦¬

1. ML ëª¨ë¸ì´ 30ë¶„ í›„ íŠ¸ë˜í”½ ì¦ê°€ë¥¼ ì˜ˆì¸¡
2. Placeholder Pod(pause container)ì˜ replicasë¥¼ ëŠ˜ë¦¼
3. Karpenterê°€ Pending Podë¥¼ ê°ì§€í•˜ì—¬ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
4. ì‹¤ì œ íŠ¸ë˜í”½ì´ ì˜¤ë©´ HPAê°€ ì‹¤ì œ Podë¥¼ ìƒì„±
5. Placeholder PodëŠ” ë‚®ì€ ìš°ì„ ìˆœìœ„ë¡œ ì¦‰ì‹œ ì¶•ì¶œë¨
6. ë…¸ë“œê°€ ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆìœ¼ë¯€ë¡œ Podê°€ ì¦‰ì‹œ ìŠ¤ì¼€ì¤„ë§ë¨
:::

### 3.5 ARC + Karpenter í†µí•© ìë™ AZ ëŒ€í”¼

**ARC(Application Recovery Controller)**ëŠ” AWSì˜ ê³ ê°€ìš©ì„± ì„œë¹„ìŠ¤ë¡œ, AZ ì¥ì• ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  íŠ¸ë˜í”½ì„ ê±´ê°•í•œ AZë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤. Karpenterì™€ í†µí•©í•˜ë©´ **ë…¸ë“œ ë ˆë²¨ì˜ ìë™ ë³µêµ¬**ê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

#### ARC ê°œìš”

Application Recovery ControllerëŠ” ë‹¤ìŒ 3ê°€ì§€ í•µì‹¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- **Readiness Check**: ì• í”Œë¦¬ì¼€ì´ì…˜ í—¬ìŠ¤ ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§
- **Routing Control**: Route 53 ë˜ëŠ” ALBë¥¼ í†µí•´ íŠ¸ë˜í”½ ë¼ìš°íŒ… ì œì–´
- **Zonal Shift**: AZ ë‹¨ìœ„ë¡œ íŠ¸ë˜í”½ì„ ìë™ ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ ì´ë™

#### Karpenter í†µí•© íŒ¨í„´

```yaml
# ARC Zonal Shift ì‹œê·¸ë„ì„ ê°ì§€í•˜ëŠ” Controller
apiVersion: v1
kind: ConfigMap
metadata:
  name: arc-karpenter-controller
  namespace: kube-system
data:
  config.yaml: |
    arcCluster: arn:aws:route53-recovery-control::ACCOUNT:cluster/CLUSTER_ID
    routingControls:
      - name: az-a-routing
        arn: arn:aws:route53-recovery-control::ACCOUNT:controlpanel/PANEL/routingcontrol/CONTROL_A
      - name: az-b-routing
        arn: arn:aws:route53-recovery-control::ACCOUNT:controlpanel/PANEL/routingcontrol/CONTROL_B
      - name: az-c-routing
        arn: arn:aws:route53-recovery-control::ACCOUNT:controlpanel/PANEL/routingcontrol/CONTROL_C
    karpenterNodePools:
      - default
      - gpu-pool
```

#### AZ ì¥ì•  ìë™ ë³µêµ¬ ì‹œí€€ìŠ¤

```mermaid
sequenceDiagram
    participant AZ_A as AZ-A (ì¥ì• )
    participant ARC as ARC Controller
    participant R53 as Route 53
    participant K8s as Kubernetes API
    participant Karp as Karpenter
    participant AZ_B as AZ-B (ê±´ê°•)
    participant Pod as Workload Pods

    AZ_A->>ARC: Readiness Check ì‹¤íŒ¨
    ARC->>ARC: Gray Failure íŒ¨í„´ ê°ì§€
    ARC->>R53: Zonal Shift ì‹œì‘ (AZ-A OUT)
    ARC->>K8s: Node Taint ì¶”ê°€ (NoSchedule)
    K8s->>Karp: Pending Pod ì´ë²¤íŠ¸ ë°œìƒ
    Karp->>AZ_B: ëŒ€ì²´ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
    AZ_B-->>K8s: ìƒˆ ë…¸ë“œ ë“±ë¡ ì™„ë£Œ
    K8s->>Pod: Pod ì•ˆì „ ì´ë™ (PDB ì¤€ìˆ˜)
    Pod-->>AZ_B: ì„œë¹„ìŠ¤ ë³µêµ¬ ì™„ë£Œ
    ARC->>K8s: AZ-A ë…¸ë“œ Drain ì‹œì‘
    K8s->>AZ_A: ë…¸ë“œ ì •ë¦¬ ë° ì œê±°
```

#### Gray Failure ì²˜ë¦¬

**Gray Failure**ëŠ” ì™„ì „í•œ ì¥ì• ê°€ ì•„ë‹Œ ì„±ëŠ¥ ì €í•˜ ìƒíƒœë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ARCëŠ” ë‹¤ìŒ íŒ¨í„´ì„ ê°ì§€í•©ë‹ˆë‹¤:

- **ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì¦ê°€**: í‰ì†Œ 5ms â†’ 50ms ì´ìƒ
- **ê°„í—ì  íƒ€ì„ì•„ì›ƒ**: ìš”ì²­ì˜ 1-5%ê°€ ì‹¤íŒ¨
- **ë¦¬ì†ŒìŠ¤ ê²½í•©**: CPU steal time ì¦ê°€, ë„¤íŠ¸ì›Œí¬ íŒ¨í‚· ì†ì‹¤

```python
# Gray Failure ê°ì§€ Lambda í•¨ìˆ˜ ì˜ˆì‹œ
import boto3
from datetime import datetime, timedelta

def detect_gray_failure(event, context):
    """
    Container Network Observability ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
    Gray Failure íŒ¨í„´ì„ ê°ì§€í•©ë‹ˆë‹¤.
    """
    cloudwatch = boto3.client('cloudwatch')

    # AZë³„ ë„¤íŠ¸ì›Œí¬ ì§€ì—° ë©”íŠ¸ë¦­ ì¡°íšŒ
    response = cloudwatch.get_metric_statistics(
        Namespace='ContainerInsights',
        MetricName='pod_network_rx_latency_ms',
        Dimensions=[
            {'Name': 'ClusterName', 'Value': 'my-cluster'},
            {'Name': 'AvailabilityZone', 'Value': 'ap-northeast-2a'}
        ],
        StartTime=datetime.utcnow() - timedelta(minutes=15),
        EndTime=datetime.utcnow(),
        Period=60,
        Statistics=['Average', 'Maximum']
    )

    # Gray Failure ì„ê³„ê°’ ì²´í¬
    datapoints = response['Datapoints']
    if len(datapoints) < 10:
        return {'status': 'insufficient_data'}

    avg_latency = sum(d['Average'] for d in datapoints) / len(datapoints)
    max_latency = max(d['Maximum'] for d in datapoints)

    # ê¸°ì¤€: í‰ê·  ì§€ì—° > 50ms ë˜ëŠ” ìµœëŒ€ ì§€ì—° > 200ms
    if avg_latency > 50 or max_latency > 200:
        trigger_zonal_shift('ap-northeast-2a')
        return {'status': 'gray_failure_detected', 'action': 'zonal_shift'}

    return {'status': 'healthy'}

def trigger_zonal_shift(az):
    """ARC Zonal Shiftë¥¼ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤."""
    arc = boto3.client('route53-recovery-cluster')
    arc.update_routing_control_state(
        RoutingControlArn='arn:aws:route53-recovery-control::ACCOUNT:...',
        RoutingControlState='Off'  # AZ-A íŠ¸ë˜í”½ ì°¨ë‹¨
    )
```

#### Istio í†µí•© End-to-end ë³µêµ¬

Istio ì„œë¹„ìŠ¤ ë©”ì‹œë¥¼ ì‚¬ìš©í•˜ë©´ **L7 ë ˆë²¨ì˜ íŠ¸ë˜í”½ ì œì–´**ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤:

```yaml
# Istio DestinationRule: AZ ì¥ì•  ì‹œ ìë™ í˜ì¼ì˜¤ë²„
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: payment-service-dr
spec:
  host: payment-service
  trafficPolicy:
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
    loadBalancer:
      localityLbSetting:
        enabled: true
        failover:
          - from: ap-northeast-2a
            to: ap-northeast-2c
```

**End-to-end ë³µêµ¬ íë¦„**:

1. **ARC Readiness Check ì‹¤íŒ¨** â†’ Zonal Shift ì‹œì‘
2. **Route 53** â†’ AZ-Aë¡œ ê°€ëŠ” ì™¸ë¶€ íŠ¸ë˜í”½ ì°¨ë‹¨
3. **Istio Envoy** â†’ AZ-A ë‚´ë¶€ Podë¡œ ê°€ëŠ” East-West íŠ¸ë˜í”½ ì°¨ë‹¨
4. **Karpenter** â†’ AZ-Cì— ëŒ€ì²´ ë…¸ë“œ í”„ë¡œë¹„ì €ë‹
5. **Kubernetes** â†’ PDBë¥¼ ì¤€ìˆ˜í•˜ë©° Pod ì•ˆì „ ì´ë™
6. **Istio** â†’ ìƒˆ Podë¡œ íŠ¸ë˜í”½ ìë™ ë¼ìš°íŒ…

#### ì˜ˆì¸¡ì  AZ ê´€ë¦¬

Container Network Observability ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ **AZ ì„±ëŠ¥ ì´ìƒì„ ì„ ì œì ìœ¼ë¡œ ê°ì§€**í•©ë‹ˆë‹¤:

```promql
# AZë³„ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ìœ¨ ì¶”ì´
sum(rate(pod_network_rx_errors_total[5m])) by (availability_zone)
/ sum(rate(pod_network_rx_packets_total[5m])) by (availability_zone)
* 100

# AZë³„ í‰ê·  Pod-to-Pod ë ˆì´í„´ì‹œ
histogram_quantile(0.99,
  sum(rate(pod_network_latency_bucket[5m])) by (availability_zone, le)
)
```

**ì˜ˆì¸¡ì  AZ ê´€ë¦¬ ì „ëµ**:

- **íŠ¸ë Œë“œ ë¶„ì„**: ì§€ë‚œ 7ì¼ê°„ AZë³„ ì„±ëŠ¥ íŒ¨í„´ í•™ìŠµ
- **ì¡°ê¸° ê²½ë³´**: ì„±ëŠ¥ì´ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ 20% ì €í•˜ ì‹œ ì•Œë¦¼
- **ì„ ì œì  Shift**: 30% ì €í•˜ ì‹œ ìë™ Zonal Shift ê³ ë ¤
- **ë¹„ìš© ìµœì í™”**: Cross-AZ íŠ¸ë˜í”½ ë¹„ìš©ì„ ê³ ë ¤í•œ ìµœì  ë°°ì¹˜

:::warning ARC + Karpenter í†µí•© ì£¼ì˜ì‚¬í•­
ARC + Karpenter í†µí•©ì€ PDBê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ì•ˆì „í•œ Pod ì´ë™ì„ ë³´ì¥í•©ë‹ˆë‹¤. ëª¨ë“  í”„ë¡œë•ì…˜ ì›Œí¬ë¡œë“œì— PDBë¥¼ ì„¤ì •í•˜ì„¸ìš”.

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: payment-service-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: payment-service
```
:::

---

## 4. CloudWatch Anomaly Detection

### 4.1 ì´ìƒ íƒì§€ ë°´ë“œ

CloudWatch Anomaly Detectionì€ MLì„ ì‚¬ìš©í•˜ì—¬ ë©”íŠ¸ë¦­ì˜ **ì •ìƒ ë²”ìœ„ ë°´ë“œ**ë¥¼ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ê³ , ë°´ë“œë¥¼ ë²—ì–´ë‚˜ëŠ” ì´ìƒì„ íƒì§€í•©ë‹ˆë‹¤.

```bash
# Anomaly Detection ëª¨ë¸ ìƒì„±
aws cloudwatch put-anomaly-detector \
  --namespace "ContainerInsights" \
  --metric-name "pod_cpu_utilization" \
  --dimensions Name=ClusterName,Value=my-cluster \
  --stat "Average" \
  --configuration '{
    "ExcludedTimeRanges": [
      {
        "StartTime": "2026-01-01T00:00:00Z",
        "EndTime": "2026-01-02T00:00:00Z"
      }
    ],
    "MetricTimezone": "Asia/Seoul"
  }'
```

### 4.2 EKS ë©”íŠ¸ë¦­ ì ìš©

Anomaly Detectionì„ ì ìš©í•  í•µì‹¬ EKS ë©”íŠ¸ë¦­:

<AnomalyMetrics />

### 4.3 Anomaly Detection ê¸°ë°˜ ì•ŒëŒ

```bash
# Anomaly Detection ê¸°ë°˜ CloudWatch Alarm
aws cloudwatch put-metric-alarm \
  --alarm-name "EKS-CPU-Anomaly" \
  --comparison-operator GreaterThanUpperThreshold \
  --threshold-metric-id ad1 \
  --evaluation-periods 3 \
  --datapoints-to-alarm 2 \
  --metrics '[
    {
      "Id": "m1",
      "MetricStat": {
        "Metric": {
          "Namespace": "ContainerInsights",
          "MetricName": "pod_cpu_utilization",
          "Dimensions": [
            {"Name": "ClusterName", "Value": "my-cluster"}
          ]
        },
        "Period": 300,
        "Stat": "Average"
      }
    },
    {
      "Id": "ad1",
      "Expression": "ANOMALY_DETECTION_BAND(m1, 2)"
    }
  ]' \
  --alarm-actions "arn:aws:sns:ap-northeast-2:ACCOUNT_ID:ops-alerts"
```

---

## 5. AI Agent ìë™ ì¸ì‹œë˜íŠ¸ ëŒ€ì‘

### 5.1 ê¸°ì¡´ ìë™í™”ì˜ í•œê³„

EventBridge + Lambda ê¸°ë°˜ ìë™í™”ëŠ” **ê·œì¹™ ê¸°ë°˜**ì´ë¯€ë¡œ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤:

```
[ê¸°ì¡´ ë°©ì‹: ê·œì¹™ ê¸°ë°˜ ìë™í™”]
CloudWatch Alarm â†’ EventBridge Rule â†’ Lambda â†’ ê³ ì •ëœ ì¡°ì¹˜

ë¬¸ì œì :
  âœ— "CPU > 80%ì´ë©´ ìŠ¤ì¼€ì¼ì•„ì›ƒ" â€” ì›ì¸ì´ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ì¼ ìˆ˜ë„ ìˆìŒ
  âœ— "Pod ì¬ì‹œì‘ > 5ì´ë©´ ì•Œë¦¼" â€” ì›ì¸ë³„ ëŒ€ì‘ì´ ë‹¤ë¦„
  âœ— ë³µí•© ì¥ì•  ëŒ€ì‘ ë¶ˆê°€
  âœ— ìƒˆë¡œìš´ íŒ¨í„´ì— ì ì‘ ë¶ˆê°€
```

### 5.2 AI Agent ê¸°ë°˜ ììœ¨ ëŒ€ì‘

<ResponsePatterns />

AI AgentëŠ” **ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒë‹¨**ìœ¼ë¡œ ììœ¨ì ìœ¼ë¡œ ëŒ€ì‘í•©ë‹ˆë‹¤.

```mermaid
graph TD
    subgraph Trigger["ğŸ”” íŠ¸ë¦¬ê±°"]
        CWA["CloudWatch<br/>Alarm"]
        DGA["DevOps Guru<br/>Insight"]
        K8SE["K8s<br/>Event"]
    end

    subgraph Agent["ğŸ¤– AI Agent"]
        COLLECT["ë°ì´í„° ìˆ˜ì§‘<br/>(MCP í†µí•©)"]
        ANALYZE["AI ë¶„ì„<br/>(ê·¼ë³¸ ì›ì¸)"]
        DECIDE["íŒë‹¨<br/>(ìë™/ìˆ˜ë™)"]
        ACT["ì‹¤í–‰<br/>(ì•ˆì „í•œ ì¡°ì¹˜)"]
        REPORT["ë³´ê³ <br/>(Slack/Jira)"]
    end

    subgraph Actions["âš¡ ëŒ€ì‘ ì¡°ì¹˜"]
        SCALE["ìŠ¤ì¼€ì¼ë§<br/>ì¡°ì •"]
        RESTART["Pod<br/>ì¬ì‹œì‘"]
        ROLLBACK["ë°°í¬<br/>ë¡¤ë°±"]
        ESCALATE["ì‚¬ëŒì—ê²Œ<br/>ì—ìŠ¤ì»¬ë ˆì´ì…˜"]
    end

    CWA --> COLLECT
    DGA --> COLLECT
    K8SE --> COLLECT
    COLLECT --> ANALYZE --> DECIDE
    DECIDE --> ACT --> REPORT
    ACT --> SCALE
    ACT --> RESTART
    ACT --> ROLLBACK
    ACT --> ESCALATE
```

### 5.3 Kagent ìë™ ì¸ì‹œë˜íŠ¸ ëŒ€ì‘

```yaml
# Kagent: ìë™ ì¸ì‹œë˜íŠ¸ ëŒ€ì‘ ì—ì´ì „íŠ¸
apiVersion: kagent.dev/v1alpha1
kind: Agent
metadata:
  name: incident-responder
  namespace: kagent-system
spec:
  description: "EKS ì¸ì‹œë˜íŠ¸ ìë™ ëŒ€ì‘ ì—ì´ì „íŠ¸"
  modelConfig:
    provider: bedrock
    model: anthropic.claude-sonnet
    region: ap-northeast-2
  systemPrompt: |
    ë‹¹ì‹ ì€ EKS ì¸ì‹œë˜íŠ¸ ëŒ€ì‘ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

    ## ëŒ€ì‘ ì›ì¹™
    1. ì•ˆì „ ìš°ì„ : ìœ„í—˜í•œ ë³€ê²½ì€ ì‚¬ëŒì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜
    2. ê·¼ë³¸ ì›ì¸ ìš°ì„ : ì¦ìƒì´ ì•„ë‹Œ ì›ì¸ì— ëŒ€ì‘
    3. ìµœì†Œ ê°œì…: í•„ìš”í•œ ìµœì†Œí•œì˜ ì¡°ì¹˜ë§Œ ìˆ˜í–‰
    4. ëª¨ë“  ì¡°ì¹˜ ê¸°ë¡: Slackê³¼ JIRAì— ìë™ ë³´ê³ 

    ## ìë™ ì¡°ì¹˜ í—ˆìš© ë²”ìœ„
    - Pod ì¬ì‹œì‘ (CrashLoopBackOff, 5íšŒ ì´ìƒ)
    - HPA min/max ì¡°ì • (í˜„ì¬ê°’ì˜ Â±50% ë²”ìœ„)
    - Deployment rollback (ì´ì „ ë²„ì „ìœ¼ë¡œ)
    - ë…¸ë“œ drain (MemoryPressure/DiskPressure)

    ## ì—ìŠ¤ì»¬ë ˆì´ì…˜ ëŒ€ìƒ
    - ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì¡°ì¹˜
    - 50% ì´ìƒì˜ replicas ì˜í–¥
    - StatefulSet ê´€ë ¨ ë³€ê²½
    - ë„¤íŠ¸ì›Œí¬ ì •ì±… ë³€ê²½

  tools:
    - name: kubectl
      type: kmcp
      config:
        allowedVerbs: ["get", "describe", "logs", "top", "rollout", "scale", "delete"]
        deniedResources: ["secrets", "configmaps"]
    - name: cloudwatch
      type: kmcp
      config:
        actions: ["GetMetricData", "DescribeAlarms", "GetInsight"]
    - name: slack
      type: mcp
      config:
        webhook_url: "${SLACK_WEBHOOK}"
        channel: "#incidents"

  triggers:
    - type: cloudwatch-alarm
      filter:
        severity: ["CRITICAL", "HIGH"]
    - type: kubernetes-event
      filter:
        reason: ["CrashLoopBackOff", "OOMKilled", "FailedScheduling"]
```

### 5.4 Strands Agent SOP: ë³µí•© ì¥ì•  ëŒ€ì‘

```python
# Strands Agent: ë³µí•© ì¥ì•  ìë™ ëŒ€ì‘
from strands import Agent
from strands.tools import eks_tool, cloudwatch_tool, slack_tool, jira_tool

incident_agent = Agent(
    name="complex-incident-handler",
    model="bedrock/anthropic.claude-sonnet",
    tools=[eks_tool, cloudwatch_tool, slack_tool, jira_tool],
    sop="""
    ## ë³µí•© ì¥ì•  ëŒ€ì‘ SOP

    ### Phase 1: ìƒí™© íŒŒì•… (30ì´ˆ ì´ë‚´)
    1. CloudWatch ì•ŒëŒ ë° DevOps Guru ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ
    2. ê´€ë ¨ ì„œë¹„ìŠ¤ì˜ Pod ìƒíƒœ í™•ì¸
    3. ë…¸ë“œ ìƒíƒœ ë° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  í™•ì¸
    4. ìµœê·¼ ë°°í¬ ì´ë ¥ í™•ì¸ (10ë¶„ ì´ë‚´ ë³€ê²½ ì‚¬í•­)

    ### Phase 2: ê·¼ë³¸ ì›ì¸ ë¶„ì„ (2ë¶„ ì´ë‚´)
    1. ë¡œê·¸ì—ì„œ ì—ëŸ¬ íŒ¨í„´ ì¶”ì¶œ
    2. ë©”íŠ¸ë¦­ ìƒê´€ ë¶„ì„ (CPU, Memory, Network, Disk)
    3. ë°°í¬ ë³€ê²½ê³¼ì˜ ì‹œê°„ì  ìƒê´€ê´€ê³„ ë¶„ì„
    4. ì˜ì¡´ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸

    ### Phase 3: ìë™ ëŒ€ì‘
    ì›ì¸ë³„ ìë™ ì¡°ì¹˜:

    **ë°°í¬ ê´€ë ¨ ì¥ì• :**
    - ìµœê·¼ 10ë¶„ ì´ë‚´ ë°°í¬ ì¡´ì¬ â†’ ìë™ ë¡¤ë°±
    - ë¡¤ë°± í›„ ìƒíƒœ í™•ì¸ â†’ ì •ìƒí™”ë˜ë©´ ì™„ë£Œ

    **ë¦¬ì†ŒìŠ¤ ë¶€ì¡±:**
    - CPU/Memory > 90% â†’ HPA ì¡°ì • ë˜ëŠ” Karpenter ë…¸ë“œ ì¶”ê°€
    - Disk > 85% â†’ ë¶ˆí•„ìš” ë¡œê·¸/ì´ë¯¸ì§€ ì •ë¦¬

    **ì˜ì¡´ ì„œë¹„ìŠ¤ ì¥ì• :**
    - RDS ì—°ê²° ì‹¤íŒ¨ â†’ ì—°ê²° í’€ ì„¤ì • í™•ì¸, í•„ìš”ì‹œ ì¬ì‹œì‘
    - SQS ì§€ì—° â†’ DLQ í™•ì¸, ì†Œë¹„ì ìŠ¤ì¼€ì¼ì•„ì›ƒ

    **ì›ì¸ ë¶ˆëª…:**
    - ì‚¬ëŒì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜
    - ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„°ë¥¼ Slackì— ê³µìœ 

    ### Phase 4: ì‚¬í›„ ì²˜ë¦¬
    1. ì¸ì‹œë˜íŠ¸ íƒ€ì„ë¼ì¸ ìƒì„±
    2. JIRA ì¸ì‹œë˜íŠ¸ í‹°ì¼“ ìƒì„±
    3. Slack #incidents ì±„ë„ì— ë³´ê³ ì„œ ê²Œì‹œ
    4. í•™ìŠµ ë°ì´í„°ë¡œ ì €ì¥ (í”¼ë“œë°± ë£¨í”„)
    """
)
```

:::info AI Agentì˜ í•µì‹¬ ê°€ì¹˜
EventBridge+Lambdaë¥¼ ë„˜ì–´ AI ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ììœ¨ ëŒ€ì‘ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. **ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤**(CloudWatch, EKS API, X-Ray, ë°°í¬ ì´ë ¥)ë¥¼ **MCPë¡œ í†µí•© ì¡°íšŒ**í•˜ì—¬, ê·œì¹™ìœ¼ë¡œëŠ” ëŒ€ì‘í•  ìˆ˜ ì—†ëŠ” ë³µí•© ì¥ì• ë„ ê·¼ë³¸ ì›ì¸ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì¡°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
:::

### 5.5 CloudWatch Investigations â€” AI ê¸°ë°˜ ìë™ ê·¼ë³¸ ì›ì¸ ë¶„ì„

**CloudWatch Investigations**ëŠ” AWSê°€ 17ë…„ê°„ ì¶•ì í•œ ìš´ì˜ ê²½í—˜ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•í•œ **ìƒì„±í˜• AI ê¸°ë°˜ ìë™ ì¡°ì‚¬ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤. ì¸ì‹œë˜íŠ¸ ë°œìƒ ì‹œ AIê°€ ìë™ìœ¼ë¡œ ê°€ì„¤ì„ ìƒì„±í•˜ê³ , ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ë©°, ê²€ì¦í•˜ëŠ” ì¡°ì‚¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

#### CloudWatch Investigations ê°œìš”

```mermaid
graph TD
    subgraph Trigger["ğŸ”” ì¸ì‹œë˜íŠ¸ ê°ì§€"]
        ALARM["CloudWatch<br/>Alarm"]
        SIGNAL["Application<br/>Signals"]
    end

    subgraph Investigation["ğŸ” AI ì¡°ì‚¬ í”„ë¡œì„¸ìŠ¤"]
        HYPO["ê°€ì„¤ ìƒì„±<br/>(AI)"]
        COLLECT["ë°ì´í„° ìˆ˜ì§‘<br/>(ìë™)"]
        ANALYZE["ìƒê´€ ë¶„ì„<br/>(AI)"]
        ROOT["ê·¼ë³¸ ì›ì¸<br/>ì¶”ë¡ "]
    end

    subgraph Evidence["ğŸ“Š ì¦ê±° ìˆ˜ì§‘"]
        METRICS["ê´€ë ¨ ë©”íŠ¸ë¦­"]
        LOGS["ê´€ë ¨ ë¡œê·¸"]
        TRACES["ê´€ë ¨ íŠ¸ë ˆì´ìŠ¤"]
        DEPLOY["ë°°í¬ ì´ë ¥"]
    end

    subgraph Output["ğŸ“ ê²°ê³¼ ë° ì¡°ì¹˜"]
        SUMMARY["ì¡°ì‚¬ ê²°ê³¼<br/>ìš”ì•½"]
        REMEDIATION["ë³µêµ¬ ì œì•ˆ<br/>(Runbook)"]
        REPORT["ìƒì„¸ ë³´ê³ ì„œ"]
    end

    ALARM --> HYPO
    SIGNAL --> HYPO
    HYPO --> COLLECT
    COLLECT --> METRICS
    COLLECT --> LOGS
    COLLECT --> TRACES
    COLLECT --> DEPLOY
    METRICS --> ANALYZE
    LOGS --> ANALYZE
    TRACES --> ANALYZE
    DEPLOY --> ANALYZE
    ANALYZE --> ROOT
    ROOT --> SUMMARY
    ROOT --> REMEDIATION
    ROOT --> REPORT
```

#### í•µì‹¬ ê¸°ëŠ¥

**1. Application Signals í†µí•©: ì„œë¹„ìŠ¤ ë§µ ê¸°ë°˜ ì˜í–¥ë„ ìë™ ë¶„ì„**

CloudWatch InvestigationsëŠ” Application Signalsê°€ ìë™ ìƒì„±í•œ ì„œë¹„ìŠ¤ ë§µì„ í™œìš©í•˜ì—¬ **ì¥ì•  ì „íŒŒ ê²½ë¡œ**ë¥¼ ì¶”ì í•©ë‹ˆë‹¤:

```yaml
# Application Signals ìë™ ì„œë¹„ìŠ¤ ë§µ ì˜ˆì‹œ
payment-gateway (ì—ëŸ¬ìœ¨ ì¦ê°€ 25%)
  â””â”€> payment-service (ë ˆì´í„´ì‹œ ì¦ê°€ 300%)
       â”œâ”€> postgres-db (ì—°ê²° í’€ ê³ ê°ˆ)
       â””â”€> redis-cache (ì •ìƒ)
            â””â”€> dynamodb (ì •ìƒ)
```

InvestigationsëŠ” ì´ ë§µì„ ë¶„ì„í•˜ì—¬:
- **Root Cause**: `postgres-db` ì—°ê²° í’€ ê³ ê°ˆ
- **Impacted Services**: `payment-service`, `payment-gateway`
- **Propagation Path**: DB â†’ Service â†’ Gateway

**2. ê´€ë ¨ ë©”íŠ¸ë¦­/ë¡œê·¸/íŠ¸ë ˆì´ìŠ¤ ìë™ ìƒê´€ ë¶„ì„**

```python
# Investigationsê°€ ìˆ˜í–‰í•˜ëŠ” ìë™ ìƒê´€ ë¶„ì„ ì˜ˆì‹œ

# ì‹œê°„ì  ìƒê´€ê´€ê³„
payment_service_errors.spike_at = "2026-02-12 14:23:00"
db_connection_pool.exhausted_at = "2026-02-12 14:22:55"
# â†’ 5ì´ˆ ì°¨ì´: DB ë¬¸ì œê°€ ì„œë¹„ìŠ¤ ì—ëŸ¬ë³´ë‹¤ ë¨¼ì € ë°œìƒ

# ë©”íŠ¸ë¦­ ìƒê´€ê´€ê³„
db_active_connections = 100 (max_connections ë„ë‹¬)
payment_service_response_time = 5000ms (í‰ì†Œ 50ms ëŒ€ë¹„ 100ë°°)
# â†’ ê°•í•œ ìƒê´€ê´€ê³„: DB ì—°ê²° ê³ ê°ˆ â†’ ì„œë¹„ìŠ¤ ì§€ì—°

# ë¡œê·¸ íŒ¨í„´ ë¶„ì„
logs.error_pattern = "CannotGetJdbcConnectionException"
logs.frequency = 1,234 occurrences in last 5 minutes
# â†’ ëª…í™•í•œ ì¦ê±°: DB ì—°ê²° ë¶ˆê°€ ì—ëŸ¬
```

**3. ê°€ì„¤ ê¸°ë°˜ ê·¼ë³¸ ì›ì¸ ì¶”ë¡ **

InvestigationsëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê°€ì„¤ì„ ìë™ ìƒì„±í•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤:

| ê°€ì„¤ | ê²€ì¦ ë°©ë²• | ê²°ê³¼ |
|------|----------|------|
| DB ì—°ê²° í’€ ê³ ê°ˆ | `db_connections` ë©”íŠ¸ë¦­ í™•ì¸ | âœ“ í™•ì¸ë¨ |
| ë„¤íŠ¸ì›Œí¬ ì§€ì—° | VPC Flow Logs ë¶„ì„ | âœ— ì •ìƒ |
| OOM(ë©”ëª¨ë¦¬ ë¶€ì¡±) | ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ í™•ì¸ | âœ— ì •ìƒ |
| ë°°í¬ í›„ ë²„ê·¸ | ìµœê·¼ ë°°í¬ ì´ë ¥ ì¡°íšŒ | âœ“ 10ë¶„ ì „ ë°°í¬ í™•ì¸ |

**ìµœì¢… ê²°ë¡ **: ìµœê·¼ ë°°í¬ì—ì„œ DB ì—°ê²° í’€ ì„¤ì •ì´ `maxPoolSize=50`ì—ì„œ `maxPoolSize=10`ìœ¼ë¡œ ì˜ëª» ë³€ê²½ë¨.

**4. ì¡°ì‚¬ ê²°ê³¼ ìš”ì•½ ë° ë³µêµ¬ ì œì•ˆ**

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  CloudWatch Investigations ê²°ê³¼ ìš”ì•½
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”´ ê·¼ë³¸ ì›ì¸ (Root Cause):
   payment-serviceì˜ DB ì—°ê²° í’€ ì„¤ì • ì˜¤ë¥˜
   (maxPoolSize: 50 â†’ 10ìœ¼ë¡œ ì˜ëª» ë³€ê²½)

ğŸ“Š ì˜í–¥ë„ (Impact):
   - payment-gateway: ì—ëŸ¬ìœ¨ 25% ì¦ê°€
   - payment-service: ë ˆì´í„´ì‹œ 300% ì¦ê°€
   - ì˜í–¥ë°›ì€ ìš”ì²­: ì•½ 15,000ê±´

â±ï¸ íƒ€ì„ë¼ì¸:
   14:10 - ë°°í¬ ì‹œì‘ (v1.2.3 â†’ v1.2.4)
   14:22 - DB ì—°ê²° í’€ ê³ ê°ˆ ì‹œì‘
   14:23 - ì„œë¹„ìŠ¤ ì—ëŸ¬ ê¸‰ì¦ ì•ŒëŒ ë°œìƒ
   14:25 - Investigations ìë™ ì‹œì‘

ğŸ’¡ ê¶Œì¥ ì¡°ì¹˜:
   1. ì¦‰ì‹œ ë¡¤ë°±: kubectl rollout undo deployment/payment-service
   2. DB ì—°ê²° í’€ ì„¤ì • ë³µêµ¬: maxPoolSize=50
   3. ë°°í¬ ì „ í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ë‹¨ê³„ ì¶”ê°€
   4. ConfigMap ë³€ê²½ ì‹œ ìë™ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì ìš©

ğŸ“‹ ê´€ë ¨ ë¦¬ì†ŒìŠ¤:
   - Runbook: https://wiki/db-connection-pool-issue
   - ë¡œê·¸: CloudWatch Logs Insights ì¿¼ë¦¬ ë§í¬
   - ë©”íŠ¸ë¦­: CloudWatch Dashboard ë§í¬
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

#### DevOps Agentì™€ì˜ ì°¨ì´ì 

| ì¸¡ë©´ | CloudWatch Investigations | Kagent / Strands Agent |
|------|--------------------------|------------------------|
| **ìš´ì˜ ë°©ì‹** | AWS ê´€ë¦¬í˜• (ì„¤ì • ë¶ˆí•„ìš”) | ì‚¬ìš©ìê°€ ì„¤ì¹˜Â·ìš´ì˜ |
| **ë¶„ì„ ë²”ìœ„** | AWS ì „ì—­ ë°ì´í„° ìë™ ìˆ˜ì§‘ | ì„¤ì •í•œ ë°ì´í„° ì†ŒìŠ¤ë§Œ |
| **ê·¼ë³¸ ì›ì¸ ë¶„ì„** | AI ê¸°ë°˜ ìë™ ê°€ì„¤ ìƒì„±Â·ê²€ì¦ | SOP ê¸°ë°˜ ê·œì¹™ ì‹¤í–‰ |
| **ì»¤ìŠ¤í„°ë§ˆì´ì§•** | ì œí•œì  (AWS í”„ë¦¬ì…‹) | ë†’ìŒ (ì™„ì „í•œ ììœ ë„) |
| **ìë™ ë³µêµ¬** | ì œì•ˆë§Œ ì œê³µ (ì‹¤í–‰ ì•ˆ í•¨) | ìë™ ì‹¤í–‰ ê°€ëŠ¥ |
| **ë¹„ìš©** | CloudWatch ì‚¬ìš©ëŸ‰ ê¸°ë°˜ | ì¸í”„ë¼ ë¹„ìš©ë§Œ |
| **í•™ìŠµ ê³¡ì„ ** | ì—†ìŒ (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥) | ì¤‘ê°„ (YAML ì‘ì„± í•„ìš”) |

**ì¶”ì²œ í†µí•© íŒ¨í„´**:

```mermaid
graph LR
    A[CloudWatch Alarm] --> B[Investigations]
    B --> C{ê·¼ë³¸ ì›ì¸ ì‹ë³„}
    C -->|ëª…í™•í•œ ì›ì¸| D[EventBridge]
    C -->|ë¶ˆëª…í™•| E[ì‚¬ëŒ ì—ìŠ¤ì»¬ë ˆì´ì…˜]
    D --> F[Kagent ìë™ ë³µêµ¬]
    F --> G[ë³µêµ¬ ì™„ë£Œ]
    G --> H[Investigations ì¬ê²€ì¦]
```

**í†µí•© ì˜ˆì‹œ: EventBridge Rule**

```json
{
  "source": ["aws.cloudwatch"],
  "detail-type": ["CloudWatch Investigation Complete"],
  "detail": {
    "conclusion": {
      "rootCauseType": ["Configuration Error", "Resource Exhaustion"]
    }
  }
}
```

```python
# EventBridge â†’ Kagent ìë™ ë³µêµ¬ Lambda
def lambda_handler(event, context):
    """
    CloudWatch Investigations ê²°ê³¼ë¥¼ ë°›ì•„
    Kagentë¥¼ í†µí•´ ìë™ ë³µêµ¬ë¥¼ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.
    """
    investigation = event['detail']
    root_cause = investigation['conclusion']['rootCauseType']

    if root_cause == "Configuration Error":
        # Kagentì— ConfigMap ë¡¤ë°± ìš”ì²­
        trigger_kagent_task(
            task_type="rollback_config",
            resource=investigation['affectedResources'][0],
            reason=investigation['conclusion']['summary']
        )
    elif root_cause == "Resource Exhaustion":
        # Kagentì— ìŠ¤ì¼€ì¼ë§ ìš”ì²­
        trigger_kagent_task(
            task_type="scale_up",
            resource=investigation['affectedResources'][0],
            target_replicas=calculate_required_replicas()
        )
```

:::tip CloudWatch Investigations í™œìš© ì „ëµ
CloudWatch InvestigationsëŠ” ì„¤ì • ì—†ì´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê´€ë¦¬í˜• AI ë¶„ì„ì…ë‹ˆë‹¤. ì»¤ìŠ¤í…€ ìë™í™”ê°€ í•„ìš”í•œ ê²½ìš° Kagent/Strands Agentì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì„¸ìš”.

**ê¶Œì¥ ì›Œí¬í”Œë¡œìš°**:
1. **1ì°¨ ë¶„ì„**: CloudWatch Investigationsë¡œ ê·¼ë³¸ ì›ì¸ ìë™ ì‹ë³„
2. **2ì°¨ ëŒ€ì‘**: ëª…í™•í•œ ì›ì¸ì¸ ê²½ìš° â†’ Kagent/Strandsë¡œ ìë™ ë³µêµ¬
3. **ì—ìŠ¤ì»¬ë ˆì´ì…˜**: ë¶ˆëª…í™•í•œ ê²½ìš° â†’ ì‚¬ëŒì—ê²Œ ì¡°ì‚¬ ê²°ê³¼ ì „ë‹¬
:::

#### ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤: EKS Pod OOMKilled ì¡°ì‚¬

```
[ì¸ì‹œë˜íŠ¸] 14:45 - payment-service Pod OOMKilled

[Investigations ìë™ ì¡°ì‚¬]

ë‹¨ê³„ 1: ê°€ì„¤ ìƒì„±
  - ê°€ì„¤ A: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜
  - ê°€ì„¤ B: íŠ¸ë˜í”½ ê¸‰ì¦ìœ¼ë¡œ ì¸í•œ ì •ìƒ ë©”ëª¨ë¦¬ ì¦ê°€
  - ê°€ì„¤ C: ë©”ëª¨ë¦¬ limits ì„¤ì • ì˜¤ë¥˜

ë‹¨ê³„ 2: ë°ì´í„° ìˆ˜ì§‘
  - Pod ë©”ëª¨ë¦¬ ì‚¬ìš© ì¶”ì´: 100Mi â†’ 512Mi (4ì‹œê°„)
  - íŠ¸ë˜í”½ ì¶”ì´: ë³€í™” ì—†ìŒ (ì•ˆì •ì )
  - Heap dump ë¶„ì„: Redis ì—°ê²° ê°ì²´ 10,000ê°œ ëˆ„ì 

ë‹¨ê³„ 3: ê·¼ë³¸ ì›ì¸ ì‹ë³„
  âœ“ ê°€ì„¤ A í™•ì¸: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ (Redis ì—°ê²° ë¯¸í•´ì œ)
  âœ— ê°€ì„¤ B ê¸°ê°: íŠ¸ë˜í”½ ë³€í™” ì—†ìŒ
  âœ— ê°€ì„¤ C ê¸°ê°: limitsëŠ” ì ì ˆ (512Mi)

ë‹¨ê³„ 4: ë³µêµ¬ ì œì•ˆ
  ì¦‰ì‹œ ì¡°ì¹˜:
    - kubectl rollout restart deployment/payment-service
    - ë©”ëª¨ë¦¬ limitsë¥¼ ì„ì‹œë¡œ 1Gië¡œ ì¦ê°€

  ê·¼ë³¸ì  í•´ê²°:
    - Redis í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ ìˆ˜ì • (ì—°ê²° í’€ ì œëŒ€ë¡œ ë‹«ê¸°)
    - ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ë„êµ¬ ì¶”ê°€
    - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ëª¨ë‹ˆí„°ë§ ì•ŒëŒ ì„¤ì •

  ê´€ë ¨ ì½”ë“œ:
    íŒŒì¼: src/cache/redis_client.go
    ë¬¸ì œ: defer conn.Close() ëˆ„ë½
    ìˆ˜ì • PR: https://github.com/...
```

### 5.6 Amazon Q Developer ìì—°ì–´ ê¸°ë°˜ ìš´ì˜ ìë™í™”

**Amazon Q Developer**ëŠ” AWSì˜ ì°¨ì„¸ëŒ€ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œ, **ìì—°ì–´ ì¸í„°í˜ì´ìŠ¤**ë¥¼ í†µí•´ EKS ìš´ì˜ì„ í˜ì‹ ì ìœ¼ë¡œ ê°„ì†Œí™”í•©ë‹ˆë‹¤. ì½˜ì†” íƒìƒ‰ì´ë‚˜ ë³µì¡í•œ ëª…ë ¹ì–´ ì—†ì´, ì¼ìƒ ì–¸ì–´ë¡œ ì§ˆë¬¸í•˜ê³  ì¡°ì¹˜ë¥¼ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ìì—°ì–´ ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ EKS íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

Amazon Q DeveloperëŠ” **ìì—°ì–´ ì§ˆì˜**ë¥¼ AWS API í˜¸ì¶œê³¼ kubectl ëª…ë ¹ìœ¼ë¡œ ìë™ ë³€í™˜í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤:

**ì‹œë‚˜ë¦¬ì˜¤ 1: OOM ë°œìƒ Pod ì°¾ê¸°**

```
[ì‚¬ìš©ì ì§ˆë¬¸]
"ì´ í´ëŸ¬ìŠ¤í„°ì—ì„œ OOMì´ ë°œìƒí•˜ëŠ” Podë¥¼ ì°¾ì•„ì¤˜"

[Amazon Q Developer ì‘ë‹µ]
ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...

ë°œê²¬ëœ OOM ì´ë²¤íŠ¸:
- payment-service-7d8f9c4b-xyz (namespace: payment)
  â””â”€ ìµœê·¼ 3íšŒ OOMKilled (ì§€ë‚œ 1ì‹œê°„)
  â””â”€ ë©”ëª¨ë¦¬ limits: 512Mi, ì‹¤ì œ ì‚¬ìš©: 520Mi
  â””â”€ ê¶Œì¥: memory limitsë¥¼ 1Gië¡œ ì¦ê°€

- cart-service-5a6b2d1c-abc (namespace: shopping)
  â””â”€ ìµœê·¼ 1íšŒ OOMKilled (ì§€ë‚œ 30ë¶„)
  â””â”€ ë©”ëª¨ë¦¬ limits: 256Mi, ì‹¤ì œ ì‚¬ìš©: 280Mi
  â””â”€ ê¶Œì¥: memory limitsë¥¼ 512Mië¡œ ì¦ê°€

ì‹¤í–‰ëœ ëª…ë ¹:
$ kubectl get events --all-namespaces --field-selector reason=OOMKilled
$ kubectl top pod -n payment payment-service-7d8f9c4b-xyz
$ kubectl describe pod -n payment payment-service-7d8f9c4b-xyz

ë‹¤ìŒ ì¡°ì¹˜ë¥¼ ì›í•˜ì‹œë‚˜ìš”?
1. memory limits ìë™ ì¡°ì • (VPA ì ìš©)
2. ìƒì„¸ ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì‹œì‘
3. ê´€ë ¨ ë¡œê·¸ ì „ì²´ ë¶„ì„
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ê³ ë¶€í•˜ ë…¸ë“œ ì‹ë³„**

```
[ì‚¬ìš©ì ì§ˆë¬¸]
"ì§€ë‚œ 1ì‹œê°„ ë™ì•ˆ CPU ì‚¬ìš©ë¥ ì´ 80% ì´ìƒì¸ ë…¸ë“œëŠ”?"

[Amazon Q Developer ì‘ë‹µ]
CloudWatch Container Insights ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ìˆìŠµë‹ˆë‹¤...

ê³ ë¶€í•˜ ë…¸ë“œ ëª©ë¡:

1. ip-10-0-1-45.ap-northeast-2.compute.internal
   - í‰ê·  CPU: 87.3% (ì§€ë‚œ 1ì‹œê°„)
   - ìµœëŒ€ CPU: 94.1%
   - ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: m5.xlarge (4 vCPU)
   - ì‹¤í–‰ ì¤‘ì¸ Pod: 23ê°œ
   - ê¶Œì¥ ì¡°ì¹˜: Karpenterë¥¼ í†µí•œ ë…¸ë“œ ì¶”ê°€ ë˜ëŠ” ì›Œí¬ë¡œë“œ ì¬ë¶„ì‚°

2. ip-10-0-2-78.ap-northeast-2.compute.internal
   - í‰ê·  CPU: 82.5% (ì§€ë‚œ 1ì‹œê°„)
   - ìµœëŒ€ CPU: 89.7%
   - ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: c5.2xlarge (8 vCPU)
   - ì‹¤í–‰ ì¤‘ì¸ Pod: 31ê°œ
   - ê¶Œì¥ ì¡°ì¹˜: CPU intensive Podë¥¼ ë‹¤ë¥¸ ë…¸ë“œë¡œ ì´ë™

ì‹¤í–‰ëœ CloudWatch ì¿¼ë¦¬:
namespace: ContainerInsights
metric: node_cpu_utilization
period: 1h
threshold: > 80%

ë‹¤ìŒ ì¡°ì¹˜ë¥¼ ì‹¤í–‰í• ê¹Œìš”?
1. Karpenterë¡œ ìƒˆ ë…¸ë“œ ìë™ í”„ë¡œë¹„ì €ë‹
2. ê³ ë¶€í•˜ Podë¥¼ ë‹¤ë¥¸ ë…¸ë“œë¡œ ìˆ˜ë™ ì´ë™
3. HPA ì„¤ì • ê²€í†  ë° ì¡°ì •
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: Deployment ë¡¤ë°±**

```
[ì‚¬ìš©ì ì§ˆë¬¸]
"ì´ Deploymentë¥¼ ë¡¤ë°±í•´ì¤˜"

[Amazon Q Developer ì‘ë‹µ]
payment-service Deploymentì˜ ë¡¤ë°±ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...

í˜„ì¬ ë°°í¬ ìƒíƒœ:
- Revision: 15
- ë°°í¬ ì‹œê°„: 2026-02-12 14:30:00 (15ë¶„ ì „)
- ì—ëŸ¬ìœ¨: 12.3% (í‰ì†Œ 0.5% ëŒ€ë¹„ ê¸‰ì¦)
- ì´ì „ ì•ˆì • ë²„ì „: Revision 14 (ì—ëŸ¬ìœ¨ 0.4%)

ë¡¤ë°± ëŒ€ìƒ:
- í˜„ì¬: payment-service:v2.3.1 (Revision 15)
- ë¡¤ë°±: payment-service:v2.3.0 (Revision 14)

PDB í™•ì¸:
- minAvailable: 2
- í˜„ì¬ Running: 5
- ë¡¤ë°± ì¤‘ ìµœì†Œ ë³´ì¥: 2ê°œ ìœ ì§€ âœ“

ì‹¤í–‰ ëª…ë ¹:
$ kubectl rollout undo deployment/payment-service -n payment --to-revision=14

ë¡¤ë°±ì„ ì‹¤í–‰í• ê¹Œìš”? (yes/no)
```

**ì‹œë‚˜ë¦¬ì˜¤ 4: ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ ì§„ë‹¨**

```
[ì‚¬ìš©ì ì§ˆë¬¸]
"payment-serviceê°€ databaseì— ì—°ê²°í•  ìˆ˜ ì—†ëŠ” ì´ìœ ë¥¼ ì°¾ì•„ì¤˜"

[Amazon Q Developer ì‘ë‹µ]
ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œë¥¼ ì§„ë‹¨í•˜ê³  ìˆìŠµë‹ˆë‹¤...

ì§„ë‹¨ ê²°ê³¼:

1. DNS í™•ì¸: âœ“ ì •ìƒ
   - database.payment.svc.cluster.local í•´ì„ ì„±ê³µ
   - IP: 10.100.15.234

2. ë„¤íŠ¸ì›Œí¬ ì •ì±… í™•ì¸: âœ— ë¬¸ì œ ë°œê²¬
   - payment-service â†’ database ì—°ê²° ì°¨ë‹¨ë¨
   - ì›ì¸: NetworkPolicy "database-access-policy"ì—ì„œ
     payment-serviceì˜ ë¼ë²¨ì´ í—ˆìš© ëª©ë¡ì— ì—†ìŒ

3. SecurityGroup í™•ì¸: âœ“ ì •ìƒ
   - RDS SecurityGroupì—ì„œ EKS ë…¸ë“œ í—ˆìš©

4. ë¡œê·¸ ë¶„ì„:
   - ì—ëŸ¬: "dial tcp 10.100.15.234:5432: i/o timeout"
   - ë°œìƒ ë¹ˆë„: ì§€ë‚œ 10ë¶„ê°„ 1,247íšŒ

í•´ê²° ë°©ë²•:
$ kubectl edit networkpolicy database-access-policy -n payment

ë‹¤ìŒ ë¼ë²¨ì„ ingress.from.podSelectorì— ì¶”ê°€:
  matchLabels:
    app: payment-service  # â† ì´ ë¼ë²¨ ì¶”ê°€

ìˆ˜ì •ëœ YAML ë¯¸ë¦¬ë³´ê¸°:
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-access-policy
  namespace: payment
spec:
  podSelector:
    matchLabels:
      app: database
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: payment-service  # â† ì¶”ê°€ë¨
        - podSelector:
            matchLabels:
              app: order-service
---

ìë™ìœ¼ë¡œ ìˆ˜ì •í• ê¹Œìš”? (yes/no)
```

#### Cost Explorer í†µí•©

Amazon Q DeveloperëŠ” **AWS Cost Explorer**ì™€ í†µí•©ë˜ì–´ ìì—°ì–´ë¡œ ë¹„ìš© ì§ˆì˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤:

**ì‹œë‚˜ë¦¬ì˜¤ 1: ë¹„ìš© ì¦ê°€ ì›ì¸ ë¶„ì„**

```
[ì‚¬ìš©ì ì§ˆë¬¸]
"ì´ë²ˆ ë‹¬ EKS ë¹„ìš©ì´ ì™œ ì¦ê°€í–ˆì–´?"

[Amazon Q Developer ì‘ë‹µ]
Cost Explorer ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...

ë¹„ìš© ì¦ê°€ ë¶„ì„ (2026ë…„ 2ì›”):

ì´ EKS ë¹„ìš©: $12,450 (ì „ì›” ëŒ€ë¹„ +45%)
ì¦ê°€ ì›ì¸:

1. ì»´í“¨íŒ… ë¹„ìš© ì¦ê°€: +$3,200 (52% ê¸°ì—¬)
   - m5.4xlarge ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© ì¦ê°€
   - ì›ì¸: ML ì¶”ë¡  ì›Œí¬ë¡œë“œ ì¦ê°€ (vLLM ë°°í¬)
   - ê¶Œì¥: Graviton (m7g) ë§ˆì´ê·¸ë ˆì´ì…˜ìœ¼ë¡œ 40% ì ˆê° ê°€ëŠ¥

2. ë°ì´í„° ì „ì†¡ ë¹„ìš© ì¦ê°€: +$1,800 (29% ê¸°ì—¬)
   - Cross-AZ íŠ¸ë˜í”½ ì¦ê°€ (150GB â†’ 450GB)
   - ì›ì¸: ìƒˆë¡œìš´ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ í†µì‹  íŒ¨í„´
   - ê¶Œì¥: Topology Aware Hintsë¡œ ë™ì¼ AZ í†µì‹  ì„ í˜¸

3. EBS ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì¦ê°€: +$950 (15% ê¸°ì—¬)
   - gp3 ë³¼ë¥¨ ì‚¬ìš©ëŸ‰ ì¦ê°€ (500GB â†’ 1,200GB)
   - ì›ì¸: Prometheus ë©”íŠ¸ë¦­ ë³´ê´€ ê¸°ê°„ ì—°ì¥ (7ì¼ â†’ 30ì¼)
   - ê¶Œì¥: S3ë¡œ ì¥ê¸° ë³´ê´€ ë°ì´í„° ì´ê´€

4. ë¡œë“œ ë°¸ëŸ°ì„œ ë¹„ìš© ì¦ê°€: +$250 (4% ê¸°ì—¬)
   - ALB LCU ì‚¬ìš© ì¦ê°€
   - ì›ì¸: API íŠ¸ë˜í”½ ì¦ê°€

ìƒì„¸ ë¹„ìš© ëŒ€ì‹œë³´ë“œ: (ë§í¬)
ë¹„ìš© ìµœì í™” ì œì•ˆ ë³´ê³ ì„œ ìƒì„± ì¤‘...
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: Spot vs On-Demand ë¹„ìš© ë¹„êµ**

```
[ì‚¬ìš©ì ì§ˆë¬¸]
"Spot ì¸ìŠ¤í„´ìŠ¤ë¡œ ì „í™˜í•˜ë©´ ì–¼ë§ˆë‚˜ ì ˆê°í•  ìˆ˜ ìˆì–´?"

[Amazon Q Developer ì‘ë‹µ]
Spot ì ˆê° ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...

í˜„ì¬ í´ëŸ¬ìŠ¤í„° êµ¬ì„±:
- On-Demand ë…¸ë“œ: 45ê°œ
- ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: m5.xlarge, m5.2xlarge, c5.2xlarge
- ì›” ë¹„ìš©: $8,500

Spot ì „í™˜ ì‹œë®¬ë ˆì´ì…˜ (70% Spot, 30% On-Demand):

ì‹œë‚˜ë¦¬ì˜¤ A: ë³´ìˆ˜ì  ì „í™˜ (Stateless ì›Œí¬ë¡œë“œë§Œ)
- Spot ë…¸ë“œ: 32ê°œ (70%)
- On-Demand ë…¸ë“œ: 13ê°œ (30%)
- ì˜ˆìƒ ì›” ë¹„ìš©: $3,200
- ì ˆê°ì•¡: $5,300/ì›” (62% ì ˆê°)
- ìœ„í—˜ë„: ë‚®ìŒ (Stateful ì›Œí¬ë¡œë“œëŠ” On-Demand ìœ ì§€)

ì‹œë‚˜ë¦¬ì˜¤ B: ì ê·¹ì  ì „í™˜ (ëŒ€ë¶€ë¶„ Spot)
- Spot ë…¸ë“œ: 40ê°œ (89%)
- On-Demand ë…¸ë“œ: 5ê°œ (11%)
- ì˜ˆìƒ ì›” ë¹„ìš©: $1,900
- ì ˆê°ì•¡: $6,600/ì›” (78% ì ˆê°)
- ìœ„í—˜ë„: ì¤‘ê°„ (Spot ì¤‘ë‹¨ ì‹œ ì¼ë¶€ ì„œë¹„ìŠ¤ ì˜í–¥)

Spot ì¤‘ë‹¨ ëŒ€ì‘:
- Karpenter ìë™ ì¬í”„ë¡œë¹„ì €ë‹: í‰ê·  90ì´ˆ
- Spot Instance Interruption Warning: 2ë¶„ ì „ ì•Œë¦¼
- PDBë¡œ ìµœì†Œ ê°€ìš©ì„± ë³´ì¥

ê¶Œì¥ ì‚¬í•­:
1. ì‹œë‚˜ë¦¬ì˜¤ Aë¡œ ì‹œì‘ (ì•ˆì •ì„± ìš°ì„ )
2. 2ì£¼ ë™ì•ˆ ëª¨ë‹ˆí„°ë§
3. ì•ˆì •ì ì´ë©´ ì‹œë‚˜ë¦¬ì˜¤ Bë¡œ í™•ëŒ€

Spot ì „í™˜ ê³„íšì„œ ìƒì„± ì¤‘...
```

#### MCP ì„œë²„ í†µí•© ì €ì½”ë“œ AIOps

Amazon Q DeveloperëŠ” **Model Context Protocol(MCP)**ë¥¼ í†µí•´ ë‹¤ì–‘í•œ AWS ì„œë¹„ìŠ¤ì™€ í†µí•©ë˜ì–´, **ì €ì½”ë“œ AIOps ì†”ë£¨ì…˜**ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**MCP ì„œë²„ í†µí•© ì•„í‚¤í…ì²˜**:

```mermaid
graph TD
    subgraph User["ğŸ‘¤ ìš´ì˜ì"]
        NL["ìì—°ì–´ ì§ˆë¬¸"]
    end

    subgraph QDev["ğŸ¤– Amazon Q Developer"]
        INTENT["ì˜ë„ ë¶„ì„"]
        ORCHESTRATE["MCP ì„œë²„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"]
        SYNTHESIZE["ì‘ë‹µ í•©ì„±"]
    end

    subgraph MCP["ğŸ“¡ MCP ì„œë²„ë“¤"]
        EKS_MCP["EKS MCP<br/>(kubectl)"]
        CW_MCP["CloudWatch MCP<br/>(ë©”íŠ¸ë¦­/ë¡œê·¸)"]
        CE_MCP["Cost Explorer MCP<br/>(ë¹„ìš©)"]
        BEDROCK_MCP["Bedrock MCP<br/>(AI ë¶„ì„)"]
    end

    subgraph AWS["â˜ï¸ AWS ì„œë¹„ìŠ¤"]
        EKS["Amazon EKS"]
        CW["CloudWatch"]
        CE["Cost Explorer"]
        BEDROCK["Bedrock"]
    end

    NL --> INTENT
    INTENT --> ORCHESTRATE
    ORCHESTRATE --> EKS_MCP
    ORCHESTRATE --> CW_MCP
    ORCHESTRATE --> CE_MCP
    ORCHESTRATE --> BEDROCK_MCP
    EKS_MCP --> EKS
    CW_MCP --> CW
    CE_MCP --> CE
    BEDROCK_MCP --> BEDROCK
    EKS --> SYNTHESIZE
    CW --> SYNTHESIZE
    CE --> SYNTHESIZE
    BEDROCK --> SYNTHESIZE
    SYNTHESIZE --> NL
```

**í†µí•© ìš´ì˜ ë·° ì˜ˆì‹œ**:

```
[ì‚¬ìš©ì ì§ˆë¬¸]
"payment-serviceì˜ í˜„ì¬ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë³´ì—¬ì¤˜"

[Amazon Q Developer ì‘ë‹µ]
payment-service í†µí•© ìƒíƒœ ëŒ€ì‹œë³´ë“œ:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  PAYMENT-SERVICE ìƒíƒœ (2026-02-12 15:30)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Kubernetes ìƒíƒœ (EKS MCP)
   - Deployment: payment-service
   - Replicas: 5/5 Running
   - Image: payment-service:v2.3.1
   - ìµœê·¼ ì¬ì‹œì‘: ì—†ìŒ (ì•ˆì •ì )
   - PDB: minAvailable=2 (ë§Œì¡±)

2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ (CloudWatch MCP)
   - Request Rate: 1,234 req/s
   - P50 Latency: 45ms
   - P99 Latency: 280ms (SLO: 500ms) âœ“
   - Error Rate: 0.3% (SLO: 1%) âœ“
   - CPU ì‚¬ìš©ë¥ : 65% (í‰ê· )
   - Memory ì‚¬ìš©ë¥ : 72% (í‰ê· )

3. ë¹„ìš© (Cost Explorer MCP)
   - ì´ë²ˆ ë‹¬ ë¹„ìš©: $1,245
   - ì „ì›” ëŒ€ë¹„: +12%
   - ì£¼ìš” ë¹„ìš©: ì»´í“¨íŒ… (m5.xlarge x 5)
   - ìµœì í™” ê¸°íšŒ: Spot ì „í™˜ ì‹œ $750 ì ˆê° ê°€ëŠ¥

4. AI ë¶„ì„ (Bedrock MCP)
   - ìƒíƒœ: ì •ìƒ
   - íŠ¸ë Œë“œ: íŠ¸ë˜í”½ ì¦ê°€ ì¶”ì„¸ (ì£¼ê°„ +15%)
   - ì˜ˆì¸¡: 2ì£¼ ë‚´ ìŠ¤ì¼€ì¼ë§ í•„ìš”
   - ê¶Œì¥: HPA maxReplicasë¥¼ 10ìœ¼ë¡œ ì¦ê°€

5. ìµœê·¼ ì´ë²¤íŠ¸ (CloudWatch Logs MCP)
   - ë°°í¬: 2ì¼ ì „ (v2.3.0 â†’ v2.3.1) ì„±ê³µ
   - ì•ŒëŒ: ì—†ìŒ
   - ì—ëŸ¬ ë¡œê·¸: 15ê±´ (ê²½ë¯¸, DB ì—°ê²° íƒ€ì„ì•„ì›ƒ)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì¢…í•© í‰ê°€: ğŸŸ¢ ì •ìƒ
ë‹¤ìŒ ì¡°ì¹˜ í•„ìš”: ì—†ìŒ
ê´€ì°° ê³„ì†: CPU/Memory ì¶”ì„¸ ëª¨ë‹ˆí„°ë§

ìì„¸íˆ ë³´ë ¤ëŠ” í•­ëª©ì´ ìˆë‚˜ìš”? (1-5)
```

**ìì—°ì–´ â†’ MCP í˜¸ì¶œ â†’ ê²°ê³¼ ë¶„ì„ â†’ ì•¡ì…˜ ì œì•ˆì˜ ìë™ ë£¨í”„**:

```python
# Amazon Q Developerì˜ ë‚´ë¶€ ë™ì‘ (ê°œë…ì )
class QDeveloperAIOpsLoop:
    def process_query(self, user_query: str):
        """ìì—°ì–´ ì§ˆì˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ìë™ ë£¨í”„"""

        # 1. ì˜ë„ ë¶„ì„
        intent = self.analyze_intent(user_query)
        # ì˜ˆ: "payment-service ìƒíƒœ" â†’ intents: ["k8s_status", "metrics", "cost"]

        # 2. í•„ìš”í•œ MCP ì„œë²„ ì‹ë³„
        required_mcps = self.identify_mcps(intent)
        # ì˜ˆ: ["eks-mcp", "cloudwatch-mcp", "cost-explorer-mcp"]

        # 3. MCP í˜¸ì¶œ (ë³‘ë ¬)
        results = await asyncio.gather(
            self.eks_mcp.get_deployment_status("payment-service"),
            self.cloudwatch_mcp.get_metrics("payment-service", period="1h"),
            self.cost_explorer_mcp.get_service_cost("payment-service")
        )

        # 4. ê²°ê³¼ í†µí•© ë¶„ì„ (Bedrock Claude ì‚¬ìš©)
        analysis = self.bedrock_mcp.analyze(
            prompt=f"ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¢…í•© ìƒíƒœë¥¼ í‰ê°€í•˜ê³  ì•¡ì…˜ì„ ì œì•ˆí•´ì£¼ì„¸ìš”:\n{results}",
            model="anthropic.claude-sonnet-4.0"
        )

        # 5. ì•¡ì…˜ ì œì•ˆ ìƒì„±
        actions = self.generate_actions(analysis)
        # ì˜ˆ: ["HPA ì¡°ì •", "Spot ì „í™˜ ê³ ë ¤", "ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ê°•í™”"]

        # 6. ì‚¬ìš©ìì—ê²Œ ì‘ë‹µ
        return self.format_response(analysis, actions)
```

**MCP ì„œë²„ ì¡°í•© ì˜ˆì‹œ**:

| ì§ˆë¬¸ ìœ í˜• | ì‚¬ìš©ë˜ëŠ” MCP ì„œë²„ | í†µí•© ë¶„ì„ |
|----------|----------------|----------|
| "Podê°€ ì™œ ì¬ì‹œì‘í•˜ë‚˜ìš”?" | EKS MCP + CloudWatch Logs MCP | ì´ë²¤íŠ¸ + ë¡œê·¸ ìƒê´€ ë¶„ì„ |
| "ë¹„ìš©ì´ ì™œ ì¦ê°€í–ˆë‚˜ìš”?" | Cost Explorer MCP + EKS MCP | ë¹„ìš© ì¦ê°€ + ë¦¬ì†ŒìŠ¤ ë³€ê²½ ìƒê´€ ë¶„ì„ |
| "ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì´ ë°œìƒí•˜ë‚˜ìš”?" | CloudWatch MCP + EKS MCP | ë©”íŠ¸ë¦­ + ë„¤íŠ¸ì›Œí¬ ì •ì±… ë¶„ì„ |
| "ë³´ì•ˆ ìœ„í˜‘ì´ ìˆë‚˜ìš”?" | GuardDuty MCP + EKS MCP | ìœ„í˜‘ íƒì§€ + Pod ìƒíƒœ ë¶„ì„ |

#### Kagent/Strandsì™€ì˜ ì°¨ì´ì 

| ì¸¡ë©´ | Amazon Q Developer | Kagent / Strands |
|------|-------------------|------------------|
| **ìš´ì˜ ë°©ì‹** | ëŒ€í™”í˜• ë„êµ¬ (Interactive) | ìë™í™” ì—ì´ì „íŠ¸ (Autonomous) |
| **íŠ¸ë¦¬ê±°** | ì‚¬ìš©ì ì§ˆë¬¸ (On-demand) | ì´ë²¤íŠ¸ ê¸°ë°˜ (Event-driven) |
| **ì£¼ìš” ìš©ë„** | ìˆ˜ë™ ì¡°ì‚¬ ë° ë¶„ì„ | ìë™ ëŒ€ì‘ ë° ë³µêµ¬ |
| **ì‹¤í–‰ ê¶Œí•œ** | ì½ê¸° ì¤‘ì‹¬ (ì¼ë¶€ ì“°ê¸°) | ì“°ê¸° ê¶Œí•œ í•„ìš” (ìë™ ì¡°ì¹˜) |
| **ì„¤ì • ë³µì¡ë„** | ë‚®ìŒ (ì¦‰ì‹œ ì‚¬ìš©) | ì¤‘ê°„ (YAML ì„¤ì • í•„ìš”) |
| **ì»¤ìŠ¤í„°ë§ˆì´ì§•** | ì œí•œì  (AWS í”„ë¦¬ì…‹) | ë†’ìŒ (SOP ê¸°ë°˜ ì™„ì „ ì œì–´) |
| **ë¹„ìš©** | Q Developer êµ¬ë… ë¹„ìš© | ì¸í”„ë¼ ë¹„ìš©ë§Œ |
| **í•™ìŠµ ê³¡ì„ ** | ì—†ìŒ (ìì—°ì–´) | ì¤‘ê°„ (Kubernetes ì§€ì‹ í•„ìš”) |

**ì¶”ì²œ ì¡°í•© íŒ¨í„´**:

```
[ì‹œë‚˜ë¦¬ì˜¤ 1: ì¸ì‹œë˜íŠ¸ ë°œìƒ]

1. Kagent/Strands (ìë™ ëŒ€ì‘)
   - ì•ŒëŒ ê°ì§€ â†’ ì¦‰ì‹œ ìë™ ì¡°ì¹˜ ì‹œì‘
   - ì˜ˆ: Pod ì¬ì‹œì‘, ìŠ¤ì¼€ì¼ë§, ë¡¤ë°±

2. Amazon Q Developer (ìˆ˜ë™ ì¡°ì‚¬)
   - ë³µì¡í•œ ì›ì¸ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°
   - ì˜ˆ: "ì™œ ì´ Podê°€ ê³„ì† ì¬ì‹œì‘í•˜ë‚˜ìš”?"

[ì‹œë‚˜ë¦¬ì˜¤ 2: ì •ê¸° ì ê²€]

1. Amazon Q Developer (ìˆ˜ë™ ì¡°ì‚¬)
   - "ì´ë²ˆ ì£¼ ë¹„ìš© ì¦ê°€ ì›ì¸ì„ ë¶„ì„í•´ì¤˜"
   - "ì„±ëŠ¥ ì €í•˜ê°€ ìˆëŠ” ì„œë¹„ìŠ¤ë¥¼ ì°¾ì•„ì¤˜"

2. Kagent/Strands (ìë™ ëŒ€ì‘)
   - Q Developerì˜ ì œì•ˆì„ ë°›ì•„ ìë™ ì ìš©
   - ì˜ˆ: VPA ì¡°ì •, HPA ì„¤ì • ë³€ê²½

[ì‹œë‚˜ë¦¬ì˜¤ 3: ì˜ˆì¸¡ ìš´ì˜]

1. CloudWatch Anomaly Detection
   - ì´ìƒ ì§•í›„ ìë™ ê°ì§€

2. Amazon Q Developer (ë¶„ì„)
   - "ì´ ì´ìƒ ì§•í›„ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”?"
   - "ê³¼ê±°ì— ìœ ì‚¬í•œ íŒ¨í„´ì´ ìˆì—ˆë‚˜ìš”?"

3. Kagent/Strands (ì„ ì œì  ì¡°ì¹˜)
   - ì˜ˆì¸¡ëœ ë¬¸ì œì— ëŒ€í•œ ì„ ì œì  ìŠ¤ì¼€ì¼ë§
```

**í†µí•© ì›Œí¬í”Œë¡œìš° ì˜ˆì‹œ**:

```yaml
# Kagent Agent: Amazon Q Developer ì œì•ˆì„ ìë™ ì‹¤í–‰
apiVersion: kagent.dev/v1alpha1
kind: Agent
metadata:
  name: q-developer-executor
spec:
  description: "Amazon Q Developerì˜ ì œì•ˆì„ ìë™ ì‹¤í–‰"
  triggers:
    - type: slack-command
      filter:
        command: "/q-execute"
  tools:
    - name: kubectl
      type: kmcp
    - name: amazon-q
      type: custom
      config:
        endpoint: "https://q.aws.amazon.com/api"
  workflow: |
    ## Q Developer ì œì•ˆ ìë™ ì‹¤í–‰ ì›Œí¬í”Œë¡œìš°

    1. Slackì—ì„œ Q Developerì—ê²Œ ì§ˆë¬¸
       ì˜ˆ: "@q payment-service ìµœì í™” ë°©ì•ˆì„ ì œì•ˆí•´ì¤˜"

    2. Q Developerê°€ ì œì•ˆ ìƒì„±
       ì˜ˆ: "HPA maxReplicasë¥¼ 10ìœ¼ë¡œ ì¦ê°€, VPA ì ìš©"

    3. ì‚¬ìš©ìê°€ ìŠ¹ì¸
       ëª…ë ¹: "/q-execute ì œì•ˆë²ˆí˜¸"

    4. Kagentê°€ ìë™ ì‹¤í–‰
       - HPA ì„¤ì • ë³€ê²½
       - VPA ìƒì„± ë° ì ìš©
       - ì‹¤í–‰ ê²°ê³¼ Slackì— ë³´ê³ 
```

:::tip Amazon Q Developerì˜ í•µì‹¬ ê°€ì¹˜
Amazon Q DeveloperëŠ” **ìì—°ì–´ ì¸í„°í˜ì´ìŠ¤**ë¥¼ í†µí•´ EKS ìš´ì˜ì˜ ì§„ì… ì¥ë²½ì„ ëŒ€í­ ë‚®ì¶¥ë‹ˆë‹¤. kubectl ëª…ë ¹ì–´ë‚˜ CloudWatch ì¿¼ë¦¬ ë¬¸ë²•ì„ ëª°ë¼ë„, ì¼ìƒ ì–¸ì–´ë¡œ ì§ˆë¬¸í•˜ê³  ì¡°ì¹˜ë¥¼ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. **MCP ì„œë²„ í†µí•©**ì„ í†µí•´ ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ì¡°í•©í•˜ì—¬, **ì €ì½”ë“œ AIOps ì†”ë£¨ì…˜**ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ê¶Œì¥ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**:
1. **ìˆ˜ë™ ì¡°ì‚¬**: ë³µì¡í•œ ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸ ë¶„ì„
2. **ë¹„ìš© ìµœì í™”**: Cost Explorerì™€ ì—°ë™í•œ ë¹„ìš© ì¸ì‚¬ì´íŠ¸
3. **í•™ìŠµ ë„êµ¬**: ì‹ ê·œ íŒ€ì›ì˜ EKS ìš´ì˜ í•™ìŠµ
4. **Kagent/Strands ì¡°í•©**: Q Developer(ì¡°ì‚¬) + Kagent(ìë™ ëŒ€ì‘)
:::

### 5.7 Bedrock AgentCore ê¸°ë°˜ ììœ¨ ìš´ì˜

**Amazon Bedrock AgentCore**ëŠ” Bedrock Agentsì˜ í•µì‹¬ ì—”ì§„ìœ¼ë¡œ, í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ **ì™„ì „ ììœ¨ ìš´ì˜ ì—ì´ì „íŠ¸**ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. Kagent/Strandsê°€ Kubernetes ë„¤ì´í‹°ë¸Œ ì ‘ê·¼ì´ë¼ë©´, Bedrock AgentCoreëŠ” AWS ë„¤ì´í‹°ë¸Œ ì ‘ê·¼ìœ¼ë¡œ **guardrails**ì™€ **action groups**ë¥¼ í†µí•´ ì•ˆì „í•œ ìë™í™” ë²”ìœ„ë¥¼ ëª…í™•íˆ ì œì–´í•©ë‹ˆë‹¤.

#### 5.6.1 Bedrock AgentCore ì•„í‚¤í…ì²˜

```mermaid
graph TD
    subgraph Trigger["ğŸ”” ì´ë²¤íŠ¸ ê°ì§€"]
        EVB["EventBridge<br/>(Alarm/Insight)"]
        CWA["CloudWatch<br/>Alarm"]
    end

    subgraph AgentCore["ğŸ¤– Bedrock AgentCore"]
        AGENT["Agent<br/>(claude-sonnet)"]
        KB["Knowledge Base<br/>(Runbook)"]
        AG["Action Groups<br/>(Lambda)"]
        GR["Guardrails<br/>(ì•ˆì „ ë²”ìœ„)"]
    end

    subgraph Actions["âš¡ ë³µêµ¬ ì¡°ì¹˜"]
        EKS_A["EKS API<br/>(kubectl)"]
        AWS_A["AWS API<br/>(RDS/SQS)"]
        NOTIFY["Slack/JIRA<br/>(ì•Œë¦¼)"]
    end

    EVB --> AGENT
    CWA --> AGENT
    AGENT --> KB
    AGENT --> AG
    AG --> GR
    GR --> EKS_A
    GR --> AWS_A
    GR --> NOTIFY

    style AgentCore fill:#fff3cd,stroke:#ff9800
```

#### 5.6.2 Bedrock Agent ì •ì˜ â€” ì¸ì‹œë˜íŠ¸ ììœ¨ ë³µêµ¬

```python
# Bedrock Agent ìƒì„± â€” ì¸ì‹œë˜íŠ¸ ìë™ ëŒ€ì‘
import boto3

bedrock = boto3.client('bedrock-agent', region_name='ap-northeast-2')

response = bedrock.create_agent(
    agentName='incident-auto-remediation',
    foundationModel='anthropic.claude-sonnet-v3',
    instruction="""
    ë‹¹ì‹ ì€ EKS ì¸ì‹œë˜íŠ¸ ìë™ ë³µêµ¬ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

    ## í•µì‹¬ ì›ì¹™
    1. ì•ˆì „ ìš°ì„ : guardrails ë²”ìœ„ ë‚´ì—ì„œë§Œ ì¡°ì¹˜
    2. ê·¼ë³¸ ì›ì¸ ë¶„ì„: ì¦ìƒì´ ì•„ë‹Œ ì›ì¸ í•´ê²°
    3. ìµœì†Œ ê°œì…: í•„ìš”í•œ ìµœì†Œí•œì˜ ë³€ê²½ë§Œ ìˆ˜í–‰
    4. ì™„ì „ íˆ¬ëª…ì„±: ëª¨ë“  ì¡°ì¹˜ë¥¼ Slackê³¼ JIRAì— ì¦‰ì‹œ ë³´ê³ 

    ## ìë™ ë³µêµ¬ ì›Œí¬í”Œë¡œìš°
    Phase 1: ê°ì§€ (30ì´ˆ ì´ë‚´)
    - CloudWatch Alarm ë¶„ì„
    - DevOps Guru Insight ìˆ˜ì§‘
    - ê´€ë ¨ EKS ë¦¬ì†ŒìŠ¤ ìƒíƒœ ì¡°íšŒ

    Phase 2: ì§„ë‹¨ (2ë¶„ ì´ë‚´)
    - Pod ë¡œê·¸ ë° ì´ë²¤íŠ¸ ë¶„ì„
    - ë©”íŠ¸ë¦­ ìƒê´€ ë¶„ì„ (CPU/Memory/Network)
    - ë°°í¬ ì´ë ¥ í™•ì¸ (ìµœê·¼ 10ë¶„ ë³€ê²½ ì‚¬í•­)
    - Knowledge Baseì—ì„œ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰

    Phase 3: ìë™ ë³µêµ¬ (5ë¶„ ì´ë‚´)
    - ë°°í¬ ì¥ì•  â†’ ìë™ ë¡¤ë°± (to previous stable revision)
    - ë¦¬ì†ŒìŠ¤ ë¶€ì¡± â†’ HPA ì¡°ì • ë˜ëŠ” Pod ì¬ì‹œì‘
    - ì˜ì¡´ ì„œë¹„ìŠ¤ ì¥ì•  â†’ ì¬ì‹œì‘ ë˜ëŠ” ì—°ê²° ì¬ì„¤ì •
    - ì›ì¸ ë¶ˆëª… â†’ ì‚¬ëŒì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜

    Phase 4: ê²€ì¦ ë° ë³´ê³ 
    - ë³µêµ¬ í›„ ìƒíƒœ í™•ì¸ (ë©”íŠ¸ë¦­ ì •ìƒí™” í™•ì¸)
    - ì¸ì‹œë˜íŠ¸ íƒ€ì„ë¼ì¸ ìƒì„±
    - Slack/JIRA ìë™ ë³´ê³ 
    """,
    idleSessionTTLInSeconds=600,
    agentResourceRoleArn='arn:aws:iam::ACCOUNT_ID:role/BedrockAgentRole'
)

agent_id = response['agent']['agentId']
```

#### 5.6.3 Action Groups â€” ì•ˆì „í•œ ë³µêµ¬ ì¡°ì¹˜ ë²”ìœ„

```python
# Action Group 1: EKS ì½ê¸° ì¡°íšŒ
bedrock.create_agent_action_group(
    agentId=agent_id,
    agentVersion='DRAFT',
    actionGroupName='eks-read-actions',
    actionGroupExecutor={
        'lambda': 'arn:aws:lambda:ap-northeast-2:ACCOUNT_ID:function:eks-read-handler'
    },
    apiSchema={
        'payload': '''
        {
          "openapi": "3.0.0",
          "info": {"title": "EKS Read API", "version": "1.0.0"},
          "paths": {
            "/pods": {
              "get": {
                "summary": "Get Pod list",
                "parameters": [
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}}
                ],
                "responses": {"200": {"description": "Pod list"}}
              }
            },
            "/pods/{name}/logs": {
              "get": {
                "summary": "Get Pod logs",
                "parameters": [
                  {"name": "name", "in": "path", "required": true, "schema": {"type": "string"}},
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}}
                ],
                "responses": {"200": {"description": "Pod logs"}}
              }
            },
            "/deployments/{name}/revisions": {
              "get": {
                "summary": "Get deployment revision history",
                "parameters": [
                  {"name": "name", "in": "path", "required": true, "schema": {"type": "string"}},
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}}
                ],
                "responses": {"200": {"description": "Revision list"}}
              }
            }
          }
        }
        '''
    }
)

# Action Group 2: EKS ë³µêµ¬ ì¡°ì¹˜ (guardrails ì ìš©)
bedrock.create_agent_action_group(
    agentId=agent_id,
    agentVersion='DRAFT',
    actionGroupName='eks-remediation-actions',
    actionGroupExecutor={
        'lambda': 'arn:aws:lambda:ap-northeast-2:ACCOUNT_ID:function:eks-remediation-handler'
    },
    apiSchema={
        'payload': '''
        {
          "openapi": "3.0.0",
          "info": {"title": "EKS Remediation API", "version": "1.0.0"},
          "paths": {
            "/deployments/{name}/rollback": {
              "post": {
                "summary": "Rollback deployment to previous revision",
                "parameters": [
                  {"name": "name", "in": "path", "required": true, "schema": {"type": "string"}},
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}},
                  {"name": "to_revision", "in": "query", "schema": {"type": "integer"}}
                ],
                "responses": {"200": {"description": "Rollback initiated"}}
              }
            },
            "/pods/{name}/restart": {
              "post": {
                "summary": "Restart Pod (delete and let controller recreate)",
                "parameters": [
                  {"name": "name", "in": "path", "required": true, "schema": {"type": "string"}},
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}}
                ],
                "responses": {"200": {"description": "Pod restarted"}}
              }
            },
            "/hpa/{name}/adjust": {
              "post": {
                "summary": "Adjust HPA min/max replicas",
                "parameters": [
                  {"name": "name", "in": "path", "required": true, "schema": {"type": "string"}},
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}},
                  {"name": "min_replicas", "in": "query", "schema": {"type": "integer"}},
                  {"name": "max_replicas", "in": "query", "schema": {"type": "integer"}}
                ],
                "responses": {"200": {"description": "HPA adjusted"}}
              }
            }
          }
        }
        '''
    }
)

# Action Group 3: ì•Œë¦¼ ë° ë³´ê³ 
bedrock.create_agent_action_group(
    agentId=agent_id,
    agentVersion='DRAFT',
    actionGroupName='notification-actions',
    actionGroupExecutor={
        'lambda': 'arn:aws:lambda:ap-northeast-2:ACCOUNT_ID:function:notification-handler'
    },
    apiSchema={
        'payload': '''
        {
          "openapi": "3.0.0",
          "info": {"title": "Notification API", "version": "1.0.0"},
          "paths": {
            "/slack/send": {
              "post": {
                "summary": "Send Slack notification",
                "requestBody": {
                  "required": true,
                  "content": {
                    "application/json": {
                      "schema": {
                        "type": "object",
                        "properties": {
                          "channel": {"type": "string"},
                          "message": {"type": "string"},
                          "severity": {"type": "string", "enum": ["info", "warning", "critical"]}
                        }
                      }
                    }
                  }
                },
                "responses": {"200": {"description": "Message sent"}}
              }
            },
            "/jira/create-incident": {
              "post": {
                "summary": "Create JIRA incident ticket",
                "requestBody": {
                  "required": true,
                  "content": {
                    "application/json": {
                      "schema": {
                        "type": "object",
                        "properties": {
                          "title": {"type": "string"},
                          "description": {"type": "string"},
                          "severity": {"type": "string"}
                        }
                      }
                    }
                  }
                },
                "responses": {"200": {"description": "Ticket created"}}
              }
            }
          }
        }
        '''
    }
)
```

#### 5.6.4 Guardrails â€” ì•ˆì „ ë²”ìœ„ ì œí•œ

```python
# Guardrails ì •ì˜ â€” ì•ˆì „í•œ ìë™í™” ë²”ìœ„ ì œí•œ
bedrock_guardrails = boto3.client('bedrock', region_name='ap-northeast-2')

guardrail_response = bedrock_guardrails.create_guardrail(
    name='incident-remediation-guardrails',
    description='ì¸ì‹œë˜íŠ¸ ìë™ ë³µêµ¬ ì•ˆì „ ë²”ìœ„ ì œí•œ',
    topicPolicyConfig={
        'topicsConfig': [
            {
                'name': 'data-deletion',
                'definition': 'Any action that deletes persistent data, such as PV, StatefulSet, or database',
                'type': 'DENY'
            },
            {
                'name': 'security-policy-change',
                'definition': 'Changes to SecurityGroup, NetworkPolicy, RBAC, or IAM roles',
                'type': 'DENY'
            },
            {
                'name': 'namespace-critical',
                'definition': 'Actions on kube-system or critical infrastructure namespaces',
                'type': 'DENY'
            }
        ]
    },
    contentPolicyConfig={
        'filtersConfig': [
            {'type': 'HATE', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},
            {'type': 'VIOLENCE', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'}
        ]
    },
    wordPolicyConfig={
        'wordsConfig': [
            {'text': 'delete pv'},
            {'text': 'delete statefulset'},
            {'text': 'drop database'},
            {'text': 'rm -rf'},
            {'text': 'delete namespace kube-system'}
        ],
        'managedWordListsConfig': [
            {'type': 'PROFANITY'}
        ]
    }
)

# Guardrailsë¥¼ Agentì— ì—°ê²°
bedrock.associate_agent_guardrail(
    agentId=agent_id,
    agentVersion='DRAFT',
    guardrailIdentifier=guardrail_response['guardrailId'],
    guardrailVersion='DRAFT'
)
```

#### 5.6.5 Knowledge Base í†µí•© â€” Runbook ìë™ ì°¸ì¡°

```python
# Knowledge Base ìƒì„± â€” Runbook ì €ì¥ì†Œ
bedrock.create_knowledge_base(
    name='incident-runbook-kb',
    description='ì¸ì‹œë˜íŠ¸ ëŒ€ì‘ Runbook ì €ì¥ì†Œ',
    roleArn='arn:aws:iam::ACCOUNT_ID:role/BedrockKBRole',
    knowledgeBaseConfiguration={
        'type': 'VECTOR',
        'vectorKnowledgeBaseConfiguration': {
            'embeddingModelArn': 'arn:aws:bedrock:ap-northeast-2::foundation-model/amazon.titan-embed-text-v1'
        }
    },
    storageConfiguration={
        'type': 'OPENSEARCH_SERVERLESS',
        'opensearchServerlessConfiguration': {
            'collectionArn': 'arn:aws:aoss:ap-northeast-2:ACCOUNT_ID:collection/runbook-kb',
            'vectorIndexName': 'runbook-index',
            'fieldMapping': {
                'vectorField': 'embedding',
                'textField': 'text',
                'metadataField': 'metadata'
            }
        }
    }
)

# Knowledge Baseë¥¼ Agentì— ì—°ê²°
bedrock.associate_agent_knowledge_base(
    agentId=agent_id,
    agentVersion='DRAFT',
    knowledgeBaseId='KB_ID',
    description='ì¸ì‹œë˜íŠ¸ ëŒ€ì‘ Runbook ìë™ ì°¸ì¡°',
    knowledgeBaseState='ENABLED'
)
```

**Runbook ì˜ˆì‹œ (Knowledge Baseì— ì €ì¥)**:

```markdown
# Runbook: OOMKilled Pod ë³µêµ¬

## ì¦ìƒ
- Pod Status: OOMKilled
- Event Reason: OOMKilled
- Container Exit Code: 137

## ê·¼ë³¸ ì›ì¸ ë¶„ì„
1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ íŠ¸ë Œë“œ í™•ì¸ (ì§€ë‚œ 24ì‹œê°„)
2. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íŒ¨í„´ í™•ì¸ (ì ì§„ì  ì¦ê°€ vs ê¸‰ì¦)
3. ë¡œê·¸ì—ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í™•ì¸

## ìë™ ë³µêµ¬ ì¡°ì¹˜
1. ì„ì‹œ ì¡°ì¹˜: memory limits 2ë°° ì¦ê°€ (ìµœëŒ€ 4Gi)
2. Pod ì¬ì‹œì‘
3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (30ë¶„)

## ê·¼ë³¸ ì›ì¸ í•´ê²°
1. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬: ê°œë°œíŒ€ì— ì—ìŠ¤ì»¬ë ˆì´ì…˜
2. ë°ì´í„° í¬ê¸° ì¦ê°€: VPA ì ìš© ê¶Œì¥
3. ì˜ëª»ëœ limits: Right-sizing ê¶Œì¥
```

#### 5.6.6 EventBridge í†µí•© â€” ìë™ íŠ¸ë¦¬ê±°

```json
{
  "source": ["aws.cloudwatch"],
  "detail-type": ["CloudWatch Alarm State Change"],
  "detail": {
    "alarmName": [{"prefix": "EKS-"}],
    "state": {
      "value": ["ALARM"]
    }
  }
}
```

**Lambda í•¨ìˆ˜ â€” Bedrock Agent í˜¸ì¶œ**:

```python
import boto3
import json

bedrock_runtime = boto3.client('bedrock-agent-runtime', region_name='ap-northeast-2')

def lambda_handler(event, context):
    alarm_name = event['detail']['alarmName']
    alarm_description = event['detail']['alarmDescription']

    # Bedrock Agent í˜¸ì¶œ
    response = bedrock_runtime.invoke_agent(
        agentId='AGENT_ID',
        agentAliasId='PROD',
        sessionId=f"incident-{alarm_name}-{event['time']}",
        inputText=f"""
        CloudWatch ì•ŒëŒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

        ì•ŒëŒ ì´ë¦„: {alarm_name}
        ì„¤ëª…: {alarm_description}
        ë°œìƒ ì‹œê°„: {event['time']}

        ì´ ì¸ì‹œë˜íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì§„ë‹¨í•˜ê³  ë³µêµ¬í•˜ì„¸ìš”.
        ëª¨ë“  ì¡°ì¹˜ëŠ” Slack #incidents ì±„ë„ì— ë³´ê³ í•˜ì„¸ìš”.
        """
    )

    return {
        'statusCode': 200,
        'body': json.dumps('Agent invoked successfully')
    }
```

#### 5.6.7 Kagent + Bedrock Agent í•˜ì´ë¸Œë¦¬ë“œ íŒ¨í„´

Kagent(K8s ë„¤ì´í‹°ë¸Œ)ì™€ Bedrock Agent(AWS ë„¤ì´í‹°ë¸Œ)ë¥¼ ê²°í•©í•˜ë©´ ìµœìƒì˜ ììœ¨ ìš´ì˜ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

| ì¸¡ë©´ | Kagent | Bedrock Agent | ê¶Œì¥ ì‚¬ìš© |
|------|--------|---------------|----------|
| **ë°°í¬ ë°©ì‹** | Kubernetes CRD | AWS ì„œë¹„ìŠ¤ | Kagent: í´ëŸ¬ìŠ¤í„° ë‚´ ì¡°ì¹˜<br/>Bedrock: AWS ë¦¬ì†ŒìŠ¤ ì¡°ì¹˜ |
| **ê¶Œí•œ ì œì–´** | RBAC | IAM + Guardrails | Kagent: Pod/Deployment<br/>Bedrock: RDS/SQS/Lambda |
| **ì»¨í…ìŠ¤íŠ¸** | K8s API ì§ì ‘ ì ‘ê·¼ | Action Groups í†µí•´ ì ‘ê·¼ | Kagent: K8s ì´ë²¤íŠ¸ ìš°ì„ <br/>Bedrock: CloudWatch ìš°ì„  |
| **ì•ˆì „ ì¥ì¹˜** | RBAC + NetworkPolicy | Guardrails + Word Policy | ë‘ ê°€ì§€ ëª¨ë‘ í™œìš© |
| **Knowledge Base** | ConfigMap/Custom | OpenSearch Serverless | Bedrock: ëŒ€ê·œëª¨ Runbook |
| **ë¹„ìš©** | ì¸í”„ë¼ ë¹„ìš©ë§Œ | Bedrock API í˜¸ì¶œ ë¹„ìš© | Kagent: ë¹ˆë²ˆí•œ ì¡°ì¹˜<br/>Bedrock: ë³µì¡í•œ ë¶„ì„ |

**í•˜ì´ë¸Œë¦¬ë“œ íŒ¨í„´ ì˜ˆì‹œ**:

```yaml
# Kagent: K8s ë¦¬ì†ŒìŠ¤ ìë™ ë³µêµ¬
apiVersion: kagent.dev/v1alpha1
kind: Agent
metadata:
  name: k8s-remediation
spec:
  triggers:
    - type: kubernetes-event
      filter:
        reason: ["OOMKilled", "CrashLoopBackOff"]
  tools:
    - name: kubectl
      type: kmcp
  workflow: |
    ## K8s ë¦¬ì†ŒìŠ¤ ìë™ ë³µêµ¬
    1. Pod ì¬ì‹œì‘
    2. HPA ì¡°ì •
    3. VPA ì ìš©
    4. Bedrock Agent í˜¸ì¶œ (AWS ë¦¬ì†ŒìŠ¤ ì¡°ì¹˜ í•„ìš” ì‹œ)
---
# EventBridge Rule: CloudWatch â†’ Bedrock Agent
{
  "source": ["aws.cloudwatch"],
  "detail-type": ["CloudWatch Alarm State Change"],
  "detail": {
    "alarmName": [{"prefix": "RDS-"}, {"prefix": "SQS-"}]
  }
}
```

**í†µí•© ì›Œí¬í”Œë¡œìš°**:

```
[ì¸ì‹œë˜íŠ¸ ë°œìƒ]
      â†“
[K8s Event?]  YES â†’ Kagent ìë™ ëŒ€ì‘ (Pod/Deployment ì¡°ì¹˜)
      â†“ NO
[CloudWatch Alarm?]  YES â†’ Bedrock Agent í˜¸ì¶œ (AWS ë¦¬ì†ŒìŠ¤ ì¡°ì¹˜)
      â†“
[ë³µì¡í•œ ê·¼ë³¸ ì›ì¸ ë¶„ì„ í•„ìš”?]
      â†“ YES
Bedrock Agentì˜ Knowledge Base ì°¸ì¡° â†’ Runbook ìë™ ì ìš©
      â†“
[Kagent + Bedrock Agent í˜‘ì—…]
Kagent: K8s ë¦¬ì†ŒìŠ¤ ë³µêµ¬
Bedrock Agent: RDS/SQS/Lambda ì¡°ì • + Slack ë³´ê³ 
```

:::info Bedrock AgentCoreì˜ í•µì‹¬ ê°€ì¹˜
Bedrock AgentCoreëŠ” **guardrails**ì™€ **action groups**ë¥¼ í†µí•´ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê²Œ ì™„ì „ ììœ¨ ìš´ì˜ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Kagent/Strandsê°€ K8s ë„¤ì´í‹°ë¸Œ ì ‘ê·¼ì´ë¼ë©´, Bedrock AgentCoreëŠ” AWS ë„¤ì´í‹°ë¸Œ ì ‘ê·¼ìœ¼ë¡œ **AWS ë¦¬ì†ŒìŠ¤(RDS, SQS, Lambda)**ê¹Œì§€ í†µí•© ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. **Knowledge Base í†µí•©**ì„ í†µí•´ ê³¼ê±° Runbookì„ ìë™ìœ¼ë¡œ ì°¸ì¡°í•˜ì—¬, ì¸ê°„ ìš´ì˜ìì˜ ì˜ì‚¬ê²°ì • íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ì¬í˜„í•©ë‹ˆë‹¤.
:::

#### 5.7.1 Node Readiness Controllerì™€ ì˜ˆì¸¡ì  ë…¸ë“œ ê´€ë¦¬

**Node Readiness Controller(NRC)**ëŠ” Kubernetes 1.33+ì—ì„œ ì œê³µë˜ëŠ” ë…¸ë“œ ì¤€ë¹„ ìƒíƒœ ìë™ ê´€ë¦¬ ë„êµ¬ì…ë‹ˆë‹¤. ë…¸ë“œ ì»¨ë””ì…˜(Node Condition) ë³€í™”ë¥¼ ê°ì§€í•˜ì—¬ ìë™ìœ¼ë¡œ taint/cordon ì‘ì—…ì„ ìˆ˜í–‰í•˜ì—¬, **ë°˜ì‘í˜• ìš´ì˜ì—ì„œ ì˜ˆì¸¡í˜• ìš´ì˜ìœ¼ë¡œ ì „í™˜**í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.

**ì˜ˆì¸¡ì  ìš´ì˜ì—ì„œì˜ NRC ì—­í• **:

```
[ë°˜ì‘í˜• ìš´ì˜]
ë…¸ë“œ ì¥ì•  ë°œìƒ â†’ ìˆ˜ë™ìœ¼ë¡œ kubectl cordon â†’ ìˆ˜ë™ drain â†’ ìˆ˜ë™ ë³µêµ¬
â€¢ ê°ì§€ ì§€ì—°: 5-10ë¶„
â€¢ ìˆ˜ë™ ê°œì…: í•„ìˆ˜
â€¢ MTTR: 20-30ë¶„

[NRC ê¸°ë°˜ ë°˜ìë™ ìš´ì˜]
Node Condition ë³€í™” â†’ NRCê°€ ìë™ taint ì ìš© â†’ ìƒˆ Pod ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨
â€¢ ê°ì§€ ì§€ì—°: 30ì´ˆ
â€¢ ìˆ˜ë™ ê°œì…: ë³µêµ¬ ì‹œì—ë§Œ
â€¢ MTTR: 5-10ë¶„

[AI + NRC ì˜ˆì¸¡ ìš´ì˜]
AIê°€ ì¥ì•  ì˜ˆì¸¡ â†’ Node Condition ì‚¬ì „ ì—…ë°ì´íŠ¸ â†’ NRCê°€ proactive taint
â€¢ ê°ì§€ ì§€ì—°: 0ë¶„ (ì˜ˆì¸¡)
â€¢ ìˆ˜ë™ ê°œì…: ì—†ìŒ
â€¢ MTTR: 2-5ë¶„ (ì‚¬ì „ ëŒ€í”¼)
```

**Continuous ëª¨ë“œì™€ ìë™ ë³µêµ¬ ë£¨í”„**:

NRCëŠ” **Continuous ëª¨ë“œ**ë¥¼ ì§€ì›í•˜ì—¬ Node Conditionì´ ë³µêµ¬ë˜ë©´ taintë¥¼ ìë™ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.

```yaml
apiVersion: nrc.k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-driver-health
spec:
  mode: Continuous  # í•µì‹¬: ìë™ ë³µêµ¬
  conditions:
    - type: GPUDriverHealthy
      status: "False"
  action:
    taint:
      key: gpu-driver-unhealthy
      effect: NoSchedule
```

**ìë™ ë³µêµ¬ ì‹œí€€ìŠ¤**:

```mermaid
graph LR
    A[GPU ë“œë¼ì´ë²„ í¬ë˜ì‹œ] --> B[NPDê°€ Condition False]
    B --> C[NRCê°€ NoSchedule taint]
    C --> D[ìƒˆ Pod ìŠ¤ì¼€ì¤„ë§ ì°¨ë‹¨]
    D --> E[ë“œë¼ì´ë²„ ìë™ ì¬ì‹œì‘]
    E --> F[NPDê°€ Condition True]
    F --> G[NRCê°€ taint ì œê±°]
    G --> H[ì„œë¹„ìŠ¤ ì •ìƒí™”]
```

**ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤: GPU ë…¸ë“œ ìë™ ë³µêµ¬**:

```bash
# 1. ì¥ì•  ê°ì§€ (NPDê°€ GPU ë“œë¼ì´ë²„ í¬ë˜ì‹œ ê°ì§€)
kubectl get node gpu-node-1 -o jsonpath='{.status.conditions[?(@.type=="GPUDriverHealthy")]}'
# Output: {"type":"GPUDriverHealthy","status":"False","reason":"DriverCrash"}

# 2. NRCê°€ ìë™ taint ì ìš© (30ì´ˆ ì´ë‚´)
kubectl describe node gpu-node-1 | grep Taints
# Output: gpu-driver-unhealthy:NoSchedule

# 3. ë“œë¼ì´ë²„ ìë™ ë³µêµ¬ (DaemonSet watchdog)
kubectl logs -n kube-system nvidia-driver-watchdog-xxx
# Output: "Restarting nvidia-driver.service..."

# 4. NPDê°€ ë³µêµ¬ ê°ì§€
kubectl get node gpu-node-1 -o jsonpath='{.status.conditions[?(@.type=="GPUDriverHealthy")]}'
# Output: {"type":"GPUDriverHealthy","status":"True","reason":"DriverHealthy"}

# 5. NRCê°€ taint ìë™ ì œê±°
kubectl describe node gpu-node-1 | grep Taints
# Output: <none>
```

**í•µì‹¬: ìˆ˜ë™ ê°œì… ì—†ëŠ” ì™„ì „ ìë™ ë³µêµ¬**ì…ë‹ˆë‹¤.

**Chaos Engineering í†µí•©**:

NRCëŠ” Chaos Engineeringê³¼ ê²°í•©í•˜ì—¬ **ì¥ì•  ëŒ€ì‘ ëŠ¥ë ¥ì„ ì‚¬ì „ ê²€ì¦**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
# AWS FIS Experiment: ë…¸ë“œ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
apiVersion: fis.aws.amazon.com/v1
kind: ExperimentTemplate
metadata:
  name: nrc-response-test
spec:
  description: "NRCì˜ ìë™ taint ë°˜ì‘ ì†ë„ ì¸¡ì •"
  actions:
    - name: inject-node-condition-failure
      actionId: aws:eks:inject-node-condition
      parameters:
        nodeSelector: gpu=true
        conditionType: GPUDriverHealthy
        conditionStatus: "False"
        duration: PT5M
  stopConditions:
    - source: aws:cloudwatch:alarm
      value: arn:aws:cloudwatch:...:alarm/pod-eviction-rate-high
  targets:
    - resourceType: aws:eks:node
      selectionMode: COUNT(1)
      resourceTags:
        gpu: "true"
```

**NRC dry-run ëª¨ë“œë¡œ ì˜í–¥ ë²”ìœ„ ì‚¬ì „ íŒŒì•…**:

```yaml
apiVersion: nrc.k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: memory-pressure-dryrun
spec:
  mode: DryRun  # ì‹¤ì œ taint ì ìš© ì—†ì´ ë¡œê·¸ë§Œ ê¸°ë¡
  conditions:
    - type: MemoryPressure
      status: "True"
  action:
    taint:
      key: memory-pressure
      effect: NoExecute  # ê°•ì œ Pod ì¢…ë£Œ ì‹œë®¬ë ˆì´ì…˜
```

```bash
# DryRun ëª¨ë“œ ê²°ê³¼ ë¶„ì„
kubectl logs -n kube-system node-readiness-controller | grep "DryRun"
# Output:
# [DryRun] Would apply taint to node-1: memory-pressure:NoExecute
# [DryRun] 15 pods would be evicted: [payment-service-xxx, order-service-yyy, ...]
# [DryRun] Estimated MTTR: 45 seconds
```

**AIê°€ ê³¼ê±° NRC ì´ë²¤íŠ¸ íŒ¨í„´ í•™ìŠµ â†’ ì¥ì•  ì˜ˆì¸¡ ëª¨ë¸ ê°œì„ **:

```python
# CloudWatch Logs Insights: NRC taint íŒ¨í„´ ë¶„ì„
query = """
fields @timestamp, node_name, condition_type, taint_key, pods_affected
| filter action = "taint_applied"
| stats count() by condition_type, bin(1h)
"""

# AI í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
import pandas as pd

nrc_events = cloudwatch_logs.query(query)
df = pd.DataFrame(nrc_events)

# ì¥ì•  ì˜ˆì¸¡ ëª¨ë¸ ì…ë ¥ í”¼ì²˜
features = [
    'condition_type',           # GPUDriverHealthy, MemoryPressure, DiskPressure
    'taint_frequency_1h',       # ì§€ë‚œ 1ì‹œê°„ taint ë¹ˆë„
    'node_age_days',            # ë…¸ë“œ ìƒì„± ì´í›„ ê²½ê³¼ ì¼ìˆ˜
    'pods_affected_avg',        # í‰ê·  ì˜í–¥ ë°›ëŠ” Pod ìˆ˜
]

# Prophet ê¸°ë°˜ ì¥ì•  ì˜ˆì¸¡
model = Prophet()
model.fit(df[['timestamp', 'taint_frequency_1h']].rename(columns={'timestamp': 'ds', 'taint_frequency_1h': 'y'}))
forecast = model.predict(future)

# ì˜ˆì¸¡ ê²°ê³¼ â†’ Node Condition ì‚¬ì „ ì—…ë°ì´íŠ¸
if forecast['yhat'].iloc[-1] > threshold:
    k8s.patch_node_condition(
        node_name='gpu-node-1',
        condition_type='GPUDriverHealthy',
        status='False',
        reason='PredictedFailure'
    )
    # NRCê°€ ìë™ìœ¼ë¡œ proactive taint ì ìš©
```

**Karpenter + NRC ììœ¨ ë…¸ë“œ ê´€ë¦¬**:

NRCì™€ Karpenterë¥¼ ê²°í•©í•˜ë©´ **ì™„ì „ ììœ¨ ë…¸ë“œ ìƒëª…ì£¼ê¸° ê´€ë¦¬**ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-pool
spec:
  disruption:
    consolidationPolicy: WhenEmpty
    budgets:
      - nodes: "1"
        schedule: "* * * * *"  # ë§¤ ë¶„ ì²´í¬
  template:
    metadata:
      labels:
        workload-type: gpu-inference
    spec:
      nodeClassRef:
        name: gpu-class
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["g5.xlarge", "g5.2xlarge"]
      taints:
        - key: gpu-not-ready
          effect: NoSchedule
          # NRCê°€ GPU ì¤€ë¹„ ì™„ë£Œ í›„ ì œê±°
```

**ììœ¨ ë…¸ë“œ êµì²´ ì‹œí€€ìŠ¤**:

```
1. NRCê°€ gpu-node-1ì— taint ì ìš© (GPU ë“œë¼ì´ë²„ ì¥ì• )
2. Karpenterê°€ ëŒ€ì²´ ë…¸ë“œ ìë™ í”„ë¡œë¹„ì €ë‹ (gpu-node-2)
3. gpu-node-2ì— NRC bootstrap ê·œì¹™ ì ìš©
   â†’ GPU ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ ì „ê¹Œì§€ gpu-not-ready:NoSchedule
4. NPDê°€ GPU ì¤€ë¹„ ì™„ë£Œ í™•ì¸ â†’ Condition True
5. NRCê°€ gpu-not-ready taint ì œê±°
6. Schedulerê°€ ì›Œí¬ë¡œë“œë¥¼ gpu-node-2ë¡œ ì´ë™
7. gpu-node-1ì˜ ëª¨ë“  Pod ì¢…ë£Œ í›„ Karpenterê°€ ë…¸ë“œ ì‚­ì œ
```

**ì „ì²´ ê³¼ì • ìë™: ê°ì§€ â†’ ê²©ë¦¬ â†’ ëŒ€ì²´ â†’ ë³µêµ¬ â†’ ì •ë¦¬**

:::tip NRC + AIì˜ í•µì‹¬ ê°€ì¹˜
Node Readiness ControllerëŠ” **ë°˜ì‘í˜• ìë™í™”**ë¥¼ ì œê³µí•˜ì§€ë§Œ, AIì™€ ê²°í•©í•˜ë©´ **ì˜ˆì¸¡í˜• ìë™í™”**ë¡œ ì§„í™”í•©ë‹ˆë‹¤. AIê°€ ê³¼ê±° NRC ì´ë²¤íŠ¸ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì¥ì• ë¥¼ ì˜ˆì¸¡í•˜ê³ , NRCê°€ ì‚¬ì „ì— taintë¥¼ ì ìš©í•˜ì—¬ **ì¥ì•  ë°œìƒ ì „ì— ì›Œí¬ë¡œë“œë¥¼ ëŒ€í”¼**ì‹œí‚µë‹ˆë‹¤. Karpenterì™€ í†µí•©í•˜ë©´ ë…¸ë“œ ìƒëª…ì£¼ê¸° ì „ì²´ë¥¼ ì™„ì „ ììœ¨í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

**ì°¸ì¡°**: [Introducing Node Readiness Controller](https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/)

---

## 6. Kiro í”„ë¡œê·¸ë˜ë¨¸í‹± ë””ë²„ê¹…

### 6.1 ë””ë ‰íŒ… vs í”„ë¡œê·¸ë˜ë¨¸í‹± ëŒ€ì‘ ë¹„êµ

```
[ë””ë ‰íŒ… ê¸°ë°˜ ëŒ€ì‘] â€” ìˆ˜ë™, ë°˜ë³µì , ë¹„ìš© ë†’ìŒ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ìš´ì˜ì: "payment-service 500 ì—ëŸ¬ ë°œìƒ"
  AI:     "ì–´ë–¤ Podì—ì„œ ë°œìƒí•˜ë‚˜ìš”?"
  ìš´ì˜ì: "payment-xxx Pod"
  AI:     "ë¡œê·¸ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
  ìš´ì˜ì: (kubectl logs ì‹¤í–‰ í›„ ë³µì‚¬-ë¶™ì—¬ë„£ê¸°)
  AI:     "DB ì—°ê²° ì˜¤ë¥˜ ê°™ìŠµë‹ˆë‹¤. RDS ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”"
  ìš´ì˜ì: (AWS ì½˜ì†”ì—ì„œ RDS í™•ì¸)
  ...ë°˜ë³µ...

  ì´ ì†Œìš”: 15-30ë¶„, ìˆ˜ë™ ì‘ì—… ë‹¤ìˆ˜

[í”„ë¡œê·¸ë˜ë¨¸í‹± ëŒ€ì‘] â€” ìë™, ì²´ê³„ì , ë¹„ìš© íš¨ìœ¨ì 
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ì•Œë¦¼: "payment-service 500 ì—ëŸ¬ ë°œìƒ"

  Kiro Spec:
    1. EKS MCPë¡œ Pod ìƒíƒœ ì¡°íšŒ
    2. ì—ëŸ¬ ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„
    3. ê´€ë ¨ AWS ì„œë¹„ìŠ¤ (RDS, SQS) ìƒíƒœ í™•ì¸
    4. ê·¼ë³¸ ì›ì¸ ì§„ë‹¨
    5. ìë™ ìˆ˜ì • ì½”ë“œ ìƒì„±
    6. PR ìƒì„± ë° ê²€ì¦

  ì´ ì†Œìš”: 2-5ë¶„, ìë™í™”
```

### 6.2 Kiro + MCP ë””ë²„ê¹… ì›Œí¬í”Œë¡œìš°

```mermaid
graph TD
    subgraph Trigger2["ğŸ”” ì´ìŠˆ ê°ì§€"]
        ALERT["CloudWatch<br/>Alarm"]
        GURU["DevOps Guru<br/>Insight"]
    end

    subgraph Kiro2["ğŸ¤– Kiro (í”„ë¡œê·¸ë˜ë¨¸í‹±)"]
        SPEC["Spec ê¸°ë°˜<br/>ì§„ë‹¨ ê³„íš"]
        MCP_Q["MCP í†µí•©<br/>ë°ì´í„° ìˆ˜ì§‘"]
        ANALYZE2["AI ë¶„ì„<br/>ê·¼ë³¸ ì›ì¸"]
        FIX["ìˆ˜ì • ì½”ë“œ<br/>ìë™ ìƒì„±"]
        PR["PR ìƒì„±<br/>+ ê²€ì¦"]
    end

    subgraph Deploy2["ğŸš€ ë°°í¬"]
        REVIEW["AI ë¦¬ë·°<br/>+ ìŠ¹ì¸"]
        ARGO2["Argo CD<br/>ìë™ ë°°í¬"]
        VERIFY["ë°°í¬ í›„<br/>ê²€ì¦"]
    end

    ALERT --> SPEC
    GURU --> SPEC
    SPEC --> MCP_Q --> ANALYZE2 --> FIX --> PR
    PR --> REVIEW --> ARGO2 --> VERIFY
    VERIFY -.->|ë¬¸ì œ ì§€ì†| SPEC

    style Kiro2 fill:#e3f2fd,stroke:#2196f3
```

### 6.3 êµ¬ì²´ì  ì‹œë‚˜ë¦¬ì˜¤: OOMKilled ìë™ ëŒ€ì‘

```
[Kiro í”„ë¡œê·¸ë˜ë¨¸í‹± ë””ë²„ê¹…: OOMKilled]

1. ê°ì§€: payment-service Pod OOMKilled ì´ë²¤íŠ¸

2. Kiro Spec ì‹¤í–‰:
   â†’ EKS MCP: get_events(namespace="payment", reason="OOMKilled")
   â†’ EKS MCP: get_pod_logs(pod="payment-xxx", previous=true)
   â†’ CloudWatch MCP: query_metrics("pod_memory_utilization", last="1h")

3. AI ë¶„ì„:
   "payment-serviceì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì‹œì‘ í›„ 2ì‹œê°„ë§ˆë‹¤
    256Miì”© ì¦ê°€í•˜ëŠ” ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íŒ¨í„´ ê°ì§€.
    ë¡œê·¸ì—ì„œ Redis ì—°ê²°ì´ ì œëŒ€ë¡œ ì¢…ë£Œë˜ì§€ ì•ŠëŠ” ê²ƒ í™•ì¸."

4. ìë™ ìˆ˜ì •:
   - memory limits 256Mi â†’ 512Mi (ì„ì‹œ ì¡°ì¹˜)
   - Redis ì—°ê²° í’€ ì •ë¦¬ ì½”ë“œ íŒ¨ì¹˜ ìƒì„±
   - ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì„¤ì • ì¶”ê°€

5. PR ìƒì„±:
   Title: "fix: payment-service Redis connection leak"
   - deployment.yaml: memory limits ì¡°ì •
   - redis_client.go: defer conn.Close() ì¶”ê°€
   - monitoring: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€ì‹œë³´ë“œ ì¶”ê°€
```

:::tip í”„ë¡œê·¸ë˜ë¨¸í‹± ë””ë²„ê¹…ì˜ í•µì‹¬
Kiro + EKS MCPë¥¼ í†µí•´ ì´ìŠˆë¥¼ **í”„ë¡œê·¸ë˜ë¨¸í‹±í•˜ê²Œ ë¶„ì„Â·í•´ê²°**í•©ë‹ˆë‹¤. ë””ë ‰íŒ… ë°©ì‹ì˜ ìˆ˜ë™ ëŒ€ì‘ ëŒ€ë¹„ **ë¹„ìš© íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ ìë™í™”**ê°€ ê°€ëŠ¥í•˜ë©°, ë™ì¼í•œ ì´ìŠˆê°€ ë°˜ë³µë  ë•Œ í•™ìŠµëœ Specì„ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

---

## 7. AI Right-Sizing

### 7.1 Container Insights ê¸°ë°˜ ì¶”ì²œ

CloudWatch Container InsightsëŠ” Podì˜ ì‹¤ì œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì ì • í¬ê¸°ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.

```promql
# ì‹¤ì œ CPU ì‚¬ìš©ëŸ‰ vs requests ë¹„êµ
avg(rate(container_cpu_usage_seconds_total{namespace="payment"}[1h]))
  by (pod)
/ avg(kube_pod_container_resource_requests{resource="cpu", namespace="payment"})
  by (pod)
* 100

# ì‹¤ì œ Memory ì‚¬ìš©ëŸ‰ vs requests ë¹„êµ
avg(container_memory_working_set_bytes{namespace="payment"})
  by (pod)
/ avg(kube_pod_container_resource_requests{resource="memory", namespace="payment"})
  by (pod)
* 100
```

### 7.2 VPA + ML ê¸°ë°˜ ìë™ Right-Sizing

```yaml
# VPA (Vertical Pod Autoscaler) ì„¤ì •
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: payment-service-vpa
  namespace: payment
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: payment-service
  updatePolicy:
    updateMode: "Auto"  # Off, Initial, Auto
  resourcePolicy:
    containerPolicies:
      - containerName: app
        minAllowed:
          cpu: 100m
          memory: 128Mi
        maxAllowed:
          cpu: "2"
          memory: 4Gi
        controlledResources: ["cpu", "memory"]
```

### 7.3 Right-Sizing íš¨ê³¼

<RightSizingResults />

:::tip K8s 1.35: In-Place Pod Resource Updates
K8s 1.35(2026.01, EKS ì§€ì›)ë¶€í„° **In-Place Pod Resource Updates** ê¸°ëŠ¥ì´ ë„ì…ë˜ì–´, Podë¥¼ ì¬ì‹œì‘í•˜ì§€ ì•Šê³ ë„ CPUì™€ ë©”ëª¨ë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” VPAì˜ ê°€ì¥ í° í•œê³„ì˜€ë˜ "ë¦¬ì†ŒìŠ¤ ë³€ê²½ ì‹œ Pod ì¬ì‹œì‘" ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. StatefulSetì´ë‚˜ ì¬ì‹œì‘ì— ë¯¼ê°í•œ ì›Œí¬ë¡œë“œì—ì„œë„ ì•ˆì „í•˜ê²Œ ìˆ˜ì§ ìŠ¤ì¼€ì¼ë§ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.
:::

:::warning VPA ì£¼ì˜ì‚¬í•­ (K8s 1.34 ì´í•˜)
K8s 1.34 ì´í•˜ì—ì„œ VPA `Auto` ëª¨ë“œëŠ” Podë¥¼ ì¬ì‹œì‘í•˜ì—¬ ë¦¬ì†ŒìŠ¤ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤. StatefulSetì´ë‚˜ ì¬ì‹œì‘ì— ë¯¼ê°í•œ ì›Œí¬ë¡œë“œì—ëŠ” `Off` ëª¨ë“œë¡œ ì¶”ì²œê°’ë§Œ í™•ì¸í•˜ê³ , ìˆ˜ë™ìœ¼ë¡œ ì ìš©í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤. VPAì™€ HPAë¥¼ ë™ì¼ ë©”íŠ¸ë¦­(CPU/Memory)ìœ¼ë¡œ ë™ì‹œì— ì‚¬ìš©í•˜ë©´ ì¶©ëŒì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
:::

### 7.4 In-Place Pod Vertical Scaling (K8s 1.33+)

Kubernetes 1.33ë¶€í„° **In-Place Pod Vertical Scaling**ì´ Betaë¡œ ì§„ì…í•˜ë©´ì„œ, VPAì˜ ê°€ì¥ í° ë‹¨ì ì´ì—ˆë˜ **Pod ì¬ì‹œì‘ ë¬¸ì œ**ê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì‹¤í–‰ ì¤‘ì¸ Podì˜ CPUì™€ ë©”ëª¨ë¦¬ë¥¼ ì¬ì‹œì‘ ì—†ì´ ë™ì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### In-Place Pod Resize ê°œìš”

ê¸°ì¡´ VPAì˜ ë¬¸ì œì :
- Pod ë¦¬ì†ŒìŠ¤ ë³€ê²½ ì‹œ **ë°˜ë“œì‹œ ì¬ì‹œì‘** í•„ìš”
- StatefulSet, ë°ì´í„°ë² ì´ìŠ¤, ìºì‹œ ë“± **ìƒíƒœ ìœ ì§€ê°€ ì¤‘ìš”í•œ ì›Œí¬ë¡œë“œ**ì—ì„œ ì‚¬ìš© ì–´ë ¤ì›€
- ì¬ì‹œì‘ ì¤‘ ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ê°€ëŠ¥ì„±
- PDB(Pod Disruption Budget)ì™€ì˜ ì¶©ëŒ

In-Place Resizeì˜ í•´ê²°ì±…:
- **ì‹¤í–‰ ì¤‘ì¸ Podì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •**
- cgroup ì œí•œì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³€ê²½
- ì¬ì‹œì‘ ì—†ì´ ë¦¬ì†ŒìŠ¤ ì¦ê°€/ê°ì†Œ
- **QoS Class ìœ ì§€** ì‹œ ì¬ì‹œì‘ ë¶ˆí•„ìš”

#### Kubernetes ë²„ì „ë³„ ìƒíƒœ

| Kubernetes ë²„ì „ | ìƒíƒœ | Feature Gate | ë¹„ê³  |
|----------------|------|--------------|------|
| 1.27 | Alpha | `InPlacePodVerticalScaling` | ì‹¤í—˜ì  ê¸°ëŠ¥ |
| 1.33 | Beta | ê¸°ë³¸ í™œì„±í™” | í”„ë¡œë•ì…˜ í…ŒìŠ¤íŠ¸ ê¶Œì¥ |
| 1.35+ (ì˜ˆìƒ) | Stable | ê¸°ë³¸ í™œì„±í™” | í”„ë¡œë•ì…˜ ì•ˆì „ ì‚¬ìš© |

**EKS ì§€ì› í˜„í™©**:
- **EKS 1.33** (2026ë…„ 4ì›” ì˜ˆìƒ): Beta ê¸°ëŠ¥ í™œì„±í™” ê°€ëŠ¥
- **EKS 1.35** (2026ë…„ 11ì›” ì˜ˆìƒ): Stable ë²„ì „ ì§€ì›

EKSì—ì„œ Feature Gate í™œì„±í™” ë°©ë²• (1.33 Beta):
```bash
# EKS í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ Feature Gate í™œì„±í™” (ì˜ˆì •)
aws eks create-cluster \
  --name my-cluster \
  --kubernetes-version 1.33 \
  --kubernetes-network-config '{"serviceIpv4Cidr":"10.100.0.0/16"}' \
  --role-arn arn:aws:iam::ACCOUNT_ID:role/EKSClusterRole \
  --resources-vpc-config subnetIds=subnet-xxx,subnet-yyy \
  --feature-gates InPlacePodVerticalScaling=true
```

:::info EKS Feature Gate ì§€ì›
EKSëŠ” Kubernetes ë²„ì „ì´ GAëœ í›„ ì¼ì • ê¸°ê°„ í›„ì— Feature Gateë¥¼ ì§€ì›í•©ë‹ˆë‹¤. 1.33 Beta ê¸°ëŠ¥ì€ EKS 1.33 ì¶œì‹œì™€ ë™ì‹œì— í™œì„±í™”ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, AWS ê³µì‹ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.
:::

#### ë™ì‘ ë°©ì‹

In-Place ResizeëŠ” **`resize` subresource**ë¥¼ í†µí•´ ì‹¤í–‰ ì¤‘ì¸ Podì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤:

```yaml
# Podì˜ resize ìƒíƒœ í™•ì¸
apiVersion: v1
kind: Pod
metadata:
  name: payment-service-abc123
spec:
  containers:
    - name: app
      resources:
        requests:
          cpu: "1"
          memory: 2Gi
        limits:
          cpu: "2"
          memory: 4Gi
status:
  resize: InProgress  # Proposed, InProgress, Deferred, Infeasible
  containerStatuses:
    - name: app
      allocatedResources:
        cpu: "1"
        memory: 2Gi
      resources:
        requests:
          cpu: "1.5"  # ìƒˆë¡œìš´ ìš”ì²­ê°’
          memory: 3Gi
```

**Resize ìƒíƒœ ì „ì´**:

```
Proposed (ì œì•ˆë¨)
  â†“
InProgress (ì§„í–‰ ì¤‘) â€” kubeletì´ cgroup ì œí•œ ë³€ê²½
  â†“
[ì„±ê³µ] Pod.spec.resources == Pod.status.allocatedResources
  ë˜ëŠ”
[ì‹¤íŒ¨] Deferred (ì§€ì—°ë¨) â€” ë¦¬ì†ŒìŠ¤ ë¶€ì¡±, ë‚˜ì¤‘ì— ì¬ì‹œë„
  ë˜ëŠ”
[ì‹¤íŒ¨] Infeasible (ë¶ˆê°€ëŠ¥) â€” QoS Class ë³€ê²½ í•„ìš”, ì¬ì‹œì‘ í•„ìš”
```

#### VPA Auto ëª¨ë“œì™€ í†µí•©

VPAëŠ” In-Place Resizeê°€ ê°€ëŠ¥í•œ ê²½ìš° **ìë™ìœ¼ë¡œ ì¬ì‹œì‘ ì—†ì´ ë¦¬ì†ŒìŠ¤ë¥¼ ì¡°ì •**í•©ë‹ˆë‹¤:

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: payment-service-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: payment-service
  updatePolicy:
    updateMode: "Auto"  # In-Place Resize ì§€ì› ì‹œ ì¬ì‹œì‘ ì—†ì´ ì¡°ì •
  resourcePolicy:
    containerPolicies:
      - containerName: app
        minAllowed:
          cpu: 100m
          memory: 128Mi
        maxAllowed:
          cpu: "4"
          memory: 8Gi
        controlledResources: ["cpu", "memory"]
        mode: Auto  # In-Place Resize ìë™ ì ìš©
```

**VPA ë™ì‘ íë¦„**:

```mermaid
graph TD
    A[VPA Recommender] -->|ë¦¬ì†ŒìŠ¤ ì¶”ì²œ| B{In-Place Resize ê°€ëŠ¥?}
    B -->|Yes| C[resize subresource ì—…ë°ì´íŠ¸]
    B -->|No| D[Pod ì¬ìƒì„±]
    C --> E[kubeletì´ cgroup ë³€ê²½]
    E --> F[Pod ì‹¤í–‰ ìœ ì§€]
    D --> G[Pod ì¬ì‹œì‘]
```

#### ì œì•½ì‚¬í•­

1. **CPUëŠ” ììœ ë¡­ê²Œ resize ê°€ëŠ¥**
   - CPU shares, CPU quota ë™ì  ë³€ê²½ ê°€ëŠ¥
   - cgroup CPU ì»¨íŠ¸ë¡¤ëŸ¬ê°€ ì‹¤ì‹œê°„ ë³€ê²½ ì§€ì›

2. **MemoryëŠ” ì¦ê°€ë§Œ ê°€ëŠ¥, ê°ì†Œ ë¶ˆê°€**
   - Linux cgroup v1/v2 ì œí•œìœ¼ë¡œ ë©”ëª¨ë¦¬ limit **ê°ì†Œ ì‹œ ì¬ì‹œì‘ í•„ìš”**
   - ë©”ëª¨ë¦¬ ì¦ê°€ëŠ” ê°€ëŠ¥ (cgroup memory.limit_in_bytes ì¦ê°€)
   - ë©”ëª¨ë¦¬ ê°ì†ŒëŠ” Infeasible ìƒíƒœë¡œ ì „í™˜ â†’ Pod ì¬ìƒì„± í•„ìš”

```yaml
# Memory ì¦ê°€: In-Place Resize ê°€ëŠ¥ âœ…
resources:
  requests:
    memory: 2Gi â†’ 4Gi  # OK, ì¬ì‹œì‘ ì—†ìŒ

# Memory ê°ì†Œ: In-Place Resize ë¶ˆê°€ âŒ
resources:
  requests:
    memory: 4Gi â†’ 2Gi  # Infeasible, Pod ì¬ìƒì„± í•„ìš”
```

3. **QoS Class ë³€ê²½ ì‹œ ì¬ì‹œì‘ í•„ìš”**

QoS ClassëŠ” Podì˜ ë¦¬ì†ŒìŠ¤ ë³´ì¥ ìˆ˜ì¤€ì„ ê²°ì •í•˜ë¯€ë¡œ, ë³€ê²½ ì‹œ ì¬ì‹œì‘ì´ í•„ìš”í•©ë‹ˆë‹¤:

| ê¸°ì¡´ QoS | ìƒˆë¡œìš´ QoS | In-Place Resize ê°€ëŠ¥? |
|----------|------------|---------------------|
| Guaranteed | Guaranteed | âœ… ê°€ëŠ¥ (requests == limits ìœ ì§€) |
| Burstable | Burstable | âœ… ê°€ëŠ¥ |
| BestEffort | BestEffort | âœ… ê°€ëŠ¥ |
| Guaranteed | Burstable | âŒ ë¶ˆê°€ (ì¬ì‹œì‘ í•„ìš”) |
| Burstable | Guaranteed | âŒ ë¶ˆê°€ (ì¬ì‹œì‘ í•„ìš”) |

```yaml
# QoS Class ìœ ì§€: In-Place Resize ê°€ëŠ¥ âœ…
# Guaranteed â†’ Guaranteed
resources:
  requests:
    cpu: "1"
    memory: 2Gi
  limits:
    cpu: "1"    # requests == limits ìœ ì§€
    memory: 2Gi
# â†’ (ë³€ê²½ í›„)
resources:
  requests:
    cpu: "2"
    memory: 4Gi
  limits:
    cpu: "2"    # requests == limits ìœ ì§€
    memory: 4Gi

# QoS Class ë³€ê²½: In-Place Resize ë¶ˆê°€ âŒ
# Guaranteed â†’ Burstable
resources:
  requests:
    cpu: "1"
    memory: 2Gi
  limits:
    cpu: "1"
    memory: 2Gi
# â†’ (ë³€ê²½ í›„)
resources:
  requests:
    cpu: "1"
    memory: 2Gi
  limits:
    cpu: "2"    # requests != limits â†’ QoS ë³€ê²½
    memory: 4Gi
# â†’ Infeasible, Pod ì¬ìƒì„± í•„ìš”
```

#### StatefulSetì˜ ì•ˆì „í•œ ìˆ˜ì§ ìŠ¤ì¼€ì¼ë§ íŒ¨í„´

StatefulSetì€ ìƒíƒœ ìœ ì§€ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ, In-Place Resizeë¥¼ í™œìš©í•œ ì•ˆì „í•œ íŒ¨í„´ì„ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤:

**íŒ¨í„´ 1: Guaranteed QoS ìœ ì§€**

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  replicas: 3
  template:
    spec:
      containers:
        - name: postgres
          image: postgres:15
          resources:
            requests:
              cpu: "2"
              memory: 4Gi
            limits:
              cpu: "2"    # requests == limits (Guaranteed QoS)
              memory: 4Gi
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: postgres-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: postgres
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: postgres
        minAllowed:
          cpu: "1"
          memory: 2Gi
        maxAllowed:
          cpu: "4"
          memory: 8Gi
        controlledResources: ["cpu", "memory"]
        controlledValues: RequestsAndLimits  # requestsì™€ limitsë¥¼ í•¨ê»˜ ì¡°ì •
```

**íŒ¨í„´ 2: ì ì§„ì  ë©”ëª¨ë¦¬ ì¦ê°€ (ê°ì†Œ ë°©ì§€)**

```python
# VPA ì¶”ì²œê°’ì„ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ë©”ëª¨ë¦¬ ê°ì†Œ ë°©ì§€
import boto3
from kubernetes import client, config

def safe_vpa_update(namespace, statefulset_name):
    """
    VPA ì¶”ì²œê°’ì„ í™•ì¸í•˜ì—¬ ë©”ëª¨ë¦¬ ê°ì†Œê°€ í•„ìš”í•œ ê²½ìš° ì•Œë¦¼ë§Œ ë³´ë‚´ê³ ,
    ì¦ê°€ê°€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ In-Place Resize ìˆ˜í–‰
    """
    config.load_kube_config()
    v1 = client.CoreV1Api()

    # í˜„ì¬ Podì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ
    pods = v1.list_namespaced_pod(
        namespace=namespace,
        label_selector=f"app={statefulset_name}"
    )

    for pod in pods.items:
        current_memory = pod.spec.containers[0].resources.requests['memory']
        vpa_recommendation = get_vpa_recommendation(namespace, statefulset_name)

        if vpa_recommendation['memory'] < current_memory:
            # ë©”ëª¨ë¦¬ ê°ì†ŒëŠ” ì•Œë¦¼ë§Œ
            send_alert(
                f"[WARNING] {pod.metadata.name}: VPA recommends memory decrease "
                f"({current_memory} â†’ {vpa_recommendation['memory']}). "
                f"Manual Pod restart required for memory decrease."
            )
        elif vpa_recommendation['memory'] > current_memory:
            # ë©”ëª¨ë¦¬ ì¦ê°€ëŠ” In-Place Resize ìˆ˜í–‰
            apply_in_place_resize(pod.metadata.name, vpa_recommendation)
```

**íŒ¨í„´ 3: ë¡¤ë§ ì—…ë°ì´íŠ¸ì™€ In-Place Resize ì¡°í•©**

```yaml
# StatefulSet ì—…ë°ì´íŠ¸ ì „ëµ: ë¡¤ë§ ì—…ë°ì´íŠ¸ + In-Place Resize
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
spec:
  replicas: 5
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0  # ëª¨ë“  Pod ì—…ë°ì´íŠ¸ ëŒ€ìƒ
  podManagementPolicy: OrderedReady
  template:
    spec:
      containers:
        - name: cassandra
          resources:
            requests:
              cpu: "4"
              memory: 8Gi
            limits:
              cpu: "4"
              memory: 8Gi
```

**ì—…ë°ì´íŠ¸ ì‹œë‚˜ë¦¬ì˜¤**:

1. **CPU ì¦ê°€**: In-Place Resizeë¡œ ì¦‰ì‹œ ì ìš© (ì¬ì‹œì‘ ì—†ìŒ)
2. **Memory ì¦ê°€**: In-Place Resizeë¡œ ì¦‰ì‹œ ì ìš© (ì¬ì‹œì‘ ì—†ìŒ)
3. **Memory ê°ì†Œ**: ë¡¤ë§ ì—…ë°ì´íŠ¸ë¡œ Podë¥¼ í•˜ë‚˜ì”© ì¬ì‹œì‘ (Quorum ìœ ì§€)

```bash
# ë©”ëª¨ë¦¬ ê°ì†Œ ì‹œ ì•ˆì „í•œ ë¡¤ë§ ì¬ì‹œì‘
kubectl rollout restart statefulset/cassandra -n database

# ë¡¤ë§ ì¬ì‹œì‘ ìƒíƒœ ëª¨ë‹ˆí„°ë§
kubectl rollout status statefulset/cassandra -n database

# Podë³„ ì¬ì‹œì‘ í™•ì¸ (Quorum ìœ ì§€)
# cassandra-4 â†’ cassandra-3 â†’ cassandra-2 â†’ cassandra-1 â†’ cassandra-0
```

#### ì‹¤ì „ ì˜ˆì œ: Redis í´ëŸ¬ìŠ¤í„° ë©”ëª¨ë¦¬ ì¦ê°€

```yaml
# Redis StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
  namespace: cache
spec:
  replicas: 6
  serviceName: redis-cluster
  template:
    spec:
      containers:
        - name: redis
          image: redis:7
          resources:
            requests:
              cpu: "1"
              memory: 4Gi
            limits:
              cpu: "1"
              memory: 4Gi
---
# VPAë¡œ ìë™ ë©”ëª¨ë¦¬ ì¦ê°€
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: redis-cluster-vpa
  namespace: cache
spec:
  targetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: redis-cluster
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: redis
        minAllowed:
          memory: 4Gi
        maxAllowed:
          memory: 16Gi
        controlledResources: ["memory"]
        controlledValues: RequestsAndLimits
```

**In-Place Resize ìˆ˜í–‰ ê²°ê³¼**:

```bash
# 1. VPAê°€ ë©”ëª¨ë¦¬ ì¦ê°€ ê°ì§€
$ kubectl describe vpa redis-cluster-vpa -n cache
Recommendation:
  Container Recommendations:
    Container Name:  redis
    Target:
      Memory:  8Gi  # 4Gi â†’ 8Gi ì¦ê°€ ê¶Œì¥

# 2. VPAê°€ ìë™ìœ¼ë¡œ In-Place Resize ìˆ˜í–‰
$ kubectl get pod redis-cluster-0 -n cache -o yaml
status:
  resize: InProgress
  containerStatuses:
    - allocatedResources:
        memory: 4Gi
      resources:
        requests:
          memory: 8Gi  # ìƒˆë¡œìš´ ìš”ì²­ê°’

# 3. Kubeletì´ cgroup ë³€ê²½ ì™„ë£Œ
$ kubectl get pod redis-cluster-0 -n cache -o yaml
status:
  resize: ""  # ì™„ë£Œë˜ë©´ ë¹„ì›Œì§
  containerStatuses:
    - allocatedResources:
        memory: 8Gi  # ìƒˆë¡œìš´ ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì™„ë£Œ

# 4. Pod ì¬ì‹œì‘ ì—†ì´ ë©”ëª¨ë¦¬ ì¦ê°€ í™•ì¸
$ kubectl exec redis-cluster-0 -n cache -- redis-cli INFO memory
used_memory:8589934592  # 8GB
maxmemory:8589934592

# 5. Pod ì—…íƒ€ì„ í™•ì¸ (ì¬ì‹œì‘ ì—†ìŒ)
$ kubectl get pod redis-cluster-0 -n cache
NAME              READY   STATUS    RESTARTS   AGE
redis-cluster-0   1/1     Running   0          15d  # 15ì¼ê°„ ì¬ì‹œì‘ ì—†ìŒ
```

:::warning In-Place Pod Vertical Scalingì€ ì•„ì§ Beta ë‹¨ê³„ì…ë‹ˆë‹¤
In-Place Pod Vertical Scalingì€ Kubernetes 1.33ì—ì„œ Betaë¡œ ì§„ì…í–ˆìŠµë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” **Kubernetes 1.35+ Stable ì´í›„ ë„ì…**ì„ ê¶Œì¥í•©ë‹ˆë‹¤. Beta ê¸°ê°„ ë™ì•ˆ API ë³€ê²½ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, EKSëŠ” Kubernetes GA ì´í›„ ì¼ì • ê¸°ê°„ í›„ì— ì§€ì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ê¶Œì¥ ì‚¬í•­**:
- **K8s 1.33-1.34**: ê°œë°œ/ìŠ¤í…Œì´ì§• í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸
- **K8s 1.35+**: í”„ë¡œë•ì…˜ í™˜ê²½ ë„ì… ê³ ë ¤
- **EKS ì‚¬ìš©ì**: AWS ê³µì‹ ë¬¸ì„œì—ì„œ Feature Gate ì§€ì› ì‹œì  í™•ì¸
:::

:::tip In-Place Resizeì˜ í•µì‹¬ ê°€ì¹˜
VPAì˜ ê°€ì¥ í° ë‹¨ì ì´ì—ˆë˜ **Pod ì¬ì‹œì‘ ë¬¸ì œ**ê°€ í•´ê²°ë˜ë©´ì„œ, StatefulSet, ë°ì´í„°ë² ì´ìŠ¤, ìºì‹œ, ML ì¶”ë¡  ì„œë¹„ìŠ¤ ë“± **ìƒíƒœ ìœ ì§€ê°€ ì¤‘ìš”í•œ ì›Œí¬ë¡œë“œ**ì—ì„œë„ ì•ˆì „í•˜ê²Œ ìˆ˜ì§ ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë©”ëª¨ë¦¬ ì¦ê°€ëŠ” ì¬ì‹œì‘ ì—†ì´ ì¦‰ì‹œ ë°˜ì˜ë˜ë¯€ë¡œ, íŠ¸ë˜í”½ ê¸‰ì¦ ì‹œ ë¹ ë¥¸ ëŒ€ì‘ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
:::

---

## 8. í”¼ë“œë°± ë£¨í”„

### 8.1 ì˜ˆì¸¡ ì •í™•ë„ ì¸¡ì •

```python
# ì˜ˆì¸¡ ì •í™•ë„ ì¸¡ì • ë° ëª¨ë¸ ì¬í•™ìŠµ
import numpy as np

def calculate_accuracy(predicted, actual):
    """MAPE (Mean Absolute Percentage Error) ê³„ì‚°"""
    mape = np.mean(np.abs((actual - predicted) / actual)) * 100
    return {
        'mape': mape,
        'accuracy': 100 - mape,
        'over_prediction_rate': np.mean(predicted > actual) * 100,
        'under_prediction_rate': np.mean(predicted < actual) * 100
    }

def should_retrain(accuracy_history, threshold=85):
    """ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
    recent_accuracy = np.mean(accuracy_history[-10:])
    if recent_accuracy < threshold:
        return True, f"ìµœê·¼ ì •í™•ë„ {recent_accuracy:.1f}% < ì„ê³„ê°’ {threshold}%"
    return False, f"ì •í™•ë„ ì–‘í˜¸: {recent_accuracy:.1f}%"
```

### 8.2 ìë™ ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸

```yaml
# ì˜ˆì¸¡ ëª¨ë¸ ìë™ ì¬í•™ìŠµ CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: model-retrainer
  namespace: scaling
spec:
  schedule: "0 2 * * 0"  # ë§¤ì£¼ ì¼ìš”ì¼ 02:00
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: retrainer
              image: my-registry/model-retrainer:latest
              env:
                - name: AMP_WORKSPACE_ID
                  value: "ws-xxxxx"
                - name: TRAINING_WEEKS
                  value: "4"
                - name: ACCURACY_THRESHOLD
                  value: "85"
              resources:
                requests:
                  cpu: "2"
                  memory: 4Gi
          restartPolicy: OnFailure
```

### 8.3 A/B ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸

```
[A/B ìŠ¤ì¼€ì¼ë§]

ê·¸ë£¹ A (50% íŠ¸ë˜í”½): HPA ê¸°ë°˜ ë°˜ì‘í˜• ìŠ¤ì¼€ì¼ë§
ê·¸ë£¹ B (50% íŠ¸ë˜í”½): ML ì˜ˆì¸¡ ê¸°ë°˜ ì„ ì œ ìŠ¤ì¼€ì¼ë§

ë¹„êµ ì§€í‘œ:
  - P99 ë ˆì´í„´ì‹œ ì°¨ì´
  - ìŠ¤ì¼€ì¼ ì´ë²¤íŠ¸ íšŸìˆ˜
  - ë¦¬ì†ŒìŠ¤ ì‚¬ìš© íš¨ìœ¨
  - ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥
```

---

## 9. Chaos Engineering + AI

### 9.1 AWS Fault Injection Service (FIS)

```json
{
  "description": "EKS Pod ì¥ì•  ì£¼ì… í…ŒìŠ¤íŠ¸",
  "targets": {
    "eks-pods": {
      "resourceType": "aws:eks:pod",
      "selectionMode": "COUNT(2)",
      "resourceTags": {
        "app": "payment-service"
      },
      "parameters": {
        "clusterIdentifier": "my-cluster",
        "namespace": "payment"
      }
    }
  },
  "actions": {
    "terminate-pods": {
      "actionId": "aws:eks:terminate-pod",
      "parameters": {},
      "targets": {
        "Pods": "eks-pods"
      }
    }
  },
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentServiceSLO"
    }
  ],
  "roleArn": "arn:aws:iam::ACCOUNT_ID:role/FISRole",
  "tags": {
    "Environment": "staging",
    "Team": "platform"
  }
}
```

### 9.2 AI ê¸°ë°˜ ì¥ì•  íŒ¨í„´ í•™ìŠµ

Chaos Engineering ì‹¤í—˜ ê²°ê³¼ë¥¼ AIê°€ í•™ìŠµí•˜ì—¬ ëŒ€ì‘ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

<ChaosExperiments />

```python
# FIS ì‹¤í—˜ í›„ AI í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
from strands import Agent

chaos_analyzer = Agent(
    name="chaos-pattern-analyzer",
    model="bedrock/anthropic.claude-sonnet",
    sop="""
    ## Chaos Engineering ê²°ê³¼ ë¶„ì„

    1. FIS ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘
       - ì£¼ì…ëœ ì¥ì•  ìœ í˜•
       - ì‹œìŠ¤í…œ ë°˜ì‘ ì‹œê°„
       - ë³µêµ¬ ì‹œê°„
       - ì˜í–¥ ë²”ìœ„

    2. íŒ¨í„´ ë¶„ì„
       - ì¥ì•  ì „íŒŒ ê²½ë¡œ ë§µí•‘
       - ì·¨ì•½ ì§€ì  ì‹ë³„
       - ë³µêµ¬ ë³‘ëª© ì§€ì  íŒŒì•…

    3. ëŒ€ì‘ ê·œì¹™ ì—…ë°ì´íŠ¸
       - ê¸°ì¡´ SOPì— í•™ìŠµ ë‚´ìš© ì¶”ê°€
       - ìƒˆë¡œìš´ íŒ¨í„´ì— ëŒ€í•œ ëŒ€ì‘ ê·œì¹™ ìƒì„±
       - ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì„ê³„ê°’ ì¡°ì •

    4. ë³´ê³ ì„œ ìƒì„±
       - ì‹¤í—˜ ìš”ì•½
       - ë°œê²¬ëœ ì·¨ì•½ì 
       - ê¶Œì¥ ê°œì„  ì‚¬í•­
    """
)
```

:::tip Chaos Engineering + AI í”¼ë“œë°± ë£¨í”„
FISë¡œ ì¥ì• ë¥¼ ì£¼ì…í•˜ê³ , AIê°€ ì‹œìŠ¤í…œ ë°˜ì‘ íŒ¨í„´ì„ í•™ìŠµí•˜ë©´, AI Agentì˜ ìë™ ëŒ€ì‘ ëŠ¥ë ¥ì´ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒë©ë‹ˆë‹¤. "ì¥ì•  ì£¼ì… â†’ ê´€ì°° â†’ í•™ìŠµ â†’ ëŒ€ì‘ ê°œì„ "ì˜ í”¼ë“œë°± ë£¨í”„ê°€ ììœ¨ ìš´ì˜ì˜ í•µì‹¬ì…ë‹ˆë‹¤.
:::

### 9.4 AWS FIS ìµœì‹  ê¸°ëŠ¥ ë° í”„ë¡œë•ì…˜ ì•ˆì „ ì¥ì¹˜

AWS Fault Injection Service(FIS)ëŠ” 2025-2026ë…„ ê¸°ì¤€ìœ¼ë¡œ **EKS ì „ìš© ì•¡ì…˜ íƒ€ì…**ê³¼ **ìë™ ì¤‘ë‹¨ ë©”ì»¤ë‹ˆì¦˜**ì„ ì œê³µí•˜ì—¬, í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê²Œ Chaos Engineeringì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### FIS ìµœì‹  EKS ì•¡ì…˜ íƒ€ì…

FISëŠ” EKS ì›Œí¬ë¡œë“œì— íŠ¹í™”ëœ ì¥ì•  ì£¼ì… ì•¡ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤:

| ì•¡ì…˜ íƒ€ì… | ì„¤ëª… | ì ìš© ëŒ€ìƒ | ì‚¬ìš© ì‚¬ë¡€ |
|----------|------|----------|----------|
| `aws:eks:pod-delete` | íŠ¹ì • Pod ì‚­ì œ | Pod | Pod ì¬ì‹œì‘ íšŒë³µë ¥ í…ŒìŠ¤íŠ¸ |
| `aws:eks:pod-network-latency` | Pod ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì£¼ì… | Pod | ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì‹œ ì• í”Œë¦¬ì¼€ì´ì…˜ ë™ì‘ ê²€ì¦ |
| `aws:eks:pod-network-packet-loss` | Pod ë„¤íŠ¸ì›Œí¬ íŒ¨í‚· ì†ì‹¤ ì£¼ì… | Pod | ë¶ˆì•ˆì •í•œ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜ |
| `aws:eks:node-drain` | ë…¸ë“œ ë“œë ˆì¸ (ì•ˆì „í•œ Pod ì´ë™) | Node | ë…¸ë“œ ìœ ì§€ë³´ìˆ˜ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ |
| `aws:eks:terminate-nodegroup-instances` | ë…¸ë“œ ê·¸ë£¹ ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ | Node Group | ëŒ€ê·œëª¨ ë…¸ë“œ ì¥ì•  ë³µêµ¬ í…ŒìŠ¤íŠ¸ |

**Pod ì‚­ì œ ì•¡ì…˜ ìƒì„¸**:

```json
{
  "actionId": "aws:eks:pod-delete",
  "description": "EKS Pod ì‚­ì œë¥¼ í†µí•œ ì¬ì‹œì‘ íšŒë³µë ¥ í…ŒìŠ¤íŠ¸",
  "targets": {
    "Pods": "eks-payment-pods"
  },
  "parameters": {
    "kubernetesServiceAccount": "fis-experiment-role",
    "maxPodsToDelete": "2",
    "podDeletionMode": "all-at-once"
  }
}
```

**ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì£¼ì… ì•¡ì…˜**:

```json
{
  "actionId": "aws:eks:pod-network-latency",
  "description": "Pod ë„¤íŠ¸ì›Œí¬ ì§€ì—° 200ms ì£¼ì…",
  "targets": {
    "Pods": "eks-payment-pods"
  },
  "parameters": {
    "kubernetesServiceAccount": "fis-experiment-role",
    "duration": "PT5M",
    "delayMilliseconds": "200",
    "jitterMilliseconds": "50",
    "sources": "all",
    "destinations": "all"
  }
}
```

**íŒ¨í‚· ì†ì‹¤ ì£¼ì… ì•¡ì…˜**:

```json
{
  "actionId": "aws:eks:pod-network-packet-loss",
  "description": "5% íŒ¨í‚· ì†ì‹¤ ì£¼ì…",
  "targets": {
    "Pods": "eks-payment-pods"
  },
  "parameters": {
    "kubernetesServiceAccount": "fis-experiment-role",
    "duration": "PT3M",
    "lossPercent": "5",
    "sources": "all",
    "destinations": "all"
  }
}
```

**ë…¸ë“œ ë“œë ˆì¸ ì•¡ì…˜**:

```json
{
  "actionId": "aws:eks:node-drain",
  "description": "ë…¸ë“œ ì•ˆì „ ë“œë ˆì¸ (PDB ì¤€ìˆ˜)",
  "targets": {
    "Nodes": "eks-worker-nodes"
  },
  "parameters": {
    "kubernetesServiceAccount": "fis-experiment-role",
    "gracePeriodSeconds": "300",
    "skipWaitForDeleteTimeout": "false"
  }
}
```

#### stopConditions ê¸°ë°˜ ìë™ ì¤‘ë‹¨

FISì˜ **stopConditions** ê¸°ëŠ¥ì€ SLO ìœ„ë°˜ ì‹œ ì‹¤í—˜ì„ ìë™ìœ¼ë¡œ ì¤‘ë‹¨í•˜ì—¬ í”„ë¡œë•ì…˜ ì•ˆì „ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤:

```json
{
  "description": "EKS Pod ì¥ì•  ì£¼ì… with SLO ë³´í˜¸",
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-ErrorRate-SLO"
    },
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-Latency-P99-SLO"
    },
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-Availability-SLO"
    }
  ]
}
```

**CloudWatch Alarm ì„¤ì • ì˜ˆì‹œ**:

```bash
# Error Rate SLO Alarm (ì—ëŸ¬ìœ¨ > 5%)
aws cloudwatch put-metric-alarm \
  --alarm-name "PaymentService-ErrorRate-SLO" \
  --alarm-description "Stop FIS if error rate exceeds 5%" \
  --namespace "AWS/ApplicationELB" \
  --metric-name "HTTPCode_Target_5XX_Count" \
  --dimensions Name=LoadBalancer,Value=app/payment-lb/xxx \
  --statistic Sum \
  --period 60 \
  --evaluation-periods 2 \
  --threshold 50 \
  --comparison-operator GreaterThanThreshold \
  --treat-missing-data notBreaching

# Latency P99 SLO Alarm (P99 > 500ms)
aws cloudwatch put-metric-alarm \
  --alarm-name "PaymentService-Latency-P99-SLO" \
  --alarm-description "Stop FIS if P99 latency exceeds 500ms" \
  --namespace "ContainerInsights" \
  --metric-name "pod_http_request_duration_p99" \
  --dimensions Name=Service,Value=payment-service \
  --statistic Average \
  --period 60 \
  --evaluation-periods 3 \
  --threshold 500 \
  --comparison-operator GreaterThanThreshold

# Availability SLO Alarm (ê°€ìš©ì„± < 99.9%)
aws cloudwatch put-metric-alarm \
  --alarm-name "PaymentService-Availability-SLO" \
  --alarm-description "Stop FIS if availability drops below 99.9%" \
  --metric-name "AvailabilityRate" \
  --namespace "CustomMetrics" \
  --dimensions Name=Service,Value=payment-service \
  --statistic Average \
  --period 300 \
  --evaluation-periods 1 \
  --threshold 99.9 \
  --comparison-operator LessThanThreshold
```

#### í”„ë¡œë•ì…˜ ì•ˆì „ ì¥ì¹˜ íŒ¨í„´

**íŒ¨í„´ 1: PDB í†µí•© â€” FIS ì‹¤í—˜ ì¤‘ PDB ì¤€ìˆ˜ ë³´ì¥**

```yaml
# Pod Disruption Budget ì„¤ì •
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: payment-service-pdb
  namespace: payment
spec:
  minAvailable: 2  # ìµœì†Œ 2ê°œ PodëŠ” í•­ìƒ Running ìœ ì§€
  selector:
    matchLabels:
      app: payment-service
---
# FIS Experiment Template (PDB ìë™ ì¤€ìˆ˜)
{
  "description": "Pod ì‚­ì œ ì‹¤í—˜ (PDB ì¤€ìˆ˜)",
  "targets": {
    "eks-payment-pods": {
      "resourceType": "aws:eks:pod",
      "selectionMode": "COUNT(1)",
      "resourceTags": {
        "app": "payment-service"
      },
      "parameters": {
        "clusterIdentifier": "my-cluster",
        "namespace": "payment"
      }
    }
  },
  "actions": {
    "delete-pod-safely": {
      "actionId": "aws:eks:pod-delete",
      "parameters": {
        "kubernetesServiceAccount": "fis-experiment-role",
        "maxPodsToDelete": "1",
        "podDeletionMode": "one-at-a-time"
      },
      "targets": {
        "Pods": "eks-payment-pods"
      }
    }
  }
}
```

**FIS + PDB ë™ì‘ íë¦„**:

```mermaid
sequenceDiagram
    participant FIS as AWS FIS
    participant K8s as Kubernetes API
    participant PDB as PodDisruptionBudget
    participant Pod as Payment Pods

    FIS->>K8s: Pod ì‚­ì œ ìš”ì²­ (payment-pod-1)
    K8s->>PDB: Disruption í—ˆìš© ì—¬ë¶€ í™•ì¸
    PDB->>K8s: minAvailable=2, í˜„ì¬ Running=3
    K8s->>PDB: 1ê°œ ì‚­ì œ ê°€ëŠ¥ (3-1=2 â‰¥ minAvailable)
    PDB->>K8s: Disruption í—ˆìš©
    K8s->>Pod: payment-pod-1 ì‚­ì œ
    Pod->>Pod: Graceful Shutdown (30s)
    Pod->>K8s: Pod ì¢…ë£Œ ì™„ë£Œ
    K8s->>K8s: ìƒˆ Pod ìŠ¤ì¼€ì¤„ë§ (payment-pod-4)
    K8s->>FIS: ì‹¤í—˜ ì™„ë£Œ
```

**PDB ìœ„ë°˜ ì‹œë‚˜ë¦¬ì˜¤**:

```bash
# í˜„ì¬ Running Pods: 2ê°œ (ìµœì†Œê°’)
$ kubectl get pods -n payment -l app=payment-service
NAME                READY   STATUS    RESTARTS   AGE
payment-pod-2       1/1     Running   0          5m
payment-pod-3       1/1     Running   0          5m

# FISê°€ Pod ì‚­ì œ ì‹œë„
$ aws fis start-experiment --experiment-template-id EXT123456

# Kubernetesê°€ PDBë¥¼ í™•ì¸í•˜ê³  ê±°ë¶€
# minAvailable=2, í˜„ì¬=2 â†’ 1ê°œ ì‚­ì œ ì‹œ 1ê°œë§Œ ë‚¨ìŒ â†’ PDB ìœ„ë°˜
# â†’ FIS ì‹¤í—˜ ì‹¤íŒ¨ (PDBê°€ Disruption ì°¨ë‹¨)

# FIS ì‹¤í—˜ ë¡œê·¸
{
  "state": "failed",
  "reason": "PodDisruptionBudget prevents pod deletion. Current: 2, Required: 2"
}
```

**íŒ¨í„´ 2: í­ë°œ ë°˜ê²½ ì œí•œ â€” íƒœê·¸/ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ì‹¤í—˜ ë²”ìœ„ ì œí•œ**

```json
{
  "description": "ì œí•œëœ ë²”ìœ„ì˜ Pod ì¥ì•  ì‹¤í—˜",
  "targets": {
    "eks-test-pods": {
      "resourceType": "aws:eks:pod",
      "selectionMode": "PERCENT(25)",
      "resourceTags": {
        "environment": "staging",
        "chaos-experiment": "enabled",
        "team": "payments"
      },
      "filters": [
        {
          "path": "Namespace",
          "values": ["payment-staging"]
        },
        {
          "path": "Labels.version",
          "values": ["canary"]
        }
      ],
      "parameters": {
        "clusterIdentifier": "staging-cluster",
        "namespace": "payment-staging"
      }
    }
  }
}
```

**í­ë°œ ë°˜ê²½ ì œí•œ ì „ëµ**:

| ì œí•œ ë°©ì‹ | ì„¤ì • ë°©ë²• | ì˜ˆì‹œ |
|----------|----------|------|
| **ë„¤ì„ìŠ¤í˜ì´ìŠ¤** | `filters.Namespace` | `payment-staging` (í”„ë¡œë•ì…˜ ì œì™¸) |
| **ë¼ë²¨ ì„ íƒ** | `filters.Labels` | `version=canary` (ì¹´ë‚˜ë¦¬ ë°°í¬ë§Œ) |
| **íƒœê·¸ ê¸°ë°˜** | `resourceTags` | `chaos-experiment=enabled` (ëª…ì‹œì  ì˜µíŠ¸ì¸) |
| **ë¹„ìœ¨ ì œí•œ** | `selectionMode: PERCENT(N)` | `PERCENT(25)` (ìµœëŒ€ 25%ë§Œ ì˜í–¥) |
| **ê°œìˆ˜ ì œí•œ** | `selectionMode: COUNT(N)` | `COUNT(2)` (ìµœëŒ€ 2ê°œë§Œ) |

**íŒ¨í„´ 3: ì ì§„ì  í™•ì¥ â€” 1ê°œ Pod â†’ 10% Pod â†’ 25% Pod ë‹¨ê³„ë³„ í™•ì¥**

```json
{
  "description": "ì ì§„ì  Pod ì‚­ì œ ì‹¤í—˜",
  "actions": {
    "phase-1-single-pod": {
      "actionId": "aws:eks:pod-delete",
      "description": "Phase 1: 1ê°œ Pod ì‚­ì œ",
      "parameters": {
        "kubernetesServiceAccount": "fis-experiment-role",
        "maxPodsToDelete": "1"
      },
      "targets": {
        "Pods": "eks-payment-pods-phase1"
      }
    },
    "wait-1": {
      "actionId": "aws:fis:wait",
      "parameters": {
        "duration": "PT2M"
      },
      "startAfter": ["phase-1-single-pod"]
    },
    "phase-2-10-percent": {
      "actionId": "aws:eks:pod-delete",
      "description": "Phase 2: 10% Pod ì‚­ì œ",
      "parameters": {
        "kubernetesServiceAccount": "fis-experiment-role",
        "selectionMode": "PERCENT(10)"
      },
      "targets": {
        "Pods": "eks-payment-pods-phase2"
      },
      "startAfter": ["wait-1"]
    },
    "wait-2": {
      "actionId": "aws:fis:wait",
      "parameters": {
        "duration": "PT3M"
      },
      "startAfter": ["phase-2-10-percent"]
    },
    "phase-3-25-percent": {
      "actionId": "aws:eks:pod-delete",
      "description": "Phase 3: 25% Pod ì‚­ì œ",
      "parameters": {
        "kubernetesServiceAccount": "fis-experiment-role",
        "selectionMode": "PERCENT(25)"
      },
      "targets": {
        "Pods": "eks-payment-pods-phase3"
      },
      "startAfter": ["wait-2"]
    }
  },
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-ErrorRate-SLO"
    }
  ]
}
```

**ì ì§„ì  í™•ì¥ íë¦„**:

```
Phase 1: 1ê°œ Pod ì‚­ì œ
  â†“ (2ë¶„ ëŒ€ê¸°, SLO ëª¨ë‹ˆí„°ë§)
Phase 2: 10% Pod ì‚­ì œ
  â†“ (3ë¶„ ëŒ€ê¸°, SLO ëª¨ë‹ˆí„°ë§)
Phase 3: 25% Pod ì‚­ì œ
  â†“
[ì„±ê³µ] ëª¨ë“  ë‹¨ê³„ í†µê³¼ â†’ ì‹œìŠ¤í…œ íšŒë³µë ¥ ê²€ì¦ ì™„ë£Œ
[ì‹¤íŒ¨] SLO ìœ„ë°˜ â†’ ìë™ ì¤‘ë‹¨, ë¡¤ë°±
```

**íŒ¨í„´ 4: ë¡¤ë°± ì¡°ê±´ â€” latency P99 > 500ms ë˜ëŠ” error rate > 5% ì‹œ ìë™ ì¤‘ë‹¨**

```json
{
  "description": "ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì‹¤í—˜ with ìë™ ë¡¤ë°±",
  "actions": {
    "inject-latency": {
      "actionId": "aws:eks:pod-network-latency",
      "description": "200ms ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì£¼ì…",
      "parameters": {
        "kubernetesServiceAccount": "fis-experiment-role",
        "duration": "PT10M",
        "delayMilliseconds": "200",
        "jitterMilliseconds": "50"
      },
      "targets": {
        "Pods": "eks-payment-pods"
      }
    }
  },
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-Latency-P99-SLO"
    },
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-ErrorRate-SLO"
    }
  ],
  "roleArn": "arn:aws:iam::ACCOUNT_ID:role/FISExperimentRole",
  "tags": {
    "Environment": "production",
    "Team": "platform",
    "ChaosExperimentType": "network-latency"
  }
}
```

**ìë™ ë¡¤ë°± ì‹œë‚˜ë¦¬ì˜¤**:

```
[00:00] FIS ì‹¤í—˜ ì‹œì‘ â€” 200ms ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì£¼ì…
[00:00] CloudWatch Alarms ëª¨ë‹ˆí„°ë§ ì‹œì‘
  - Latency P99 SLO: ì •ìƒ (250ms < 500ms)
  - Error Rate SLO: ì •ìƒ (2% < 5%)
[00:03] Latency P99 ì¦ê°€ ê°ì§€: 450ms
[00:05] Latency P99 SLO ìœ„ë°˜: 520ms > 500ms
[00:05] CloudWatch Alarm íŠ¸ë¦¬ê±°: "PaymentService-Latency-P99-SLO"
[00:05] FIS ìë™ ì¤‘ë‹¨ (stopCondition ë§Œì¡±)
[00:05] ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì œê±° (ìë™ ë¡¤ë°±)
[00:06] Latency P99 ë³µêµ¬: 280ms
[00:08] ì‹œìŠ¤í…œ ì •ìƒ ìƒíƒœ ë³µêµ¬
```

#### FIS Experiment Template YAML ì˜ˆì‹œ

```yaml
# FIS Experiment Template: EKS Pod ì¥ì•  ì£¼ì… + stopConditions
AWSTemplateFormatVersion: '2010-09-09'
Description: 'FIS Experiment Template for EKS Pod Fault Injection'

Resources:
  PaymentServiceFISExperiment:
    Type: AWS::FIS::ExperimentTemplate
    Properties:
      Description: 'EKS Pod ì‚­ì œ ì‹¤í—˜ with SLO ë³´í˜¸'
      StopConditions:
        - Source: 'aws:cloudwatch:alarm'
          Value: !GetAtt PaymentServiceErrorRateAlarm.Arn
        - Source: 'aws:cloudwatch:alarm'
          Value: !GetAtt PaymentServiceLatencyAlarm.Arn
      Targets:
        PaymentPods:
          ResourceType: 'aws:eks:pod'
          SelectionMode: 'COUNT(2)'
          ResourceTags:
            app: 'payment-service'
          Parameters:
            clusterIdentifier: !Ref EKSClusterName
            namespace: 'payment'
      Actions:
        DeletePods:
          ActionId: 'aws:eks:pod-delete'
          Parameters:
            kubernetesServiceAccount: !GetAtt FISServiceAccount.Name
            maxPodsToDelete: '2'
            podDeletionMode: 'one-at-a-time'
          Targets:
            Pods: 'PaymentPods'
      RoleArn: !GetAtt FISExperimentRole.Arn
      Tags:
        Environment: 'production'
        Team: 'platform'

  PaymentServiceErrorRateAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: 'PaymentService-ErrorRate-SLO'
      AlarmDescription: 'Stop FIS if error rate exceeds 5%'
      MetricName: 'HTTPCode_Target_5XX_Count'
      Namespace: 'AWS/ApplicationELB'
      Statistic: Sum
      Period: 60
      EvaluationPeriods: 2
      Threshold: 50
      ComparisonOperator: GreaterThanThreshold
      TreatMissingData: notBreaching

  PaymentServiceLatencyAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: 'PaymentService-Latency-P99-SLO'
      AlarmDescription: 'Stop FIS if P99 latency exceeds 500ms'
      MetricName: 'pod_http_request_duration_p99'
      Namespace: 'ContainerInsights'
      Statistic: Average
      Period: 60
      EvaluationPeriods: 3
      Threshold: 500
      ComparisonOperator: GreaterThanThreshold

  FISExperimentRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: fis.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AWSFaultInjectionSimulatorEKSAccess'
      Policies:
        - PolicyName: FISCloudWatchAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'cloudwatch:DescribeAlarms'
                  - 'cloudwatch:GetMetricData'
                Resource: '*'

  FISServiceAccount:
    Type: AWS::EKS::ServiceAccount
    Properties:
      ClusterName: !Ref EKSClusterName
      Name: 'fis-experiment-role'
      Namespace: 'kube-system'
      RoleArn: !GetAtt FISExperimentRole.Arn

Parameters:
  EKSClusterName:
    Type: String
    Description: 'Name of the EKS cluster'
    Default: 'my-cluster'

Outputs:
  ExperimentTemplateId:
    Description: 'FIS Experiment Template ID'
    Value: !GetAtt PaymentServiceFISExperiment.Id
    Export:
      Name: !Sub '${AWS::StackName}-ExperimentTemplateId'
```

:::tip FIS í”„ë¡œë•ì…˜ ì•ˆì „ ì¥ì¹˜ì˜ í•µì‹¬
AWS FISì˜ **stopConditions**ì™€ **PDB í†µí•©**ì€ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ Chaos Engineeringì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ê¸°ëŠ¥ì…ë‹ˆë‹¤. SLO ìœ„ë°˜ ì‹œ ìë™ ì¤‘ë‹¨, ì ì§„ì  í™•ì¥, í­ë°œ ë°˜ê²½ ì œí•œì„ ì¡°í•©í•˜ë©´, **ì‚¬ìš©ì ì˜í–¥ ì—†ì´** ì‹œìŠ¤í…œ íšŒë³µë ¥ì„ ê²€ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ê¶Œì¥ ì‚¬í•­**:
1. **í•­ìƒ stopConditions ì„¤ì •**: CloudWatch Alarmê³¼ ì—°ë™í•˜ì—¬ SLO ìœ„ë°˜ ì‹œ ìë™ ì¤‘ë‹¨
2. **PDB í•„ìˆ˜ ì„¤ì •**: ëª¨ë“  í”„ë¡œë•ì…˜ ì›Œí¬ë¡œë“œì— PDB ì ìš©
3. **ì ì§„ì  í™•ì¥**: 1ê°œ â†’ 10% â†’ 25% ë‹¨ê³„ë³„ í™•ì¥ìœ¼ë¡œ ì•ˆì „ì„± í™•ë³´
4. **ë¹„í”„ë¡œë•ì…˜ í™˜ê²½ ìš°ì„ **: ìŠ¤í…Œì´ì§• í™˜ê²½ì—ì„œ ì¶©ë¶„íˆ í…ŒìŠ¤íŠ¸ í›„ í”„ë¡œë•ì…˜ ì ìš©
:::

### 9.5 AI ê¸°ë°˜ ê³ ê¸‰ Chaos Engineering

AIë¥¼ í™œìš©í•˜ë©´ Chaos Engineeringì´ **ìˆ˜ë™ ì‹¤í—˜ ì„¤ê³„ â†’ ì§€ëŠ¥í˜• ìë™ ì„¤ê³„**ë¡œ ì§„í™”í•©ë‹ˆë‹¤. ê³¼ê±° ì¥ì•  íŒ¨í„´ í•™ìŠµ, Steady State Hypothesis ìë™ ì •ì˜, GameDay ìë™í™”ë¥¼ í†µí•´ ì‹œìŠ¤í…œ íšŒë³µë ¥ì„ ì²´ê³„ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 9.5.1 ê³¼ê±° ì¥ì•  íŒ¨í„´ í•™ìŠµ â†’ ìƒˆë¡œìš´ ì¹´ì˜¤ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ìë™ ì œì•ˆ

AIê°€ ê³¼ê±° ì¸ì‹œë˜íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬, ì‹¤ì œ ë°œìƒ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì¹´ì˜¤ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìë™ìœ¼ë¡œ ì œì•ˆí•©ë‹ˆë‹¤.

```python
# AI ê¸°ë°˜ ì¹´ì˜¤ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±ê¸°
from strands import Agent
import boto3

fis_client = boto3.client('fis', region_name='ap-northeast-2')
cloudwatch_client = boto3.client('cloudwatch', region_name='ap-northeast-2')

chaos_designer = Agent(
    name="chaos-scenario-designer",
    model="bedrock/anthropic.claude-sonnet",
    sop="""
    ## AI ê¸°ë°˜ ì¹´ì˜¤ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ìë™ ì„¤ê³„

    ### Phase 1: ê³¼ê±° ì¸ì‹œë˜íŠ¸ ë¶„ì„ (í•™ìŠµ)
    1. CloudWatch Logs Insightsë¡œ ê³¼ê±° 6ê°œì›” ì¸ì‹œë˜íŠ¸ ìˆ˜ì§‘
       - ì¥ì•  ìœ í˜•ë³„ ë¹ˆë„ ë¶„ì„
       - ì˜í–¥ ë²”ìœ„ ë° ë³µêµ¬ ì‹œê°„ ë¶„ì„
       - ê·¼ë³¸ ì›ì¸ë³„ ë¶„ë¥˜ (ë„¤íŠ¸ì›Œí¬/ë¦¬ì†ŒìŠ¤/ë°°í¬)

    2. ì¸ì‹œë˜íŠ¸ íŒ¨í„´ ì¶”ì¶œ
       - ë°˜ë³µ ë°œìƒ íŒ¨í„´ ì‹ë³„
       - ê³„ì ˆì /ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„
       - ì˜ì¡´ì„± ê¸°ë°˜ ì—°ì‡„ ì¥ì•  íŒ¨í„´

    ### Phase 2: ì¹´ì˜¤ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ìë™ ìƒì„±
    1. ì¥ì•  íŒ¨í„´ë³„ FIS ì‹¤í—˜ í…œí”Œë¦¿ ìë™ ìƒì„±
       - Pod OOMKilled íŒ¨í„´ â†’ ë©”ëª¨ë¦¬ ì••ë°• ì‹¤í—˜
       - ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ íŒ¨í„´ â†’ ë ˆì´í„´ì‹œ ì£¼ì… ì‹¤í—˜
       - ë…¸ë“œ ì¥ì•  íŒ¨í„´ â†’ ë…¸ë“œ ì¢…ë£Œ ì‹¤í—˜

    2. Steady State Hypothesis ìë™ ì •ì˜
       - ê³¼ê±° SLO ë°ì´í„° ê¸°ë°˜ ì •ìƒ ìƒíƒœ ì •ì˜
       - CloudWatch Alarm ê¸°ë°˜ ì¤‘ë‹¨ ì¡°ê±´ ìë™ ìƒì„±

    3. ì‹¤í—˜ ìš°ì„ ìˆœìœ„ ì œì•ˆ
       - ë¹ˆë„ Ã— ì˜í–¥ë„ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ê³„ì‚°
       - ë¯¸ê²€ì¦ ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ ìš°ì„  ì œì•ˆ

    ### Phase 3: ì‹¤í—˜ ìë™ ì‹¤í–‰ ë° ë¶„ì„
    1. FIS ì‹¤í—˜ ìë™ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ë§)
    2. ì‹œìŠ¤í…œ ë°˜ì‘ ê´€ì°° ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    3. ì˜ˆìƒ ëŒ€ë¹„ ì‹¤ì œ ê²°ê³¼ ë¹„êµ ë¶„ì„
    4. ë¯¸í¡í•œ íšŒë³µë ¥ ì˜ì—­ ì‹ë³„ ë° ê°œì„  ê¶Œì¥
    """
)
```

**ì‹¤ì „ ì˜ˆì‹œ: ê³¼ê±° ì¸ì‹œë˜íŠ¸ ê¸°ë°˜ ì¹´ì˜¤ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ìë™ ìƒì„±**

```python
# Step 1: ê³¼ê±° ì¸ì‹œë˜íŠ¸ ë°ì´í„° ìˆ˜ì§‘
import json
from datetime import datetime, timedelta

def analyze_past_incidents():
    """CloudWatch Logs Insightsë¡œ ê³¼ê±° ì¸ì‹œë˜íŠ¸ ë¶„ì„"""
    logs_client = boto3.client('logs', region_name='ap-northeast-2')

    query = """
    fields @timestamp, detail.alarmName, detail.state.value, detail.state.reason
    | filter detail-type = "CloudWatch Alarm State Change"
    | filter detail.state.value = "ALARM"
    | stats count(*) as incident_count by detail.state.reason as failure_pattern
    | sort incident_count desc
    """

    start_time = int((datetime.now() - timedelta(days=180)).timestamp())
    end_time = int(datetime.now().timestamp())

    response = logs_client.start_query(
        logGroupName='/aws/events/cloudwatch-alarms',
        startTime=start_time,
        endTime=end_time,
        queryString=query
    )

    query_id = response['queryId']

    # ì¿¼ë¦¬ ê²°ê³¼ ëŒ€ê¸° ë° ë°˜í™˜
    import time
    while True:
        result = logs_client.get_query_results(queryId=query_id)
        if result['status'] == 'Complete':
            return result['results']
        time.sleep(2)

# Step 2: AIê°€ ì¸ì‹œë˜íŠ¸ íŒ¨í„´ ê¸°ë°˜ ì¹´ì˜¤ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ì œì•ˆ
incident_patterns = analyze_past_incidents()

scenario_prompt = f"""
ê³¼ê±° 6ê°œì›”ê°„ ë°œìƒí•œ ì¸ì‹œë˜íŠ¸ íŒ¨í„´:
{json.dumps(incident_patterns, indent=2)}

ì´ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:
1. ê°€ì¥ ë¹ˆë²ˆí•œ ì¥ì•  íŒ¨í„´ Top 5 ì‹ë³„
2. ê° íŒ¨í„´ì— ëŒ€í•œ AWS FIS ì‹¤í—˜ í…œí”Œë¦¿ ìƒì„±
3. Steady State Hypothesis ì •ì˜ (SLO ê¸°ë°˜)
4. ì‹¤í—˜ ìš°ì„ ìˆœìœ„ ì œì•ˆ (ë¹ˆë„ Ã— ì˜í–¥ë„)
"""

response = chaos_designer.run(scenario_prompt)

# Step 3: AIê°€ ì œì•ˆí•œ FIS ì‹¤í—˜ í…œí”Œë¦¿ ìë™ ìƒì„±
# ì˜ˆì‹œ ì¶œë ¥:
"""
[AI ë¶„ì„ ê²°ê³¼]

Top 5 ì¥ì•  íŒ¨í„´:
1. Pod OOMKilled (37íšŒ) â€” ë©”ëª¨ë¦¬ ë¶€ì¡±
2. Network Timeout (24íšŒ) â€” ì™¸ë¶€ API ì§€ì—°
3. Node NotReady (18íšŒ) â€” ë…¸ë“œ ì¥ì• 
4. Deployment Failed (12íšŒ) â€” ì´ë¯¸ì§€ Pull ì‹¤íŒ¨
5. RDS Connection Timeout (9íšŒ) â€” ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨

ê¶Œì¥ ì¹´ì˜¤ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤:

[ì‹œë‚˜ë¦¬ì˜¤ 1: ë©”ëª¨ë¦¬ ì••ë°• ì‹¤í—˜]
ëª©ì : Pod OOMKilled ëŒ€ì‘ ëŠ¥ë ¥ ê²€ì¦
FIS ì•¡ì…˜: aws:eks:inject-pod-memory-stress
ëŒ€ìƒ: payment-service (ê³¼ê±° OOMKilled 37íšŒ ë°œìƒ)
Steady State: memory_utilization < 85%, pod_restart_count < 5
ìš°ì„ ìˆœìœ„: ë†’ìŒ (ë¹ˆë„ 37 Ã— ì˜í–¥ë„ 9 = 333)

[ì‹œë‚˜ë¦¬ì˜¤ 2: ë„¤íŠ¸ì›Œí¬ ë ˆì´í„´ì‹œ ì‹¤í—˜]
ëª©ì : ì™¸ë¶€ API ì§€ì—° ì‹œ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ ê²€ì¦
FIS ì•¡ì…˜: aws:eks:pod-network-latency
ëŒ€ìƒ: order-service (ì™¸ë¶€ payment API í˜¸ì¶œ)
Steady State: p99_latency < 500ms, error_rate < 1%
ìš°ì„ ìˆœìœ„: ì¤‘ê°„ (ë¹ˆë„ 24 Ã— ì˜í–¥ë„ 7 = 168)

[ì‹œë‚˜ë¦¬ì˜¤ 3: ë…¸ë“œ ì¢…ë£Œ ì‹¤í—˜]
ëª©ì : ë…¸ë“œ ì¥ì•  ì‹œ Pod ì¬ìŠ¤ì¼€ì¤„ë§ ê²€ì¦
FIS ì•¡ì…˜: aws:eks:terminate-nodegroup-instances
ëŒ€ìƒ: worker-node-group (25% ì¢…ë£Œ)
Steady State: available_pods >= minAvailable (PDB), scheduling_time < 60s
ìš°ì„ ìˆœìœ„: ë†’ìŒ (ë¹ˆë„ 18 Ã— ì˜í–¥ë„ 10 = 180)
"""
```

#### 9.5.2 Steady State Hypothesisì˜ AI ìë™ ì •ì˜

Chaos Engineeringì˜ í•µì‹¬ì¸ **Steady State Hypothesis**(ì •ìƒ ìƒíƒœ ê°€ì„¤)ë¥¼ AIê°€ ê³¼ê±° ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ì •ì˜í•©ë‹ˆë‹¤.

```python
# Steady State Hypothesis ìë™ ìƒì„±
steady_state_agent = Agent(
    name="steady-state-generator",
    model="bedrock/anthropic.claude-sonnet",
    sop="""
    ## Steady State Hypothesis ìë™ ì •ì˜

    ### ì…ë ¥ ë°ì´í„°
    1. ê³¼ê±° 30ì¼ CloudWatch ë©”íŠ¸ë¦­ (ì •ìƒ ìƒíƒœ ê¸°ê°„)
       - RPS (Requests Per Second)
       - Error Rate
       - P50/P95/P99 Latency
       - CPU/Memory Utilization
       - Pod Restart Count

    2. í˜„ì¬ SLO ì„¤ì •
       - Availability SLO: 99.9%
       - Latency SLO: P99 < 500ms
       - Error Budget: 0.1%

    ### ì •ìƒ ìƒíƒœ ì •ì˜ ë¡œì§
    1. ë©”íŠ¸ë¦­ë³„ ì •ìƒ ë²”ìœ„ ê³„ì‚°
       - Baseline: ê³¼ê±° 30ì¼ í‰ê· 
       - Acceptable Range: í‰ê·  Â± 2Ïƒ (í‘œì¤€í¸ì°¨)
       - Alert Threshold: í‰ê·  + 3Ïƒ

    2. SLO ê¸°ë°˜ ìƒí•œì„  ì„¤ì •
       - Error Rate: max(SLO threshold, í‰ê·  + 2Ïƒ)
       - Latency: min(SLO threshold, í‰ê·  + 2Ïƒ)

    3. CloudWatch Alarmìœ¼ë¡œ ë³€í™˜
       - Steady State ìœ„ë°˜ ì‹œ FIS ì‹¤í—˜ ìë™ ì¤‘ë‹¨

    ### ì¶œë ¥
    - Steady State Hypothesis YAML
    - CloudWatch Alarm ì •ì˜ (FIS stopConditions)
    """
)
```

**ì‹¤ì „ ì˜ˆì‹œ: Steady State ìë™ ìƒì„±**

```python
def generate_steady_state_hypothesis(service_name: str, lookback_days: int = 30):
    """AI ê¸°ë°˜ Steady State Hypothesis ìë™ ìƒì„±"""

    # Step 1: ê³¼ê±° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    end_time = datetime.now()
    start_time = end_time - timedelta(days=lookback_days)

    metrics = {
        'error_rate': cloudwatch_client.get_metric_statistics(
            Namespace='AWS/ApplicationELB',
            MetricName='HTTPCode_Target_5XX_Count',
            Dimensions=[{'Name': 'LoadBalancer', 'Value': f'app/{service_name}-lb'}],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Average', 'Maximum']
        ),
        'latency_p99': cloudwatch_client.get_metric_statistics(
            Namespace='ContainerInsights',
            MetricName='pod_http_request_duration_p99',
            Dimensions=[{'Name': 'Service', 'Value': service_name}],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Average']
        )
    }

    # Step 2: AIê°€ ì •ìƒ ìƒíƒœ ì •ì˜
    prompt = f"""
    ì„œë¹„ìŠ¤: {service_name}
    ê³¼ê±° {lookback_days}ì¼ ë©”íŠ¸ë¦­ ë°ì´í„°:
    {json.dumps(metrics, indent=2, default=str)}

    ë‹¤ìŒì„ ìƒì„±í•˜ì„¸ìš”:
    1. Steady State Hypothesis (ì •ìƒ ìƒíƒœ ê¸°ì¤€)
    2. FIS stopConditionsìš© CloudWatch Alarm ì •ì˜
    3. ì‹¤í—˜ ì¤‘ ëª¨ë‹ˆí„°ë§í•  í•µì‹¬ ë©”íŠ¸ë¦­ ëª©ë¡
    """

    response = steady_state_agent.run(prompt)

    # ì˜ˆì‹œ ì¶œë ¥:
    """
    [Steady State Hypothesis: payment-service]

    ## ì •ìƒ ìƒíƒœ ê¸°ì¤€ (Baseline: ê³¼ê±° 30ì¼ í‰ê· )

    1. Error Rate
       - Baseline: 0.3%
       - Acceptable Range: 0% - 0.8% (í‰ê·  Â± 2Ïƒ)
       - Alert Threshold: 1.2% (í‰ê·  + 3Ïƒ)
       â†’ FIS stopCondition: error_rate > 1.2%

    2. Latency P99
       - Baseline: 320ms
       - Acceptable Range: 200ms - 440ms
       - Alert Threshold: 560ms
       â†’ FIS stopCondition: p99_latency > 560ms

    3. Availability
       - Baseline: 99.97%
       - Acceptable Range: 99.9% - 100%
       - Alert Threshold: 99.8%
       â†’ FIS stopCondition: availability < 99.8%

    4. Pod Restart Count (5ë¶„ ìœˆë„ìš°)
       - Baseline: 0.1íšŒ
       - Acceptable Range: 0 - 1íšŒ
       - Alert Threshold: 3íšŒ
       â†’ FIS stopCondition: restart_count > 3

    ## CloudWatch Alarm ì •ì˜ (FIS stopConditions)

    ```yaml
    stopConditions:
      - source: aws:cloudwatch:alarm
        value: arn:aws:cloudwatch:region:account:alarm:payment-ErrorRate-SSH
      - source: aws:cloudwatch:alarm
        value: arn:aws:cloudwatch:region:account:alarm:payment-LatencyP99-SSH
      - source: aws:cloudwatch:alarm
        value: arn:aws:cloudwatch:region:account:alarm:payment-Availability-SSH
      - source: aws:cloudwatch:alarm
        value: arn:aws:cloudwatch:region:account:alarm:payment-RestartCount-SSH
    ```

    ## í•µì‹¬ ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­
    1. RPS (ì •ìƒ ë²”ìœ„: 800-1200 req/s)
    2. Active Connections (ì •ìƒ ë²”ìœ„: 50-150)
    3. Database Connection Pool (ì •ìƒ ë²”ìœ„: 10-30)
    """

    return response
```

#### 9.5.3 GameDay ìë™í™” â€” AIê°€ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± + ì‹¤í–‰ + ë¶„ì„

**GameDay**(ì¬ë‚œ ë³µêµ¬ í›ˆë ¨)ë¥¼ AIê°€ ì™„ì „ ìë™í™”í•©ë‹ˆë‹¤. ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±ë¶€í„° ì‹¤í–‰, ê²°ê³¼ ë¶„ì„ê¹Œì§€ ììœ¨ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
# GameDay ìë™í™” ì—ì´ì „íŠ¸
gameday_orchestrator = Agent(
    name="gameday-orchestrator",
    model="bedrock/anthropic.claude-opus",  # ë³µì¡í•œ ì˜ì‚¬ê²°ì • â†’ Opus ì‚¬ìš©
    sop="""
    ## GameDay ìë™í™” ì›Œí¬í”Œë¡œìš°

    ### Phase 1: ì‚¬ì „ ê³„íš (D-7)
    1. ê³¼ê±° ì¸ì‹œë˜íŠ¸ ë¶„ì„ â†’ í˜„ì‹¤ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
    2. ì°¸ê°€ íŒ€ ë° ì—­í•  ì •ì˜ (ìë™ ì•Œë¦¼)
    3. Steady State Hypothesis ì •ì˜
    4. Rollback Plan ì¤€ë¹„

    ### Phase 2: ì‹¤í–‰ ì¤€ë¹„ (D-1)
    1. ìŠ¤í…Œì´ì§• í™˜ê²½ ìƒíƒœ í™•ì¸
    2. Monitoring Dashboard ì¤€ë¹„ (AMG)
    3. ì°¸ê°€ìì—ê²Œ GameDay ë¸Œë¦¬í•‘ ì „ì†¡ (Slack)
    4. stopConditions ê²€ì¦

    ### Phase 3: GameDay ì‹¤í–‰ (D-Day)
    1. ì‹œë‚˜ë¦¬ì˜¤ 1: Pod ì¥ì•  ì£¼ì… (FIS ì‹¤í–‰)
       - ê´€ì°° ì‹œê°„: 10ë¶„
       - ìë™ ë³µêµ¬ ê²€ì¦
       - ë©”íŠ¸ë¦­ ìˆ˜ì§‘

    2. ì‹œë‚˜ë¦¬ì˜¤ 2: ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì£¼ì…
       - ê´€ì°° ì‹œê°„: 15ë¶„
       - íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ ê²€ì¦
       - ì‚¬ìš©ì ì˜í–¥ ë¶„ì„

    3. ì‹œë‚˜ë¦¬ì˜¤ 3: ë°ì´í„°ë² ì´ìŠ¤ ì¥ì• 
       - ê´€ì°° ì‹œê°„: 20ë¶„
       - Failover ê²€ì¦
       - ë³µêµ¬ ì‹œê°„ ì¸¡ì •

    ### Phase 4: ì‚¬í›„ ë¶„ì„ (D+1)
    1. íƒ€ì„ë¼ì¸ ì¬êµ¬ì„±
    2. ë³µêµ¬ ì‹œê°„ ë¶„ì„ (MTTR)
    3. ì·¨ì•½ì  ì‹ë³„ ë° ê°œì„  ê¶Œì¥
    4. Post-Mortem ë³´ê³ ì„œ ìë™ ìƒì„±
    5. JIRA í‹°ì¼“ ìƒì„± (ê°œì„  ê³¼ì œ)
    """
)
```

**ì‹¤ì „ ì˜ˆì‹œ: ìë™í™”ëœ GameDay ì‹¤í–‰**

```python
# GameDay ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
gameday_scenario = {
    "name": "EKS ë³µí•© ì¥ì•  ëŒ€ì‘ í›ˆë ¨",
    "date": "2026-02-20",
    "environment": "staging",
    "scenarios": [
        {
            "id": "scenario-1",
            "name": "Pod ëŒ€ëŸ‰ ì¢…ë£Œ (25% ë™ì‹œ ì¥ì• )",
            "fis_template_id": "EXT-pod-termination-25pct",
            "duration": "10m",
            "expected_behavior": "HPA ìë™ ìŠ¤ì¼€ì¼ì•„ì›ƒ, 60ì´ˆ ì´ë‚´ ë³µêµ¬",
            "success_criteria": "error_rate < 2%, p99_latency < 800ms"
        },
        {
            "id": "scenario-2",
            "name": "ë„¤íŠ¸ì›Œí¬ ë ˆì´í„´ì‹œ 300ms ì£¼ì…",
            "fis_template_id": "EXT-network-latency-300ms",
            "duration": "15m",
            "expected_behavior": "Circuit Breaker ë™ì‘, Fallback ì‘ë‹µ",
            "success_criteria": "timeout_rate < 5%, fallback_success > 95%"
        },
        {
            "id": "scenario-3",
            "name": "RDS Failover ì‹œë®¬ë ˆì´ì…˜",
            "fis_template_id": "EXT-rds-failover",
            "duration": "20m",
            "expected_behavior": "Connection Pool ì¬ì—°ê²°, ë°ì´í„° ì†ì‹¤ ì—†ìŒ",
            "success_criteria": "connection_retry_success > 99%, data_consistency = 100%"
        }
    ]
}

# GameDay ìë™ ì‹¤í–‰
def run_automated_gameday(scenario):
    """AI ê¸°ë°˜ GameDay ìë™ ì‹¤í–‰"""

    # Phase 1: ì‚¬ì „ ì¤€ë¹„
    print("[Phase 1] GameDay ì‚¬ì „ ì¤€ë¹„ ì‹œì‘...")
    gameday_orchestrator.run(f"""
    GameDay ì‹œë‚˜ë¦¬ì˜¤:
    {json.dumps(scenario, indent=2)}

    ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:
    1. ì°¸ê°€ íŒ€ì—ê²Œ Slack ì•Œë¦¼ ì „ì†¡ (ë‚ ì§œ, ì‹œë‚˜ë¦¬ì˜¤ ê°œìš”)
    2. AMG ëŒ€ì‹œë³´ë“œ ìƒì„± (ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§)
    3. stopConditions ê²€ì¦
    """)

    # Phase 2: ì‹œë‚˜ë¦¬ì˜¤ë³„ ì‹¤í–‰
    print("[Phase 2] GameDay ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ ì‹œì‘...")
    results = []

    for scenario_item in scenario['scenarios']:
        print(f"  â†’ ì‹¤í–‰ ì¤‘: {scenario_item['name']}")

        # FIS ì‹¤í—˜ ì‹œì‘
        experiment = fis_client.start_experiment(
            experimentTemplateId=scenario_item['fis_template_id']
        )

        experiment_id = experiment['experiment']['id']

        # ì‹¤í—˜ ì™„ë£Œ ëŒ€ê¸°
        import time
        while True:
            status = fis_client.get_experiment(id=experiment_id)
            state = status['experiment']['state']['status']

            if state in ['completed', 'stopped', 'failed']:
                break

            time.sleep(10)

        # ê²°ê³¼ ìˆ˜ì§‘
        result = {
            'scenario_id': scenario_item['id'],
            'experiment_id': experiment_id,
            'state': state,
            'metrics': collect_metrics_during_experiment(experiment_id)
        }
        results.append(result)

        # AI ë¶„ì„
        analysis_prompt = f"""
        ì‹œë‚˜ë¦¬ì˜¤: {scenario_item['name']}
        ì˜ˆìƒ ë™ì‘: {scenario_item['expected_behavior']}
        ì„±ê³µ ê¸°ì¤€: {scenario_item['success_criteria']}
        ì‹¤ì œ ê²°ê³¼:
        {json.dumps(result, indent=2)}

        ë‹¤ìŒì„ ë¶„ì„í•˜ì„¸ìš”:
        1. ì„±ê³µ ê¸°ì¤€ ì¶©ì¡± ì—¬ë¶€
        2. ì˜ˆìƒ ëŒ€ë¹„ ì‹¤ì œ ë™ì‘ ë¹„êµ
        3. ë°œê²¬ëœ ì·¨ì•½ì 
        4. ê°œì„  ê¶Œì¥ ì‚¬í•­
        """

        scenario_analysis = gameday_orchestrator.run(analysis_prompt)
        result['ai_analysis'] = scenario_analysis

    # Phase 3: ì¢…í•© ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±
    print("[Phase 3] GameDay ê²°ê³¼ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±...")

    final_report_prompt = f"""
    GameDay ì „ì²´ ê²°ê³¼:
    {json.dumps(results, indent=2)}

    ë‹¤ìŒì„ í¬í•¨í•œ Post-Mortem ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì„¸ìš”:
    1. Executive Summary (ê²½ì˜ì§„ìš© ìš”ì•½)
    2. ì‹œë‚˜ë¦¬ì˜¤ë³„ ìƒì„¸ ê²°ê³¼
    3. íƒ€ì„ë¼ì¸ ì¬êµ¬ì„±
    4. ì·¨ì•½ì  ë° ê°œì„  ê³¼ì œ (ìš°ì„ ìˆœìœ„ë³„)
    5. JIRA í‹°ì¼“ ìƒì„±í•  ê°œì„  ê³¼ì œ ëª©ë¡
    """

    final_report = gameday_orchestrator.run(final_report_prompt)

    # Slack ë³´ê³ 
    slack_client = boto3.client('chatbot', region_name='ap-northeast-2')
    slack_client.send_message(
        Channel='#gameday-results',
        Message=final_report
    )

    # JIRA í‹°ì¼“ ìë™ ìƒì„±
    create_jira_tickets_from_report(final_report)

    return final_report

# ì‹¤í–‰
report = run_automated_gameday(gameday_scenario)
```

**AI ìƒì„± GameDay ë³´ê³ ì„œ ì˜ˆì‹œ**:

```markdown
# GameDay Post-Mortem ë³´ê³ ì„œ
Date: 2026-02-20 | Environment: Staging | Duration: 45ë¶„

## Executive Summary
3ê°œ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰, 2ê°œ ì„±ê³µ, 1ê°œ ë¶€ë¶„ ì„±ê³µ.
- Pod ëŒ€ëŸ‰ ì¢…ë£Œ: âœ… ì„±ê³µ (ë³µêµ¬ ì‹œê°„ 45ì´ˆ)
- ë„¤íŠ¸ì›Œí¬ ë ˆì´í„´ì‹œ: âš ï¸ ë¶€ë¶„ ì„±ê³µ (Timeout 7% ë°œìƒ)
- RDS Failover: âœ… ì„±ê³µ (Failover ì‹œê°„ 18ì´ˆ)

ì£¼ìš” ë°œê²¬: Circuit Breaker íƒ€ì„ì•„ì›ƒ ì„¤ì • ë¯¸í¡

## ì‹œë‚˜ë¦¬ì˜¤ 1: Pod ëŒ€ëŸ‰ ì¢…ë£Œ
ëª©í‘œ: 25% Pod ë™ì‹œ ì¢…ë£Œ ì‹œ ìë™ ë³µêµ¬ ê²€ì¦
ê²°ê³¼: âœ… ì„±ê³µ
- ë³µêµ¬ ì‹œê°„: 45ì´ˆ (ëª©í‘œ: 60ì´ˆ ì´ë‚´)
- Error Rate: 1.2% (ëª©í‘œ: < 2%)
- P99 Latency: 680ms (ëª©í‘œ: < 800ms)

ë°œê²¬ ì‚¬í•­:
- HPAê°€ 40ì´ˆ ë§Œì— ìƒˆ Pod ìƒì„± ì™„ë£Œ
- PDBê°€ ë™ì‹œ ì¢…ë£Œë¥¼ ì ì ˆíˆ ì œí•œ
- ì‚¬ìš©ì ì˜í–¥ ìµœì†Œí™” ì„±ê³µ

## ì‹œë‚˜ë¦¬ì˜¤ 2: ë„¤íŠ¸ì›Œí¬ ë ˆì´í„´ì‹œ
ëª©í‘œ: 300ms ë ˆì´í„´ì‹œ ì£¼ì… ì‹œ Circuit Breaker ë™ì‘ ê²€ì¦
ê²°ê³¼: âš ï¸ ë¶€ë¶„ ì„±ê³µ
- Timeout Rate: 7% (ëª©í‘œ: < 5%)
- Fallback Success: 98% (ëª©í‘œ: > 95%)

ë°œê²¬ ì‚¬í•­:
- Circuit Breaker ë™ì‘ì€ ì •ìƒ
- í•˜ì§€ë§Œ íƒ€ì„ì•„ì›ƒ ì„¤ì •ì´ ë„ˆë¬´ ì§§ìŒ (í˜„ì¬: 500ms)
- ê¶Œì¥: íƒ€ì„ì•„ì›ƒì„ 800msë¡œ ì¦ê°€

ì·¨ì•½ì :
- order-serviceì˜ payment-api í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ ì„¤ì • ë¯¸í¡
- ì¬ì‹œë„ ë¡œì§ ì—†ìŒ (503 ì—ëŸ¬ ì¦‰ì‹œ ë°˜í™˜)

## ì‹œë‚˜ë¦¬ì˜¤ 3: RDS Failover
ëª©í‘œ: RDS Failover ì‹œ ì—°ê²° ì¬ì‹œë„ ê²€ì¦
ê²°ê³¼: âœ… ì„±ê³µ
- Failover ì‹œê°„: 18ì´ˆ
- Connection Retry Success: 100%
- Data Consistency: 100%

ë°œê²¬ ì‚¬í•­:
- Connection Poolì´ ìë™ìœ¼ë¡œ ì¬ì—°ê²° ì„±ê³µ
- íŠ¸ëœì­ì…˜ ì¤‘ë‹¨ëœ ìš”ì²­ ìë™ ì¬ì‹œë„ ì„±ê³µ

## ê°œì„  ê³¼ì œ (ìš°ì„ ìˆœìœ„ë³„)

### P0 (ê¸´ê¸‰)
- [ ] order-service: payment-api íƒ€ì„ì•„ì›ƒ 500ms â†’ 800ms ì¦ê°€
- [ ] order-service: ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ (exponential backoff)

### P1 (ë†’ìŒ)
- [ ] Circuit Breaker ì„¤ì • í‘œì¤€í™” ë¬¸ì„œ ì‘ì„±
- [ ] ì „ì‚¬ ì„œë¹„ìŠ¤ íƒ€ì„ì•„ì›ƒ ì„¤ì • ê²€í† 

### P2 (ì¤‘ê°„)
- [ ] GameDay ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ê°œì„  (ë” ë§ì€ ì‹œë‚˜ë¦¬ì˜¤)
- [ ] Observability ëŒ€ì‹œë³´ë“œì— Circuit Breaker ìƒíƒœ ì¶”ê°€

## JIRA í‹°ì¼“ ìƒì„±
- INFRA-1234: order-service íƒ€ì„ì•„ì›ƒ ì„¤ì • ê°œì„ 
- INFRA-1235: Circuit Breaker ì„¤ì • í‘œì¤€í™” ë¬¸ì„œ
- INFRA-1236: ì „ì‚¬ ì„œë¹„ìŠ¤ íƒ€ì„ì•„ì›ƒ ê°ì‚¬
```

:::tip AI ê¸°ë°˜ ê³ ê¸‰ Chaos Engineeringì˜ í•µì‹¬
AIë¥¼ í™œìš©í•˜ë©´ Chaos Engineeringì´ **ìˆ˜ë™ ì‹¤í—˜ ì„¤ê³„ â†’ ì§€ëŠ¥í˜• ìë™ ì„¤ê³„**ë¡œ ì§„í™”í•©ë‹ˆë‹¤. ê³¼ê±° ì¥ì•  íŒ¨í„´ í•™ìŠµì„ í†µí•´ ì‹¤ì œ ë°œìƒ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìë™ ì œì•ˆí•˜ê³ , Steady State Hypothesisë¥¼ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì •ì˜í•˜ë©°, GameDayë¥¼ ì™„ì „ ìë™í™”í•˜ì—¬ ì²´ê³„ì ìœ¼ë¡œ ì‹œìŠ¤í…œ íšŒë³µë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**í•µì‹¬ ê°€ì¹˜**:
1. **ë°ì´í„° ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤**: ê³¼ê±° ì¸ì‹œë˜íŠ¸ ë¶„ì„ â†’ í˜„ì‹¤ì ì¸ ì¹´ì˜¤ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤
2. **ìë™ ì •ìƒ ìƒíƒœ ì •ì˜**: ë©”íŠ¸ë¦­ ê¸°ë°˜ Steady State Hypothesis ìë™ ìƒì„±
3. **GameDay ìë™í™”**: ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± â†’ ì‹¤í–‰ â†’ ë¶„ì„ â†’ ë³´ê³ ì„œ ìƒì„± ì „ì²´ ìë™í™”
4. **ì§€ì†ì  ê°œì„ **: AIê°€ ì‹¤í—˜ ê²°ê³¼ í•™ìŠµ â†’ ë‹¤ìŒ ì‹¤í—˜ ê°œì„ 
:::

### 9.6 ì˜ˆì¸¡ ê¸°ë°˜ ë¹„ìš© ìµœì í™”

ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§ê³¼ AI ë¶„ì„ì„ ê²°í•©í•˜ë©´, **ì„±ëŠ¥ ìœ ì§€ + ë¹„ìš© ìµœì í™”**ë¥¼ ë™ì‹œì— ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¸ë˜í”½ ì˜ˆì¸¡ê³¼ Spot ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬, On-Demand ëŒ€ë¹„ Spot ë¹„ìœ¨ì„ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ê³ , ì˜ˆì‚° ì´ˆê³¼ë¥¼ ì‚¬ì „ì— ë°©ì§€í•©ë‹ˆë‹¤.

#### 9.6.1 íŠ¸ë˜í”½ ì˜ˆì¸¡ + Spot ì¤‘ë‹¨ ì˜ˆì¸¡ ê²°í•©

Karpenterì˜ Spot ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©ê³¼ íŠ¸ë˜í”½ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬, **ë¹„ìš© íš¨ìœ¨ì„±ê³¼ ì•ˆì •ì„±**ì„ ê· í˜•ìˆê²Œ ìœ ì§€í•©ë‹ˆë‹¤.

```mermaid
graph TD
    subgraph Prediction["ğŸ“Š ì˜ˆì¸¡ ê³„ì¸µ"]
        TRAFFIC["íŠ¸ë˜í”½ ì˜ˆì¸¡<br/>(Prophet)"]
        SPOT["Spot ì¤‘ë‹¨ ì˜ˆì¸¡<br/>(AWS API)"]
    end

    subgraph Decision["ğŸ¤– AI ì˜ì‚¬ê²°ì •"]
        ANALYZE["ë¹„ìš©-ì•ˆì •ì„±<br/>íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„"]
        DECIDE["Spot/OnDemand<br/>ë¹„ìœ¨ ê²°ì •"]
    end

    subgraph Action["âš¡ ìë™ ì¡°ì¹˜"]
        KARP["Karpenter<br/>NodePool ì¡°ì •"]
        HPA_ADJ["HPA ì„¤ì •<br/>ìµœì í™”"]
    end

    TRAFFIC --> ANALYZE
    SPOT --> ANALYZE
    ANALYZE --> DECIDE
    DECIDE --> KARP
    DECIDE --> HPA_ADJ

    style Decision fill:#fff3cd,stroke:#ff9800
```

**Spot ì¤‘ë‹¨ ì˜ˆì¸¡ ê¸°ë°˜ ë¹„ìœ¨ ì¡°ì •**:

```python
# Spot ì¤‘ë‹¨ ì˜ˆì¸¡ + íŠ¸ë˜í”½ ì˜ˆì¸¡ í†µí•© ìŠ¤ì¼€ì¼ëŸ¬
import boto3
from datetime import datetime, timedelta

ec2_client = boto3.client('ec2', region_name='ap-northeast-2')
cloudwatch_client = boto3.client('cloudwatch', region_name='ap-northeast-2')

def predict_spot_interruption_risk(instance_types: list[str], availability_zones: list[str]) -> dict:
    """Spot ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ìœ„í—˜ë„ ì˜ˆì¸¡"""

    # Spot ì¤‘ë‹¨ ê¶Œê³  ì¡°íšŒ (ìµœê·¼ 5ë¶„ ë°ì´í„°)
    risk_scores = {}

    for az in availability_zones:
        for instance_type in instance_types:
            # CloudWatchì—ì„œ Spot ì¤‘ë‹¨ ë¹ˆë„ ì¡°íšŒ
            response = cloudwatch_client.get_metric_statistics(
                Namespace='AWS/EC2Spot',
                MetricName='InterruptionRate',
                Dimensions=[
                    {'Name': 'AvailabilityZone', 'Value': az},
                    {'Name': 'InstanceType', 'Value': instance_type}
                ],
                StartTime=datetime.now() - timedelta(hours=24),
                EndTime=datetime.now(),
                Period=3600,
                Statistics=['Average']
            )

            if response['Datapoints']:
                avg_interruption_rate = sum(dp['Average'] for dp in response['Datapoints']) / len(response['Datapoints'])
                risk_scores[f"{instance_type}/{az}"] = avg_interruption_rate
            else:
                risk_scores[f"{instance_type}/{az}"] = 0.0

    return risk_scores

def calculate_optimal_spot_ratio(traffic_prediction: dict, spot_risk: dict) -> dict:
    """íŠ¸ë˜í”½ ì˜ˆì¸¡ + Spot ìœ„í—˜ë„ ê¸°ë°˜ ìµœì  Spot ë¹„ìœ¨ ê³„ì‚°"""

    predicted_rps = traffic_prediction['predicted_rps']
    prediction_confidence = traffic_prediction['confidence']  # 0.0 - 1.0

    # í‰ê·  Spot ì¤‘ë‹¨ ìœ„í—˜ë„
    avg_spot_risk = sum(spot_risk.values()) / len(spot_risk) if spot_risk else 0.0

    # ê²°ì • ë¡œì§
    if avg_spot_risk > 0.05:  # 5% ì´ìƒ ì¤‘ë‹¨ ìœ„í—˜
        # ê³ ìœ„í—˜: On-Demand ë¹„ìœ¨ ì¦ê°€
        spot_ratio = 0.3
        ondemand_ratio = 0.7
        reason = "Spot ì¤‘ë‹¨ ìœ„í—˜ ë†’ìŒ (>5%)"

    elif prediction_confidence < 0.7:  # ì˜ˆì¸¡ ì‹ ë¢°ë„ ë‚®ìŒ
        # ë¶ˆí™•ì‹¤ì„± ë†’ìŒ: On-Demand ë¹„ìœ¨ ì¦ê°€ (ì•ˆì •ì„± ìš°ì„ )
        spot_ratio = 0.5
        ondemand_ratio = 0.5
        reason = "íŠ¸ë˜í”½ ì˜ˆì¸¡ ì‹ ë¢°ë„ ë‚®ìŒ (<70%)"

    elif predicted_rps > 5000:  # ê³ íŠ¸ë˜í”½ ì˜ˆìƒ
        # í”¼í¬ íƒ€ì„: On-Demand ë¹„ìœ¨ ì¦ê°€ (ì„±ëŠ¥ ìš°ì„ )
        spot_ratio = 0.4
        ondemand_ratio = 0.6
        reason = "ê³ íŠ¸ë˜í”½ ì˜ˆìƒ (>5000 RPS)"

    else:
        # ì •ìƒ: Spot ë¹„ìœ¨ ìµœëŒ€í™” (ë¹„ìš© ìµœì í™”)
        spot_ratio = 0.8
        ondemand_ratio = 0.2
        reason = "ì •ìƒ ìš´ì˜ ì¡°ê±´ (ë¹„ìš© ìµœì í™”)"

    return {
        'spot_ratio': spot_ratio,
        'ondemand_ratio': ondemand_ratio,
        'reason': reason,
        'estimated_cost_saving': calculate_cost_saving(spot_ratio)
    }

def calculate_cost_saving(spot_ratio: float) -> float:
    """Spot ë¹„ìœ¨ ê¸°ë°˜ ë¹„ìš© ì ˆê°ì•¡ ì¶”ì •"""
    # ê°€ì •: Spot ì¸ìŠ¤í„´ìŠ¤ëŠ” On-Demand ëŒ€ë¹„ 70% ì €ë ´
    spot_discount = 0.7
    return spot_ratio * spot_discount * 100  # ë°±ë¶„ìœ¨

# ì‹¤í–‰ ì˜ˆì‹œ
spot_risk = predict_spot_interruption_risk(
    instance_types=['c6i.xlarge', 'c5.xlarge'],
    availability_zones=['ap-northeast-2a', 'ap-northeast-2b', 'ap-northeast-2c']
)

traffic_pred = {
    'predicted_rps': 3500,
    'confidence': 0.85
}

optimal_ratio = calculate_optimal_spot_ratio(traffic_pred, spot_risk)

print(f"""
[ì˜ˆì¸¡ ê¸°ë°˜ Spot ë¹„ìœ¨ ì¡°ì •]
íŠ¸ë˜í”½ ì˜ˆì¸¡: {traffic_pred['predicted_rps']} RPS (ì‹ ë¢°ë„: {traffic_pred['confidence']:.0%})
Spot ì¤‘ë‹¨ ìœ„í—˜: {sum(spot_risk.values()) / len(spot_risk):.2%}

ê¶Œì¥ ë¹„ìœ¨:
- Spot: {optimal_ratio['spot_ratio']:.0%}
- On-Demand: {optimal_ratio['ondemand_ratio']:.0%}

ê·¼ê±°: {optimal_ratio['reason']}
ì˜ˆìƒ ë¹„ìš© ì ˆê°: {optimal_ratio['estimated_cost_saving']:.1f}%
""")
```

#### 9.6.2 ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ On-Demand ëŒ€ë¹„ Spot ë¹„ìœ¨ ë™ì  ì¡°ì •

Karpenter NodePool ì„¤ì •ì„ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬, ì˜ˆì¸¡ëœ íŠ¸ë˜í”½ê³¼ Spot ìœ„í—˜ë„ì— ë”°ë¼ ìµœì  ë¹„ìœ¨ì„ ìœ ì§€í•©ë‹ˆë‹¤.

```yaml
# Karpenter NodePool: ë™ì  Spot ë¹„ìœ¨ ì¡°ì •
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: dynamic-spot-pool
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["c6i.xlarge", "c5.xlarge", "c6a.xlarge"]

      # Spot ë¹„ìœ¨ ë™ì  ì¡°ì • (ê¸°ë³¸ê°’: 70% Spot, 30% On-Demand)
      kubelet:
        systemReserved:
          cpu: 100m
          memory: 100Mi

  # Spot ì¤‘ë‹¨ ì²˜ë¦¬ ì „ëµ
  disruption:
    consolidationPolicy: WhenUnderutilized
    expireAfter: 720h  # 30ì¼

  # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¹„ìœ¨ ì œì–´
  weight: 100
---
# Lambda í•¨ìˆ˜: Karpenter NodePool ë™ì  ì—…ë°ì´íŠ¸
import boto3
import json

eks_client = boto3.client('eks', region_name='ap-northeast-2')
k8s_client = boto3.client('eks', region_name='ap-northeast-2')  # kubectl ëŒ€ì‹  ì‚¬ìš©

def update_karpenter_nodepool_weights(optimal_ratio: dict):
    """Karpenter NodePoolì˜ Spot/OnDemand ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""

    spot_weight = int(optimal_ratio['spot_ratio'] * 100)
    ondemand_weight = int(optimal_ratio['ondemand_ratio'] * 100)

    # NodePool ì—…ë°ì´íŠ¸ (kubectl apply ëŒ€ì‹  API ì‚¬ìš©)
    nodepool_patch = {
        "spec": {
            "template": {
                "spec": {
                    "requirements": [
                        {
                            "key": "karpenter.sh/capacity-type",
                            "operator": "In",
                            "values": ["spot", "on-demand"],
                            "weight": {
                                "spot": spot_weight,
                                "on-demand": ondemand_weight
                            }
                        }
                    ]
                }
            }
        }
    }

    # CloudWatch ë©”íŠ¸ë¦­ ê¸°ë¡
    cloudwatch_client.put_metric_data(
        Namespace='Karpenter/CostOptimization',
        MetricData=[
            {
                'MetricName': 'SpotRatio',
                'Value': optimal_ratio['spot_ratio'],
                'Unit': 'Percent',
                'Timestamp': datetime.now()
            },
            {
                'MetricName': 'EstimatedCostSaving',
                'Value': optimal_ratio['estimated_cost_saving'],
                'Unit': 'Percent',
                'Timestamp': datetime.now()
            }
        ]
    )

    print(f"Karpenter NodePool ì—…ë°ì´íŠ¸: Spot {spot_weight}%, OnDemand {ondemand_weight}%")

# EventBridge Rule: 5ë¶„ë§ˆë‹¤ ì‹¤í–‰
def lambda_handler(event, context):
    # 1. íŠ¸ë˜í”½ ì˜ˆì¸¡ ê°€ì ¸ì˜¤ê¸°
    traffic_pred = get_traffic_prediction()

    # 2. Spot ì¤‘ë‹¨ ìœ„í—˜ ì˜ˆì¸¡
    spot_risk = predict_spot_interruption_risk(
        instance_types=['c6i.xlarge', 'c5.xlarge'],
        availability_zones=['ap-northeast-2a', 'ap-northeast-2b', 'ap-northeast-2c']
    )

    # 3. ìµœì  ë¹„ìœ¨ ê³„ì‚°
    optimal_ratio = calculate_optimal_spot_ratio(traffic_pred, spot_risk)

    # 4. Karpenter NodePool ì—…ë°ì´íŠ¸
    update_karpenter_nodepool_weights(optimal_ratio)

    # 5. Slack ì•Œë¦¼ (ë¹„ìœ¨ ë³€ê²½ ì‹œ)
    if abs(optimal_ratio['spot_ratio'] - 0.7) > 0.1:  # ê¸°ë³¸ê°’ ëŒ€ë¹„ 10% ì´ìƒ ë³€ê²½
        send_slack_notification(
            channel='#cost-optimization',
            message=f"""
            ğŸ”„ Karpenter Spot ë¹„ìœ¨ ìë™ ì¡°ì •

            **ì¡°ì • ê·¼ê±°**: {optimal_ratio['reason']}
            **ìƒˆ ë¹„ìœ¨**: Spot {optimal_ratio['spot_ratio']:.0%}, On-Demand {optimal_ratio['ondemand_ratio']:.0%}
            **ì˜ˆìƒ ë¹„ìš© ì ˆê°**: {optimal_ratio['estimated_cost_saving']:.1f}%

            íŠ¸ë˜í”½ ì˜ˆì¸¡: {traffic_pred['predicted_rps']} RPS (ì‹ ë¢°ë„ {traffic_pred['confidence']:.0%})
            Spot ì¤‘ë‹¨ ìœ„í—˜: {sum(spot_risk.values()) / len(spot_risk):.2%}
            """
        )

    return {
        'statusCode': 200,
        'body': json.dumps(optimal_ratio)
    }
```

#### 9.6.3 CloudWatch ë©”íŠ¸ë¦­ ê¸°ë°˜ ë¹„ìš© ì´ìƒ íƒì§€

CloudWatch Anomaly Detectionì„ í™œìš©í•˜ì—¬ ì˜ˆì‚° ì´ˆê³¼ë¥¼ ì‚¬ì „ì— ê°ì§€í•˜ê³  ìë™ ì•Œë¦¼í•©ë‹ˆë‹¤.

```python
# ë¹„ìš© ì´ìƒ íƒì§€ ì„¤ì •
import boto3

cloudwatch_client = boto3.client('cloudwatch', region_name='ap-northeast-2')
ce_client = boto3.client('ce', region_name='ap-northeast-2')  # Cost Explorer

# Step 1: ì¼ì¼ ë¹„ìš© ë©”íŠ¸ë¦­ì„ CloudWatchì— ê¸°ë¡
def record_daily_cost_to_cloudwatch():
    """Cost Explorer ë°ì´í„°ë¥¼ CloudWatch Custom Metricìœ¼ë¡œ ê¸°ë¡"""

    # ì–´ì œ ë¹„ìš© ì¡°íšŒ
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    today = datetime.now().strftime('%Y-%m-%d')

    response = ce_client.get_cost_and_usage(
        TimePeriod={
            'Start': yesterday,
            'End': today
        },
        Granularity='DAILY',
        Metrics=['UnblendedCost'],
        Filter={
            'Dimensions': {
                'Key': 'SERVICE',
                'Values': ['Amazon Elastic Kubernetes Service', 'Amazon EC2']
            }
        }
    )

    total_cost = float(response['ResultsByTime'][0]['Total']['UnblendedCost']['Amount'])

    # CloudWatch ë©”íŠ¸ë¦­ ê¸°ë¡
    cloudwatch_client.put_metric_data(
        Namespace='AWS/Billing',
        MetricData=[
            {
                'MetricName': 'DailyEKSCost',
                'Value': total_cost,
                'Unit': 'None',
                'Timestamp': datetime.now()
            }
        ]
    )

    return total_cost

# Step 2: Anomaly Detection ì„¤ì •
cloudwatch_client.put_anomaly_detector(
    Namespace='AWS/Billing',
    MetricName='DailyEKSCost',
    Stat='Sum'
)

# Step 3: ì´ìƒ ë¹„ìš© ì•ŒëŒ ì„¤ì •
cloudwatch_client.put_metric_alarm(
    AlarmName='EKS-Cost-Anomaly-Detection',
    AlarmDescription='EKS ì¼ì¼ ë¹„ìš© ì´ìƒ íƒì§€ (Anomaly Detection)',
    ActionsEnabled=True,
    AlarmActions=[
        'arn:aws:sns:ap-northeast-2:ACCOUNT_ID:cost-alerts'
    ],
    MetricName='DailyEKSCost',
    Namespace='AWS/Billing',
    Statistic='Sum',
    Period=86400,  # 24ì‹œê°„
    EvaluationPeriods=1,
    ThresholdMetricId='ad1',
    ComparisonOperator='LessThanLowerOrGreaterThanUpperThreshold',
    Metrics=[
        {
            'Id': 'm1',
            'ReturnData': True,
            'MetricStat': {
                'Metric': {
                    'Namespace': 'AWS/Billing',
                    'MetricName': 'DailyEKSCost'
                },
                'Period': 86400,
                'Stat': 'Sum'
            }
        },
        {
            'Id': 'ad1',
            'Expression': 'ANOMALY_DETECTION_BAND(m1, 2)',  # 2 standard deviations
            'Label': 'DailyEKSCost (expected)'
        }
    ]
)

print("ë¹„ìš© ì´ìƒ íƒì§€ ì„¤ì • ì™„ë£Œ: CloudWatch Anomaly Detection + Alarm")
```

#### 9.6.4 ì˜ˆì¸¡ ëª¨ë¸ ê¸°ë°˜ Reserved Instances/Savings Plans ìµœì í™”

ML ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë¯¸ë˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ì˜ˆì¸¡í•˜ê³ , Reserved Instances ë˜ëŠ” Savings Plans êµ¬ë§¤ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.

```python
# RI/Savings Plans êµ¬ë§¤ ìµœì í™”
from prophet import Prophet
import pandas as pd

def predict_baseline_capacity(historical_data: pd.DataFrame) -> dict:
    """ê³¼ê±° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ Baseline ìš©ëŸ‰ ì˜ˆì¸¡"""

    # Prophet ëª¨ë¸ í•™ìŠµ
    model = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False
    )

    # ê³¼ê±° ì¸ìŠ¤í„´ìŠ¤ ì‹œê°„(instance-hours) ë°ì´í„°
    df = historical_data[['ds', 'y']].copy()  # ds: ë‚ ì§œ, y: ì¸ìŠ¤í„´ìŠ¤ ì‹œê°„
    model.fit(df)

    # í–¥í›„ 90ì¼ ì˜ˆì¸¡
    future = model.make_future_dataframe(periods=90)
    forecast = model.predict(future)

    # Baseline ê³„ì‚°: í•˜ìœ„ 20% percentile (í•­ìƒ í•„ìš”í•œ ìµœì†Œ ìš©ëŸ‰)
    baseline_capacity = forecast['yhat'].quantile(0.20)

    # í”¼í¬ ìš©ëŸ‰: ìƒìœ„ 95% percentile
    peak_capacity = forecast['yhat'].quantile(0.95)

    return {
        'baseline_capacity': baseline_capacity,
        'peak_capacity': peak_capacity,
        'forecast': forecast
    }

# ì‹¤í–‰ ì˜ˆì‹œ
historical_data = pd.DataFrame({
    'ds': pd.date_range(start='2025-08-01', end='2026-02-01', freq='H'),
    'y': [50, 52, 48, 55, 60, 58, 62, ...]  # ì‹œê°„ë‹¹ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜
})

prediction = predict_baseline_capacity(historical_data)

print(f"""
[RI/Savings Plans êµ¬ë§¤ ê¶Œì¥]

Baseline ìš©ëŸ‰ (í•˜ìœ„ 20%): {prediction['baseline_capacity']:.0f} ì¸ìŠ¤í„´ìŠ¤
â†’ ê¶Œì¥: {prediction['baseline_capacity']:.0f}ê°œ ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•´ 1ë…„ RI êµ¬ë§¤

Peak ìš©ëŸ‰ (ìƒìœ„ 95%): {prediction['peak_capacity']:.0f} ì¸ìŠ¤í„´ìŠ¤
â†’ Baseline ì´ˆê³¼ë¶„: {prediction['peak_capacity'] - prediction['baseline_capacity']:.0f}ê°œ
â†’ ì´ˆê³¼ë¶„ì€ Spot + On-Demand ì¡°í•© ì‚¬ìš©

ì˜ˆìƒ ë¹„ìš© ì ˆê°:
- RI ì ìš© ì‹œ: 30-40% ì ˆê°
- Spot ì ìš© ì‹œ: 60-70% ì ˆê° (í”¼í¬ ì‹œê°„ëŒ€)
- ì´ ì˜ˆìƒ ì ˆê°: ì•½ 45% (í˜¼í•© ì „ëµ)
""")
```

**Cost Explorer í†µí•© â€” ì‹¤ì‹œê°„ ë¹„ìš© ì¶”ì **

```yaml
# CloudWatch Dashboard: ë¹„ìš© ìµœì í™” í˜„í™©
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-optimization-dashboard
data:
  dashboard.json: |
    {
      "widgets": [
        {
          "type": "metric",
          "properties": {
            "title": "ì¼ì¼ EKS ë¹„ìš© ì¶”ì´",
            "metrics": [
              ["AWS/Billing", "DailyEKSCost", {"stat": "Sum"}],
              [".", ".", {"stat": "Sum", "id": "ad1", "expression": "ANOMALY_DETECTION_BAND(m1, 2)"}]
            ],
            "period": 86400,
            "region": "ap-northeast-2"
          }
        },
        {
          "type": "metric",
          "properties": {
            "title": "Spot vs On-Demand ë¹„ìœ¨",
            "metrics": [
              ["Karpenter/CostOptimization", "SpotRatio"],
              [".", "OnDemandRatio"]
            ],
            "period": 300,
            "region": "ap-northeast-2"
          }
        },
        {
          "type": "metric",
          "properties": {
            "title": "ëˆ„ì  ë¹„ìš© ì ˆê°ì•¡",
            "metrics": [
              ["Karpenter/CostOptimization", "EstimatedCostSaving"]
            ],
            "period": 86400,
            "stat": "Sum",
            "region": "ap-northeast-2"
          }
        },
        {
          "type": "metric",
          "properties": {
            "title": "Spot ì¤‘ë‹¨ ë¹ˆë„",
            "metrics": [
              ["AWS/EC2Spot", "InterruptionRate", {"stat": "Average"}]
            ],
            "period": 3600,
            "region": "ap-northeast-2"
          }
        }
      ]
    }
```

:::info ì˜ˆì¸¡ ê¸°ë°˜ ë¹„ìš© ìµœì í™”ì˜ í•µì‹¬
íŠ¸ë˜í”½ ì˜ˆì¸¡ê³¼ Spot ì¤‘ë‹¨ ì˜ˆì¸¡ì„ ê²°í•©í•˜ë©´, **ì„±ëŠ¥ ì €í•˜ ì—†ì´** ë¹„ìš©ì„ ëŒ€í­ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Karpenterì˜ ë™ì  Spot ë¹„ìœ¨ ì¡°ì •ìœ¼ë¡œ ë¹„ìš© íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê³ , CloudWatch Anomaly Detectionìœ¼ë¡œ ì˜ˆì‚° ì´ˆê³¼ë¥¼ ì‚¬ì „ì— ë°©ì§€í•˜ë©°, ML ê¸°ë°˜ ìš©ëŸ‰ ì˜ˆì¸¡ìœ¼ë¡œ RI/Savings Plans êµ¬ë§¤ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.

**ë¹„ìš© ì ˆê° ì „ëµ**:
1. **Spot ë¹„ìœ¨ ìµœëŒ€í™”**: ì •ìƒ ì‹œê°„ëŒ€ 80% Spot, í”¼í¬ ì‹œê°„ëŒ€ 40% Spot
2. **Baseline RI êµ¬ë§¤**: í•˜ìœ„ 20% percentile ìš©ëŸ‰ì— ëŒ€í•´ 1ë…„ RI
3. **ì´ìƒ íƒì§€**: CloudWatch Anomaly Detectionìœ¼ë¡œ ì˜ˆì‚° ì´ˆê³¼ ì‚¬ì „ ê²½ê³ 
4. **ë™ì  ì¡°ì •**: 5ë¶„ë§ˆë‹¤ íŠ¸ë˜í”½ ì˜ˆì¸¡ + Spot ìœ„í—˜ë„ ê¸°ë°˜ ë¹„ìœ¨ ì¡°ì •

**ì˜ˆìƒ íš¨ê³¼**:
- Spot í™œìš©: 60-70% ë¹„ìš© ì ˆê° (On-Demand ëŒ€ë¹„)
- RI í™œìš©: 30-40% ë¹„ìš© ì ˆê° (On-Demand ëŒ€ë¹„)
- í˜¼í•© ì „ëµ: ì´ 45-50% ë¹„ìš© ì ˆê° (ì˜ˆì¸¡ ê¸°ë°˜ ìµœì í™”)
:::

---

## 10. í†µí•© ìš´ì˜ ëŒ€ì‹œë³´ë“œ

### 10.1 AMG ëŒ€ì‹œë³´ë“œ êµ¬ì„±

<MaturityTable />

í†µí•© ìš´ì˜ ëŒ€ì‹œë³´ë“œëŠ” ì˜ˆì¸¡ ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„°ë¥¼ í•¨ê»˜ í‘œì‹œí•©ë‹ˆë‹¤.

```json
{
  "dashboard": {
    "title": "EKS ì˜ˆì¸¡ ìš´ì˜ ëŒ€ì‹œë³´ë“œ",
    "panels": [
      {
        "title": "íŠ¸ë˜í”½ ì˜ˆì¸¡ vs ì‹¤ì œ",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{namespace='payment'}[5m]))",
            "legendFormat": "ì‹¤ì œ RPS"
          },
          {
            "expr": "predicted_rps{service='payment'}",
            "legendFormat": "ì˜ˆì¸¡ RPS"
          }
        ]
      },
      {
        "title": "ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸",
        "type": "timeseries",
        "targets": [
          {
            "expr": "kube_deployment_spec_replicas{deployment='payment-service'}",
            "legendFormat": "í˜„ì¬ Replicas"
          },
          {
            "expr": "predicted_replicas{deployment='payment-service'}",
            "legendFormat": "ì˜ˆì¸¡ í•„ìš” Replicas"
          }
        ]
      },
      {
        "title": "SLO í˜„í™©",
        "type": "gauge",
        "targets": [
          {
            "expr": "1 - (sum(rate(http_requests_total{status=~'5..'}[30d])) / sum(rate(http_requests_total[30d])))",
            "legendFormat": "ê°€ìš©ì„± SLO"
          }
        ],
        "thresholds": {
          "steps": [
            {"value": 0.999, "color": "green"},
            {"value": 0.995, "color": "yellow"},
            {"value": 0, "color": "red"}
          ]
        }
      },
      {
        "title": "Error Budget ì”ëŸ‰",
        "type": "stat",
        "targets": [
          {
            "expr": "error_budget_remaining_percent{service='payment'}",
            "legendFormat": "ë‚¨ì€ Error Budget"
          }
        ]
      },
      {
        "title": "ì˜ˆì¸¡ ì •í™•ë„",
        "type": "stat",
        "targets": [
          {
            "expr": "prediction_accuracy_percent",
            "legendFormat": "ì •í™•ë„"
          }
        ]
      },
      {
        "title": "ì¸ì‹œë˜íŠ¸ ìë™ ëŒ€ì‘ë¥ ",
        "type": "stat",
        "targets": [
          {
            "expr": "auto_remediation_success_rate",
            "legendFormat": "ìë™ ëŒ€ì‘ ì„±ê³µë¥ "
          }
        ]
      }
    ]
  }
}
```

### 10.2 í•µì‹¬ ëŒ€ì‹œë³´ë“œ íŒ¨ë„

<DashboardPanels />

---

## 11. ë§ˆë¬´ë¦¬

### 11.1 ë„ì… ë¡œë“œë§µ

```
Phase 1: ê´€ì°°ì„± ê¸°ë°˜ êµ¬ì¶•
  â””â”€â”€ AMP/AMG + CloudWatch + Anomaly Detection

Phase 2: ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§
  â””â”€â”€ Prophet/ARIMA + Karpenter ì„ ì œ í”„ë¡œë¹„ì €ë‹

Phase 3: AI Agent í™•ì¥
  â””â”€â”€ Q Developer + Strands + Kagent + MCP í†µí•©

Phase 4: Kiro í”„ë¡œê·¸ë˜ë¨¸í‹± ë””ë²„ê¹…
  â””â”€â”€ Kiro Spec â†’ ìë™ ì§„ë‹¨ â†’ ìë™ ìˆ˜ì •

Phase 5: Chaos Engineering + í”¼ë“œë°± ë£¨í”„
  â””â”€â”€ FIS ì‹¤í—˜ â†’ AI í•™ìŠµ â†’ ììœ¨ ìš´ì˜ ì§„í™”
```

### 11.2 ë‹¤ìŒ ë‹¨ê³„

- **[1. AIOps ì „ëµ ê°€ì´ë“œ](./aiops-introduction.md)**: ì˜ˆì¸¡ ìš´ì˜ì˜ ìƒìœ„ ì „ëµ â€” AIOps ì „ì²´ ë§¥ë½
- **[2. ì§€ëŠ¥í˜• ê´€ì°°ì„± ìŠ¤íƒ](./aiops-observability-stack.md)**: ì˜ˆì¸¡ ìš´ì˜ì˜ ë°ì´í„° ê¸°ë°˜ â€” ê´€ì°°ì„± êµ¬ì¶•
- **[3. AIDLC í”„ë ˆì„ì›Œí¬](./aidlc-framework.md)**: ì˜ˆì¸¡ ìš´ì˜ì„ í¬í•¨í•œ AI ê°œë°œ ë¼ì´í”„ì‚¬ì´í´

### 11.3 í•™ìŠµ ê²½ë¡œ

```
[ì´ì „] 1. AIOps ì „ëµ ê°€ì´ë“œ â€” ì „ëµê³¼ ë°©í–¥ì„± ì´í•´
     â†“
[ì´ì „] 2. ì§€ëŠ¥í˜• ê´€ì°°ì„± ìŠ¤íƒ â€” ë°ì´í„° ìˆ˜ì§‘Â·ë¶„ì„ ê¸°ë°˜ êµ¬ì¶•
     â†“
[ì´ì „] 3. AIDLC í”„ë ˆì„ì›Œí¬ â€” AI ì£¼ë„ ê°œë°œ ë°©ë²•ë¡ 
     â†“
[í˜„ì¬ ë¬¸ì„œ] 4. ì˜ˆì¸¡ ìŠ¤ì¼€ì¼ë§ ë° ìë™ ë³µêµ¬ â€” ììœ¨ ìš´ì˜ ì‹¤í˜„
```

:::info ê´€ë ¨ ë¬¸ì„œ

- [1. AIOps ì „ëµ ê°€ì´ë“œ](./aiops-introduction.md) â€” AIOps ì „ì²´ ì „ëµ
- [2. ì§€ëŠ¥í˜• ê´€ì°°ì„± ìŠ¤íƒ](./aiops-observability-stack.md) â€” ê´€ì°°ì„± ê¸°ë°˜ ì¸í”„ë¼
- [3. AIDLC í”„ë ˆì„ì›Œí¬](./aidlc-framework.md) â€” AI ì£¼ë„ ê°œë°œ ë°©ë²•ë¡ 
:::
