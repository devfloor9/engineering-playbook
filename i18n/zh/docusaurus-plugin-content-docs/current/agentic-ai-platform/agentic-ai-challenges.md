---
title: "Agentic AI å·¥ä½œè´Ÿè½½çš„æŠ€æœ¯æŒ‘æˆ˜"
sidebar_label: "1. Technical Challenges"
description: "è¿è¥ Agentic AI å·¥ä½œè´Ÿè½½çš„ 4 å¤§æ ¸å¿ƒæŠ€æœ¯æŒ‘æˆ˜åŠåŸºäº Kubernetes çš„å¼€æºç”Ÿæ€ç³»ç»Ÿ"
tags: [kubernetes, genai, agentic-ai, gpu, challenges, open-source]
category: "genai-aiml"
sidebar_position: 1
last_update:
  date: 2026-02-14
  author: devfloor9
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { ChallengeSummary, K8sCoreFeatures, SolutionMapping, ModelServingComparison, InferenceGatewayComparison, ObservabilityComparison, KAgentFeatures, ObservabilityLayerStack, LlmdFeatures, DistributedTrainingStack, GpuInfraStack } from '@site/src/components/AgenticChallengesTables';

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2025-02-05 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 7 åˆ†é’Ÿ

## ç®€ä»‹

åœ¨æ„å»ºå’Œè¿è¥ Agentic AI å¹³å°æ—¶ï¼Œå¹³å°å·¥ç¨‹å¸ˆå’Œæ¶æ„å¸ˆé¢ä¸´ç€ä¸ä¼ ç»Ÿ Web åº”ç”¨æ ¹æœ¬ä¸åŒçš„ç‹¬ç‰¹æŠ€æœ¯æŒ‘æˆ˜ã€‚æœ¬æ–‡åˆ†æäº† **4 å¤§æ ¸å¿ƒæŒ‘æˆ˜**ï¼Œå¹¶æ¢è®¨äº†æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜çš„**åŸºäº Kubernetes çš„å¼€æºç”Ÿæ€ç³»ç»Ÿ**ã€‚

## Agentic AI å¹³å°çš„ 4 å¤§æ ¸å¿ƒæŠ€æœ¯æŒ‘æˆ˜

åˆ©ç”¨å‰æ²¿æ¨¡å‹ï¼ˆæœ€æ–°å¤§è¯­è¨€æ¨¡å‹ï¼‰çš„ Agentic AI ç³»ç»Ÿä¸ä¼ ç»Ÿ Web åº”ç”¨æœ‰ç€**æ ¹æœ¬ä¸åŒçš„åŸºç¡€è®¾æ–½éœ€æ±‚**ã€‚

```mermaid
graph TB
    subgraph "4 Key Technical Challenges"
        C1["ğŸ–¥ï¸ Challenge 1<br/>GPU Monitoring & Resource Scheduling"]
        C2["ğŸ”€ Challenge 2<br/>Agentic AI Request Dynamic Routing & Scaling"]
        C3["ğŸ“Š Challenge 3<br/>Token/Session Level Monitoring & Cost Control"]
        C4["ğŸ”§ Challenge 4<br/>FM Fine-tuning & Automation Pipeline"]
    end

    subgraph "Common Characteristics"
        COMMON["GPU Resource Intensive<br/>Unpredictable Workloads<br/>High Infrastructure Costs<br/>Complex Distributed Systems"]
    end

    C1 --> COMMON
    C2 --> COMMON
    C3 --> COMMON
    C4 --> COMMON

    style C1 fill:#ff6b6b
    style C2 fill:#4ecdc4
    style C3 fill:#45b7d1
    style C4 fill:#96ceb4
    style COMMON fill:#f9f9f9
```

### æŒ‘æˆ˜æ¦‚è¿°

<ChallengeSummary />

:::warning ä¼ ç»ŸåŸºç¡€è®¾æ–½æ–¹æ¡ˆçš„å±€é™æ€§
ä¼ ç»Ÿçš„åŸºäºè™šæ‹Ÿæœºçš„åŸºç¡€è®¾æ–½æˆ–æ‰‹åŠ¨ç®¡ç†æ–¹å¼æ— æ³•æœ‰æ•ˆåº”å¯¹ Agentic AI çš„**åŠ¨æ€ä¸”ä¸å¯é¢„æµ‹çš„å·¥ä½œè´Ÿè½½æ¨¡å¼**ã€‚GPU èµ„æºçš„é«˜æ˜‚æˆæœ¬å’Œå¤æ‚çš„åˆ†å¸ƒå¼ç³»ç»Ÿè¦æ±‚ä½¿å¾—**è‡ªåŠ¨åŒ–åŸºç¡€è®¾æ–½ç®¡ç†**æˆä¸ºå¿…éœ€ã€‚
:::

---

## è§£å†³ä¹‹é“ï¼šäº‘åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–ä¸ AI å¹³å°çš„èåˆ

è§£å†³ Agentic AI å¹³å°æŒ‘æˆ˜çš„å…³é”®åœ¨äº**äº‘åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–ä¸ AI å·¥ä½œè´Ÿè½½çš„æœ‰æœºèåˆ**ã€‚ä»¥ä¸‹æ˜¯è¿™ç§èåˆè‡³å…³é‡è¦çš„åŸå› ï¼š

```mermaid
graph LR
    subgraph "AI Workload Characteristics"
        AI1["Dynamic Resource Demands"]
        AI2["Unpredictable Traffic"]
        AI3["High-cost GPU Resources"]
        AI4["Complex Distributed Processing"]
    end

    subgraph "Infrastructure Automation Requirements"
        INF1["Real-time Provisioning"]
        INF2["Automatic Scaling"]
        INF3["Cost Optimization"]
        INF4["Declarative Management"]
    end

    subgraph "Integration Platform"
        PLATFORM["Kubernetes<br/>Container Orchestration"]
    end

    AI1 --> PLATFORM
    AI2 --> PLATFORM
    AI3 --> PLATFORM
    AI4 --> PLATFORM
    PLATFORM --> INF1
    PLATFORM --> INF2
    PLATFORM --> INF3
    PLATFORM --> INF4

    style PLATFORM fill:#326ce5
```

## ä¸ºä»€ä¹ˆé€‰æ‹© Kubernetesï¼Ÿ

Kubernetes æ˜¯è§£å†³ Agentic AI å¹³å°æ‰€æœ‰æŒ‘æˆ˜çš„**ç†æƒ³åŸºç¡€å¹³å°**ï¼š

<K8sCoreFeatures />

```mermaid
graph TB
    subgraph "Kubernetes Core Components"
        API["API Server<br/>Declarative Resource Management"]
        SCHED["Scheduler<br/>GPU-aware Scheduling"]
        CTRL["Controller Manager<br/>State Reconciliation Loop"]
        ETCD["etcd<br/>Cluster State Storage"]
    end

    subgraph "AI Workload Support"
        GPU["GPU Device Plugin<br/>GPU Resource Abstraction"]
        HPA["HPA/KEDA<br/>Metrics-based Scaling"]
        OP["Operators<br/>Complex Workflow Automation"]
    end

    subgraph "Challenge Resolution"
        S1["âœ… Integrated GPU Resource Management"]
        S2["âœ… Dynamic Scaling"]
        S3["âœ… Resource Quota Management"]
        S4["âœ… Distributed Learning Automation"]
    end

    API --> GPU
    SCHED --> GPU
    CTRL --> HPA
    CTRL --> OP
    GPU --> S1
    HPA --> S2
    API --> S3
    OP --> S4

    style API fill:#326ce5
    style SCHED fill:#326ce5
    style CTRL fill:#326ce5
```

:::info Kubernetes å¯¹ AI å·¥ä½œè´Ÿè½½çš„æ”¯æŒ
Kubernetes æä¾›äº†ä¸ AI/ML ç”Ÿæ€ç³»ç»Ÿçš„ä¸°å¯Œé›†æˆï¼ŒåŒ…æ‹¬ NVIDIA GPU Operatorã€Kubeflow å’Œ KEDAã€‚é€šè¿‡è¿™äº›é›†æˆï¼ŒGPU èµ„æºç®¡ç†ã€åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨¡å‹æœåŠ¡å¯ä»¥åœ¨**å•ä¸€å¹³å°ä¸Šç»Ÿä¸€ç®¡ç†**ã€‚
:::

---

æ—¢ç„¶æˆ‘ä»¬å·²ç»äº†è§£äº†ä¸ºä»€ä¹ˆ Kubernetes æ˜¯ AI å·¥ä½œè´Ÿè½½çš„ç†æƒ³é€‰æ‹©ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹çœ‹**é’ˆå¯¹æ¯ä¸ªæŒ‘æˆ˜çš„å…·ä½“å¼€æºè§£å†³æ–¹æ¡ˆ**ã€‚

## Kubernetes Agentic AI è§£å†³æ–¹æ¡ˆé¸Ÿç°å›¾

Kubernetes ç”Ÿæ€ç³»ç»Ÿæ‹¥æœ‰**ä¸“é—¨çš„å¼€æºè§£å†³æ–¹æ¡ˆ**æ¥è§£å†³ Agentic AI å¹³å°çš„æ¯ä¸ªæŒ‘æˆ˜ã€‚è¿™äº›è§£å†³æ–¹æ¡ˆè¢«è®¾è®¡ä¸º Kubernetes åŸç”Ÿçš„ï¼Œè®©æ‚¨èƒ½å¤Ÿå……åˆ†åˆ©ç”¨**å£°æ˜å¼ç®¡ç†ã€è‡ªåŠ¨æ‰©å±•å’Œé«˜å¯ç”¨æ€§**çš„ä¼˜åŠ¿ã€‚

### è§£å†³æ–¹æ¡ˆæ˜ å°„æ¦‚è§ˆ

```mermaid
graph TB
    subgraph "4 Key Technical Challenges"
        C1["ğŸ–¥ï¸ GPU Monitoring &<br/>Resource Scheduling"]
        C2["ğŸ”€ Dynamic Routing &<br/>Scaling"]
        C3["ğŸ“Š Token/Session Monitoring<br/>& Cost Control"]
        C4["ğŸ”§ FM Fine-tuning &<br/>Automation Pipeline"]
    end

    subgraph "Kubernetes Native Solutions"
        S1["Karpenter<br/>GPU Node Auto Provisioning"]
        S2["Kgateway + LiteLLM<br/>Inference Gateway"]
        S3["LangFuse / LangSmith<br/>LLM Observability"]
        S4["NeMo + Kubeflow<br/>Distributed Training Pipeline"]
    end

    subgraph "Model Serving Layer"
        VLLM["vLLM<br/>High-Performance Inference Engine"]
        LLMD["llm-d<br/>Distributed Inference Scheduler"]
    end

    subgraph "Agent Orchestration"
        KAGENT["KAgent<br/>Kubernetes Agent Framework"]
    end

    C1 --> S1
    C2 --> S2
    C3 --> S3
    C4 --> S4

    S2 --> VLLM
    S2 --> LLMD
    KAGENT --> S2
    KAGENT --> S3

    style C1 fill:#ff6b6b
    style C2 fill:#4ecdc4
    style C3 fill:#45b7d1
    style C4 fill:#96ceb4
    style S1 fill:#ffd93d
    style S2 fill:#4286f4
    style S3 fill:#9b59b6
    style S4 fill:#76b900
    style VLLM fill:#e74c3c
    style LLMD fill:#e74c3c
    style KAGENT fill:#2ecc71
```

### æŒ‘æˆ˜å¯¹åº”çš„è§£å†³æ–¹æ¡ˆè¯¦ç»†æ˜ å°„

<SolutionMapping />

---

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»æ¦‚è§ˆäº† Kubernetes ç”Ÿæ€ç³»ç»Ÿä¸­çš„å„ç§è§£å†³æ–¹æ¡ˆã€‚ç°åœ¨è®©æˆ‘ä»¬ä»å¼€æºæ¶æ„çš„è§’åº¦ï¼Œæ·±å…¥äº†è§£**è¿™äº›è§£å†³æ–¹æ¡ˆå¦‚ä½•å®é™…é›†æˆå’ŒååŒå·¥ä½œ**ã€‚

## å¼€æºç”Ÿæ€ç³»ç»Ÿä¸ Kubernetes é›†æˆæ¶æ„

Agentic AI å¹³å°ç”±å„ç§å¼€æºé¡¹ç›®ç»„æˆï¼Œå®ƒä»¬å›´ç»• Kubernetes æœ‰æœºåœ°é›†æˆåœ¨ä¸€èµ·ã€‚æœ¬èŠ‚è¯´æ˜ **LLM å¯è§‚æµ‹æ€§ã€æ¨¡å‹æœåŠ¡ã€å‘é‡æ•°æ®åº“å’Œ GPU åŸºç¡€è®¾æ–½**ä¸­çš„æ ¸å¿ƒå¼€æºé¡¹ç›®å¦‚ä½•åä½œï¼Œå½¢æˆå®Œæ•´çš„ Agentic AI å¹³å°ã€‚

### 1. æ¨¡å‹æœåŠ¡ï¼švLLM + llm-d

**vLLM** æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„ LLM æ¨ç†æœåŠ¡å¼•æ“ï¼Œé€šè¿‡ PagedAttention **æœ€å¤§åŒ–å†…å­˜æ•ˆç‡**ã€‚

**llm-d** æ˜¯ä¸€ä¸ªåœ¨ Kubernetes ç¯å¢ƒä¸­**æ™ºèƒ½åˆ†å‘** LLM æ¨ç†è¯·æ±‚çš„è°ƒåº¦å™¨ã€‚

```mermaid
graph LR
    subgraph "Inference Request Flow"
        REQ["Client Request"]
        LLMD["llm-d<br/>Request Router"]

        subgraph "vLLM Instances"
            V1["vLLM Pod 1<br/>GPU: A100"]
            V2["vLLM Pod 2<br/>GPU: A100"]
            V3["vLLM Pod 3<br/>GPU: H100"]
        end
    end

    REQ --> LLMD
    LLMD --> V1
    LLMD --> V2
    LLMD --> V3

    style LLMD fill:#e74c3c
    style V1 fill:#3498db
    style V2 fill:#3498db
    style V3 fill:#3498db
```

<ModelServingComparison />

**Kubernetes é›†æˆï¼š**

- ä»¥ Kubernetes Deployment å½¢å¼éƒ¨ç½²
- é€šè¿‡ Service æš´éœ²æœåŠ¡
- åŸºäºé˜Ÿåˆ—æ·±åº¦æŒ‡æ ‡é€šè¿‡ HPA è¿›è¡Œæ‰©å±•
- é€šè¿‡èµ„æºè¯·æ±‚/é™åˆ¶è¿›è¡Œ GPU åˆ†é…

### 2. æ¨ç†ç½‘å…³ï¼šKgateway + LiteLLM

**Kgateway** æ˜¯ä¸€ä¸ªåŸºäº Kubernetes Gateway API çš„ AI æ¨ç†ç½‘å…³ï¼Œæä¾›**å¤šæ¨¡å‹è·¯ç”±å’Œæµé‡ç®¡ç†**ã€‚

**LiteLLM** é€šè¿‡ç»Ÿä¸€ API **æŠ½è±¡å„ç§ LLM æä¾›å•†**ï¼Œä½¿æ¨¡å‹åˆ‡æ¢å˜å¾—ç®€å•ã€‚

```mermaid
graph TB
    subgraph "Gateway Layer"
        CLIENT["Client Applications"]
        KGW["Kgateway<br/>Inference Gateway"]
        LITE["LiteLLM<br/>Provider Abstraction"]
    end

    subgraph "Model Backends"
        SELF["Self-hosted<br/>vLLM / TGI"]
        BEDROCK["Amazon Bedrock"]
        OPENAI["OpenAI API"]
    end

    CLIENT --> KGW
    KGW --> LITE
    LITE --> SELF
    LITE --> BEDROCK
    LITE --> OPENAI

    style KGW fill:#4286f4
    style LITE fill:#9b59b6
```

<InferenceGatewayComparison />

**Kubernetes é›†æˆï¼š**

- å®ç° Kubernetes Gateway APIï¼ˆæ ‡å‡†ï¼‰
- é€šè¿‡ HTTPRoute èµ„æºè¿›è¡Œå£°æ˜å¼è·¯ç”±
- ä¸ Kubernetes Service åŸç”Ÿé›†æˆ
- æ”¯æŒè·¨å‘½åç©ºé—´è·¯ç”±

### 3. LLM å¯è§‚æµ‹æ€§ï¼šLangFuse + LangSmith

**LangFuse** å’Œ **LangSmith** æ˜¯**è¿½è¸ª LLM åº”ç”¨å…¨ç”Ÿå‘½å‘¨æœŸ**çš„å¯è§‚æµ‹æ€§å¹³å°ã€‚

```mermaid
graph LR
    subgraph "LLM Application"
        APP["Agent Application"]
        CHAIN["LangChain / LlamaIndex"]
    end

    subgraph "Observability Platform"
        LF["LangFuse<br/>(Self-hosted)"]
        LS["LangSmith<br/>(Managed)"]
    end

    subgraph "Analysis Features"
        TRACE["Trace Analysis"]
        COST["Cost Tracking"]
        EVAL["Quality Evaluation"]
        DEBUG["Debugging"]
    end

    APP --> CHAIN
    CHAIN --> LF
    CHAIN --> LS
    LF --> TRACE & COST & EVAL & DEBUG
    LS --> TRACE & COST & EVAL & DEBUG

    style LF fill:#45b7d1
    style LS fill:#9b59b6
```

<ObservabilityComparison />

**Kubernetes é›†æˆï¼ˆLangFuseï¼‰ï¼š**

- ä»¥ StatefulSet æˆ– Deployment å½¢å¼éƒ¨ç½²
- éœ€è¦ PostgreSQL åç«¯ï¼ˆå¯ä½¿ç”¨æ‰˜ç®¡ RDS æˆ–é›†ç¾¤å†…éƒ¨ç½²ï¼‰
- ä»¥ Prometheus æ ¼å¼æš´éœ²æŒ‡æ ‡
- é€šè¿‡ Pod ä¸­çš„ç¯å¢ƒå˜é‡è¿›è¡Œ SDK é›†æˆ

### 4. Agent ç¼–æ’ï¼šKAgent

**KAgent** æ˜¯ä¸€ä¸ª Kubernetes åŸç”Ÿçš„ AI Agent æ¡†æ¶ï¼Œ**å°† Agent å·¥ä½œæµå®šä¹‰ä¸º CRD å¹¶è¿›è¡Œç®¡ç†**ã€‚

```mermaid
graph TB
    subgraph "KAgent Architecture"
        CRD["Agent CRD<br/>Declarative Definition"]
        CTRL["KAgent Controller<br/>State Management"]

        subgraph "Agent Components"
            TOOL["Tool Definitions"]
            MEM["Memory Store"]
            LLM["LLM Backend"]
        end
    end

    subgraph "Integration"
        KGW["Kgateway"]
        OBS["LangFuse"]
    end

    CRD --> CTRL
    CTRL --> TOOL & MEM & LLM
    CTRL --> KGW
    CTRL --> OBS

    style CRD fill:#2ecc71
    style CTRL fill:#2ecc71
```

<KAgentFeatures />

**Kubernetes é›†æˆï¼š**

- é€šè¿‡è‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼ˆCRDï¼‰æ‰©å±• Kubernetes
- æ§åˆ¶å™¨æ¨¡å¼å®ç°çŠ¶æ€åè°ƒ
- ä¸ Kubernetes RBAC åŸç”Ÿé›†æˆ
- åˆ©ç”¨ Kubernetes Secrets ç®¡ç† API å¯†é’¥

### è§£å†³æ–¹æ¡ˆæ ˆé›†æˆæ¶æ„

```mermaid
graph TB
    subgraph "Client Layer"
        WEB["Web Application"]
        API["API Clients"]
        AGENT["Agent Applications"]
    end

    subgraph "Gateway Layer"
        KGW["Kgateway<br/>Traffic Management"]
        LITE["LiteLLM<br/>Provider Abstraction"]
    end

    subgraph "Orchestration Layer"
        KAGENT["KAgent<br/>Agent Framework"]
        KEDA["KEDA<br/>Event-driven Scaling"]
    end

    subgraph "Serving Layer"
        LLMD["llm-d<br/>Request Scheduler"]
        VLLM1["vLLM Instance 1"]
        VLLM2["vLLM Instance 2"]
        VLLM3["vLLM Instance 3"]
    end

    subgraph "Infrastructure Layer"
        KARP["Karpenter<br/>Node Provisioning"]
        GPU1["GPU Node 1"]
        GPU2["GPU Node 2"]
        GPU3["GPU Node 3"]
    end

    subgraph "Observability Layer"
        LF["LangFuse<br/>LLM Tracing"]
        PROM["Prometheus<br/>Metrics"]
        GRAF["Grafana<br/>Dashboards"]
    end

    WEB & API & AGENT --> KGW
    KGW --> LITE
    LITE --> KAGENT
    KAGENT --> LLMD
    LLMD --> VLLM1 & VLLM2 & VLLM3
    VLLM1 --> GPU1
    VLLM2 --> GPU2
    VLLM3 --> GPU3
    KARP --> GPU1 & GPU2 & GPU3
    KEDA --> VLLM1 & VLLM2 & VLLM3

    KAGENT -.-> LF
    VLLM1 & VLLM2 & VLLM3 -.-> PROM
    PROM --> GRAF
    LF --> GRAF

    style KGW fill:#4286f4
    style KAGENT fill:#2ecc71
    style KARP fill:#ffd93d
    style LF fill:#45b7d1
    style LLMD fill:#e74c3c
```

---

### å®Œæ•´çš„å¼€æºé›†æˆæ¶æ„

```mermaid
graph TB
    subgraph "Application Layer"
        AGENT["Agentic AI Application"]
        RAG["RAG Pipeline"]
    end

    subgraph "LLM Observability Layer"
        LF["LangFuse<br/>(Self-hosted)"]
        LS["LangSmith<br/>(Managed)"]
        RAGAS["RAGAS<br/>(RAG Quality Evaluation)"]
    end

    subgraph "Inference Gateway Layer"
        LITE["LiteLLM<br/>(Provider Abstraction)"]
        KGW["Kgateway<br/>(Traffic Management)"]
    end

    subgraph "Model Serving Layer"
        LLMD["llm-d<br/>(Distributed Scheduler)"]
        VLLM["vLLM<br/>(Inference Engine)"]
    end

    subgraph "Vector Database Layer"
        MILVUS["Milvus<br/>(Vector Store)"]
    end

    subgraph "GPU Infrastructure Layer"
        DRA["DRA<br/>(Dynamic Resource Allocation)"]
        DCGM["DCGM<br/>(GPU Monitoring)"]
        NCCL["NCCL<br/>(GPU Communication)"]
        KARP["Karpenter<br/>(Node Provisioning)"]
    end

    AGENT --> LF & LS
    AGENT --> LITE
    RAG --> MILVUS
    RAG --> RAGAS
    LITE --> KGW
    KGW --> LLMD
    LLMD --> VLLM
    VLLM --> DRA
    DRA --> DCGM
    VLLM --> NCCL
    KARP --> DRA

    style LF fill:#45b7d1
    style LS fill:#9b59b6
    style RAGAS fill:#e67e22
    style LITE fill:#9b59b6
    style LLMD fill:#e74c3c
    style MILVUS fill:#00d4aa
    style DRA fill:#326ce5
    style DCGM fill:#76b900
    style NCCL fill:#76b900
    style KARP fill:#ffd93d
```

### å„å±‚å¼€æºç»„ä»¶è§’è‰²ä¸é›†æˆ

#### LLM å¯è§‚æµ‹æ€§å±‚ï¼šLangFuseã€LangSmithã€RAGAS

**è¿½è¸ª LLM åº”ç”¨å…¨ç”Ÿå‘½å‘¨æœŸå¹¶è¯„ä¼°è´¨é‡**çš„æ ¸å¿ƒå·¥å…·ã€‚

<ObservabilityLayerStack />

```mermaid
graph LR
    subgraph "LLM Application"
        APP["Agent App"]
        SDK1["LangFuse SDK"]
        SDK2["LangSmith SDK"]
    end

    subgraph "Kubernetes Cluster"
        subgraph "LangFuse Stack"
            LF_WEB["LangFuse Web<br/>(Deployment)"]
            LF_WORKER["LangFuse Worker<br/>(Deployment)"]
            LF_DB["PostgreSQL<br/>(StatefulSet)"]
            LF_REDIS["Redis<br/>(StatefulSet)"]
        end

        subgraph "RAGAS Evaluation"
            RAGAS_JOB["RAGAS Job<br/>(CronJob)"]
        end
    end

    APP --> SDK1 --> LF_WEB
    APP --> SDK2
    LF_WEB --> LF_WORKER --> LF_DB
    LF_WORKER --> LF_REDIS
    RAGAS_JOB --> LF_DB

    style LF_WEB fill:#45b7d1
    style RAGAS_JOB fill:#e67e22
```

**LangFuse Kubernetes éƒ¨ç½²ç¤ºä¾‹ï¼š**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langfuse-web
  namespace: observability
spec:
  replicas: 2
  selector:
    matchLabels:
      app: langfuse-web
  template:
    spec:
      containers:
        - name: langfuse
          image: langfuse/langfuse:latest
          env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: database-url
            - name: NEXTAUTH_SECRET
              valueFrom:
                secretKeyRef:
                  name: langfuse-secrets
                  key: nextauth-secret
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: ragas-evaluation
  namespace: observability
spec:
  schedule: "0 */6 * * *"  # æ¯ 6 å°æ—¶è¿è¡Œä¸€æ¬¡
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: ragas
              image: ragas/ragas:latest
              command: ["python", "-m", "ragas.evaluate"]
              env:
                - name: LANGFUSE_HOST
                  value: "http://langfuse-web:3000"
          restartPolicy: OnFailure
```

#### æ¨ç†ç½‘å…³å±‚ï¼šLiteLLM

**LiteLLM** å°† 100 å¤šä¸ª LLM æä¾›å•†æŠ½è±¡ä¸º**ç»Ÿä¸€çš„ OpenAI å…¼å®¹ API**ã€‚

```mermaid
graph TB
    subgraph "LiteLLM Gateway"
        PROXY["LiteLLM Proxy<br/>(Deployment)"]
        CONFIG["Config<br/>(ConfigMap)"]
        CACHE["Redis Cache<br/>(StatefulSet)"]
    end

    subgraph "LLM Backends"
        SELF["Self-hosted<br/>vLLM / TGI"]
        BEDROCK["Amazon Bedrock"]
        OPENAI["OpenAI API"]
        ANTHROPIC["Anthropic API"]
    end

    subgraph "Features"
        LB["Load Balancing"]
        FALLBACK["Fallback Logic"]
        COST["Cost Tracking"]
        RATE["Rate Limiting"]
    end

    PROXY --> SELF & BEDROCK & OPENAI & ANTHROPIC
    CONFIG --> PROXY
    CACHE --> PROXY
    PROXY --> LB & FALLBACK & COST & RATE

    style PROXY fill:#9b59b6
```

**LiteLLM Kubernetes éƒ¨ç½²ç¤ºä¾‹ï¼š**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: litellm-proxy
  namespace: ai-gateway
spec:
  replicas: 3
  selector:
    matchLabels:
      app: litellm
  template:
    spec:
      containers:
        - name: litellm
          image: ghcr.io/berriai/litellm:main-latest
          ports:
            - containerPort: 4000
          env:
            - name: LITELLM_MASTER_KEY
              valueFrom:
                secretKeyRef:
                  name: litellm-secrets
                  key: master-key
            - name: REDIS_HOST
              value: "redis-cache"
          volumeMounts:
            - name: config
              mountPath: /app/config.yaml
              subPath: config.yaml
      volumes:
        - name: config
          configMap:
            name: litellm-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ai-gateway
data:
  config.yaml: |
    model_list:
      - model_name: gpt-4
        litellm_params:
          model: openai/gpt-4
          api_key: os.environ/OPENAI_API_KEY
      - model_name: claude-3
        litellm_params:
          model: anthropic/claude-3-opus
          api_key: os.environ/ANTHROPIC_API_KEY
      - model_name: llama-70b
        litellm_params:
          model: openai/llama-70b
          api_base: http://vllm-llama:8000/v1

    router_settings:
      routing_strategy: least-busy
      enable_fallbacks: true

    general_settings:
      master_key: os.environ/LITELLM_MASTER_KEY
```

#### åˆ†å¸ƒå¼æ¨ç†å±‚ï¼šllm-d

**llm-d** æ˜¯ä¸€ä¸ªåœ¨ Kubernetes ç¯å¢ƒä¸­**æ™ºèƒ½åˆ†å‘** LLM æ¨ç†è¯·æ±‚çš„è°ƒåº¦å™¨ã€‚

<LlmdFeatures />

```mermaid
graph LR
    subgraph "llm-d Architecture"
        ROUTER["llm-d Router<br/>(Deployment)"]
        SCHED["Scheduler Logic"]
        CACHE["Prefix Cache Index"]
    end

    subgraph "vLLM Backends"
        V1["vLLM-1<br/>GPU: A100"]
        V2["vLLM-2<br/>GPU: A100"]
        V3["vLLM-3<br/>GPU: H100"]
    end

    subgraph "Kubernetes Resources"
        SVC["Service"]
        EP["EndpointSlice"]
        HPA["HPA/KEDA"]
    end

    ROUTER --> SCHED --> CACHE
    SCHED --> V1 & V2 & V3
    SVC --> ROUTER
    EP --> V1 & V2 & V3
    HPA --> V1 & V2 & V3

    style ROUTER fill:#e74c3c
    style SCHED fill:#e74c3c
```

**llm-d Kubernetes éƒ¨ç½²ç¤ºä¾‹ï¼š**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-d-router
  namespace: ai-inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-d
  template:
    spec:
      containers:
        - name: llm-d
          image: ghcr.io/llm-d/llm-d:latest
          ports:
            - containerPort: 8080
          env:
            - name: BACKENDS
              value: "vllm-0.vllm:8000,vllm-1.vllm:8000,vllm-2.vllm:8000"
            - name: ROUTING_STRATEGY
              value: "prefix-aware"
            - name: PROMETHEUS_ENDPOINT
              value: "http://prometheus:9090"
          resources:
            requests:
              memory: "256Mi"
              cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: llm-d
  namespace: ai-inference
spec:
  selector:
    app: llm-d
  ports:
    - port: 8080
      targetPort: 8080
```

### 5. å‘é‡æ•°æ®åº“å±‚ï¼šMilvus

Milvus æ˜¯ RAG ç®¡çº¿çš„æ ¸å¿ƒç»„ä»¶ï¼Œåœ¨ Kubernetes ä¸Šä»¥åˆ†å¸ƒå¼æ¶æ„è¿è¡Œã€‚

è¯¦ç»†ä¿¡æ¯è¯·å‚é˜… **[Milvus å‘é‡æ•°æ®åº“](./milvus-vector-database.md)**ã€‚

**Milvus çš„æ ¸å¿ƒç‰¹æ€§ï¼š**

- **åˆ†å¸ƒå¼æ¶æ„**ï¼šå°†è®¿é—®å±‚ã€åè°ƒå±‚ã€å·¥ä½œèŠ‚ç‚¹å±‚å’Œå­˜å‚¨å±‚åˆ†ç¦»ï¼Œå®ç°ç‹¬ç«‹æ‰©å±•
- **Kubernetes Operator**ï¼šåŸºäº CRD çš„å£°æ˜å¼ç®¡ç†
- **GPU åŠ é€Ÿ**ï¼šåœ¨ç´¢å¼•èŠ‚ç‚¹ä¸Šè¿›è¡Œ GPU åŠ é€Ÿçš„ç´¢å¼•æ„å»º
- **S3 é›†æˆ**ï¼šä½¿ç”¨ Amazon S3 ä½œä¸ºæŒä¹…åŒ–å­˜å‚¨

### 6. åˆ†å¸ƒå¼è®­ç»ƒï¼šNeMo + Kubeflow

**NVIDIA NeMo** å’Œ **Kubeflow** ä¸ºå¤§è§„æ¨¡æ¨¡å‹æä¾›**è‡ªåŠ¨åŒ–åˆ†å¸ƒå¼è®­ç»ƒç®¡çº¿**ã€‚

<DistributedTrainingStack />

```mermaid
graph LR
    subgraph "Data Pipeline"
        DATA["Training Data"]
        PREP["Data Preprocessing"]
    end

    subgraph "Training Cluster"
        NEMO["NeMo Framework"]
        DIST["Distributed Training"]
    end

    subgraph "Model Registry"
        CKPT["Checkpoint Storage"]
        MLFLOW["MLflow Registry"]
    end

    subgraph "Deployment"
        SERVE["Model Serving"]
        CANARY["Canary Deployment"]
    end

    DATA --> PREP
    PREP --> NEMO
    NEMO --> DIST
    DIST --> CKPT
    CKPT --> MLFLOW
    MLFLOW --> SERVE
    SERVE --> CANARY

    style NEMO fill:#76b900
```

**Kubernetes é›†æˆï¼š**

- Kubeflow Training Operatorï¼ˆPyTorchJobã€MPIJob ç­‰ï¼‰
- åˆ†å¸ƒå¼å·¥ä½œè´Ÿè½½çš„ Gang è°ƒåº¦
- æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦ï¼ˆèŠ‚ç‚¹äº²å’Œæ€§ã€åäº²å’Œæ€§ï¼‰
- ä¸ CSI é©±åŠ¨é›†æˆå®ç°å…±äº«å­˜å‚¨ï¼ˆFSx for Lustreï¼‰

---

## GPU åŸºç¡€è®¾æ–½ä¸èµ„æºç®¡ç†

GPU èµ„æºç®¡ç†æ˜¯ Agentic AI å¹³å°çš„æ ¸å¿ƒã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…ï¼š

- **[GPU èµ„æºç®¡ç†](./gpu-resource-management.md)**ï¼šDevice Pluginã€DRAï¼ˆåŠ¨æ€èµ„æºåˆ†é…ï¼‰ã€GPU æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦
- **[NeMo æ¡†æ¶](./nemo-framework.md)**ï¼šåˆ†å¸ƒå¼è®­ç»ƒå’Œ NCCL ä¼˜åŒ–

:::tip å…³é”® GPU ç®¡ç†æ¦‚å¿µ

- **Device Plugin**ï¼šKubernetes åŸºç¡€ GPU åˆ†é…æœºåˆ¶
- **DRAï¼ˆåŠ¨æ€èµ„æºåˆ†é…ï¼‰**ï¼šKubernetes 1.26+ ä¸­çš„çµæ´»èµ„æºç®¡ç†
- **NCCL**ï¼šç”¨äºåˆ†å¸ƒå¼ GPU è®­ç»ƒçš„é«˜æ€§èƒ½é€šä¿¡åº“
:::

### GPU åŸºç¡€è®¾æ–½æ ˆæ¦‚è§ˆ

```mermaid
graph TB
    subgraph "GPU Infrastructure Stack"
        subgraph "Resource Allocation"
            DRA["DRA<br/>(Dynamic Resource Allocation)"]
            DRIVER["NVIDIA Device Plugin"]
        end

        subgraph "Monitoring"
            DCGM["DCGM Exporter"]
            PROM["Prometheus"]
            GRAF["Grafana"]
        end

        subgraph "Communication"
            NCCL["NCCL<br/>(GPU Collective Comm)"]
            EFA["EFA Driver"]
        end

        subgraph "Node Management"
            KARP["Karpenter"]
            GPU_OP["GPU Operator"]
        end
    end

    subgraph "GPU Nodes"
        N1["Node 1<br/>8x A100"]
        N2["Node 2<br/>8x A100"]
        N3["Node 3<br/>8x H100"]
    end

    DRA --> DRIVER --> N1 & N2 & N3
    DCGM --> N1 & N2 & N3
    DCGM --> PROM --> GRAF
    NCCL --> EFA --> N1 & N2 & N3
    KARP --> N1 & N2 & N3
    GPU_OP --> DRIVER & DCGM

    style DRA fill:#326ce5
    style DCGM fill:#76b900
    style NCCL fill:#76b900
    style KARP fill:#ffd93d
```

<GpuInfraStack />

---

## ç»“è®ºï¼šä¸ºä»€ä¹ˆ Agentic AI é€‰æ‹© Kubernetesï¼Ÿ

Kubernetes æä¾›äº†ä½¿ç°ä»£ Agentic AI å¹³å°æˆä¸ºå¯èƒ½çš„**åŸºç¡€è®¾æ–½åº•å±‚**ï¼š

### æ ¸å¿ƒä¼˜åŠ¿

1. **ç»Ÿä¸€å¹³å°**ï¼šæ¨ç†ã€è®­ç»ƒå’Œç¼–æ’çš„å•ä¸€å¹³å°
2. **å£°æ˜å¼ç®¡ç†**ï¼šåŸºç¡€è®¾æ–½å³ä»£ç ï¼Œæ”¯æŒç‰ˆæœ¬æ§åˆ¶
3. **ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿ**ï¼šé¢å‘ AI å·¥ä½œè´Ÿè½½çš„å¤§é‡å¼€æºè§£å†³æ–¹æ¡ˆ
4. **äº‘ç«¯å¯ç§»æ¤æ€§**ï¼šéšå¤„è¿è¡Œï¼ˆæœ¬åœ°ã€AWSã€GCPã€Azureï¼‰
5. **æˆç†Ÿçš„å·¥å…·é“¾**ï¼škubectlã€Helmã€Operatorã€ç›‘æ§æ ˆ
6. **æ´»è·ƒçš„ç¤¾åŒº**ï¼šKubernetes AI/ML SIG æ¨åŠ¨åˆ›æ–°

### å‰è¿›ä¹‹è·¯

```mermaid
graph LR
    START["Agentic AI<br/>Requirements"] --> K8S["Kubernetes<br/>Foundation"]
    K8S --> OSS["Open Source<br/>Ecosystem"]
    OSS --> CLOUD["Cloud Provider<br/>Integration"]
    CLOUD --> SOLUTION["Complete<br/>AI Platform"]

    style START fill:#ff6b6b
    style K8S fill:#326ce5
    style OSS fill:#2ecc71
    style CLOUD fill:#ff9900
    style SOLUTION fill:#4ecdc4
```

å¯¹äºæ„å»º Agentic AI å¹³å°çš„ç»„ç»‡ï¼š

1. **ä» Kubernetes å¼€å§‹**ï¼šåœ¨å›¢é˜Ÿä¸­å»ºç«‹ Kubernetes ä¸“ä¸šèƒ½åŠ›
2. **é‡‡ç”¨å¼€æº**ï¼šåˆ©ç”¨ç»è¿‡éªŒè¯çš„è§£å†³æ–¹æ¡ˆï¼ˆvLLMã€LangFuse ç­‰ï¼‰
3. **ä¸äº‘é›†æˆ**ï¼šå°†å¼€æºä¸æ‰˜ç®¡æœåŠ¡ç›¸ç»“åˆ
4. **è‡ªåŠ¨åŒ–åŸºç¡€è®¾æ–½**ï¼šå®æ–½è‡ªåŠ¨æ‰©å±•å’Œè‡ªåŠ¨é…ç½®
5. **å…¨é¢å¯è§‚æµ‹**ï¼šä»ç¬¬ä¸€å¤©èµ·å°±å»ºç«‹å…¨é¢çš„å¯è§‚æµ‹æ€§

:::info ä¸‹ä¸€æ­¥ï¼šåŸºäº EKS çš„è§£å†³æ–¹æ¡ˆ
æœ‰å…³ä½¿ç”¨ **Amazon EKS å’Œ AWS æœåŠ¡**åº”å¯¹è¿™äº›æŒ‘æˆ˜çš„è¯¦ç»†è§£å†³æ–¹æ¡ˆï¼Œè¯·å‚é˜… [åŸºäº EKS çš„ Agentic AI è§£å†³æ–¹æ¡ˆ](./agentic-ai-solutions-eks.md)ã€‚
:::

---

## åç»­æ­¥éª¤

æœ¬æ–‡æ¢è®¨äº† Agentic AI å·¥ä½œè´Ÿè½½çš„ 4 å¤§æ ¸å¿ƒæŒ‘æˆ˜åŠåŸºäº Kubernetes çš„å¼€æºç”Ÿæ€ç³»ç»Ÿã€‚

:::info ä¸‹ä¸€æ­¥ï¼šåŸºäº EKS çš„è§£å†³æ–¹æ¡ˆ
æœ‰å…³ä½¿ç”¨ **Amazon EKS å’Œ AWS æœåŠ¡**è§£å†³æœ¬æ–‡ä»‹ç»çš„æŒ‘æˆ˜çš„å…·ä½“æ–¹æ³•ï¼Œè¯·å‚é˜… [åŸºäº EKS çš„ Agentic AI è§£å†³æ–¹æ¡ˆ](./agentic-ai-solutions-eks.md)ã€‚

ä¸‹ä¸€ç¯‡æ–‡æ¡£æ¶µç›–çš„ä¸»é¢˜ï¼š

- ä½¿ç”¨ EKS Auto Mode æ„å»ºå…¨è‡ªåŠ¨åŒ–é›†ç¾¤
- ä½¿ç”¨ Karpenter è¿›è¡Œ GPU èŠ‚ç‚¹è‡ªåŠ¨é…ç½®
- ä¸ AWS æœåŠ¡é›†æˆï¼ˆBedrockã€S3ã€CloudWatchï¼‰
- ç”Ÿäº§ç¯å¢ƒçš„å®‰å…¨å’Œè¿ç»´ç­–ç•¥
- å®æˆ˜éƒ¨ç½²æŒ‡å—ä¸æ•…éšœæ’é™¤
:::

---

## å‚è€ƒèµ„æ–™

### Kubernetes ä¸åŸºç¡€è®¾æ–½

- [Kubernetes å®˜æ–¹æ–‡æ¡£](https://kubernetes.io/docs/)
- [Karpenter å®˜æ–¹æ–‡æ¡£](https://karpenter.sh/docs/)
- [Amazon EKS æœ€ä½³å®è·µæŒ‡å—](https://docs.aws.amazon.com/eks/latest/best-practices/introduction.html)
- [NVIDIA GPU Operator æ–‡æ¡£](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html)
- [KEDA - Kubernetes äº‹ä»¶é©±åŠ¨è‡ªåŠ¨æ‰©å±•](https://keda.sh/)

### æ¨¡å‹æœåŠ¡ä¸æ¨ç†

- [vLLM æ–‡æ¡£](https://docs.vllm.ai/)
- [llm-d é¡¹ç›®](https://github.com/llm-d/llm-d)
- [Kgateway æ–‡æ¡£](https://kgateway.io/docs/)
- [LiteLLM æ–‡æ¡£](https://docs.litellm.ai/)

### LLM å¯è§‚æµ‹æ€§

- [LangFuse æ–‡æ¡£](https://langfuse.com/docs)
- [LangSmith æ–‡æ¡£](https://docs.smith.langchain.com/)
- [RAGAS æ–‡æ¡£](https://docs.ragas.io/)

### å‘é‡æ•°æ®åº“

- [Milvus æ–‡æ¡£](https://milvus.io/docs)
- [Milvus Operator](https://github.com/milvus-io/milvus-operator)

### GPU åŸºç¡€è®¾æ–½

- [NVIDIA GPU Operator æ–‡æ¡£](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/)
- [DCGM Exporter](https://github.com/NVIDIA/dcgm-exporter)
- [NCCL æ–‡æ¡£](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html)
- [AWS EFA æ–‡æ¡£](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html)

### Agent æ¡†æ¶ä¸è®­ç»ƒ

- [KAgent - Kubernetes Agent æ¡†æ¶](https://github.com/kagent-dev/kagent)
- [NVIDIA NeMo æ¡†æ¶](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)
- [Kubeflow æ–‡æ¡£](https://www.kubeflow.org/docs/)
