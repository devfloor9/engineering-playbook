---
title: "åœ¨ EKS ä¸Šæ„å»º MLOps æµæ°´çº¿"
sidebar_label: "15. MLOps Pipeline"
description: "åŸºäº Kubeflow + MLflow + KServe çš„ç«¯åˆ°ç«¯ ML ç”Ÿå‘½å‘¨æœŸç®¡ç†"
sidebar_position: 15
category: "genai-aiml"
tags: [mlops, kubeflow, mlflow, kserve, argo-workflows, eks, ml-pipeline]
last_update:
  date: 2026-02-14
  author: devfloor9
---

import SpecificationTable from '@site/src/components/tables/SpecificationTable';
import { PipelineComponents, KServeVsSeldon } from '@site/src/components/MlOpsTables';

# åœ¨ EKS ä¸Šæ„å»º MLOps æµæ°´çº¿

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2025-02-05 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 23 åˆ†é’Ÿ

## æ¦‚è¿°

MLOps æ˜¯ä¸€å¥—å°†æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¼€å‘ã€éƒ¨ç½²å’Œè¿ç»´è¿›è¡Œè‡ªåŠ¨åŒ–ä¸æ ‡å‡†åŒ–çš„å®è·µæ–¹æ³•è®ºã€‚æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•åœ¨ Amazon EKS ç¯å¢ƒä¸­åˆ©ç”¨ Kubeflow Pipelinesã€MLflow å’Œ KServeï¼Œæ„å»ºä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹æœåŠ¡çš„ç«¯åˆ°ç«¯ ML ç”Ÿå‘½å‘¨æœŸã€‚

### ä¸»è¦ç›®æ ‡

- **å®Œå…¨è‡ªåŠ¨åŒ–**ï¼šæ„å»ºä»æ•°æ®é‡‡é›†åˆ°æ¨¡å‹éƒ¨ç½²çš„å…¨è‡ªåŠ¨æµæ°´çº¿
- **å®éªŒè¿½è¸ª**ï¼šé€šè¿‡ MLflow å®ç°ç³»ç»ŸåŒ–çš„å®éªŒç®¡ç†å’Œæ¨¡å‹ç‰ˆæœ¬ç®¡ç†
- **å¯æ‰©å±•çš„æœåŠ¡**ï¼šåŸºäº KServe çš„é«˜æ€§èƒ½æ¨¡å‹æœåŠ¡åŸºç¡€è®¾æ–½
- **GPU ä¼˜åŒ–**ï¼šåˆ©ç”¨ Karpenter å®ç°åŠ¨æ€ GPU èµ„æºç®¡ç†

---

## MLOps æ¶æ„æ¦‚è§ˆ

### ç«¯åˆ°ç«¯ ML ç”Ÿå‘½å‘¨æœŸ

```mermaid
flowchart TB
    subgraph "Data Layer"
        S3[S3 Data Lake]
        RDS[(RDS/Aurora<br/>Feature Store)]
    end

    subgraph "Development"
        NOTEBOOK[Jupyter Notebooks<br/>SageMaker Studio]
        EXPERIMENT[MLflow Tracking<br/>Experiment Management]
    end

    subgraph "Training Pipeline - Kubeflow"
        DATA_PREP[Data Preparation<br/>Component]
        FEATURE_ENG[Feature Engineering<br/>Component]
        TRAIN[Model Training<br/>GPU Jobs]
        EVAL[Model Evaluation<br/>Component]
        REGISTER[Model Registry<br/>MLflow]
    end

    subgraph "Serving Layer - KServe"
        PREDICTOR[Predictor<br/>Model Server]
        TRANSFORMER[Transformer<br/>Pre/Post Processing]
        EXPLAINER[Explainer<br/>Model Interpretability]
    end

    subgraph "Monitoring"
        PROMETHEUS[Prometheus<br/>Metrics]
        GRAFANA[Grafana<br/>Dashboards]
        DRIFT[Drift Detection<br/>Evidently AI]
    end

    S3 --> DATA_PREP
    RDS --> FEATURE_ENG
    NOTEBOOK --> EXPERIMENT
    EXPERIMENT --> TRAIN

    DATA_PREP --> FEATURE_ENG
    FEATURE_ENG --> TRAIN
    TRAIN --> EVAL
    EVAL --> REGISTER

    REGISTER --> PREDICTOR
    PREDICTOR --> TRANSFORMER
    TRANSFORMER --> EXPLAINER

    PREDICTOR --> PROMETHEUS
    PROMETHEUS --> GRAFANA
    PREDICTOR --> DRIFT
```

### æ ¸å¿ƒç»„ä»¶

<PipelineComponents />

---

## Kubeflow Pipelines æ¶æ„

### Kubeflow å®‰è£…ï¼ˆAWS å‘è¡Œç‰ˆï¼‰

AWS æä¾›äº† Kubeflow on AWS å‘è¡Œç‰ˆï¼Œæä¾›ä¸ EKS é›†æˆçš„é…ç½®ã€‚

```bash
# å®‰è£… Kubeflow on AWSï¼ˆv1.9+ï¼‰
export KUBEFLOW_RELEASE_VERSION=v1.9.0
export AWS_CLUSTER_NAME=ml-cluster
export AWS_REGION=us-west-2

# ä¸‹è½½ Kubeflow æ¸…å•
git clone https://github.com/awslabs/kubeflow-manifests.git
cd kubeflow-manifests
git checkout ${KUBEFLOW_RELEASE_VERSION}

# ä½¿ç”¨ Kustomize éƒ¨ç½²
while ! kustomize build deployments/vanilla | kubectl apply -f -; do echo "Retrying to apply resources"; sleep 10; done
```

### Kubeflow æ¶æ„

```mermaid
flowchart TB
    subgraph "Kubeflow Platform"
        UI[Central Dashboard<br/>Web UI]

        subgraph "Pipelines"
            KFP_API[Pipelines API Server]
            KFP_ENGINE[Argo Workflows<br/>Execution Engine]
            KFP_DB[(MySQL<br/>Metadata Store)]
        end

        subgraph "Notebooks"
            JUPYTER[Jupyter Notebooks<br/>Development Environment]
        end

        subgraph "Training"
            TRAINING_OP[Training Operator<br/>TFJob, PyTorchJob]
        end

        subgraph "Serving"
            KSERVE[KServe<br/>InferenceService]
        end

        subgraph "Metadata"
            MLMD[ML Metadata<br/>Artifact Tracking]
        end
    end

    UI --> KFP_API
    UI --> JUPYTER
    KFP_API --> KFP_ENGINE
    KFP_ENGINE --> KFP_DB
    KFP_ENGINE --> TRAINING_OP
    TRAINING_OP --> KSERVE
    KFP_ENGINE --> MLMD
```

### ç¼–å†™ Kubeflow Pipelines ç»„ä»¶

Kubeflow Pipelines é€šè¿‡ Python SDK å®šä¹‰å¯å¤ç”¨çš„ç»„ä»¶ã€‚

```python
# pipeline_components.py
from kfp import dsl
from kfp.dsl import Input, Output, Dataset, Model, Metrics

@dsl.component(
    base_image="python:3.10",
    packages_to_install=["pandas", "scikit-learn", "boto3"]
)
def data_preparation(
    s3_input_path: str,
    output_dataset: Output[Dataset],
    train_split: float = 0.8
):
    """æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†ç»„ä»¶"""
    import pandas as pd
    import boto3
    from sklearn.model_selection import train_test_split

    # ä» S3 åŠ è½½æ•°æ®
    s3 = boto3.client('s3')
    bucket, key = s3_input_path.replace("s3://", "").split("/", 1)

    obj = s3.get_object(Bucket=bucket, Key=key)
    df = pd.read_csv(obj['Body'])

    # æ•°æ®é¢„å¤„ç†
    df = df.dropna()
    df = df.drop_duplicates()

    # Train/Test æ‹†åˆ†
    train_df, test_df = train_test_split(df, train_size=train_split, random_state=42)

    # ä¿å­˜è¾“å‡º
    output_path = output_dataset.path
    train_df.to_csv(f"{output_path}/train.csv", index=False)
    test_df.to_csv(f"{output_path}/test.csv", index=False)

    print(f"Train samples: {len(train_df)}, Test samples: {len(test_df)}")


@dsl.component(
    base_image="python:3.10",
    packages_to_install=["pandas", "scikit-learn", "mlflow", "boto3"]
)
def feature_engineering(
    input_dataset: Input[Dataset],
    output_features: Output[Dataset],
    feature_columns: list
):
    """ç‰¹å¾å·¥ç¨‹ç»„ä»¶"""
    import pandas as pd
    from sklearn.preprocessing import StandardScaler
    import pickle

    # åŠ è½½æ•°æ®
    train_df = pd.read_csv(f"{input_dataset.path}/train.csv")
    test_df = pd.read_csv(f"{input_dataset.path}/test.csv")

    # ç‰¹å¾é€‰æ‹©
    X_train = train_df[feature_columns]
    X_test = test_df[feature_columns]

    # ç¼©æ”¾
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ä¿å­˜ç¼©æ”¾å™¨
    with open(f"{output_features.path}/scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)

    # ä¿å­˜è½¬æ¢åçš„æ•°æ®
    pd.DataFrame(X_train_scaled, columns=feature_columns).to_csv(
        f"{output_features.path}/X_train.csv", index=False
    )
    pd.DataFrame(X_test_scaled, columns=feature_columns).to_csv(
        f"{output_features.path}/X_test.csv", index=False
    )

    train_df['target'].to_csv(f"{output_features.path}/y_train.csv", index=False)
    test_df['target'].to_csv(f"{output_features.path}/y_test.csv", index=False)


@dsl.component(
    base_image="pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime",
    packages_to_install=["mlflow", "scikit-learn", "boto3"]
)
def model_training(
    input_features: Input[Dataset],
    output_model: Output[Model],
    mlflow_tracking_uri: str,
    experiment_name: str,
    learning_rate: float = 0.001,
    epochs: int = 10,
    batch_size: int = 32
):
    """æ¨¡å‹è®­ç»ƒç»„ä»¶ï¼ˆPyTorchï¼‰"""
    import pandas as pd
    import torch
    import torch.nn as nn
    import mlflow
    import mlflow.pytorch

    # MLflow é…ç½®
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    mlflow.set_experiment(experiment_name)

    # åŠ è½½æ•°æ®
    X_train = pd.read_csv(f"{input_features.path}/X_train.csv").values
    y_train = pd.read_csv(f"{input_features.path}/y_train.csv").values.ravel()

    # PyTorch æ•°æ®é›†
    X_tensor = torch.FloatTensor(X_train)
    y_tensor = torch.FloatTensor(y_train)
    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # å®šä¹‰æ¨¡å‹
    class SimpleNN(nn.Module):
        def __init__(self, input_dim):
            super().__init__()
            self.fc1 = nn.Linear(input_dim, 64)
            self.fc2 = nn.Linear(64, 32)
            self.fc3 = nn.Linear(32, 1)
            self.relu = nn.ReLU()

        def forward(self, x):
            x = self.relu(self.fc1(x))
            x = self.relu(self.fc2(x))
            return self.fc3(x)

    model = SimpleNN(X_train.shape[1])
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # å¼€å§‹ MLflow å®éªŒ
    with mlflow.start_run():
        mlflow.log_params({
            "learning_rate": learning_rate,
            "epochs": epochs,
            "batch_size": batch_size
        })

        # è®­ç»ƒ
        for epoch in range(epochs):
            total_loss = 0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = model(batch_X).squeeze()
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            mlflow.log_metric("train_loss", avg_loss, step=epoch)
            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        # ä¿å­˜æ¨¡å‹
        model_path = f"{output_model.path}/model.pth"
        torch.save(model.state_dict(), model_path)
        mlflow.pytorch.log_model(model, "model")

        # ä¿å­˜æ¨¡å‹ URI
        run_id = mlflow.active_run().info.run_id
        model_uri = f"runs:/{run_id}/model"

        with open(f"{output_model.path}/model_uri.txt", "w") as f:
            f.write(model_uri)


@dsl.component(
    base_image="python:3.10",
    packages_to_install=["pandas", "torch", "scikit-learn", "mlflow"]
)
def model_evaluation(
    input_features: Input[Dataset],
    input_model: Input[Model],
    output_metrics: Output[Metrics],
    mlflow_tracking_uri: str
):
    """æ¨¡å‹è¯„ä¼°ç»„ä»¶"""
    import pandas as pd
    import torch
    import mlflow
    from sklearn.metrics import mean_squared_error, r2_score
    import json

    mlflow.set_tracking_uri(mlflow_tracking_uri)

    # åŠ è½½æµ‹è¯•æ•°æ®
    X_test = pd.read_csv(f"{input_features.path}/X_test.csv").values
    y_test = pd.read_csv(f"{input_features.path}/y_test.csv").values.ravel()

    # åŠ è½½æ¨¡å‹
    with open(f"{input_model.path}/model_uri.txt", "r") as f:
        model_uri = f.read().strip()

    model = mlflow.pytorch.load_model(model_uri)
    model.eval()

    # é¢„æµ‹
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_test)
        predictions = model(X_tensor).squeeze().numpy()

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mse = mean_squared_error(y_test, predictions)
    rmse = mse ** 0.5
    r2 = r2_score(y_test, predictions)

    # è®°å½•æŒ‡æ ‡
    with mlflow.start_run():
        mlflow.log_metrics({
            "test_mse": mse,
            "test_rmse": rmse,
            "test_r2": r2
        })

    # ä¿å­˜è¾“å‡ºæŒ‡æ ‡
    metrics = {
        "mse": mse,
        "rmse": rmse,
        "r2": r2
    }

    with open(output_metrics.path, "w") as f:
        json.dump(metrics, f)

    print(f"Evaluation Metrics - MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}")
```

### æµæ°´çº¿å®šä¹‰

```python
# ml_pipeline.py
from kfp import dsl

@dsl.pipeline(
    name="End-to-End ML Pipeline",
    description="Complete ML pipeline from data prep to model evaluation"
)
def ml_pipeline(
    s3_input_path: str = "s3://my-bucket/data/input.csv",
    mlflow_tracking_uri: str = "http://mlflow-server.mlflow.svc.cluster.local:5000",
    experiment_name: str = "eks-ml-experiment",
    feature_columns: list = ["feature1", "feature2", "feature3"],
    learning_rate: float = 0.001,
    epochs: int = 10,
    batch_size: int = 32
):
    # 1. æ•°æ®å‡†å¤‡
    data_prep_task = data_preparation(
        s3_input_path=s3_input_path,
        train_split=0.8
    )

    # 2. ç‰¹å¾å·¥ç¨‹
    feature_eng_task = feature_engineering(
        input_dataset=data_prep_task.outputs["output_dataset"],
        feature_columns=feature_columns
    )

    # 3. æ¨¡å‹è®­ç»ƒï¼ˆåœ¨ GPU èŠ‚ç‚¹ä¸Šè¿è¡Œï¼‰
    train_task = model_training(
        input_features=feature_eng_task.outputs["output_features"],
        mlflow_tracking_uri=mlflow_tracking_uri,
        experiment_name=experiment_name,
        learning_rate=learning_rate,
        epochs=epochs,
        batch_size=batch_size
    )
    train_task.set_gpu_limit(1)
    train_task.add_node_selector_constraint("node.kubernetes.io/instance-type", "g5.xlarge")

    # 4. æ¨¡å‹è¯„ä¼°
    eval_task = model_evaluation(
        input_features=feature_eng_task.outputs["output_features"],
        input_model=train_task.outputs["output_model"],
        mlflow_tracking_uri=mlflow_tracking_uri
    )

    return eval_task.outputs["output_metrics"]


# ç¼–è¯‘å¹¶è¿è¡Œæµæ°´çº¿
if __name__ == "__main__":
    from kfp import compiler

    compiler.Compiler().compile(
        pipeline_func=ml_pipeline,
        package_path="ml_pipeline.yaml"
    )

    # ä½¿ç”¨ Kubeflow Pipelines å®¢æˆ·ç«¯è¿è¡Œ
    import kfp
    client = kfp.Client(host="http://kubeflow-pipelines.kubeflow.svc.cluster.local:8888")

    run = client.create_run_from_pipeline_func(
        ml_pipeline,
        arguments={
            "s3_input_path": "s3://my-ml-bucket/data/training_data.csv",
            "experiment_name": "production-model-v1",
            "epochs": 20,
            "learning_rate": 0.0005
        }
    )

    print(f"Pipeline run created: {run.run_id}")
```

---

## MLflow é›†æˆ

### MLflow æ¶æ„

MLflow æ˜¯ä¸€ä¸ªç”¨äº ML å®éªŒè¿½è¸ªã€æ¨¡å‹æ³¨å†Œä¸­å¿ƒå’Œæ¨¡å‹éƒ¨ç½²çš„å¼€æºå¹³å°ã€‚

```mermaid
flowchart TB
    subgraph "MLflow Platform"
        UI[MLflow UI<br/>Web Interface]

        subgraph "Tracking Server"
            API[REST API<br/>Tracking API]
            BACKEND[(PostgreSQL<br/>Metadata Store)]
        end

        subgraph "Artifact Store"
            S3[S3 Bucket<br/>Models & Artifacts]
        end

        subgraph "Model Registry"
            REGISTRY[Model Registry<br/>Version Management]
            STAGES[Staging â†’ Production<br/>Lifecycle Management]
        end
    end

    subgraph "Clients"
        NOTEBOOK[Jupyter Notebooks]
        PIPELINE[Kubeflow Pipelines]
        CICD[CI/CD Workflows]
    end

    NOTEBOOK --> API
    PIPELINE --> API
    CICD --> API

    API --> BACKEND
    API --> S3
    API --> REGISTRY
    REGISTRY --> STAGES
    UI --> API
```

### MLflow éƒ¨ç½² YAML

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: mlflow
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mlflow-config
  namespace: mlflow
data:
  MLFLOW_S3_ENDPOINT_URL: "https://s3.us-west-2.amazonaws.com"
  AWS_DEFAULT_REGION: "us-west-2"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlflow-server
  namespace: mlflow
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mlflow-server
  template:
    metadata:
      labels:
        app: mlflow-server
    spec:
      serviceAccountName: mlflow-sa
      containers:
        - name: mlflow
          image: ghcr.io/mlflow/mlflow:v2.10.2
          ports:
            - name: http
              containerPort: 5000
          env:
            - name: MLFLOW_S3_ENDPOINT_URL
              valueFrom:
                configMapKeyRef:
                  name: mlflow-config
                  key: MLFLOW_S3_ENDPOINT_URL
            - name: AWS_DEFAULT_REGION
              valueFrom:
                configMapKeyRef:
                  name: mlflow-config
                  key: AWS_DEFAULT_REGION
          command:
            - mlflow
            - server
            - --host
            - "0.0.0.0"
            - --port
            - "5000"
            - --backend-store-uri
            - "postgresql://mlflow:password@postgres-service.mlflow.svc.cluster.local:5432/mlflow"
            - --default-artifact-root
            - "s3://my-mlflow-artifacts/"
            - --serve-artifacts
          resources:
            requests:
              memory: "2Gi"
              cpu: "1"
            limits:
              memory: "4Gi"
              cpu: "2"
          livenessProbe:
            httpGet:
              path: /health
              port: 5000
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 5000
            initialDelaySeconds: 10
            periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: mlflow-server
  namespace: mlflow
spec:
  type: ClusterIP
  ports:
    - port: 5000
      targetPort: 5000
      protocol: TCP
  selector:
    app: mlflow-server
```

### PostgreSQL åç«¯éƒ¨ç½²

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: mlflow
spec:
  serviceName: postgres-service
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:15
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_DB
              value: "mlflow"
            - name: POSTGRES_USER
              value: "mlflow"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "2Gi"
              cpu: "1"
            limits:
              memory: "4Gi"
              cpu: "2"
  volumeClaimTemplates:
    - metadata:
        name: postgres-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: gp3
        resources:
          requests:
            storage: 50Gi
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  namespace: mlflow
spec:
  type: ClusterIP
  ports:
    - port: 5432
      targetPort: 5432
  selector:
    app: postgres
```

---

## KServe vs Seldon Core æ¯”è¾ƒ

### åŠŸèƒ½æ¯”è¾ƒ

<KServeVsSeldon />

### KServe æ¶æ„

```mermaid
flowchart TB
    subgraph "KServe InferenceService"
        INGRESS[Istio Ingress<br/>Gateway]

        subgraph "Data Plane"
            PREDICTOR[Predictor<br/>Model Server]
            TRANSFORMER[Transformer<br/>Pre/Post Processing]
            EXPLAINER[Explainer<br/>Model Interpretability]
        end

        subgraph "Control Plane"
            CONTROLLER[KServe Controller<br/>Reconciliation]
            KNATIVE[Knative Serving<br/>Serverless Runtime]
        end
    end

    INGRESS --> TRANSFORMER
    TRANSFORMER --> PREDICTOR
    PREDICTOR --> EXPLAINER

    CONTROLLER --> KNATIVE
    KNATIVE --> PREDICTOR
```

### KServe å®‰è£…

```bash
# å®‰è£… Knative Servingï¼ˆKServe ä¾èµ–é¡¹ï¼‰
kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.12.0/serving-crds.yaml
kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.12.0/serving-core.yaml

# Istio ç½‘ç»œå±‚
kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.12.0/net-istio.yaml

# å®‰è£… KServe
kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.12.0/kserve.yaml
kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.12.0/kserve-runtimes.yaml
```

### KServe InferenceService ç¤ºä¾‹

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: sklearn-iris
  namespace: kserve-inference
spec:
  predictor:
    model:
      modelFormat:
        name: sklearn
      storageUri: s3://my-models/sklearn/iris
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"
        limits:
          cpu: "2"
          memory: "4Gi"
  transformer:
    containers:
      - name: transformer
        image: my-registry/iris-transformer:v1
        env:
          - name: STORAGE_URI
            value: s3://my-models/sklearn/iris
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: pytorch-bert
  namespace: kserve-inference
spec:
  predictor:
    pytorch:
      storageUri: s3://my-models/pytorch/bert
      resources:
        requests:
          nvidia.com/gpu: 1
          memory: "8Gi"
        limits:
          nvidia.com/gpu: 1
          memory: "16Gi"
      env:
        - name: TORCH_SERVE_WORKERS
          value: "2"
  minReplicas: 1
  maxReplicas: 10
  scaleTarget: 80
  scaleMetric: concurrency
```

### Seldon Core éƒ¨ç½²ç¤ºä¾‹

```yaml
apiVersion: machinelearning.seldon.io/v1
kind: SeldonDeployment
metadata:
  name: sklearn-iris-seldon
  namespace: seldon-system
spec:
  name: iris-model
  predictors:
    - name: default
      replicas: 2
      graph:
        name: classifier
        implementation: SKLEARN_SERVER
        modelUri: s3://my-models/sklearn/iris
        parameters:
          - name: method
            value: predict_proba
            type: STRING
      componentSpecs:
        - spec:
            containers:
              - name: classifier
                resources:
                  requests:
                    cpu: "1"
                    memory: "2Gi"
                  limits:
                    cpu: "2"
                    memory: "4Gi"
      svcOrchSpec:
        env:
          - name: SELDON_LOG_LEVEL
            value: INFO
---
apiVersion: machinelearning.seldon.io/v1
kind: SeldonDeployment
metadata:
  name: pytorch-transformer
  namespace: seldon-system
spec:
  name: bert-model
  predictors:
    - name: default
      replicas: 1
      graph:
        name: transformer
        type: TRANSFORMER
        endpoint:
          type: REST
        children:
          - name: model
            implementation: PYTORCH_SERVER
            modelUri: s3://my-models/pytorch/bert
            parameters:
              - name: model_name
                value: bert-base-uncased
                type: STRING
      componentSpecs:
        - spec:
            containers:
              - name: model
                resources:
                  requests:
                    nvidia.com/gpu: 1
                    memory: "8Gi"
                  limits:
                    nvidia.com/gpu: 1
                    memory: "16Gi"
```

---

## Argo Workflows CI/CD é›†æˆ

### Argo Workflows æ¶æ„

```mermaid
flowchart TB
    subgraph "CI/CD Pipeline"
        GIT[Git Repository<br/>Model Code]
        TRIGGER[Webhook Trigger<br/>GitHub/GitLab]

        subgraph "Argo Workflows"
            WORKFLOW[Workflow Controller]

            subgraph "Pipeline Steps"
                BUILD[Build Container<br/>Docker Image]
                TEST[Run Tests<br/>Unit + Integration]
                TRAIN[Train Model<br/>Kubeflow Pipeline]
                VALIDATE[Validate Model<br/>Performance Checks]
                DEPLOY[Deploy to KServe<br/>InferenceService]
            end
        end

        REGISTRY[Container Registry<br/>ECR/Docker Hub]
        KSERVE[KServe<br/>Production Serving]
    end

    GIT --> TRIGGER
    TRIGGER --> WORKFLOW
    WORKFLOW --> BUILD
    BUILD --> TEST
    TEST --> TRAIN
    TRAIN --> VALIDATE
    VALIDATE --> DEPLOY

    BUILD --> REGISTRY
    DEPLOY --> KSERVE
```

### Argo Workflow ç¤ºä¾‹

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ml-cicd-pipeline-
  namespace: argo
spec:
  entrypoint: ml-pipeline
  serviceAccountName: argo-workflow-sa

  arguments:
    parameters:
      - name: git-repo
        value: "https://github.com/myorg/ml-model.git"
      - name: git-branch
        value: "main"
      - name: model-name
        value: "fraud-detection-v2"
      - name: s3-model-path
        value: "s3://my-models/fraud-detection/v2"

  templates:
    - name: ml-pipeline
      steps:
        - - name: clone-repo
            template: git-clone

        - - name: build-image
            template: docker-build

        - - name: run-tests
            template: pytest-tests

        - - name: train-model
            template: kubeflow-training

        - - name: validate-model
            template: model-validation

        - - name: deploy-model
            template: kserve-deployment
            when: "{{steps.validate-model.outputs.result}} == passed"

    - name: git-clone
      container:
        image: alpine/git:latest
        command: [sh, -c]
        args:
          - |
            git clone {{workflow.parameters.git-repo}} /workspace
            cd /workspace && git checkout {{workflow.parameters.git-branch}}
        volumeMounts:
          - name: workspace
            mountPath: /workspace

    - name: docker-build
      container:
        image: gcr.io/kaniko-project/executor:latest
        args:
          - --dockerfile=/workspace/Dockerfile
          - --context=/workspace
          - --destination=my-registry/{{workflow.parameters.model-name}}:{{workflow.uid}}
          - --cache=true
        volumeMounts:
          - name: workspace
            mountPath: /workspace
          - name: docker-config
            mountPath: /kaniko/.docker

    - name: pytest-tests
      container:
        image: python:3.10
        command: [sh, -c]
        args:
          - |
            cd /workspace
            pip install -r requirements.txt
            pytest tests/ --junitxml=test-results.xml
        volumeMounts:
          - name: workspace
            mountPath: /workspace

    - name: kubeflow-training
      resource:
        action: create
        manifest: |
          apiVersion: kubeflow.org/v1
          kind: PyTorchJob
          metadata:
            name: {{workflow.parameters.model-name}}-{{workflow.uid}}
            namespace: kubeflow
          spec:
            pytorchReplicaSpecs:
              Master:
                replicas: 1
                template:
                  spec:
                    containers:
                      - name: pytorch
                        image: my-registry/{{workflow.parameters.model-name}}:{{workflow.uid}}
                        command:
                          - python
                          - train.py
                          - --output-path
                          - {{workflow.parameters.s3-model-path}}
                        resources:
                          requests:
                            nvidia.com/gpu: 1
                          limits:
                            nvidia.com/gpu: 1

    - name: model-validation
      script:
        image: python:3.10
        command: [python]
        source: |
          import mlflow
          import json

          mlflow.set_tracking_uri("http://mlflow-server.mlflow.svc.cluster.local:5000")

          # åŠ è½½æœ€æ–°æ¨¡å‹
          model_uri = "{{workflow.parameters.s3-model-path}}"

          # æ£€æŸ¥éªŒè¯æŒ‡æ ‡
          # ï¼ˆå®é™…ä¸Šä½¿ç”¨æµ‹è¯•æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼‰
          metrics = {
              "accuracy": 0.95,
              "precision": 0.93,
              "recall": 0.94
          }

          # é˜ˆå€¼éªŒè¯
          if metrics["accuracy"] >= 0.90:
              print("passed")
          else:
              print("failed")

    - name: kserve-deployment
      resource:
        action: apply
        manifest: |
          apiVersion: serving.kserve.io/v1beta1
          kind: InferenceService
          metadata:
            name: {{workflow.parameters.model-name}}
            namespace: kserve-inference
          spec:
            predictor:
              pytorch:
                storageUri: {{workflow.parameters.s3-model-path}}
                resources:
                  requests:
                    nvidia.com/gpu: 1
                    memory: "8Gi"
                  limits:
                    nvidia.com/gpu: 1
                    memory: "16Gi"
            minReplicas: 2
            maxReplicas: 10

  volumeClaimTemplates:
    - metadata:
        name: workspace
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 10Gi
```

---

## GPU èµ„æºè°ƒåº¦ï¼ˆKarpenterï¼‰

### Karpenter æ¶æ„

```mermaid
flowchart TB
    subgraph "Karpenter Autoscaling"
        CONTROLLER[Karpenter Controller<br/>Node Provisioner]

        subgraph "Node Provisioning"
            PENDING[Pending Pods<br/>GPU Required]
            PROVISION[Provision Decision<br/>Instance Selection]
            EC2[EC2 Launch<br/>g5/p4/p5 Instances]
        end

        subgraph "Cost Optimization"
            SPOT[Spot Instances<br/>70% Cost Savings]
            CONSOLIDATION[Node Consolidation<br/>Bin Packing]
            TERMINATION[Graceful Termination<br/>Workload Migration]
        end
    end

    PENDING --> CONTROLLER
    CONTROLLER --> PROVISION
    PROVISION --> EC2
    PROVISION --> SPOT

    CONTROLLER --> CONSOLIDATION
    CONSOLIDATION --> TERMINATION
```

### Karpenter NodePool é…ç½®

```yaml
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: gpu-training
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["g5.xlarge", "g5.2xlarge", "g5.4xlarge", "g5.8xlarge"]
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.k8s.aws/instance-gpu-count
          operator: Gt
          values: ["0"]

      nodeClassRef:
        name: gpu-node-class

      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule

  limits:
    cpu: "1000"
    memory: "4000Gi"
    nvidia.com/gpu: "50"

  disruption:
    consolidationPolicy: WhenUnderutilized
    expireAfter: 720h  # 30 å¤©

  weight: 10
---
apiVersion: karpenter.k8s.aws/v1beta1
kind: EC2NodeClass
metadata:
  name: gpu-node-class
spec:
  amiFamily: AL2
  role: KarpenterNodeRole-ml-cluster

  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: ml-cluster

  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: ml-cluster

  userData: |
    #!/bin/bash
    # å®‰è£… NVIDIA é©±åŠ¨
    /etc/eks/bootstrap.sh ml-cluster \
      --kubelet-extra-args '--max-pods=110'

    # NVIDIA Container Toolkit
    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
    curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | \
      sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

    sudo yum install -y nvidia-container-toolkit
    sudo nvidia-ctk runtime configure --runtime=containerd
    sudo systemctl restart containerd

  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 100Gi
        volumeType: gp3
        iops: 3000
        throughput: 125
        encrypted: true

  metadataOptions:
    httpEndpoint: enabled
    httpProtocolIPv6: disabled
    httpPutResponseHopLimit: 2
    httpTokens: required

  tags:
    Environment: production
    Team: ml-platform
    ManagedBy: karpenter
```

### GPU å·¥ä½œè´Ÿè½½è°ƒåº¦ç¤ºä¾‹

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: gpu-training-job
  namespace: ml-training
spec:
  template:
    metadata:
      labels:
        app: gpu-training
    spec:
      nodeSelector:
        karpenter.sh/capacity-type: spot
        node.kubernetes.io/instance-type: g5.2xlarge

      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

      containers:
        - name: trainer
          image: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime
          command:
            - python
            - train.py
          resources:
            requests:
              nvidia.com/gpu: 1
              memory: "16Gi"
              cpu: "8"
            limits:
              nvidia.com/gpu: 1
              memory: "32Gi"
              cpu: "16"

          env:
            - name: CUDA_VISIBLE_DEVICES
              value: "0"

      restartPolicy: OnFailure
  backoffLimit: 3
```

---

## ç«¯åˆ°ç«¯æµæ°´çº¿ç¤ºä¾‹

### å®Œæ•´å·¥ä½œæµ

```python
# complete_ml_workflow.py
from kfp import dsl, compiler
import kfp

@dsl.pipeline(
    name="Production ML Pipeline",
    description="Complete production-ready ML pipeline with monitoring"
)
def production_ml_pipeline(
    data_source: str = "s3://prod-data/transactions.parquet",
    model_name: str = "fraud-detection",
    experiment_name: str = "fraud-detection-prod",
    deploy_threshold: float = 0.92
):
    # 1. æ•°æ®éªŒè¯
    data_validation = data_quality_check(
        data_source=data_source
    )

    # 2. æ•°æ®å‡†å¤‡
    data_prep = data_preparation(
        s3_input_path=data_source,
        train_split=0.8
    ).after(data_validation)

    # 3. ç‰¹å¾å·¥ç¨‹
    feature_eng = feature_engineering(
        input_dataset=data_prep.outputs["output_dataset"],
        feature_columns=["amount", "merchant_id", "user_age", "transaction_hour"]
    )

    # 4. æ¨¡å‹è®­ç»ƒï¼ˆGPUï¼‰
    training = model_training(
        input_features=feature_eng.outputs["output_features"],
        mlflow_tracking_uri="http://mlflow-server.mlflow.svc.cluster.local:5000",
        experiment_name=experiment_name,
        learning_rate=0.0005,
        epochs=50,
        batch_size=64
    )
    training.set_gpu_limit(1)
    training.add_node_selector_constraint("karpenter.sh/capacity-type", "spot")

    # 5. æ¨¡å‹è¯„ä¼°
    evaluation = model_evaluation(
        input_features=feature_eng.outputs["output_features"],
        input_model=training.outputs["output_model"],
        mlflow_tracking_uri="http://mlflow-server.mlflow.svc.cluster.local:5000"
    )

    # 6. æ¨¡å‹æ³¨å†Œï¼ˆæ¡ä»¶æ€§ï¼‰
    with dsl.Condition(evaluation.outputs["output_metrics"].outputs["accuracy"] >= deploy_threshold):
        registration = register_model(
            model_uri=training.outputs["output_model"].uri,
            model_name=model_name,
            mlflow_tracking_uri="http://mlflow-server.mlflow.svc.cluster.local:5000"
        )

        # 7. KServe éƒ¨ç½²
        deployment = deploy_to_kserve(
            model_name=model_name,
            model_uri=registration.outputs["registered_model_uri"],
            namespace="kserve-inference"
        )

if __name__ == "__main__":
    compiler.Compiler().compile(
        pipeline_func=production_ml_pipeline,
        package_path="production_ml_pipeline.yaml"
    )
```

---

## ç›‘æ§ä¸å‘Šè­¦

### MLflow æŒ‡æ ‡ç›‘æ§

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: mlflow-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: mlflow-server
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: mlflow-alerts
  namespace: monitoring
spec:
  groups:
    - name: mlflow-alerts
      rules:
        - alert: MLflowServerDown
          expr: up{job="mlflow-server"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "MLflow æœåŠ¡å™¨å®•æœº"
            description: "MLflow è¿½è¸ªæœåŠ¡å™¨å·²åœæœºè¶…è¿‡ 5 åˆ†é’Ÿ"

        - alert: ModelDriftDetected
          expr: model_drift_score > 0.3
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "æ£€æµ‹åˆ°æ¨¡å‹æ¼‚ç§»"
            description: "æ¨¡å‹ {{ $labels.model_name }} å‡ºç°æ˜¾è‘—æ¼‚ç§»"
```

---

## æ€»ç»“

åŸºäº EKS çš„ MLOps æµæ°´çº¿é€šè¿‡é›†æˆ Kubeflowã€MLflow å’Œ KServeï¼Œæä¾›å®Œå…¨è‡ªåŠ¨åŒ–çš„ ML ç”Ÿå‘½å‘¨æœŸã€‚

### æ ¸å¿ƒè¦ç‚¹

1. **Kubeflow Pipelines**ï¼šåŸºäºå¯å¤ç”¨ç»„ä»¶çš„ ML å·¥ä½œæµ
2. **MLflow**ï¼šé€šè¿‡å®éªŒè¿½è¸ªå’Œæ¨¡å‹æ³¨å†Œä¸­å¿ƒå¢å¼ºæ²»ç†èƒ½åŠ›
3. **KServe**ï¼šæ”¯æŒè‡ªåŠ¨æ‰©ç¼©å®¹çš„ç”Ÿäº§çº§æ¨¡å‹æœåŠ¡
4. **Karpenter**ï¼šé€šè¿‡ GPU èµ„æºåŠ¨æ€é…ç½®å®ç°æˆæœ¬ä¼˜åŒ–
5. **Argo Workflows**ï¼šé€šè¿‡ CI/CD è‡ªåŠ¨åŒ–ç¼©çŸ­éƒ¨ç½²å‘¨æœŸ

### ä¸‹ä¸€æ­¥

- [SageMaker-EKS é›†æˆ](./sagemaker-eks-integration.md) - æ··åˆ ML æ¶æ„
- [GPU èµ„æºç®¡ç†](./gpu-resource-management.md) - GPU é›†ç¾¤ä¼˜åŒ–
- [æ¨¡å‹ç›‘æ§](./agent-monitoring.md) - ç”Ÿäº§æ¨¡å‹å¯è§‚æµ‹æ€§

---

## å‚è€ƒèµ„æ–™

- [Kubeflow å®˜æ–¹æ–‡æ¡£](https://www.kubeflow.org/docs/)
- [MLflow å®˜æ–¹æ–‡æ¡£](https://mlflow.org/docs/latest/index.html)
- [KServe å®˜æ–¹æ–‡æ¡£](https://kserve.github.io/website/)
- [Karpenter å®˜æ–¹æ–‡æ¡£](https://karpenter.sh/)
- [Argo Workflows å®˜æ–¹æ–‡æ¡£](https://argoproj.github.io/workflows/)
