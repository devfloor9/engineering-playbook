---
title: "åŸºäº vLLM çš„åŸºç¡€æ¨¡å‹éƒ¨ç½²ä¸æ€§èƒ½ä¼˜åŒ–"
sidebar_label: "5. vLLM Model Serving"
description: "ä½¿ç”¨ vLLM éƒ¨ç½²åŸºç¡€æ¨¡å‹ã€Kubernetes é›†æˆåŠæ€§èƒ½ä¼˜åŒ–ç­–ç•¥"
sidebar_position: 5
last_update:
  date: 2026-02-14
  author: devfloor9
category: "genai-aiml"
tags: [vllm, model-serving, gpu, inference, optimization, foundation-model, eks]
---

# åŸºäº vLLM çš„åŸºç¡€æ¨¡å‹éƒ¨ç½²ä¸æ€§èƒ½ä¼˜åŒ–

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2026-02-14 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 6 åˆ†é’Ÿ


vLLM æ˜¯ä¸€æ¬¾é«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“ï¼Œé€šè¿‡ PagedAttention ç®—æ³•å°† KV Cache å†…å­˜æµªè´¹å‡å°‘ 60-80%ï¼Œå¹¶é€šè¿‡ Continuous Batching å®ç°æ¯”ä¼ ç»Ÿæ–¹æ³•é«˜ 2-24 å€çš„ååé‡ã€‚Metaã€Mistral AIã€Cohere å’Œ IBM ç­‰ä¸»è¦ä¼ä¸šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ vLLMï¼Œå¹¶ä¸”å®ƒæä¾› OpenAI å…¼å®¹ APIï¼Œæ–¹ä¾¿ç°æœ‰åº”ç”¨çš„è¿ç§»ã€‚

æœ¬æ–‡æ¡£æä¾›åœ¨ Amazon EKS ç¯å¢ƒä¸­éƒ¨ç½²å’Œè¿ç»´ vLLM çš„å®ç”¨æŒ‡å—ã€‚æ¶µç›– GPU å†…å­˜è®¡ç®—ã€å¹¶è¡ŒåŒ–ç­–ç•¥é€‰æ‹©ã€Kubernetes éƒ¨ç½²æ¨¡å¼ä»¥åŠç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½è°ƒä¼˜æ–¹æ³•ã€‚

## æ ¸å¿ƒæ¶æ„ç†è§£

### PagedAttention ä¸å†…å­˜æ•ˆç‡

ä¼ ç»Ÿ LLM æœåŠ¡çš„æœ€å¤§ç“¶é¢ˆæ˜¯ KV Cache å†…å­˜ç®¡ç†ã€‚ç”±äº Transformer æ¶æ„çš„è‡ªå›å½’ç‰¹æ€§ï¼Œæ¯ä¸ªè¯·æ±‚å¿…é¡»å­˜å‚¨æ‰€æœ‰å…ˆå‰ Token çš„é”®å€¼å¯¹ï¼Œè€Œè¿™ä¸ª KV Cache ä¼šéšè¾“å…¥åºåˆ—é•¿åº¦å’Œå¹¶å‘ç”¨æˆ·æ•°çº¿æ€§å¢é•¿ã€‚

vLLM çš„ PagedAttention å—æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜ç®¡ç†çš„å¯å‘ï¼Œå°† KV Cache å­˜å‚¨åœ¨éè¿ç»­çš„å—ä¸­ã€‚è¿™æ¶ˆé™¤äº†å†…å­˜ç¢ç‰‡ï¼Œå®ç°åŠ¨æ€å†…å­˜åˆ†é…ï¼Œå¹¶æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸­ 60-80% çš„å†…å­˜æµªè´¹è¢«æ¶ˆé™¤ï¼Œç›¸åŒçš„ç¡¬ä»¶å¯ä»¥å¤„ç†æ›´å¤šå¹¶å‘è¯·æ±‚ã€‚

### Continuous Batching

é™æ€æ‰¹å¤„ç†ä¼šç­‰åˆ°å›ºå®šæ•°é‡çš„è¯·æ±‚åˆ°è¾¾åæ‰å¼€å§‹å¤„ç†ã€‚å¦‚æœæ‰¹å¤§å°ä¸º 32ï¼Œç¬¬ 31 ä¸ªè¯·æ±‚å¿…é¡»ç­‰å¾…ç¬¬ 32 ä¸ªè¯·æ±‚åˆ°è¾¾ã€‚å½“è¯·æ±‚ä¸è§„åˆ™åˆ°è¾¾æ—¶ï¼ŒGPU ä»…éƒ¨åˆ†åˆ©ç”¨ï¼Œååé‡ä¸‹é™ã€‚

vLLM çš„ Continuous Batching å®Œå…¨æ¶ˆé™¤äº†æ‰¹å¤„ç†è¾¹ç•Œã€‚è°ƒåº¦å™¨åœ¨è¿­ä»£çº§åˆ«è¿è¡Œï¼Œç«‹å³ç§»é™¤å·²å®Œæˆçš„è¯·æ±‚å¹¶åŠ¨æ€æ·»åŠ æ–°è¯·æ±‚ã€‚è¿™ç¡®ä¿ GPU å§‹ç»ˆä»¥æ»¡è´Ÿè·è¿è¡Œï¼ŒåŒæ—¶æ”¹å–„å¹³å‡å»¶è¿Ÿå’Œååé‡ã€‚

## GPU å†…å­˜éœ€æ±‚è®¡ç®—

åœ¨éƒ¨ç½²æ¨¡å‹ä¹‹å‰ï¼Œå¿…é¡»å‡†ç¡®è®¡ç®— GPU å†…å­˜éœ€æ±‚ã€‚å†…å­˜ä½¿ç”¨åˆ†ä¸ºä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šæ¨¡å‹æƒé‡ã€é Torch å†…å­˜å’Œ KV Cacheã€‚

```
æ‰€éœ€ GPU å†…å­˜ = æ¨¡å‹æƒé‡ + é Torch å†…å­˜ + PyTorch æ¿€æ´»å³°å€¼å†…å­˜ + (æ¯æ‰¹æ¬¡ KV Cache å†…å­˜ Ã— æ‰¹å¤§å°)
```

æ¨¡å‹æƒé‡å†…å­˜ç”±å‚æ•°æ•°é‡å’Œç²¾åº¦å†³å®šã€‚

| ç²¾åº¦ | æ¯å‚æ•°å­—èŠ‚æ•° | 70B æ¨¡å‹å†…å­˜ |
|-----------|---------------------|------------------|
| FP32 | 4 | 280GB |
| FP16/BF16 | 2 | 140GB |
| INT8 | 1 | 70GB |
| INT4 | 0.5 | 35GB |

ä»¥ FP16 éƒ¨ç½² 70B å‚æ•°æ¨¡å‹ä»…æƒé‡å°±éœ€è¦ 140GBã€‚è¿™åœ¨å•ä¸ª GPU ä¸Šä¸å¯èƒ½å®ç°ï¼Œéœ€è¦å¤š GPU å¼ é‡å¹¶è¡ŒåŒ–ã€‚å°†åŒä¸€æ¨¡å‹é‡åŒ–ä¸º INT4 å¯å‡å°‘åˆ° 35GBï¼Œä½¿å…¶å¯ä»¥åœ¨å•ä¸ª A100 80GB æˆ– H100 ä¸Šéƒ¨ç½²ï¼Œå¹¶ç•™æœ‰ KV Cache çš„ä½™é‡ã€‚

## å¹¶è¡ŒåŒ–ç­–ç•¥

### å¼ é‡å¹¶è¡Œ

å¼ é‡å¹¶è¡Œå°†æ¯ä¸ªæ¨¡å‹å±‚å†…çš„å‚æ•°åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šã€‚è¿™æ˜¯åœ¨å•èŠ‚ç‚¹å†…éƒ¨ç½²å¤§å‹æ¨¡å‹æ—¶æœ€å¸¸ç”¨çš„ç­–ç•¥ã€‚

é€‚ç”¨åœºæ™¯ï¼š

- æ¨¡å‹æ— æ³•æ”¾å…¥å•ä¸ª GPU
- å‡å°‘æ¯ä¸ª GPU çš„å†…å­˜å‹åŠ›ï¼Œé‡Šæ”¾ KV Cache ç©ºé—´ä»¥æé«˜ååé‡

```python
from vllm import LLM

# å°†æ¨¡å‹åˆ†å¸ƒåˆ° 4 ä¸ª GPU
llm = LLM(model="meta-llama/Llama-3.3-70B-Instruct", tensor_parallel_size=4)
```

å¼ é‡å¹¶è¡Œçš„çº¦æŸæ˜¯æ³¨æ„åŠ›å¤´çš„æ•°é‡ã€‚tensor_parallel_size å¿…é¡»æ˜¯æ¨¡å‹æ³¨æ„åŠ›å¤´æ•°é‡çš„å› å­ã€‚

### æµæ°´çº¿å¹¶è¡Œ

æµæ°´çº¿å¹¶è¡Œå°†æ¨¡å‹å±‚æŒ‰é¡ºåºåˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šã€‚Token æŒ‰é¡ºåºæµè¿‡æµæ°´çº¿ã€‚

é€‚ç”¨åœºæ™¯ï¼š

- å¼ é‡å¹¶è¡Œå·²å……åˆ†åˆ©ç”¨ä½†ä»éœ€æ›´å¤š GPU
- éœ€è¦å¤šèŠ‚ç‚¹éƒ¨ç½²æ—¶

```bash
# 4 ä¸ª GPU å¼ é‡å¹¶è¡Œï¼Œ2 ä¸ªèŠ‚ç‚¹æµæ°´çº¿å¹¶è¡Œ
vllm serve meta-llama/Llama-3.3-70B-Instruct \
  --tensor-parallel-size 4 \
  --pipeline-parallel-size 2
```

### æ•°æ®å¹¶è¡Œ

æ•°æ®å¹¶è¡Œå°†æ•´ä¸ªæ¨¡å‹å¤åˆ¶åˆ°å¤šä¸ªæœåŠ¡å™¨ä¸Šä»¥å¤„ç†ç‹¬ç«‹è¯·æ±‚ã€‚å¯ä»¥ä¸ Kubernetes HPAï¼ˆHorizontal Pod Autoscalerï¼‰ç»“åˆå®ç°å¼¹æ€§æ‰©å±•ã€‚

### Expert å¹¶è¡Œ

è¿™æ˜¯é’ˆå¯¹ MoEï¼ˆMixture-of-Expertsï¼‰æ¨¡å‹çš„ä¸“ç”¨ç­–ç•¥ã€‚Token ä»…è·¯ç”±åˆ°ç›¸å…³çš„ "Expert"ï¼Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ã€‚é€šè¿‡ `--enable-expert-parallel` æ ‡å¿—æ¿€æ´»ã€‚

## Kubernetes éƒ¨ç½²

### åŸºæœ¬éƒ¨ç½²é…ç½®

ä»¥ä¸‹æ˜¯åœ¨ AWS EKS ä¸Šéƒ¨ç½² vLLM çš„åŸºæœ¬é…ç½®ã€‚å®ƒéµå¾ª [GenAI on EKS Starter Kit](https://github.com/aws-samples/sample-genai-on-eks-starter-kit) çš„æ¨¡å¼ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen3-32b-fp8
  namespace: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: qwen3-32b-fp8
  template:
    metadata:
      labels:
        app: qwen3-32b-fp8
    spec:
      nodeSelector:
        karpenter.sh/instance-family: g6e
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.10.2
          command: ["vllm", "serve"]
          args:
            - Qwen/Qwen3-32B-FP8
            - --served-model-name=qwen3-32b-fp8
            - --trust-remote-code
            - --gpu-memory-utilization=0.95
            - --max-model-len=32768
            - --enable-auto-tool-choice
            - --tool-call-parser=hermes
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
          ports:
            - name: http
              containerPort: 8000
          resources:
            requests:
              cpu: 3
              memory: 24Gi
              nvidia.com/gpu: 1
            limits:
              nvidia.com/gpu: 1
          volumeMounts:
            - name: huggingface-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
      volumes:
        - name: huggingface-cache
          persistentVolumeClaim:
            claimName: huggingface-cache
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "16Gi"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: qwen3-32b-fp8
  namespace: vllm
spec:
  selector:
    app: qwen3-32b-fp8
  ports:
    - name: http
      port: 8000
```

### æ ¸å¿ƒé…ç½®å‚æ•°

**gpu-memory-utilization**ï¼šGPU VRAM åˆ†é…ç»™ KV Cache é¢„åˆ†é…çš„æ¯”ä¾‹ã€‚é»˜è®¤ 0.9ï¼Œæœ€é«˜å¯è®¾ç½® 0.95 ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚æ‰¾åˆ°ä¸å‘ç”Ÿ OOM çš„æœ€å¤§å€¼ã€‚

**max-model-len**ï¼šæ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚ç›´æ¥å½±å“ KV Cache å¤§å°ã€‚æ ¹æ®å®é™…å·¥ä½œè´Ÿè½½è¿›è¡Œè°ƒæ•´ã€‚

**max-num-seqs**ï¼šåŒæ—¶å¤„ç†çš„æœ€å¤§åºåˆ—æ•°ã€‚é»˜è®¤ 256-1024ã€‚åœ¨å†…å­˜å’Œååé‡ä¹‹é—´æƒè¡¡ã€‚

**tensor-parallel-size**ï¼šç”¨äºå¼ é‡å¹¶è¡ŒåŒ–çš„ GPU æ•°é‡ã€‚

### å¤š GPU å¼ é‡å¹¶è¡Œéƒ¨ç½²

70B åŠä»¥ä¸Šçš„å¤§å‹æ¨¡å‹éœ€è¦å¤š GPU é…ç½®ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-70b-instruct
  namespace: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-70b-instruct
  template:
    metadata:
      labels:
        app: llama-70b-instruct
    spec:
      nodeSelector:
        karpenter.sh/instance-family: p5
      hostNetwork: true
      hostIPC: true
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.10.2
          command: ["vllm", "serve"]
          args:
            - meta-llama/Llama-3.3-70B-Instruct
            - --tensor-parallel-size=4
            - --gpu-memory-utilization=0.90
            - --max-model-len=8192
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: NCCL_DEBUG
              value: "INFO"
          resources:
            requests:
              nvidia.com/gpu: 4
            limits:
              nvidia.com/gpu: 4
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "32Gi"
```

**é‡è¦æç¤º**ï¼šå¼ é‡å¹¶è¡Œæ¨ç†éœ€è¦ `hostIPC: true` å’Œè¶³å¤Ÿçš„å…±äº«å†…å­˜ï¼ˆ`/dev/shm`ï¼‰ã€‚

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### é‡åŒ–

åœ¨æ¨¡å‹è´¨é‡å’Œå†…å­˜æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

```bash
# ä½¿ç”¨ FP8 é‡åŒ–æ¨¡å‹
vllm serve Qwen/Qwen3-32B-FP8 --quantization fp8

# AWQ é‡åŒ–
vllm serve TheBloke/Llama-2-70B-AWQ --quantization awq

# GPTQ é‡åŒ–
vllm serve TheBloke/Llama-2-70B-GPTQ --quantization gptq
```

FP8 ä»¥å¯å¿½ç•¥çš„è´¨é‡ä¸‹é™å°†å†…å­˜å‡åŠã€‚INT4ï¼ˆAWQã€GPTQï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å¯èƒ½å¯¼è‡´è´¨é‡ä¸‹é™ï¼Œéœ€è¦é’ˆå¯¹æ¯ä¸ªå·¥ä½œè´Ÿè½½è¿›è¡Œæ€§èƒ½åˆ†æã€‚

### å‰ç¼€ç¼“å­˜

å¯¹äºæ ‡å‡†åŒ–ç³»ç»Ÿæç¤ºæˆ–é‡å¤ä¸Šä¸‹æ–‡ï¼Œåˆ©ç”¨ç‡æå‡è¶…è¿‡ 400%ã€‚

```bash
vllm serve model-name --enable-prefix-caching
```

ç”±äºç³»ç»Ÿæç¤ºçš„ KV Cache åªéœ€è®¡ç®—ä¸€æ¬¡å³å¯å…±äº«ï¼Œå…·æœ‰ç›¸åŒå‰ç¼€çš„è¯·æ±‚å¯ä»¥é¿å…å†—ä½™è®¡ç®—ã€‚å‘½ä¸­ç‡å› åº”ç”¨è€Œå¼‚ã€‚

### æ¨æµ‹è§£ç 

å¯¹äºå¯é¢„æµ‹çš„è¾“å‡ºï¼Œå¯æä¾› 2-3 å€çš„é€Ÿåº¦æå‡ã€‚ä¸€ä¸ªå°å‹è‰ç¨¿æ¨¡å‹é¢„æµ‹ Tokenï¼Œä¸»æ¨¡å‹è¿›è¡ŒéªŒè¯ã€‚

```bash
vllm serve large-model \
  --speculative-model small-draft-model \
  --num-speculative-tokens 5
```

å¯¹äºå¤šå˜çš„æç¤ºï¼Œç¼“å­˜ç»´æŠ¤å¼€é”€å¯èƒ½è¶…è¿‡æ”¶ç›Šã€‚

### åˆ†å—é¢„å¡«å……

é€šè¿‡åœ¨åŒä¸€æ‰¹æ¬¡ä¸­æ··åˆè®¡ç®—å¯†é›†å‹çš„é¢„å¡«å……å’Œå†…å­˜å¯†é›†å‹çš„è§£ç å·¥ä½œï¼ŒåŒæ—¶æ”¹å–„ååé‡å’Œå»¶è¿Ÿã€‚åœ¨ vLLM V1 ä¸­é»˜è®¤å¯ç”¨ã€‚

```python
from vllm import LLM

llm = LLM(
    model="model-name",
    max_num_batched_tokens=2048  # å¯è°ƒå‚æ•°
)
```

è°ƒæ•´ max_num_batched_tokens ä»¥å¹³è¡¡ TTFTï¼ˆé¦–ä¸ª Token æ—¶é—´ï¼‰å’Œååé‡ã€‚

## ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### Prometheus æŒ‡æ ‡

vLLM æš´éœ²å„ç§ Prometheus æŒ‡æ ‡ã€‚

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-monitor
  namespace: vllm
spec:
  selector:
    matchLabels:
      app: vllm
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
```

å…³é”®ç›‘æ§æŒ‡æ ‡ï¼š

- `vllm:num_requests_running`ï¼šå½“å‰æ­£åœ¨å¤„ç†çš„è¯·æ±‚æ•°
- `vllm:num_requests_waiting`ï¼šç­‰å¾…ä¸­çš„è¯·æ±‚æ•°
- `vllm:gpu_cache_usage_perc`ï¼šGPU KV Cache åˆ©ç”¨ç‡ç™¾åˆ†æ¯”
- `vllm:num_preemptions_total`ï¼šè¢«æŠ¢å çš„è¯·æ±‚æ•°ï¼ˆé«˜å€¼è¡¨ç¤ºå†…å­˜ä¸è¶³ï¼‰

### æŠ¢å å¤„ç†

å½“ KV Cache ç©ºé—´ä¸è¶³æ—¶ï¼ŒvLLM ä¼šæŠ¢å è¯·æ±‚ä»¥é‡Šæ”¾ç©ºé—´ã€‚å¦‚æœä»¥ä¸‹è­¦å‘Šé¢‘ç¹å‡ºç°ï¼Œéœ€è¦é‡‡å–æªæ–½ã€‚

```
WARNING Sequence group 0 is preempted by PreemptionMode.RECOMPUTE
```

åº”å¯¹æªæ–½ï¼š

- å¢åŠ  `gpu_memory_utilization`
- å‡å°‘ `max_num_seqs` æˆ– `max_num_batched_tokens`
- å¢åŠ  `tensor_parallel_size` ä»¥é‡Šæ”¾æ¯ä¸ª GPU çš„å†…å­˜
- å‡å°‘ `max_model_len`

## ç”Ÿäº§éƒ¨ç½²æ¸…å•

éƒ¨ç½²å‰è¯·éªŒè¯ï¼š

1. è®¡ç®— GPU å†…å­˜éœ€æ±‚å¹¶é€‰æ‹©åˆé€‚çš„å®ä¾‹ç±»å‹
2. ç¡®å®šé‡åŒ–ç­–ç•¥å¹¶éªŒè¯è´¨é‡-æ•ˆç‡æƒè¡¡
3. é…ç½®é€‚åˆå·¥ä½œè´Ÿè½½çš„ max_model_len
4. ç¡®å®šæ˜¯å¦éœ€è¦å¼ é‡å¹¶è¡ŒåŒ–å¹¶å†³å®š GPU æ•°é‡
5. åˆ†é…è¶³å¤Ÿçš„å…±äº«å†…å­˜ï¼ˆ/dev/shmï¼‰
6. è®¾ç½® Prometheus æŒ‡æ ‡æ”¶é›†å’Œä»ªè¡¨æ¿
7. é…ç½® HPA è¿›è¡Œå¼¹æ€§æ‰©å±•
8. é€šè¿‡ PVC æŒä¹…åŒ–æ¨¡å‹ç¼“å­˜

## å‚è€ƒèµ„æ–™

- [GenAI on EKS Starter Kit](https://github.com/aws-samples/sample-genai-on-eks-starter-kit)ï¼šè‡ªåŠ¨åŒ–éƒ¨ç½²åŒ…æ‹¬ LiteLLMã€vLLMã€Langfuse å’Œ Milvus åœ¨å†…çš„ GenAI ç»„ä»¶
- [Scalable Model Inference and Agentic AI on Amazon EKS](https://github.com/aws-solutions-library-samples/guidance-for-scalable-model-inference-and-agentic-ai-on-amazon-eks)ï¼šåŒ…å« llm-dã€Karpenter å’Œ RAG å·¥ä½œæµçš„ç»¼åˆæ¶æ„
- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai)ï¼šä¼˜åŒ–å’Œè°ƒä¼˜æŒ‡å—
- [vLLM Kubernetes éƒ¨ç½²æŒ‡å—](https://docs.vllm.ai/en/stable/deployment/k8s.html)
