---
title: "llm-d åœ¨ EKS Auto Mode ä¸Šçš„æ¨ç†éƒ¨ç½²æŒ‡å—"
sidebar_label: "7. llm-d EKS Auto Mode"
description: "åœ¨ EKS Auto Mode ä¸Šä½¿ç”¨ llm-d éƒ¨ç½² Kubernetes åŸç”Ÿåˆ†å¸ƒå¼æ¨ç†çš„æŒ‡å—"
tags: [eks, llm-d, vllm, inference-gateway, gpu, auto-mode, qwen, kv-cache]
category: "genai-aiml"
sidebar_position: 7
last_update:
  date: 2026-02-14
  author: devfloor9
---

import {
  WellLitPathTable,
  VllmComparisonTable,
  Qwen3SpecsTable,
  PrerequisitesTable,
  P5InstanceTable,
  P5eInstanceTable,
  GatewayCRDTable,
  DefaultDeploymentTable,
  KVCacheEffectsTable,
  MonitoringMetricsTable,
  ModelLoadingTable,
  CostOptimizationTable,
  TroubleshootingTable
} from '@site/src/components/LlmdTables';

# llm-d åœ¨ EKS Auto Mode ä¸Šçš„æ¨ç†éƒ¨ç½²æŒ‡å—

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2026-02-10 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 7 åˆ†é’Ÿ

> **ğŸ“Œ å½“å‰ç‰ˆæœ¬**: llm-d v0.4 (2025)ã€‚æœ¬æ–‡æ¡£ä¸­çš„éƒ¨ç½²ç¤ºä¾‹åŸºäº Intelligent Inference Scheduling well-lit pathã€‚

> ğŸ“… **å‘å¸ƒæ—¥æœŸ**: 2026-02-10 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 15 åˆ†é’Ÿ

## æ¦‚è¿°

llm-d æ˜¯ç”± Red Hat ä¸»å¯¼çš„ Kubernetes åŸç”Ÿåˆ†å¸ƒå¼æ¨ç†æ ˆï¼ŒåŸºäº Apache 2.0 è®¸å¯è¯ã€‚å®ƒç»“åˆäº† vLLM æ¨ç†å¼•æ“ã€åŸºäº Envoy çš„ Inference Gateway å’Œ Kubernetes Gateway APIï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹æä¾›æ™ºèƒ½æ¨ç†è·¯ç”±ã€‚

ä¼ ç»Ÿçš„ vLLM éƒ¨ç½²ä¾èµ–ç®€å•çš„ Round-Robin è´Ÿè½½å‡è¡¡ï¼Œè€Œ llm-d ä½¿ç”¨æ„ŸçŸ¥ KV Cache çŠ¶æ€çš„æ™ºèƒ½è·¯ç”±ï¼Œå°†å…·æœ‰ç›¸åŒå‰ç¼€çš„è¯·æ±‚å®šå‘åˆ°å·²æŒæœ‰å¯¹åº” KV Cache çš„ Podã€‚è¿™æ˜¾è‘—é™ä½äº† Time To First Token (TTFT) å¹¶èŠ‚çœ GPU è®¡ç®—èµ„æºã€‚

æœ¬æ–‡æ¡£æ¶µç›–åœ¨ Amazon EKS Auto Mode ç¯å¢ƒä¸­éƒ¨ç½² llm-d å¹¶ä½¿ç”¨ Qwen3-32B æ¨¡å‹é…ç½®æ¨ç†æœåŠ¡çš„å®Œæ•´æµç¨‹ã€‚EKS Auto Mode æä¾›åŸºäº Karpenter çš„è‡ªåŠ¨èŠ‚ç‚¹ä¾›åº”å’Œè‡ªåŠ¨ NVIDIA GPU é©±åŠ¨ç®¡ç†ï¼Œå¤§å¹…é™ä½äº† GPU åŸºç¡€è®¾æ–½è®¾ç½®çš„å¤æ‚æ€§ã€‚

:::warning llm-d Inference Gateway â‰  é€šç”¨ Gateway API å®ç°
llm-d åŸºäº Envoy çš„ Inference Gateway æ˜¯ä¸€ä¸ª**ä¸“é—¨ä¸º LLM æ¨ç†è¯·æ±‚è®¾è®¡çš„ä¸“ç”¨ç½‘å…³**ã€‚å®ƒåœ¨ç›®çš„å’ŒèŒƒå›´ä¸Šä¸æ›¿ä»£ NGINX Ingress Controller çš„é€šç”¨ Gateway API å®ç°ï¼ˆAWS LBC v3ã€Ciliumã€Envoy Gateway ç­‰ï¼‰æœ‰æœ¬è´¨åŒºåˆ«ã€‚

- **llm-d Gateway**ï¼šåŸºäº InferenceModel/InferencePool CRDï¼ŒKV Cache æ„ŸçŸ¥è·¯ç”±ï¼Œä»…å¤„ç†æ¨ç†æµé‡
- **é€šç”¨ Gateway API**ï¼šåŸºäº HTTPRoute/GRPCRouteï¼ŒTLS/è®¤è¯/é™æµï¼Œé›†ç¾¤èŒƒå›´çš„æµé‡ç®¡ç†

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåº”ä½¿ç”¨é€šç”¨ Gateway API å®ç°ä½œä¸ºé›†ç¾¤å…¥å£ç‚¹ï¼Œllm-d åœ¨å…¶ä¸‹æ–¹ä¼˜åŒ– AI æ¨ç†æµé‡ã€‚å…³äºé€šç”¨ Gateway API å®ç°çš„é€‰æ‹©ï¼Œè¯·å‚é˜… [Gateway API é‡‡ç”¨æŒ‡å—](/docs/infrastructure-optimization/gateway-api-adoption-guide)ã€‚
:::

### ä¸»è¦ç›®æ ‡

- **ç†è§£ llm-d æ¶æ„**ï¼šInference Gateway å’Œ KV Cache æ„ŸçŸ¥è·¯ç”±çš„å·¥ä½œåŸç†
- **EKS Auto Mode GPU é…ç½®**ï¼šè®¾ç½® p5.48xlarge èŠ‚ç‚¹çš„è‡ªåŠ¨ä¾›åº”
- **Qwen3-32B éƒ¨ç½²**ï¼šä½¿ç”¨ helmfile è¿›è¡Œé›†æˆéƒ¨ç½²å’ŒéªŒè¯
- **æ¨ç†æµ‹è¯•**ï¼šé€šè¿‡ OpenAI å…¼å®¹ API è¿›è¡Œæ¨ç†è¯·æ±‚å’Œæµå¼ä¼ è¾“
- **è¿ç»´ä¼˜åŒ–**ï¼šç›‘æ§ã€æˆæœ¬ä¼˜åŒ–å’Œæ•…éšœæ’é™¤

### llm-d çš„ 3 æ¡ Well-Lit Path

llm-d æä¾›ä¸‰æ¡ç»è¿‡éªŒè¯çš„éƒ¨ç½²è·¯å¾„ã€‚

<WellLitPathTable />

---

## æ¶æ„

llm-d çš„ Intelligent Inference Scheduling æ¶æ„å¦‚ä¸‹ã€‚

```mermaid
flowchart TB
    subgraph "Client Layer"
        CLIENT[Client Application<br/>OpenAI Compatible API]
    end

    subgraph "Gateway Layer"
        GW[Inference Gateway<br/>Envoy-based]
        IM[InferenceModel CRD<br/>Model â†” Pool Mapping]
        IP[InferencePool CRD<br/>vLLM Pod Group Management]
    end

    subgraph "Inference Layer"
        subgraph "vLLM Pod 1 â€” GPU 0,1"
            V1[vLLM Engine<br/>Qwen3-32B TP=2]
            KV1[KV Cache]
        end
        subgraph "vLLM Pod 2 â€” GPU 2,3"
            V2[vLLM Engine<br/>Qwen3-32B TP=2]
            KV2[KV Cache]
        end
        subgraph "vLLM Pod N â€” GPU 14,15"
            VN[vLLM Engine<br/>Qwen3-32B TP=2]
            KVN[KV Cache]
        end
    end

    subgraph "EKS Auto Mode"
        NP[Karpenter NodePool<br/>p5.48xlarge]
        NC[NodeClass default<br/>Auto GPU Driver]
    end

    CLIENT --> GW
    GW --> IM
    IM --> IP
    IP --> V1 & V2 & VN
    NP --> NC

    style CLIENT fill:#34a853,color:#fff
    style GW fill:#4286f4,color:#fff
    style NP fill:#fbbc04,color:#000
```

### llm-d ä¸ä¼ ç»Ÿ vLLM éƒ¨ç½²å¯¹æ¯”

<VllmComparisonTable />

### ä¸ºä»€ä¹ˆé€‰æ‹© Qwen3-32B

<Qwen3SpecsTable />

:::info ä¸ºä»€ä¹ˆé€‰æ‹© Qwen3-32B
Qwen3-32B æ˜¯ llm-d çš„å®˜æ–¹é»˜è®¤æ¨¡å‹ï¼ŒåŸºäº Apache 2.0 è®¸å¯è¯å¯å…è´¹å•†ç”¨ã€‚åœ¨ BF16 ç²¾åº¦ä¸‹éœ€è¦çº¦ 65GB VRAMï¼Œå¯ä»¥ä½¿ç”¨ TP=2ï¼ˆ2 ä¸ª GPUï¼‰åœ¨ H100 80GB GPU ä¸Šç¨³å®šè¿è¡Œã€‚
:::

---

## å‰ææ¡ä»¶

<PrerequisitesTable />

### å®¢æˆ·ç«¯å·¥å…·å®‰è£…

```bash
# å®‰è£… eksctlï¼ˆmacOSï¼‰
brew tap weaveworks/tap
brew install weaveworks/tap/eksctl

# å®‰è£… helmfile
brew install helmfile

# å®‰è£… yq
brew install yq

# éªŒè¯ç‰ˆæœ¬
eksctl version
kubectl version --client
helm version
helmfile --version
yq --version
```

:::warning éªŒè¯ p5.48xlarge é…é¢
p5.48xlarge ä½¿ç”¨ 192 ä¸ª vCPUã€‚è¯·éªŒè¯ AWS Service Quotas ä¸­çš„ **Running On-Demand P instances** é™åˆ¶è‡³å°‘ä¸º 192ã€‚é…é¢å¢åŠ è¯·æ±‚å¯èƒ½éœ€è¦ 1-3 ä¸ªå·¥ä½œæ—¥æ‰èƒ½è·æ‰¹ã€‚

```bash
# æ£€æŸ¥å½“å‰ P å®ä¾‹é…é¢
aws service-quotas get-service-quota \
  --service-code ec2 \
  --quota-code L-417A185B \
  --region us-west-2 \
  --query 'Quota.Value'
```

:::

---

## åˆ›å»º EKS Auto Mode é›†ç¾¤

### é›†ç¾¤é…ç½®æ–‡ä»¶

```yaml
# cluster-config.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: llm-d-cluster
  region: us-west-2
  version: "1.31"
autoModeConfig:
  enabled: true
```

```bash
# åˆ›å»ºé›†ç¾¤ï¼ˆå¤§çº¦éœ€è¦ 15-20 åˆ†é’Ÿï¼‰
eksctl create cluster -f cluster-config.yaml

# éªŒè¯é›†ç¾¤çŠ¶æ€
kubectl get nodes
kubectl cluster-info
```

### åˆ›å»º GPU NodePool

åœ¨ EKS Auto Mode ä¸­åˆ›å»ºç”¨äºè‡ªåŠ¨ä¾›åº” p5.48xlarge å®ä¾‹çš„ Karpenter NodePoolã€‚

```yaml
# gpu-nodepool.yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-p5
spec:
  template:
    spec:
      requirements:
        - key: eks.amazonaws.com/instance-family
          operator: In
          values: ["p5"]
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      taints:
        - key: nvidia.com/gpu
          effect: NoSchedule
  limits:
    cpu: "384"
    memory: 4096Gi
    nvidia.com/gpu: "16"
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 30s
```

```bash
kubectl apply -f gpu-nodepool.yaml

# éªŒè¯ NodePool çŠ¶æ€
kubectl get nodepool gpu-p5
```

:::info EKS Auto Mode ä¸­çš„ GPU æ”¯æŒ
EKS Auto Mode è‡ªåŠ¨å®‰è£…å’Œç®¡ç† NVIDIA GPU é©±åŠ¨ã€‚æ— éœ€å•ç‹¬å®‰è£… GPU Operator æˆ– NVIDIA Device Pluginã€‚ä½¿ç”¨ `default` NodeClass å¯è®© Auto Mode è‡ªåŠ¨é€‰æ‹©æœ€ä½³ AMI å’Œé©±åŠ¨ç‰ˆæœ¬ã€‚
:::

### p5.48xlarge å®ä¾‹è§„æ ¼

<P5InstanceTable />

---

## éƒ¨ç½² llm-d

### 5.1 åˆ›å»ºå‘½åç©ºé—´å’Œå¯†é’¥

```bash
export NAMESPACE=llm-d
kubectl create namespace ${NAMESPACE}

# åˆ›å»º HuggingFace Token å¯†é’¥
kubectl create secret generic llm-d-hf-token \
  --from-literal=HF_TOKEN=<your-huggingface-token> \
  -n ${NAMESPACE}

# éªŒè¯å¯†é’¥åˆ›å»º
kubectl get secret llm-d-hf-token -n ${NAMESPACE}
```

### 5.2 å…‹éš† llm-d ä»“åº“

```bash
git clone https://github.com/llm-d/llm-d.git
cd llm-d/guides/inference-scheduling
```

ç›®å½•ç»“æ„ï¼š

```text
guides/inference-scheduling/
â”œâ”€â”€ helmfile.yaml          # ç»Ÿä¸€éƒ¨ç½²å®šä¹‰
â”œâ”€â”€ values/
â”‚   â”œâ”€â”€ vllm-values.yaml   # vLLM æœåŠ¡å™¨é…ç½®
â”‚   â”œâ”€â”€ gateway-values.yaml # Gateway é…ç½®
â”‚   â””â”€â”€ ...
â””â”€â”€ README.md
```

### 5.3 å®‰è£… Gateway API CRD

llm-d ä½¿ç”¨ Kubernetes Gateway API å’Œ Inference Extension CRDã€‚

```bash
# å®‰è£…æ ‡å‡† Gateway API CRD
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.1/standard-install.yaml

# å®‰è£… Inference Extension CRDï¼ˆInferencePoolã€InferenceModelï¼‰
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/v0.3.0/manifests.yaml
```

å·²å®‰è£…çš„ CRDï¼š

<GatewayCRDTable />

```bash
# éªŒè¯ CRD å®‰è£…
kubectl get crd | grep -E "gateway|inference"
```

### 5.4 å®‰è£… Gateway æ§åˆ¶å¹³é¢

```bash
# å®‰è£…åŸºäº Istio çš„ Gateway æ§åˆ¶å¹³é¢
helmfile apply -n ${NAMESPACE} -l component=gateway-control-plane
```

### 5.5 å®Œæ•´ llm-d éƒ¨ç½²

```bash
# éƒ¨ç½²æ‰€æœ‰ç»„ä»¶ï¼ˆvLLM + Gateway + InferencePool + InferenceModelï¼‰
helmfile apply -n ${NAMESPACE}
```

é»˜è®¤éƒ¨ç½²é…ç½®ï¼š

<DefaultDeploymentTable />

:::tip èµ„æºè°ƒæ•´
é»˜è®¤é…ç½®ä½¿ç”¨ 8 ä¸ªå‰¯æœ¬ Ã— 2 ä¸ª GPU = 16 ä¸ª GPUã€‚ä¸ºäº†æµ‹è¯•ç›®çš„ï¼Œå¯ä»¥å‡å°‘ `helmfile.yaml` ä¸­çš„ `replicaCount` ä»¥èŠ‚çœæˆæœ¬ã€‚ä¾‹å¦‚ï¼Œè®¾ç½® 4 ä¸ªå‰¯æœ¬å¯åœ¨å•ä¸ª p5.48xlargeï¼ˆ8 ä¸ª GPUï¼‰ä¸Šè¿è¡Œã€‚
:::

### 5.6 éªŒè¯éƒ¨ç½²

```bash
# éªŒè¯ Helm å‘å¸ƒ
helm list -n ${NAMESPACE}

# æ£€æŸ¥æ‰€æœ‰èµ„æº
kubectl get all -n ${NAMESPACE}

# æ£€æŸ¥ InferencePool çŠ¶æ€
kubectl get inferencepool -n ${NAMESPACE}

# æ£€æŸ¥ InferenceModel çŠ¶æ€
kubectl get inferencemodel -n ${NAMESPACE}

# æ£€æŸ¥ vLLM Pod çŠ¶æ€ï¼ˆåŒ…æ‹¬ GPU åˆ†é…ï¼‰
kubectl get pods -n ${NAMESPACE} -o wide

# ç­‰å¾… Pod å°±ç»ªï¼ˆæ¨¡å‹åŠ è½½éœ€è¦ 5-10 åˆ†é’Ÿï¼‰
kubectl wait --for=condition=Ready pods -l app=vllm \
  -n ${NAMESPACE} --timeout=600s
```

:::warning æ¨¡å‹åŠ è½½æ—¶é—´
Qwen3-32Bï¼ˆBF16ï¼Œçº¦ 65GBï¼‰ä» HuggingFace Hub é¦–æ¬¡ä¸‹è½½å¯èƒ½éœ€è¦ 10-20 åˆ†é’Ÿï¼Œå–å†³äºç½‘ç»œé€Ÿåº¦ã€‚åç»­éƒ¨ç½²å°†åˆ©ç”¨èŠ‚ç‚¹çš„æœ¬åœ°ç¼“å­˜ï¼Œæ˜¾è‘—ç¼©çŸ­åŠ è½½æ—¶é—´ã€‚
:::

---

## æ¨ç†è¯·æ±‚æµ‹è¯•

### 6.1 ç«¯å£è½¬å‘

```bash
# å¯¹ Inference Gateway è¿›è¡Œç«¯å£è½¬å‘
kubectl port-forward svc/inference-gateway -n ${NAMESPACE} 8080:8080
```

### 6.2 åŸºæœ¬ curl æµ‹è¯•

```bash
curl -s http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-32B",
    "messages": [
      {
        "role": "user",
        "content": "What is Kubernetes? Please explain briefly."
      }
    ],
    "max_tokens": 256,
    "temperature": 0.7
  }' | jq .
```

é¢„æœŸå“åº”ç»“æ„ï¼š

```json
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "model": "Qwen/Qwen3-32B",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Kubernetes is an open-source platform for automating the deployment, scaling..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 128,
    "total_tokens": 143
  }
}
```

### 6.3 Python å®¢æˆ·ç«¯

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8080/v1",
    api_key="not-needed"  # llm-d ä¸éœ€è¦å•ç‹¬çš„è®¤è¯
)

response = client.chat.completions.create(
    model="Qwen/Qwen3-32B",
    messages=[
        {"role": "system", "content": "You are a cloud native expert."},
        {"role": "user", "content": "Explain 3 advantages of EKS Auto Mode."}
    ],
    max_tokens=512,
    temperature=0.7
)
print(response.choices[0].message.content)
```

### 6.4 æµå¼å“åº”æµ‹è¯•

```python
stream = client.chat.completions.create(
    model="Qwen/Qwen3-32B",
    messages=[
        {"role": "user", "content": "How does llm-d's KV Cache-aware routing work?"}
    ],
    max_tokens=512,
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()
```

### 6.5 æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨

```bash
curl -s http://localhost:8080/v1/models | jq .
```

:::info OpenAI å…¼å®¹ API
llm-d æä¾› OpenAI å…¼å®¹ APIã€‚ä½¿ç”¨ç°æœ‰ OpenAI SDK çš„åº”ç”¨åªéœ€æ›´æ”¹ `base_url` å³å¯ç«‹å³ä½¿ç”¨ã€‚æ”¯æŒ `/v1/chat/completions`ã€`/v1/completions` å’Œ `/v1/models` ç«¯ç‚¹ã€‚
:::

---

## ç†è§£ KV Cache æ„ŸçŸ¥è·¯ç”±

llm-d çš„æ ¸å¿ƒå·®å¼‚åŒ–ç‰¹æ€§æ˜¯å…¶æ„ŸçŸ¥ KV Cache çŠ¶æ€çš„æ™ºèƒ½è·¯ç”±ã€‚

```mermaid
sequenceDiagram
    participant C as Client
    participant GW as Inference Gateway
    participant P1 as vLLM Pod 1<br/>(Cache: "What is K8s")
    participant P2 as vLLM Pod 2<br/>(Cache: "What is AWS")
    participant P3 as vLLM Pod 3<br/>(Empty)

    Note over GW: KV Cache-aware Routing

    C->>GW: "What is K8s?"
    GW->>GW: Check Prefix Match
    GW->>P1: Route (Cache Hit!)
    P1->>C: Fast Response (TTFT â†“)

    C->>GW: "What is AWS?"
    GW->>GW: Check Prefix Match
    GW->>P2: Route (Cache Hit!)
    P2->>C: Fast Response (TTFT â†“)

    C->>GW: "A completely new question"
    GW->>GW: Cache Miss
    GW->>P3: Load Balancing Fallback
    P3->>C: Normal Response
```

### è·¯ç”±å·¥ä½œåŸç†

1. **æ¥æ”¶è¯·æ±‚**ï¼šå®¢æˆ·ç«¯å‘ Inference Gateway å‘é€æ¨ç†è¯·æ±‚
2. **å‰ç¼€åˆ†æ**ï¼šGateway å¯¹è¯·æ±‚çš„æç¤ºå‰ç¼€è¿›è¡Œå“ˆå¸Œä»¥è¿›è¡Œè¯†åˆ«
3. **ç¼“å­˜æŸ¥æ‰¾**ï¼šæ£€æŸ¥æ¯ä¸ª vLLM Pod çš„ KV Cache çŠ¶æ€ï¼Œæ‰¾åˆ°æŒæœ‰åŒ¹é…å‰ç¼€çš„ Pod
4. **æ™ºèƒ½è·¯ç”±**ï¼šç¼“å­˜å‘½ä¸­æ—¶è·¯ç”±åˆ°åŒ¹é…çš„ Podï¼›æœªå‘½ä¸­æ—¶å›é€€åˆ°åŸºäºè´Ÿè½½çš„å‡è¡¡
5. **è¿”å›å“åº”**ï¼švLLM é€šè¿‡ Gateway å°†æ¨ç†ç»“æœè¿”å›ç»™å®¢æˆ·ç«¯

### KV Cache æ„ŸçŸ¥è·¯ç”±çš„æ•ˆæœ

<KVCacheEffectsTable />

:::tip æœ€å¤§åŒ–ç¼“å­˜å‘½ä¸­ç‡
KV Cache æ„ŸçŸ¥è·¯ç”±åœ¨ä½¿ç”¨ç›¸åŒç³»ç»Ÿæç¤ºçš„åº”ç”¨ä¸­æœ€ä¸ºæœ‰æ•ˆã€‚ä¾‹å¦‚ï¼Œåœ¨é‡å¤å¼•ç”¨ç›¸åŒä¸Šä¸‹æ–‡æ–‡æ¡£çš„ RAG ç®¡é“ä¸­ï¼Œå¤ç”¨è¯¥å‰ç¼€çš„ KV Cache å¯ä»¥æ˜¾è‘—é™ä½ TTFTã€‚
:::

---

## ç›‘æ§ä¸éªŒè¯

### 8.1 æŸ¥çœ‹ vLLM æŒ‡æ ‡

```bash
# è®¿é—® vLLM Pod çš„æŒ‡æ ‡ç«¯ç‚¹
VLLM_POD=$(kubectl get pods -n ${NAMESPACE} -l app=vllm -o jsonpath='{.items[0].metadata.name}')
kubectl port-forward ${VLLM_POD} -n ${NAMESPACE} 9090:9090

# æŸ¥è¯¢æŒ‡æ ‡
curl -s http://localhost:9090/metrics | grep -E "vllm_"
```

### å…³é”®ç›‘æ§æŒ‡æ ‡

<MonitoringMetricsTable />

### 8.2 æ£€æŸ¥ GPU åˆ©ç”¨ç‡

```bash
# åœ¨ç‰¹å®š vLLM Pod ä¸Šè¿è¡Œ nvidia-smi
kubectl exec -it ${VLLM_POD} -n ${NAMESPACE} -- nvidia-smi

# å®æ—¶ GPU ç›‘æ§ï¼ˆ1 ç§’é—´éš”ï¼‰
kubectl exec -it ${VLLM_POD} -n ${NAMESPACE} -- nvidia-smi dmon -s u -d 1
```

### 8.3 æŸ¥çœ‹ Gateway æ—¥å¿—

```bash
# æŸ¥çœ‹ Inference Gateway æ—¥å¿—
kubectl logs -f deployment/inference-gateway -n ${NAMESPACE}

# è¯¦ç»†æŸ¥çœ‹ InferencePool çŠ¶æ€
kubectl describe inferencepool -n ${NAMESPACE}
```

### 8.4 Prometheus ServiceMonitor é…ç½®

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-d-vllm-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: vllm
  endpoints:
    - port: metrics
      path: /metrics
      interval: 15s
  namespaceSelector:
    matchNames:
      - llm-d
```

---

## è¿ç»´æ³¨æ„äº‹é¡¹

### 9.1 S3 æ¨¡å‹ç¼“å­˜

æ¯æ¬¡ä» HuggingFace Hub ä¸‹è½½æ¨¡å‹ä¼šå¢åŠ å†·å¯åŠ¨æ—¶é—´ã€‚å¯ä»¥å°†æ¨¡å‹æƒé‡ç¼“å­˜åˆ° S3 ä»¥å‡å°‘åŠ è½½æ—¶é—´ã€‚

```yaml
# åœ¨ vLLM ç¯å¢ƒå˜é‡ä¸­æ·»åŠ  S3 ç¼“å­˜è·¯å¾„
env:
  - name: VLLM_S3_MODEL_CACHE
    value: "s3://your-bucket/model-cache/qwen3-32b/"
```

<ModelLoadingTable />

### 9.2 HPAï¼ˆHorizontal Pod Autoscalerï¼‰é…ç½®

æ‚¨å¯ä»¥æ ¹æ® vLLM ç­‰å¾…è¯·æ±‚æ•°é…ç½®è‡ªåŠ¨æ‰©å±•ã€‚

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-hpa
  namespace: llm-d
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-deployment
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Pods
      pods:
        metric:
          name: vllm_num_requests_waiting
        target:
          type: AverageValue
          averageValue: "5"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 120
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 180
```

:::info HPA ä¸ Karpenter é›†æˆ
å½“ HPA å¢åŠ  vLLM å‰¯æœ¬æ•°ä¸”éœ€è¦é¢å¤– GPU æ—¶ï¼ŒKarpenter ä¼šè‡ªåŠ¨ä¾›åº”æ–°çš„ p5.48xlarge èŠ‚ç‚¹ã€‚åœ¨ EKS Auto Mode ä¸­ï¼Œæ­¤è¿‡ç¨‹å®Œå…¨è‡ªåŠ¨åŒ–ã€‚
:::

### 9.3 æˆæœ¬ä¼˜åŒ–

<CostOptimizationTable />

:::warning æˆæœ¬è­¦å‘Š
p5.48xlarge çš„è´¹ç”¨çº¦ä¸ºæ¯å°æ—¶ $98.32ï¼ˆus-west-2 æŒ‰éœ€å®šä»·ï¼‰ã€‚è¿è¡Œ 2 ä¸ªå®ä¾‹æ¯æœˆçº¦éœ€ **$141,580**ã€‚æµ‹è¯•å®ŒæˆååŠ¡å¿…æ¸…ç†èµ„æºã€‚

```bash
# æ¸…ç†èµ„æº
helmfile destroy -n ${NAMESPACE}
kubectl delete namespace ${NAMESPACE}
kubectl delete nodepool gpu-p5

# åˆ é™¤é›†ç¾¤ï¼ˆå¦‚éœ€è¦ï¼‰
eksctl delete cluster --name llm-d-cluster --region us-west-2
```

:::

---

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

<TroubleshootingTable />

### è°ƒè¯•å‘½ä»¤å‚è€ƒ

```bash
# æ£€æŸ¥ Pod çŠ¶æ€å’Œäº‹ä»¶
kubectl describe pod <pod-name> -n llm-d

# æŸ¥çœ‹ vLLM æ—¥å¿—ï¼ˆæœ€å 100 è¡Œï¼‰
kubectl logs <vllm-pod> -n llm-d --tail=100

# æ£€æŸ¥ GPU çŠ¶æ€
kubectl exec -it <vllm-pod> -n llm-d -- nvidia-smi

# è¯¦ç»†æŸ¥çœ‹ InferencePool çŠ¶æ€
kubectl describe inferencepool -n llm-d

# æŸ¥çœ‹ InferenceModel çŠ¶æ€
kubectl describe inferencemodel -n llm-d

# æŸ¥çœ‹ Gateway æ—¥å¿—
kubectl logs -f deployment/inference-gateway -n llm-d

# æŸ¥çœ‹èŠ‚ç‚¹ GPU èµ„æº
kubectl get nodes -o custom-columns=\
  NAME:.metadata.name,\
  GPU:.status.allocatable.nvidia\\.com/gpu,\
  STATUS:.status.conditions[-1].type

# æŸ¥çœ‹ Karpenter æ—¥å¿—ï¼ˆæ’æŸ¥èŠ‚ç‚¹ä¾›åº”é—®é¢˜ï¼‰
kubectl logs -f deployment/karpenter -n kube-system
```

:::tip NCCL è°ƒè¯•
å¦‚æœå‡ºç°å¤š GPU é€šä¿¡é—®é¢˜ï¼Œæ·»åŠ ä»¥ä¸‹ç¯å¢ƒå˜é‡ä»¥å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š

```yaml
env:
  - name: NCCL_DEBUG
    value: "INFO"
  - name: NCCL_DEBUG_SUBSYS
    value: "ALL"
```

:::

---

## åç»­æ­¥éª¤

æœ¬æŒ‡å—æ¶µç›–äº† llm-d çš„ Intelligent Inference Scheduling è·¯å¾„ã€‚ä½œä¸ºåç»­æ­¥éª¤ï¼Œæ‚¨å¯ä»¥æ¢ç´¢é«˜çº§åŠŸèƒ½ã€‚

- **Prefill/Decode åˆ†ç¦»**ï¼šå°† Prefill å’Œ Decode é˜¶æ®µåˆ†ç¦»åˆ°ä¸åŒçš„ Pod ç»„ï¼Œä¸ºå¤§æ‰¹é‡å¤„ç†å’Œé•¿ä¸Šä¸‹æ–‡å·¥ä½œè´Ÿè½½æœ€å¤§åŒ–ååé‡
- **Expert å¹¶è¡Œ**ï¼šå°† MoE æ¨¡å‹ï¼ˆMixtralã€DeepSeek ç­‰ï¼‰çš„ Expert åˆ†å¸ƒåˆ°å¤šä¸ªèŠ‚ç‚¹ï¼Œå®ç°è¶…å¤§æ¨¡å‹æœåŠ¡
- **LoRA Adapter çƒ­åˆ‡æ¢**ï¼šåœ¨å•ä¸ªåŸºç¡€æ¨¡å‹ä¸ŠåŠ¨æ€åŠ è½½/å¸è½½å¤šä¸ª LoRA é€‚é…å™¨ï¼Œå®ç°å¤šä»»åŠ¡æœåŠ¡
- **Prometheus + Grafana ä»ªè¡¨æ¿**ï¼šåŸºäº vLLM æŒ‡æ ‡æ„å»ºå®æ—¶ç›‘æ§ä»ªè¡¨æ¿
- **å¤šæ¨¡å‹æœåŠ¡**ï¼šä½¿ç”¨ InferenceModel CRD åœ¨å•ä¸ª llm-d é›†ç¾¤ä¸ŠåŒæ—¶æœåŠ¡å¤šä¸ªæ¨¡å‹

### ç›¸å…³æ–‡æ¡£

- [åŸºäº vLLM çš„åŸºç¡€æ¨¡å‹éƒ¨ç½²ä¸æ€§èƒ½ä¼˜åŒ–](./vllm-model-serving.md) â€” vLLM åŸºç¡€å’Œéƒ¨ç½²
- [MoE æ¨¡å‹æœåŠ¡æŒ‡å—](./moe-model-serving.md) â€” Mixture of Experts æ¨¡å‹æœåŠ¡
- [æ¨ç†ç½‘å…³ä¸åŠ¨æ€è·¯ç”±](./inference-gateway-routing.md) â€” æ¨ç†è·¯ç”±ç­–ç•¥
- [GPU èµ„æºç®¡ç†](./gpu-resource-management.md) â€” GPU é›†ç¾¤èµ„æºç®¡ç†

---

## å‚è€ƒèµ„æ–™

- [llm-d GitHub](https://github.com/llm-d/llm-d)
- [llm-d Deployer (Helm Charts)](https://github.com/llm-d/llm-d-deployer)
- [EKS Auto Mode æ–‡æ¡£](https://docs.aws.amazon.com/eks/latest/userguide/automode.html)
- [Gateway API Inference Extension](https://gateway-api.sigs.k8s.io/geps/gep-3567/)
- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [Qwen3-32B HuggingFace](https://huggingface.co/Qwen/Qwen3-32B)
- [Kubernetes Gateway API](https://gateway-api.sigs.k8s.io/)
