---
title: "Ragas RAG è¯„ä¼°æ¡†æ¶"
sidebar_label: "13. Ragas Evaluation"
description: "ä½¿ç”¨ Ragas è¯„ä¼°å’ŒæŒç»­æ”¹è¿› RAG ç®¡é“è´¨é‡"
sidebar_position: 13
tags:
  - ragas
  - rag
  - evaluation
  - llm
  - quality
  - genai
  - testing
last_update:
  date: 2026-02-14
  author: devfloor9
category: "genai-aiml"
---

import { RagasVsBedrockComparison, RagasMetrics, CostOptimizationStrategies, CostComparison, ImprovementChecklist } from '@site/src/components/RagasTables';

# Ragas RAG è¯„ä¼°æ¡†æ¶

Ragasï¼ˆRAG Assessmentï¼‰æ˜¯ä¸€ä¸ªç”¨äºå®¢è§‚è¯„ä¼° RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç®¡é“è´¨é‡çš„å¼€æºæ¡†æ¶ã€‚å®ƒå¯¹äºè¡¡é‡ Agentic AI å¹³å°ä¸­ RAG ç³»ç»Ÿçš„æ€§èƒ½å¹¶æŒç»­æ”¹è¿›è‡³å…³é‡è¦ã€‚

## æ¦‚è¿°

### ä¸ºä»€ä¹ˆéœ€è¦ RAG è¯„ä¼°

RAG ç³»ç»Ÿç”±å¤šä¸ªç»„ä»¶ï¼ˆæ£€ç´¢ã€ç”Ÿæˆã€ä¸Šä¸‹æ–‡å¤„ç†ï¼‰ç»„æˆï¼Œä½¿å¾—æ•´ä½“è´¨é‡éš¾ä»¥è¡¡é‡ï¼š

```mermaid
graph LR
    subgraph "RAG Pipeline"
        Q["Question"]
        R["Retrieval"]
        C["Context"]
        G["Generation"]
        A["Answer"]
    end

    subgraph "Evaluation Points"
        E1["Retrieval Quality"]
        E2["Context Relevancy"]
        E3["Answer Accuracy"]
        E4["Answer Faithfulness"]
    end

    Q --> R
    R --> C
    C --> G
    G --> A

    R -.-> E1
    C -.-> E2
    A -.-> E3
    A -.-> E4

    style E1 fill:#4285f4,stroke:#333
    style E2 fill:#34a853,stroke:#333
    style E3 fill:#fbbc04,stroke:#333
    style E4 fill:#ea4335,stroke:#333
```

### Ragas æ ¸å¿ƒæŒ‡æ ‡

<RagasMetrics />

## å®‰è£…ä¸åŸºæœ¬è®¾ç½®

### Python ç¯å¢ƒè®¾ç½®

```bash
# å®‰è£… Ragas
pip install ragas langchain-openai datasets

# é¢å¤–ä¾èµ–
pip install pandas numpy
```

### åŸºæœ¬è¯„ä¼°ä»£ç 

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset

# å‡†å¤‡è¯„ä¼°æ•°æ®é›†
eval_data = {
    "question": [
        "How do you perform GPU scheduling in Kubernetes?",
        "What are the key features of Karpenter?",
    ],
    "answer": [
        "GPU scheduling in Kubernetes is performed through NVIDIA Device Plugin...",
        "Karpenter provides automatic node provisioning, consolidation, and drift detection...",
    ],
    "contexts": [
        ["GPU scheduling is through Device Plugin...", "NVIDIA GPU Operator is..."],
        ["Karpenter is a Kubernetes node auto-scaler...", "NodePool CRD is used..."],
    ],
    "ground_truth": [
        "GPU resources are scheduled using NVIDIA Device Plugin and GPU Operator.",
        "Karpenter provides automatic node provisioning, consolidation, drift detection, and disruption handling.",
    ],
}

dataset = Dataset.from_dict(eval_data)

# è¿è¡Œè¯„ä¼°
results = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ],
)

print(results)
```

## æ ¸å¿ƒæŒ‡æ ‡è¯¦è§£

### 1. å¿ å®åº¦ï¼ˆFaithfulnessï¼‰

è¡¡é‡ç­”æ¡ˆå¯¹æ‰€æä¾›ä¸Šä¸‹æ–‡çš„å¿ å®ç¨‹åº¦ã€‚è¯¥æŒ‡æ ‡æ˜¯æ£€æµ‹å¹»è§‰çš„å…³é”®ã€‚

```python
from ragas.metrics import faithfulness

# å¿ å®åº¦è®¡ç®—è¿‡ç¨‹ï¼š
# 1. å°†ç­”æ¡ˆåˆ†è§£ä¸ºç‹¬ç«‹çš„å£°æ˜
# 2. éªŒè¯æ¯ä¸ªå£°æ˜æ˜¯å¦å¯ä»¥ä»ä¸Šä¸‹æ–‡ä¸­æ¨æ–­
# 3. å¿ å®åº¦åˆ†æ•° = å·²éªŒè¯å£°æ˜æ•° / æ€»å£°æ˜æ•°

# åˆ†æ•°è§£è¯»ï¼š
# 1.0ï¼šæ‰€æœ‰å£°æ˜éƒ½æœ‰ä¸Šä¸‹æ–‡æ”¯æŒ
# 0.5ï¼šä¸€åŠçš„å£°æ˜æœ‰ä¸Šä¸‹æ–‡æ”¯æŒ
# 0.0ï¼šæ²¡æœ‰å£°æ˜æœ‰ä¸Šä¸‹æ–‡æ”¯æŒï¼ˆä¸¥é‡å¹»è§‰ï¼‰
```

### 2. ç­”æ¡ˆç›¸å…³æ€§ï¼ˆAnswer Relevancyï¼‰

è¡¡é‡ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦ã€‚

```python
from ragas.metrics import answer_relevancy

# ç­”æ¡ˆç›¸å…³æ€§è®¡ç®—è¿‡ç¨‹ï¼š
# 1. ä»ç­”æ¡ˆç”Ÿæˆé—®é¢˜ï¼ˆé€†å‘å·¥ç¨‹ï¼‰
# 2. è®¡ç®—ç”Ÿæˆé—®é¢˜ä¸åŸå§‹é—®é¢˜çš„ç›¸ä¼¼åº¦
# 3. é‡å¤å¤šæ¬¡å–å¹³å‡å€¼

# åˆ†æ•°è§£è¯»ï¼š
# é«˜åˆ†ï¼šç­”æ¡ˆä¸é—®é¢˜ç›´æ¥ç›¸å…³
# ä½åˆ†ï¼šç­”æ¡ˆåŒ…å«æ— å…³å†…å®¹
```

### 3. ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼ˆContext Precisionï¼‰

è¡¡é‡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­å®é™…æœ‰ç”¨ä¿¡æ¯çš„å æ¯”ã€‚

```python
from ragas.metrics import context_precision

# ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦è®¡ç®—ï¼š
# - è¯†åˆ«ç”ŸæˆçœŸå®ç­”æ¡ˆæ‰€éœ€çš„ä¸Šä¸‹æ–‡
# - æ£€æŸ¥æœ‰ç”¨ä¿¡æ¯æ˜¯å¦åœ¨æ’åé å‰çš„ä¸Šä¸‹æ–‡ä¸­
# - ç›¸å…³ä¸Šä¸‹æ–‡æ’åè¶Šé å‰ï¼Œåˆ†æ•°è¶Šé«˜
```

### 4. ä¸Šä¸‹æ–‡å¬å›ç‡ï¼ˆContext Recallï¼‰

è¡¡é‡ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„ä¿¡æ¯æ˜¯å¦åŒ…å«åœ¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­ã€‚

```python
from ragas.metrics import context_recall

# ä¸Šä¸‹æ–‡å¬å›ç‡è®¡ç®—ï¼š
# 1. å°†çœŸå®ç­”æ¡ˆåˆ†è§£ä¸ºç‹¬ç«‹å¥å­
# 2. æ£€æŸ¥æ¯ä¸ªå¥å­æ˜¯å¦å¯ä»¥ä»æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­æ¨æ–­
# 3. å¬å›ç‡åˆ†æ•° = å¯æ¨æ–­å¥å­æ•° / æ€»å¥å­æ•°
```

## ç»¼åˆè¯„ä¼°ç®¡é“

### å®Œæ•´ RAG ç³»ç»Ÿè¯„ä¼°

```python
import os
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    context_relevancy,
    answer_correctness,
)
from datasets import Dataset
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# LLM é…ç½®ï¼ˆç”¨äºè¯„ä¼°ï¼‰
os.environ["OPENAI_API_KEY"] = "your-api-key"

def evaluate_rag_pipeline(questions, rag_chain, ground_truths):
    """ç»¼åˆ RAG ç®¡é“è¯„ä¼°"""

    answers = []
    contexts = []

    for question in questions:
        # è¿è¡Œ RAG é“¾
        result = rag_chain.invoke({"query": question})
        answers.append(result["result"])
        contexts.append([doc.page_content for doc in result["source_documents"]])

    # æ„å»ºè¯„ä¼°æ•°æ®é›†
    eval_dataset = Dataset.from_dict({
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths,
    })

    # ä½¿ç”¨æ‰€æœ‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°
    results = evaluate(
        eval_dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
            context_relevancy,
            answer_correctness,
        ],
    )

    return results

# ä½¿ç”¨ç¤ºä¾‹
questions = [
    "How do you set up Karpenter in EKS?",
    "How do you configure GPU node auto-scaling?",
    "How do you set up dynamic routing in Inference Gateway?",
]

ground_truths = [
    "Karpenter is installed via Helm chart and configured with NodePool CRD.",
    "Configure GPU-based scaling by integrating DCGM Exporter metrics with KEDA.",
    "Set up weighted traffic distribution using Gateway API HTTPRoute.",
]

# è¿è¡Œè¯„ä¼°
results = evaluate_rag_pipeline(questions, rag_chain, ground_truths)
print(results.to_pandas())
```

### åˆ†æè¯„ä¼°ç»“æœ

```python
import pandas as pd
import matplotlib.pyplot as plt

def analyze_evaluation_results(results):
    """åˆ†æå¹¶å¯è§†åŒ–è¯„ä¼°ç»“æœ"""

    df = results.to_pandas()

    # å„æŒ‡æ ‡å¹³å‡åˆ†
    metrics_summary = df.mean(numeric_only=True)
    print("=== å„æŒ‡æ ‡å¹³å‡åˆ† ===")
    print(metrics_summary)

    # è¯†åˆ«éœ€è¦æ”¹è¿›çš„é¢†åŸŸ
    print("\n=== éœ€è¦æ”¹è¿›çš„é¢†åŸŸ ===")
    for metric, score in metrics_summary.items():
        if score < 0.7:
            print(f"âš ï¸ {metric}: {score:.2f} - éœ€è¦æ”¹è¿›")
        elif score < 0.85:
            print(f"ğŸ“Š {metric}: {score:.2f} - å°šå¯")
        else:
            print(f"âœ… {metric}: {score:.2f} - ä¼˜ç§€")

    # å¯è§†åŒ–
    fig, ax = plt.subplots(figsize=(10, 6))
    metrics_summary.plot(kind='bar', ax=ax, color=['#4285f4', '#34a853', '#fbbc04', '#ea4335', '#9c27b0', '#00bcd4'])
    ax.set_ylabel('Score')
    ax.set_title('RAG Pipeline Evaluation Results')
    ax.set_ylim(0, 1)
    ax.axhline(y=0.7, color='r', linestyle='--', label='Minimum Threshold')
    ax.legend()
    plt.tight_layout()
    plt.savefig('rag_evaluation_results.png')

    return metrics_summary

# è¿è¡Œåˆ†æ
summary = analyze_evaluation_results(results)
```

## CI/CD ç®¡é“é›†æˆ

### GitHub Actions å·¥ä½œæµ

```yaml
# .github/workflows/rag-evaluation.yml
name: RAG Pipeline Evaluation

on:
  push:
    paths:
      - 'src/rag/**'
      - 'data/knowledge_base/**'
  pull_request:
    paths:
      - 'src/rag/**'
  schedule:
    - cron: '0 0 * * *'  # æ¯å¤©åˆå¤œæ‰§è¡Œ

jobs:
  evaluate:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install ragas langchain-openai datasets pandas

    - name: Run RAG Evaluation
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python scripts/evaluate_rag.py --output results/evaluation.json

    - name: Check Quality Gates
      run: |
        python scripts/check_quality_gates.py results/evaluation.json

    - name: Upload Results
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-results
        path: results/

    - name: Comment PR with Results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const results = JSON.parse(fs.readFileSync('results/evaluation.json'));

          let comment = '## RAG Evaluation Results\n\n';
          comment += '| Metric | Score | Status |\n';
          comment += '|--------|-------|--------|\n';

          for (const [metric, score] of Object.entries(results.metrics)) {
            const status = score >= 0.7 ? 'âœ…' : 'âš ï¸';
            comment += `| ${metric} | ${score.toFixed(2)} | ${status} |\n`;
          }

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
```

### è´¨é‡é—¨æ§è„šæœ¬

```python
# scripts/check_quality_gates.py
import json
import sys

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2026-02-13 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 2 åˆ†é’Ÿ


QUALITY_GATES = {
    "faithfulness": 0.8,
    "answer_relevancy": 0.75,
    "context_precision": 0.7,
    "context_recall": 0.7,
}

def check_quality_gates(results_file):
    with open(results_file) as f:
        results = json.load(f)

    failed_gates = []

    for metric, threshold in QUALITY_GATES.items():
        score = results["metrics"].get(metric, 0)
        if score < threshold:
            failed_gates.append({
                "metric": metric,
                "score": score,
                "threshold": threshold,
            })

    if failed_gates:
        print("âŒ è´¨é‡é—¨æ§æœªé€šè¿‡ï¼š")
        for gate in failed_gates:
            print(f"  - {gate['metric']}: {gate['score']:.2f} < {gate['threshold']}")
        sys.exit(1)
    else:
        print("âœ… æ‰€æœ‰è´¨é‡é—¨æ§å·²é€šè¿‡ï¼")
        sys.exit(0)

if __name__ == "__main__":
    check_quality_gates(sys.argv[1])
```

## Kubernetes å®šæ—¶è¯„ä¼°ä»»åŠ¡

### è¯„ä¼°ä»»åŠ¡å®šä¹‰

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: rag-evaluation
  namespace: genai-platform
spec:
  schedule: "0 6 * * *"  # æ¯å¤©æ—©ä¸Š 6 ç‚¹
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: evaluator
            image: your-registry/rag-evaluator:latest
            env:
            - name: OPENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: openai-credentials
                  key: api-key
            - name: MILVUS_HOST
              value: "milvus-proxy.milvus.svc.cluster.local"
            - name: RESULTS_BUCKET
              value: "s3://rag-evaluation-results"
            command:
            - python
            - /app/evaluate.py
            - --config=/app/config/evaluation.yaml
            - --output=s3
            resources:
              requests:
                cpu: "1"
                memory: "2Gi"
              limits:
                cpu: "2"
                memory: "4Gi"
          restartPolicy: OnFailure
          serviceAccountName: rag-evaluator
```

### è¯„ä¼°é…ç½® ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: rag-evaluation-config
  namespace: genai-platform
data:
  evaluation.yaml: |
    evaluation:
      metrics:
        - faithfulness
        - answer_relevancy
        - context_precision
        - context_recall

      test_sets:
        - name: "general_knowledge"
          path: "s3://test-data/general.json"
          weight: 0.4
        - name: "technical_docs"
          path: "s3://test-data/technical.json"
          weight: 0.6

      quality_gates:
        faithfulness: 0.8
        answer_relevancy: 0.75
        context_precision: 0.7
        context_recall: 0.7

      alerts:
        slack_webhook: "https://hooks.slack.com/..."
        threshold_drop: 0.1  # ä¸‹é™ 10% ä»¥ä¸Šæ—¶å‘Šè­¦
```

## è¯„ä¼°ç»“æœè§£è¯»ä¸æ”¹è¿›æŒ‡å—

### å„æŒ‡æ ‡æ”¹è¿›æ–¹å‘

```mermaid
graph TD
    subgraph "Low Faithfulness"
        F1["åœ¨æç¤ºä¸­å¢åŠ ä¸Šä¸‹æ–‡å¼ºè°ƒ"]
        F2["é™ä½ temperature"]
        F3["ä½¿ç”¨æ›´å¼ºçš„ LLM"]
    end

    subgraph "Low Context Precision"
        CP1["æ”¹è¿› Embedding æ¨¡å‹"]
        CP2["è°ƒæ•´åˆ†å—ç­–ç•¥"]
        CP3["æ·»åŠ é‡æ’åºæ¨¡å‹"]
    end

    subgraph "Low Context Recall"
        CR1["å¢åŠ æ£€ç´¢çš„ k å€¼"]
        CR2["åº”ç”¨æ··åˆæœç´¢"]
        CR3["æ‰©å±•çŸ¥è¯†åº“"]
    end

    subgraph "Low Answer Relevancy"
        AR1["æ˜ç¡®æç¤ºå†…å®¹"]
        AR2["æ·»åŠ  few-shot ç¤ºä¾‹"]
        AR3["æŒ‡å®šè¾“å‡ºæ ¼å¼"]
    end
```

### æ”¹è¿›æ¸…å•

<ImprovementChecklist />

## ç›¸å…³æ–‡æ¡£

- [Milvus å‘é‡æ•°æ®åº“](./milvus-vector-database.md)
- [Agent ç›‘æ§](./agent-monitoring.md)
- [Agentic AI å¹³å°æ¶æ„](./agentic-platform-architecture.md)

:::tip å»ºè®®

- è¯„ä¼°æ•°æ®é›†ä¸­è‡³å°‘åŒ…å« 50 ä¸ªå¤šæ ·åŒ–çš„é—®é¢˜
- ä½¿ç”¨é¢†åŸŸä¸“å®¶éªŒè¯è¿‡çš„ç­”æ¡ˆä½œä¸ºçœŸå®æ ‡å‡†
- é€šè¿‡å®šæœŸè¯„ä¼°è·Ÿè¸ªè´¨é‡éšæ—¶é—´çš„å˜åŒ–
:::

:::warning æ³¨æ„äº‹é¡¹

- Ragas è¯„ä¼°éœ€è¦è°ƒç”¨ LLM APIï¼Œä¼šäº§ç”Ÿè´¹ç”¨
- å¯¹äºå¤§è§„æ¨¡è¯„ä¼°ï¼Œè¯·ä½¿ç”¨æ‰¹å¤„ç†å’Œç¼“å­˜
- è¯„ä¼°ç»“æœå¯èƒ½å› ä½¿ç”¨çš„ LLM ä¸åŒè€Œæœ‰æ‰€å·®å¼‚
:::
