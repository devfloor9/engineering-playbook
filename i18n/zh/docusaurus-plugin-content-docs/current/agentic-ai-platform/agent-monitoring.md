---
title: "AI Agent ç›‘æ§ä¸è¿ç»´"
sidebar_label: "12. Agent Monitoring & Operations"
description: "ä½¿ç”¨ LangFuse å’Œ LangSmith ç›‘æ§ã€å‘Šè­¦å’Œæ’æŸ¥ Agentic AI åº”ç”¨çš„ç»¼åˆæŒ‡å—"
tags: [eks, langfuse, langsmith, monitoring, observability, tracing, opentelemetry, operations, troubleshooting, alerting]
category: "genai-aiml"
sidebar_position: 12
last_update:
  date: 2026-02-14
  author: devfloor9
---

import {
  LangFuseVsLangSmithTable,
  LatencyMetricsTable,
  TokenUsageMetricsTable,
  ErrorRateMetricsTable,
  DailyChecksTable,
  WeeklyChecksTable,
  MaturityModelTable
} from '@site/src/components/AgentMonitoringTables';

# AI Agent ç›‘æ§ä¸è¿ç»´

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2025-02-05 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 4 åˆ†é’Ÿ

æœ¬æ–‡æ¡£ä»‹ç»ä½¿ç”¨ LangFuse å’Œ LangSmith æœ‰æ•ˆè¿½è¸ªå’Œç›‘æ§ Agentic AI åº”ç”¨æ€§èƒ½ä¸è¡Œä¸ºçš„ç»¼åˆæ–¹æ³•ã€‚æˆ‘ä»¬æä¾›äº†ä» Kubernetes ç¯å¢ƒéƒ¨ç½²åˆ° Grafana ä»ªè¡¨ç›˜é…ç½®ã€å‘Šè­¦è®¾ç½®å’Œæ•…éšœæ’æŸ¥çš„å®Œæ•´è¿ç»´æŒ‡å—ã€‚

## æ¦‚è¿°

Agentic AI åº”ç”¨æ‰§è¡Œå¤æ‚çš„æ¨ç†é“¾å’Œå¤šæ ·çš„å·¥å…·è°ƒç”¨ï¼Œä»…ä¾é ä¼ ç»Ÿçš„ APMï¼ˆåº”ç”¨æ€§èƒ½ç›‘æ§ï¼‰å·¥å…·éš¾ä»¥å®ç°å……åˆ†çš„å¯è§æ€§ã€‚LangFuse å’Œ LangSmith æ˜¯ LLM ä¸“ç”¨çš„å¯è§‚æµ‹æ€§å·¥å…·ï¼Œæä¾›ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

- **Trace è¿½è¸ª**ï¼šè¿½è¸ª LLM è°ƒç”¨ã€å·¥å…·æ‰§è¡Œå’Œ Agent æ¨ç†è¿‡ç¨‹çš„å®Œæ•´æµç¨‹
- **Token ç”¨é‡åˆ†æ**ï¼šè®¡ç®—è¾“å…¥/è¾“å‡º Token æ•°é‡å’Œæˆæœ¬
- **è´¨é‡è¯„ä¼°**ï¼šå¯¹å“åº”è´¨é‡è¿›è¡Œè¯„åˆ†å¹¶æ”¶é›†åé¦ˆ
- **è°ƒè¯•**ï¼šé€šè¿‡å®¡æŸ¥æç¤ºè¯å’Œå“åº”æ¥è¯Šæ–­é—®é¢˜

:::info ç›®æ ‡å—ä¼—
æœ¬æ–‡æ¡£é¢å‘å¹³å°è¿ç»´äººå‘˜ã€MLOps å·¥ç¨‹å¸ˆå’Œ AI å¼€å‘è€…ã€‚éœ€è¦å…·å¤‡ Kubernetes å’Œ Python çš„åŸºç¡€çŸ¥è¯†ã€‚
:::

## LangFuse vs LangSmith å¯¹æ¯”

<LangFuseVsLangSmithTable />

:::tip é€‰å‹æŒ‡å—

- **LangFuse**ï¼šå½“æ•°æ®ä¸»æƒè‡³å…³é‡è¦æˆ–éœ€è¦æˆæœ¬ä¼˜åŒ–æ—¶
- **LangSmith**ï¼šå½“ä¸»è¦åŸºäº LangChain å¼€å‘ä¸”éœ€è¦å¿«é€Ÿä¸Šæ‰‹æ—¶
:::


## LangFuse Kubernetes éƒ¨ç½²

### æ¶æ„æ¦‚è¿°

LangFuse ç”±ä»¥ä¸‹ç»„ä»¶ç»„æˆï¼š

```mermaid
graph TB
    subgraph LangFuseStack["LangFuse Stack"]
        WEB["LangFuse Web<br/>(Next.js)"]
        API["LangFuse API<br/>(tRPC)"]
        WORKER["Background Worker<br/>(Queue Processing)"]
    end

    subgraph Storage["Storage Layer"]
        PG["PostgreSQL<br/>(Metadata)"]
        REDIS["Redis<br/>(Cache/Queue)"]
        S3["S3<br/>(Blob Storage)"]
    end

    subgraph Clients["AI Applications"]
        AGENT1["Agent 1"]
        AGENT2["Agent 2"]
        AGENTN["Agent N"]
    end

    AGENT1 --> API
    AGENT2 --> API
    AGENTN --> API
    WEB --> API
    API --> PG
    API --> REDIS
    WORKER --> PG
    WORKER --> REDIS
    WORKER --> S3

    style LangFuseStack fill:#e8f5e9
    style Storage fill:#e3f2fd
    style Clients fill:#fff3e0
```

### PostgreSQL éƒ¨ç½²

éƒ¨ç½² PostgreSQL ç”¨äº LangFuse å…ƒæ•°æ®å­˜å‚¨ã€‚

```yaml
# langfuse-postgres.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: observability
  labels:
    app.kubernetes.io/part-of: langfuse
---
apiVersion: v1
kind: Secret
metadata:
  name: langfuse-postgres-secret
  namespace: observability
type: Opaque
stringData:
  POSTGRES_USER: langfuse
  POSTGRES_PASSWORD: "your-secure-password-here"  # ç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨ Secrets Manager
  POSTGRES_DB: langfuse
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: langfuse-postgres-pvc
  namespace: observability
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: gp3
  resources:
    requests:
      storage: 100Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: langfuse-postgres
  namespace: observability
spec:
  serviceName: langfuse-postgres
  replicas: 1
  selector:
    matchLabels:
      app: langfuse-postgres
  template:
    metadata:
      labels:
        app: langfuse-postgres
    spec:
      containers:
        - name: postgres
          image: postgres:15-alpine
          ports:
            - containerPort: 5432
          envFrom:
            - secretRef:
                name: langfuse-postgres-secret
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
          livenessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - langfuse
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - langfuse
            initialDelaySeconds: 5
            periodSeconds: 5
      volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: langfuse-postgres-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: langfuse-postgres
  namespace: observability
spec:
  selector:
    app: langfuse-postgres
  ports:
    - port: 5432
      targetPort: 5432
  clusterIP: None
```


### LangFuse éƒ¨ç½²

éƒ¨ç½² LangFuse åº”ç”¨ã€‚

```yaml
# langfuse-deployment.yaml
apiVersion: v1
kind: Secret
metadata:
  name: langfuse-secret
  namespace: observability
type: Opaque
stringData:
  # å¿…éœ€çš„ç¯å¢ƒå˜é‡
  DATABASE_URL: "postgresql://langfuse:your-secure-password-here@langfuse-postgres:5432/langfuse"
  NEXTAUTH_SECRET: "your-nextauth-secret-32-chars-min"  # openssl rand -base64 32
  SALT: "your-salt-value-here"  # openssl rand -base64 32
  ENCRYPTION_KEY: "0000000000000000000000000000000000000000000000000000000000000000"  # 64 ä¸ªåå…­è¿›åˆ¶å­—ç¬¦

  # å¯é€‰çš„ç¯å¢ƒå˜é‡
  NEXTAUTH_URL: "https://langfuse.your-domain.com"
  LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: "true"

  # S3 é…ç½®ï¼ˆå¯é€‰ï¼‰
  S3_ENDPOINT: "https://s3.ap-northeast-2.amazonaws.com"
  S3_ACCESS_KEY_ID: "your-access-key"
  S3_SECRET_ACCESS_KEY: "your-secret-key"
  S3_BUCKET_NAME: "langfuse-traces"
  S3_REGION: "ap-northeast-2"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langfuse
  namespace: observability
  labels:
    app: langfuse
spec:
  replicas: 2
  selector:
    matchLabels:
      app: langfuse
  template:
    metadata:
      labels:
        app: langfuse
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "3000"
        prometheus.io/path: "/api/public/metrics"
    spec:
      containers:
        - name: langfuse
          image: langfuse/langfuse:2
          ports:
            - containerPort: 3000
              name: http
          envFrom:
            - secretRef:
                name: langfuse-secret
          env:
            - name: NODE_ENV
              value: "production"
            - name: PORT
              value: "3000"
            - name: HOSTNAME
              value: "0.0.0.0"
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /api/public/health
              port: 3000
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /api/public/health
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: langfuse
                topologyKey: kubernetes.io/hostname
---
apiVersion: v1
kind: Service
metadata:
  name: langfuse
  namespace: observability
spec:
  selector:
    app: langfuse
  ports:
    - port: 80
      targetPort: 3000
      name: http
  type: ClusterIP
```


### Ingress é…ç½®

é…ç½® Ingress ä»¥å®ç°å¤–éƒ¨è®¿é—®ã€‚

```yaml
# langfuse-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: langfuse-ingress
  namespace: observability
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:XXXXXXXXXXXX:certificate/xxx
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
    alb.ingress.kubernetes.io/ssl-redirect: "443"
    alb.ingress.kubernetes.io/healthcheck-path: /api/public/health
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: "15"
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: "5"
    alb.ingress.kubernetes.io/healthy-threshold-count: "2"
    alb.ingress.kubernetes.io/unhealthy-threshold-count: "2"
spec:
  ingressClassName: alb
  rules:
    - host: langfuse.your-domain.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: langfuse
                port:
                  number: 80
```

### HPA é…ç½®

åŸºäºæµé‡é…ç½®è‡ªåŠ¨æ‰©ç¼©å®¹ã€‚

```yaml
# langfuse-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langfuse-hpa
  namespace: observability
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langfuse
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 4
          periodSeconds: 15
      selectPolicy: Max
```

:::warning ç”Ÿäº§éƒ¨ç½²æ³¨æ„äº‹é¡¹

- å§‹ç»ˆå°† `NEXTAUTH_SECRET`ã€`SALT` å’Œ `ENCRYPTION_KEY` è®¾ç½®ä¸ºå®‰å…¨çš„éšæœºå€¼
- åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½¿ç”¨ AWS Secrets Manager æˆ– HashiCorp Vault è¿›è¡Œå¯†é’¥ç®¡ç†
- å¯¹äº PostgreSQLï¼Œå»ºè®®ä½¿ç”¨ RDSï¼ˆé«˜å¯ç”¨ã€è‡ªåŠ¨å¤‡ä»½ï¼‰
:::


## LangSmith é›†æˆ

LangSmith æ˜¯ LangChain æä¾›çš„æ‰˜ç®¡å¯è§‚æµ‹æ€§å¹³å°ã€‚è™½ç„¶æ²¡æœ‰è‡ªæ‰˜ç®¡é€‰é¡¹ï¼Œä½†ä¸åŸºäº LangChain çš„åº”ç”¨é›†æˆéå¸¸ç®€ä¾¿ã€‚

### ç¯å¢ƒé…ç½®

è®¾ç½®ä½¿ç”¨ LangSmith æ‰€éœ€çš„ç¯å¢ƒå˜é‡ã€‚

```yaml
# langsmith-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: langsmith-config
  namespace: ai-agents
type: Opaque
stringData:
  LANGCHAIN_TRACING_V2: "true"
  LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
  LANGCHAIN_API_KEY: "ls__your-api-key-here"
  LANGCHAIN_PROJECT: "agentic-ai-production"
```

### LangChain Agent é›†æˆ

å°† LangSmith ä¸ LangChain Agent é›†æˆçš„ Python ä»£ç ç¤ºä¾‹ã€‚

```python
# agent_with_langsmith.py
import os
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import tool
from langsmith import traceable
from langsmith.run_helpers import get_current_run_tree

# ç¯å¢ƒå˜é‡ï¼ˆä» Kubernetes Secret æ³¨å…¥ï¼‰
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
# LANGCHAIN_API_KEY=ls__xxx
# LANGCHAIN_PROJECT=agentic-ai-production

# å®šä¹‰è‡ªå®šä¹‰å·¥å…·
@tool
def search_knowledge_base(query: str) -> str:
    """Search the knowledge base for relevant information."""
    # Milvus æœç´¢é€»è¾‘
    return f"Search results: Information about {query}..."

@tool
def create_support_ticket(title: str, description: str, priority: str = "medium") -> str:
    """Create a customer support ticket."""
    # å·¥å•åˆ›å»ºé€»è¾‘
    return f"Ticket created: {title} (Priority: {priority})"

# Agent é…ç½®
llm = ChatOpenAI(
    model="gpt-4-turbo",
    temperature=0.7,
    max_tokens=4096,
)

prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a helpful and professional customer support agent.
    Always provide accurate information, and be honest about what you don't know.
    When necessary, search the knowledge base or create a ticket."""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

tools = [search_knowledge_base, create_support_ticket]
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    max_iterations=10,
    return_intermediate_steps=True,
)

# åŒ…è£…ä¸ºå¯è¿½è¸ªå‡½æ•°
@traceable(
    name="customer_support_agent",
    run_type="chain",
    tags=["production", "customer-support"],
)
def run_agent(user_input: str, chat_history: list = None, metadata: dict = None):
    """æ‰§è¡Œ Agent å¹¶å°† trace è®°å½•åˆ° LangSmithã€‚"""
    if chat_history is None:
        chat_history = []

    # å‘å½“å‰è¿è¡Œæ ‘æ·»åŠ å…ƒæ•°æ®
    run_tree = get_current_run_tree()
    if run_tree and metadata:
        run_tree.extra["metadata"] = metadata

    result = agent_executor.invoke({
        "input": user_input,
        "chat_history": chat_history,
    })

    return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    response = run_agent(
        user_input="Please check the shipping status of order #12345",
        metadata={
            "user_id": "user_123",
            "session_id": "session_456",
            "tenant_id": "tenant_abc",
        }
    )
    print(response)
```


### LangFuse Python é›†æˆ

å¦‚ä½•å°† LangFuse é›†æˆåˆ° Python åº”ç”¨ä¸­ã€‚

```python
# agent_with_langfuse.py
import os
from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context
from langfuse.openai import openai  # OpenAI wrapper
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.callbacks import LangfuseCallbackHandler

# åˆå§‹åŒ– LangFuse å®¢æˆ·ç«¯
langfuse = Langfuse(
    public_key=os.environ.get("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.environ.get("LANGFUSE_SECRET_KEY"),
    host=os.environ.get("LANGFUSE_HOST", "https://langfuse.your-domain.com"),
)

# LangChain å›è°ƒå¤„ç†å™¨
langfuse_handler = LangfuseCallbackHandler(
    public_key=os.environ.get("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.environ.get("LANGFUSE_SECRET_KEY"),
    host=os.environ.get("LANGFUSE_HOST"),
)

# Agent é…ç½®
llm = ChatOpenAI(
    model="gpt-4-turbo",
    temperature=0.7,
    callbacks=[langfuse_handler],
)

@observe(name="customer_support_agent")
def run_agent_with_langfuse(
    user_input: str,
    user_id: str = None,
    session_id: str = None,
    tenant_id: str = None,
):
    """å¸¦æœ‰ LangFuse è¿½è¸ªçš„ Agent æ‰§è¡Œã€‚"""

    # å‘ trace æ·»åŠ å…ƒæ•°æ®
    langfuse_context.update_current_trace(
        user_id=user_id,
        session_id=session_id,
        metadata={
            "tenant_id": tenant_id,
            "environment": os.environ.get("ENVIRONMENT", "production"),
        },
        tags=["customer-support", "production"],
    )

    # æ‰§è¡Œ Agent
    result = agent_executor.invoke(
        {"input": user_input, "chat_history": []},
        config={"callbacks": [langfuse_handler]},
    )

    # è®°å½•è¾“å‡º Token å’Œæˆæœ¬
    langfuse_context.update_current_observation(
        output=result["output"],
        metadata={
            "intermediate_steps": len(result.get("intermediate_steps", [])),
        },
    )

    return result

@observe(name="vector_search", as_type="span")
def search_with_tracing(query: str, collection: str, top_k: int = 5):
    """å¸¦è¿½è¸ªçš„å‘é‡æœç´¢ã€‚"""
    from pymilvus import Collection

    langfuse_context.update_current_observation(
        input={"query": query, "collection": collection, "top_k": top_k},
    )

    # æ‰§è¡Œ Milvus æœç´¢
    collection = Collection(collection)
    results = collection.search(
        data=[get_embedding(query)],
        anns_field="embedding",
        param={"metric_type": "COSINE", "params": {"ef": 64}},
        limit=top_k,
        output_fields=["content", "metadata"],
    )

    langfuse_context.update_current_observation(
        output={"num_results": len(results[0])},
    )

    return results

# è®°å½•è¯„åˆ†å’Œåé¦ˆ
def record_feedback(trace_id: str, score: float, comment: str = None):
    """å°†ç”¨æˆ·åé¦ˆè®°å½•åˆ° LangFuseã€‚"""
    langfuse.score(
        trace_id=trace_id,
        name="user_feedback",
        value=score,
        comment=comment,
    )

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    response = run_agent_with_langfuse(
        user_input="è¯·å‘Šè¯‰æˆ‘å…³äºäº§å“é€€è´§æµç¨‹çš„ä¿¡æ¯",
        user_id="user_123",
        session_id="session_456",
        tenant_id="tenant_abc",
    )

    # è®°å½•åé¦ˆï¼ˆä¾‹å¦‚ï¼Œç”¨æˆ·å¯¹å“åº”æ»¡æ„ï¼‰
    trace_id = langfuse_context.get_current_trace_id()
    record_feedback(trace_id, score=1.0, comment="å›ç­”å‡†ç¡®")

    # åˆ·æ–°ä»¥å‘é€æ‰€æœ‰äº‹ä»¶
    langfuse.flush()
```


## æ ¸å¿ƒç›‘æ§æŒ‡æ ‡

å®šä¹‰ Agentic AI åº”ç”¨éœ€è¦è¿½è¸ªçš„å…³é”®æŒ‡æ ‡ã€‚

### æŒ‡æ ‡åˆ†ç±»

```mermaid
graph TB
    subgraph Metrics["Core Metrics"]
        subgraph Latency["â±ï¸ Latency"]
            E2E["End-to-End Latency"]
            LLM_LAT["LLM Inference Time"]
            TOOL_LAT["Tool Execution Time"]
            RETRIEVAL_LAT["Retrieval Latency"]
        end

        subgraph Tokens["ğŸ”¢ Token Usage"]
            INPUT_TOK["Input Tokens"]
            OUTPUT_TOK["Output Tokens"]
            TOTAL_TOK["Total Tokens"]
            COST["Cost"]
        end

        subgraph Errors["âŒ Error Rate"]
            LLM_ERR["LLM Errors"]
            TOOL_ERR["Tool Errors"]
            TIMEOUT["Timeouts"]
            RATE_LIMIT["Rate Limit"]
        end

        subgraph Traces["ğŸ” Trace Analysis"]
            CHAIN_LEN["Chain Length"]
            TOOL_CALLS["Tool Call Count"]
            ITERATIONS["Iterations"]
            SUCCESS["Success Rate"]
        end
    end

    style Latency fill:#e3f2fd
    style Tokens fill:#e8f5e9
    style Errors fill:#ffcdd2
    style Traces fill:#fff3e0
```

### å»¶è¿ŸæŒ‡æ ‡

<LatencyMetricsTable />

### Token ç”¨é‡æŒ‡æ ‡

<TokenUsageMetricsTable />

### é”™è¯¯ç‡æŒ‡æ ‡

<ErrorRateMetricsTable />

### Prometheus æŒ‡æ ‡é‡‡é›†é…ç½®

```yaml
# prometheus-scrape-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-agent-scrape
  namespace: observability
data:
  agent-scrape.yaml: |
    scrape_configs:
      - job_name: 'langfuse'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - observability
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            regex: langfuse
            action: keep
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            regex: "3000"
            action: keep
        metrics_path: /api/public/metrics

      - job_name: 'ai-agents'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - ai-agents
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            regex: "true"
            action: keep
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
```


### Python æŒ‡æ ‡å¯¼å‡ºå™¨

ä» Agent åº”ç”¨ä¸­æš´éœ² Prometheus æŒ‡æ ‡çš„ä»£ç ã€‚

```python
# metrics_exporter.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# å®šä¹‰æŒ‡æ ‡
AGENT_REQUEST_DURATION = Histogram(
    'agent_request_duration_seconds',
    'Agent request duration in seconds',
    ['agent_name', 'model', 'tenant_id'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

LLM_INFERENCE_DURATION = Histogram(
    'llm_inference_duration_seconds',
    'LLM inference duration in seconds',
    ['model', 'provider'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

LLM_TOKENS = Counter(
    'llm_tokens_total',
    'Total LLM tokens used',
    ['model', 'token_type', 'tenant_id']  # token_type: input, output
)

LLM_COST = Counter(
    'llm_cost_dollars_total',
    'Total LLM cost in USD',
    ['model', 'tenant_id']
)

AGENT_ERRORS = Counter(
    'agent_errors_total',
    'Total agent errors',
    ['agent_name', 'error_type', 'tenant_id']
)

TOOL_EXECUTION_DURATION = Histogram(
    'tool_execution_duration_seconds',
    'Tool execution duration in seconds',
    ['tool_name', 'agent_name'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]
)

ACTIVE_SESSIONS = Gauge(
    'agent_active_sessions',
    'Number of active agent sessions',
    ['agent_name', 'tenant_id']
)

# æ¨¡å‹å®šä»·ï¼ˆæ¯ 1K Token çš„ç¾å…ƒä»·æ ¼ï¼‰
MODEL_COSTS = {
    "gpt-4-turbo": {"input": 0.01, "output": 0.03},
    "gpt-4": {"input": 0.03, "output": 0.06},
    "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
    "claude-3-opus": {"input": 0.015, "output": 0.075},
    "claude-3-sonnet": {"input": 0.003, "output": 0.015},
    "claude-3-haiku": {"input": 0.00025, "output": 0.00125},
}

def record_llm_usage(
    model: str,
    input_tokens: int,
    output_tokens: int,
    tenant_id: str,
    duration: float,
):
    """è®°å½• LLM ä½¿ç”¨æŒ‡æ ‡ã€‚"""
    # è®°å½• Token æ•°é‡
    LLM_TOKENS.labels(model=model, token_type="input", tenant_id=tenant_id).inc(input_tokens)
    LLM_TOKENS.labels(model=model, token_type="output", tenant_id=tenant_id).inc(output_tokens)

    # è®¡ç®—å¹¶è®°å½•æˆæœ¬
    if model in MODEL_COSTS:
        cost = (
            (input_tokens / 1000) * MODEL_COSTS[model]["input"] +
            (output_tokens / 1000) * MODEL_COSTS[model]["output"]
        )
        LLM_COST.labels(model=model, tenant_id=tenant_id).inc(cost)

    # è®°å½•æ¨ç†æ—¶é—´
    LLM_INFERENCE_DURATION.labels(model=model, provider="openai").observe(duration)

def record_agent_request(
    agent_name: str,
    model: str,
    tenant_id: str,
    duration: float,
    success: bool,
    error_type: str = None,
):
    """è®°å½• Agent è¯·æ±‚æŒ‡æ ‡ã€‚"""
    AGENT_REQUEST_DURATION.labels(
        agent_name=agent_name,
        model=model,
        tenant_id=tenant_id
    ).observe(duration)

    if not success and error_type:
        AGENT_ERRORS.labels(
            agent_name=agent_name,
            error_type=error_type,
            tenant_id=tenant_id
        ).inc()

# å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨
def start_metrics_server(port: int = 8000):
    """å¯åŠ¨ Prometheus æŒ‡æ ‡æœåŠ¡å™¨ã€‚"""
    start_http_server(port)
    print(f"Metrics server started on port {port}")
```


## Grafana ä»ªè¡¨ç›˜ä¸å‘Šè­¦

### ä»ªè¡¨ç›˜æ¦‚è¿°

é…ç½®ç”¨äº AI Agent ç›‘æ§çš„ Grafana ä»ªè¡¨ç›˜ã€‚

```mermaid
graph TB
    subgraph Dashboard["Grafana Dashboard"]
        subgraph Row1["Overview"]
            TOTAL_REQ["Total Requests"]
            SUCCESS_RATE["Success Rate"]
            AVG_LATENCY["Average Latency"]
            ACTIVE_USERS["Active Users"]
        end

        subgraph Row2["Performance"]
            LATENCY_CHART["Latency Distribution"]
            THROUGHPUT["Throughput Trend"]
            ERROR_RATE["Error Rate Trend"]
        end

        subgraph Row3["Cost & Usage"]
            TOKEN_USAGE["Token Usage"]
            COST_BY_MODEL["Cost by Model"]
            COST_BY_TENANT["Cost by Tenant"]
        end

        subgraph Row4["Traces"]
            TRACE_TABLE["Recent Traces"]
            SLOW_TRACES["Slow Requests"]
            ERROR_TRACES["Error Traces"]
        end
    end

    style Row1 fill:#e3f2fd
    style Row2 fill:#e8f5e9
    style Row3 fill:#fff3e0
    style Row4 fill:#fce4ec
```

### Grafana å‘Šè­¦è§„åˆ™

```yaml
# grafana-alerts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-alert-rules
  namespace: observability
data:
  ai-agent-alerts.yaml: |
    apiVersion: 1
    groups:
      - orgId: 1
        name: AI Agent Alerts
        folder: AI Monitoring
        interval: 1m
        rules:
          - uid: agent-high-latency
            title: Agent High Latency
            condition: C
            data:
              - refId: A
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: prometheus
                model:
                  expr: histogram_quantile(0.99, sum(rate(agent_request_duration_seconds_bucket[5m])) by (le, agent_name))
                  intervalMs: 1000
                  maxDataPoints: 43200
              - refId: B
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [10]
                        type: gt
                      operator:
                        type: and
                      query:
                        params: [A]
                      reducer:
                        type: last
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            noDataState: NoData
            execErrState: Error
            for: 5m
            annotations:
              summary: "Agent {{ $labels.agent_name }} P99 å»¶è¿Ÿè¶…è¿‡ 10 ç§’"
              description: "å½“å‰ P99 å»¶è¿Ÿï¼š{{ $values.A }} ç§’"
            labels:
              severity: warning

          - uid: agent-high-error-rate
            title: Agent High Error Rate
            condition: C
            data:
              - refId: A
                datasourceUid: prometheus
                model:
                  expr: |
                    sum(rate(agent_errors_total[5m])) by (agent_name) /
                    sum(rate(agent_request_duration_seconds_count[5m])) by (agent_name)
              - refId: B
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [0.05]
                        type: gt
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            for: 5m
            annotations:
              summary: "Agent {{ $labels.agent_name }} é”™è¯¯ç‡è¶…è¿‡ 5%"
              description: "å½“å‰é”™è¯¯ç‡ï¼š{{ printf \"%.2f\" $values.A }}%"
            labels:
              severity: critical

          - uid: llm-rate-limit
            title: LLM Rate Limit Errors
            condition: C
            data:
              - refId: A
                datasourceUid: prometheus
                model:
                  expr: sum(increase(llm_rate_limit_errors_total[5m])) by (model)
              - refId: B
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [10]
                        type: gt
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            for: 2m
            annotations:
              summary: "LLM {{ $labels.model }} æ£€æµ‹åˆ°é€Ÿç‡é™åˆ¶é”™è¯¯"
              description: "è¿‡å» 5 åˆ†é’Ÿå†…æœ‰ {{ $values.A }} æ¬¡é€Ÿç‡é™åˆ¶é”™è¯¯"
            labels:
              severity: warning

          - uid: cost-budget-alert
            title: Daily Cost Budget Exceeded
            condition: C
            data:
              - refId: A
                datasourceUid: prometheus
                model:
                  expr: sum(increase(llm_cost_dollars_total[24h])) by (tenant_id)
              - refId: B
                datasourceUid: __expr__
                model:
                  conditions:
                    - evaluator:
                        params: [100]  # æ¯æ—¥ $100 é¢„ç®—
                        type: gt
                  type: threshold
              - refId: C
                datasourceUid: __expr__
                model:
                  expression: B
                  type: reduce
                  reducer: last
            for: 0s
            annotations:
              summary: "ç§Ÿæˆ· {{ $labels.tenant_id }} è¶…å‡ºæ¯æ—¥æˆæœ¬é¢„ç®—"
              description: "å½“å‰æ¯æ—¥æˆæœ¬ï¼š${{ printf \"%.2f\" $values.A }}"
            labels:
              severity: warning
```


## è¿ç»´æ£€æŸ¥æ¸…å•

### æ¯æ—¥æ£€æŸ¥

<DailyChecksTable />

### æ¯å‘¨æ£€æŸ¥

<WeeklyChecksTable />


## æ•…éšœæ’æŸ¥æŒ‡å—

### GPU OOMï¼ˆå†…å­˜ä¸è¶³ï¼‰é—®é¢˜

#### ç—‡çŠ¶

```
CUDA out of memory. Tried to allocate X GiB
RuntimeError: CUDA error: out of memory
```

#### è¯Šæ–­

```bash
# æ£€æŸ¥ GPU å†…å­˜çŠ¶æ€
kubectl exec -it <pod-name> -n inference -- nvidia-smi

# æ£€æŸ¥ DCGM æŒ‡æ ‡
kubectl exec -it <dcgm-exporter-pod> -n monitoring -- dcgmi dmon -e 155,156
```

#### è§£å†³æ–¹æ¡ˆ

```yaml
# 1. å‡å°æ‰¹å¤„ç†å¤§å°
env:
- name: MAX_BATCH_SIZE
  value: "16"  # ä» 32 å‡å°‘

# 2. åº”ç”¨æ¨¡å‹é‡åŒ–
env:
- name: QUANTIZATION
  value: "int8"  # æˆ– "fp8"

# 3. é™åˆ¶ KV ç¼“å­˜å¤§å°
env:
- name: MAX_NUM_SEQS
  value: "128"  # é™åˆ¶å¹¶å‘åºåˆ—æ•°
```

### ç½‘ç»œå»¶è¿Ÿé—®é¢˜

#### ç—‡çŠ¶

- æ¨ç†è¯·æ±‚è¶…æ—¶
- æ¨¡å‹é—´å»¶è¿Ÿ
- NCCL è¶…æ—¶ï¼ˆåˆ†å¸ƒå¼æ¨ç†ï¼‰

#### è§£å†³æ–¹æ¡ˆ

```yaml
# 1. åˆ†å¸ƒå¼éƒ¨ç½²çš„ Pod åäº²å’Œæ€§
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchLabels:
            app: inference
        topologyKey: "topology.kubernetes.io/zone"

# 2. å¢åŠ è¶…æ—¶æ—¶é—´
env:
- name: NCCL_TIMEOUT
  value: "1800"  # 30 åˆ†é’Ÿ
- name: REQUEST_TIMEOUT
  value: "300"   # 5 åˆ†é’Ÿ
```

### LangFuse è¿æ¥é”™è¯¯

```bash
# ç—‡çŠ¶ï¼šTrace æœªè®°å½•åˆ° LangFuse

# 1. æ£€æŸ¥ LangFuse æœåŠ¡çŠ¶æ€
kubectl get pods -n observability -l app=langfuse

# 2. æ£€æŸ¥ LangFuse æ—¥å¿—
kubectl logs -n observability -l app=langfuse --tail=100

# 3. æµ‹è¯•ç½‘ç»œè¿æ¥
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl -v http://langfuse.observability.svc/api/public/health

# 4. éªŒè¯ç¯å¢ƒå˜é‡
kubectl exec -n ai-agents <pod-name> -- env | grep LANGFUSE
```


## æˆæœ¬è¿½è¸ª

### æŒ‰æ¨¡å‹çš„æˆæœ¬åˆ†æ

æŒ‰æ¨¡å‹è¿½è¸ªå’Œåˆ†æ LLM ä½¿ç”¨æˆæœ¬ã€‚

```python
# cost_tracker.py
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json

@dataclass
class ModelPricing:
    """æ¨¡å‹å®šä»·ä¿¡æ¯ï¼ˆæ¯ 1K Token çš„ç¾å…ƒä»·æ ¼ï¼‰"""
    input_price: float
    output_price: float

# 2024 å¹´æ¨¡å‹å®šä»·
MODEL_PRICING: Dict[str, ModelPricing] = {
    # OpenAI
    "gpt-4-turbo": ModelPricing(0.01, 0.03),
    "gpt-4": ModelPricing(0.03, 0.06),
    "gpt-3.5-turbo": ModelPricing(0.0005, 0.0015),

    # Anthropic
    "claude-3-opus": ModelPricing(0.015, 0.075),
    "claude-3-sonnet": ModelPricing(0.003, 0.015),
    "claude-3-haiku": ModelPricing(0.00025, 0.00125),
}

@dataclass
class UsageRecord:
    """ä½¿ç”¨è®°å½•"""
    timestamp: datetime
    model: str
    input_tokens: int
    output_tokens: int
    tenant_id: str
    agent_name: str
    trace_id: str

    @property
    def total_tokens(self) -> int:
        return self.input_tokens + self.output_tokens

    @property
    def cost(self) -> float:
        if self.model not in MODEL_PRICING:
            return 0.0
        pricing = MODEL_PRICING[self.model]
        return (
            (self.input_tokens / 1000) * pricing.input_price +
            (self.output_tokens / 1000) * pricing.output_price
        )

class CostTracker:
    """æˆæœ¬è¿½è¸ªå™¨"""

    def __init__(self, langfuse_client=None):
        self.langfuse = langfuse_client
        self.records: List[UsageRecord] = []

    def record_usage(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int,
        tenant_id: str,
        agent_name: str,
        trace_id: str,
    ):
        """è®°å½•ä½¿ç”¨æƒ…å†µã€‚"""
        record = UsageRecord(
            timestamp=datetime.utcnow(),
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            tenant_id=tenant_id,
            agent_name=agent_name,
            trace_id=trace_id,
        )
        self.records.append(record)

        # æ›´æ–° Prometheus æŒ‡æ ‡
        from metrics_exporter import record_llm_usage
        record_llm_usage(
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            tenant_id=tenant_id,
            duration=0,  # éœ€è¦å•ç‹¬æµ‹é‡
        )

        return record
```

### æˆæœ¬ä»ªè¡¨ç›˜æŸ¥è¯¢

```promql
# æ¯æ—¥æ€»æˆæœ¬
sum(increase(llm_cost_dollars_total[24h]))

# æŒ‰ç§Ÿæˆ·çš„æ¯æ—¥æˆæœ¬
sum(increase(llm_cost_dollars_total[24h])) by (tenant_id)

# æ¨¡å‹æˆæœ¬å æ¯”
sum(increase(llm_cost_dollars_total[24h])) by (model)
/ ignoring(model) group_left
sum(increase(llm_cost_dollars_total[24h]))

# é¢„ç®—ä½¿ç”¨ç‡ï¼ˆæœˆåº¦ï¼‰
sum(increase(llm_cost_dollars_total[30d])) by (tenant_id)
/ on(tenant_id) group_left
tenant_monthly_budget_usd
```

:::tip æˆæœ¬ä¼˜åŒ–å»ºè®®

1. **æ¨¡å‹é€‰æ‹©ä¼˜åŒ–**ï¼šç®€å•ä»»åŠ¡ä½¿ç”¨æ›´ç»æµçš„æ¨¡å‹ï¼ˆGPT-3.5ã€Claude Haikuï¼‰
2. **æç¤ºè¯ä¼˜åŒ–**ï¼šå»é™¤ä¸å¿…è¦çš„ä¸Šä¸‹æ–‡ä»¥å‡å°‘è¾“å…¥ Token
3. **ç¼“å­˜**ï¼šä¸ºé‡å¤æŸ¥è¯¢ç¼“å­˜å“åº”
4. **æ‰¹å¤„ç†**ï¼šå°½å¯èƒ½æ‰¹é‡å¤„ç†è¯·æ±‚ä»¥å‡å°‘å¼€é”€
:::


## æ€»ç»“

AI Agent ç›‘æ§å¯¹äº Agentic AI åº”ç”¨çš„ç¨³å®šè¿è¡Œå’ŒæŒç»­æ”¹è¿›è‡³å…³é‡è¦ã€‚ä»¥ä¸‹æ˜¯æœ¬æ–‡æ¡£æ¶µç›–çš„å…³é”®ä¸»é¢˜æ‘˜è¦ï¼š

### æ ¸å¿ƒè¦ç‚¹

1. **LangFuse éƒ¨ç½²**ï¼šåœ¨ Kubernetes ä¸­éƒ¨ç½²è‡ªæ‰˜ç®¡ LangFuseï¼Œç¡®ä¿æ•°æ®ä¸»æƒå¹¶ä¼˜åŒ–æˆæœ¬
2. **LangSmith é›†æˆ**ï¼šä¸ºåŸºäº LangChain çš„åº”ç”¨è½»æ¾å¯ç”¨è¿½è¸ª
3. **æ ¸å¿ƒæŒ‡æ ‡**ï¼šé€šè¿‡å»¶è¿Ÿã€Token ç”¨é‡ã€é”™è¯¯ç‡å’Œ Trace åˆ†æå®ç°å…¨é¢ç›‘æ§
4. **Grafana ä»ªè¡¨ç›˜**ï¼šå®æ—¶ç›‘æ§å’Œå‘Šè­¦ï¼Œå®ç°ä¸»åŠ¨è¿ç»´
5. **æˆæœ¬è¿½è¸ª**ï¼šåŸºäºæ¨¡å‹å’Œç§Ÿæˆ·çš„æˆæœ¬åˆ†æï¼Œç”¨äºé¢„ç®—ç®¡ç†å’Œä¼˜åŒ–
6. **æ•…éšœæ’æŸ¥**ï¼šGPU OOMã€ç½‘ç»œå»¶è¿Ÿã€æ¨¡å‹åŠ è½½å¤±è´¥ç­‰å¸¸è§é—®é¢˜çš„è§£å†³

### ç›‘æ§æˆç†Ÿåº¦æ¨¡å‹

<MaturityModelTable />

:::tip åç»­æ­¥éª¤

- [Agentic AI å¹³å°æ¶æ„](./agentic-platform-architecture.md) - æ•´ä½“å¹³å°è®¾è®¡
- [Kagent Kubernetes Agent ç®¡ç†](./kagent-kubernetes-agents.md) - Agent éƒ¨ç½²ä¸è¿ç»´
- [RAG è¯„ä¼°æ¡†æ¶](./ragas-evaluation.md) - ä½¿ç”¨ Ragas è¿›è¡Œè´¨é‡è¯„ä¼°
:::

## å‚è€ƒèµ„æ–™

- [LangFuse æ–‡æ¡£](https://langfuse.com/docs)
- [LangFuse GitHub ä»“åº“](https://github.com/langfuse/langfuse)
- [LangSmith æ–‡æ¡£](https://docs.smith.langchain.com/)
- [OpenTelemetry æ–‡æ¡£](https://opentelemetry.io/docs/)
- [Prometheus ç›‘æ§](https://prometheus.io/docs/)
- [Grafana ä»ªè¡¨ç›˜](https://grafana.com/docs/grafana/latest/dashboards/)
- [LangChain å›è°ƒ](https://python.langchain.com/docs/modules/callbacks/)
