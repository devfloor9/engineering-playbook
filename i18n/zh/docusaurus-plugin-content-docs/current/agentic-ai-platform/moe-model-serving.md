---
title: "MoE æ¨¡å‹æœåŠ¡æŒ‡å—"
sidebar_label: "6. MoE Model Serving"
description: "åŸºäº EKS çš„ Mixture of Experts æ¨¡å‹éƒ¨ç½²ä¸ä¼˜åŒ–ç­–ç•¥"
tags: [eks, moe, vllm, tgi, model-serving, gpu, mixtral]
category: "genai-aiml"
sidebar_position: 6
last_update:
  date: 2026-02-14
  author: devfloor9
---

import { RoutingMechanisms, MoeVsDense, GpuMemoryRequirements, ParallelizationStrategies, TensorParallelismConfig, VllmVsTgi, KvCacheConfig, BatchOptimization, MonitoringMetrics, GpuVsTrainium2 } from '@site/src/components/MoeModelTables';

# MoE æ¨¡å‹æœåŠ¡æŒ‡å—

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2025-02-05 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 5 åˆ†é’Ÿ

## æ¦‚è¿°

Mixture of Expertsï¼ˆMoEï¼‰æ¨¡å‹ä»£è¡¨äº†ä¸€ç§åˆ›æ–°æ¶æ„ï¼Œå¯æœ€å¤§åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚æœ¬æ–‡æ¶µç›–å¦‚ä½•åœ¨ Amazon EKS ç¯å¢ƒä¸­é«˜æ•ˆéƒ¨ç½²å’Œè¿è¥ Mixtralã€DeepSeek-MoE å’Œ Qwen-MoE ç­‰ MoE æ¨¡å‹ã€‚

### æ ¸å¿ƒç›®æ ‡

- **ç†è§£ MoE æ¶æ„**ï¼šå­¦ä¹ ä¸“å®¶ç½‘ç»œå’Œè·¯ç”±æœºåˆ¶çš„å·¥ä½œåŸç†
- **é«˜æ•ˆéƒ¨ç½²**ï¼šä½¿ç”¨ vLLM å’Œ TGI ä¼˜åŒ– MoE æ¨¡å‹æœåŠ¡
- **èµ„æºä¼˜åŒ–**ï¼šGPU å†…å­˜ç®¡ç†å’Œåˆ†å¸ƒå¼éƒ¨ç½²ç­–ç•¥
- **æ€§èƒ½è°ƒä¼˜**ï¼šKV Cache å’Œæ¨æµ‹è§£ç ç­‰é«˜çº§ä¼˜åŒ–æŠ€æœ¯

---

## ç†è§£ MoE æ¶æ„

### ä¸“å®¶ç½‘ç»œç»“æ„

MoE æ¨¡å‹ç”±å¤šä¸ª"ä¸“å®¶ï¼ˆExpertï¼‰"ç½‘ç»œå’Œä¸€ä¸ªåœ¨å…¶ä¸­è¿›è¡Œé€‰æ‹©çš„"è·¯ç”±å™¨ï¼ˆGateï¼‰"ç½‘ç»œç»„æˆã€‚

```mermaid
flowchart TB
    subgraph "MoE Layer"
        INPUT[Input Token<br/>Hidden State]

        subgraph "Router Network"
            GATE[Gating Network<br/>Softmax Router]
        end

        subgraph "Expert Networks"
            E1[Expert 1<br/>FFN]
            E2[Expert 2<br/>FFN]
            E3[Expert 3<br/>FFN]
            E4[Expert 4<br/>FFN]
            EN[Expert N<br/>FFN]
        end

        COMBINE[Weighted<br/>Combination]
        OUTPUT[Output<br/>Hidden State]
    end

    INPUT --> GATE
    GATE -->|"Top-K Selection<br/>(K=2)"| E1 & E2
    GATE -.->|"Not Selected"| E3 & E4 & EN
    E1 & E2 --> COMBINE
    COMBINE --> OUTPUT
```

### è·¯ç”±æœºåˆ¶

MoE æ¨¡å‹çš„æ ¸å¿ƒæ˜¯æ ¹æ®è¾“å…¥ Token é€‰æ‹©åˆé€‚ä¸“å®¶çš„è·¯ç”±æœºåˆ¶ã€‚

<RoutingMechanisms />

:::info è·¯ç”±å·¥ä½œåŸç†

1. **Gate è®¡ç®—**ï¼šå°†è¾“å…¥ Token çš„éšè—çŠ¶æ€é€šè¿‡ Gate ç½‘ç»œ
2. **ä¸“å®¶é€‰æ‹©**ï¼šä» Softmax è¾“å‡ºä¸­é€‰æ‹© Top-K ä¸ªä¸“å®¶
3. **å¹¶è¡Œå¤„ç†**ï¼šé€‰ä¸­çš„ä¸“å®¶å¹¶è¡Œå¤„ç†è¾“å…¥
4. **åŠ æƒç»„åˆ**ï¼šä½¿ç”¨ Gate æƒé‡ç»„åˆä¸“å®¶è¾“å‡º

:::

### MoE ä¸ Dense æ¨¡å‹å¯¹æ¯”

<MoeVsDense />

```mermaid
flowchart LR
    subgraph "Dense Model (70B)"
        D_IN[Input] --> D_ALL[All 70B<br/>Parameters<br/>Active]
        D_ALL --> D_OUT[Output]
    end

    subgraph "MoE Model (8x7B = 47B Active)"
        M_IN[Input] --> M_GATE[Router]
        M_GATE --> M_E1[Expert 1<br/>7B Active]
        M_GATE --> M_E2[Expert 2<br/>7B Active]
        M_E1 & M_E2 --> M_OUT[Output]
        M_GATE -.-> M_E3[Expert 3-8<br/>Inactive]
    end
```

:::tip MoE æ¨¡å‹çš„ä¼˜åŠ¿

- **è®¡ç®—æ•ˆç‡**ï¼šä»…æ¿€æ´»æ‰€æœ‰å‚æ•°çš„ä¸€ä¸ªå­é›†ï¼Œæå‡æ¨ç†é€Ÿåº¦
- **å¯æ‰©å±•æ€§**ï¼šé€šè¿‡å¢åŠ ä¸“å®¶æ¥æ‰©å±•æ¨¡å‹å®¹é‡
- **ä¸“ä¸šåŒ–**ï¼šæ¯ä¸ªä¸“å®¶å¯ä»¥ä¸“æ³¨äºç‰¹å®šé¢†åŸŸ/ä»»åŠ¡

:::

---

## MoE æ¨¡å‹æœåŠ¡æ³¨æ„äº‹é¡¹

### GPU å†…å­˜éœ€æ±‚

MoE æ¨¡å‹æ¿€æ´»çš„å‚æ•°è¾ƒå°‘ï¼Œä½†å¿…é¡»å°†æ‰€æœ‰ä¸“å®¶åŠ è½½åˆ°å†…å­˜ä¸­ã€‚

<GpuMemoryRequirements />

:::warning é‡è¦çš„å†…å­˜è®¡ç®—è¯´æ˜

- **KV Cache**ï¼šæ ¹æ®æ‰¹å¤„ç†å¤§å°å’Œåºåˆ—é•¿åº¦éœ€è¦é¢å¤–å†…å­˜
- **æ¿€æ´»å†…å­˜**ï¼šæ¨ç†æœŸé—´ä¸­é—´æ¿€æ´»å€¼çš„ç©ºé—´
- **CUDA ä¸Šä¸‹æ–‡**ï¼šæ¯ä¸ª GPU å¤§çº¦ 1-2GB CUDA å¼€é”€
- **å®‰å…¨ä½™é‡**ï¼šç”Ÿäº§è¿è¥ä¸­é¢„ç•™ 10-20% çš„é¢å¤–ç©ºé—´

:::

### åˆ†å¸ƒå¼éƒ¨ç½²ç­–ç•¥

å¤§å‹ MoE æ¨¡å‹æ— æ³•åŠ è½½åˆ°å•ä¸ª GPU ä¸Šï¼Œå› æ­¤åˆ†å¸ƒå¼éƒ¨ç½²è‡³å…³é‡è¦ã€‚

```mermaid
flowchart TB
    subgraph "Tensor Parallelism (TP=4)"
        direction LR
        TP1[GPU 0<br/>Layer 1-N<br/>Shard 1/4]
        TP2[GPU 1<br/>Layer 1-N<br/>Shard 2/4]
        TP3[GPU 2<br/>Layer 1-N<br/>Shard 3/4]
        TP4[GPU 3<br/>Layer 1-N<br/>Shard 4/4]
        TP1 <--> TP2 <--> TP3 <--> TP4
    end

    subgraph "Expert Parallelism (EP=2)"
        direction LR
        EP1[GPU 0-1<br/>Expert 1-4]
        EP2[GPU 2-3<br/>Expert 5-8]
        EP1 <-.-> EP2
    end

    subgraph "Pipeline Parallelism (PP=2)"
        direction TB
        PP1[GPU 0-3<br/>Layer 1-16]
        PP2[GPU 4-7<br/>Layer 17-32]
        PP1 --> PP2
    end
```

<ParallelizationStrategies />

### ä¸“å®¶æ¿€æ´»æ¨¡å¼

ç†è§£ä¸“å®¶æ¿€æ´»æ¨¡å¼å¯¹äºä¼˜åŒ– MoE æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚

```mermaid
flowchart LR
    subgraph "Token Distribution"
        T1[Token 1] --> E1[Expert 1]
        T2[Token 2] --> E3[Expert 3]
        T3[Token 3] --> E1[Expert 1]
        T4[Token 4] --> E2[Expert 2]
        T5[Token 5] --> E4[Expert 4]
    end

    subgraph "Load Imbalance"
        E1_LOAD[Expert 1<br/>40% Load]
        E2_LOAD[Expert 2<br/>20% Load]
        E3_LOAD[Expert 3<br/>25% Load]
        E4_LOAD[Expert 4<br/>15% Load]
    end
```

:::info ä¸“å®¶è´Ÿè½½å‡è¡¡

- **è¾…åŠ©æŸå¤±**ï¼šè®­ç»ƒæ—¶ä½¿ç”¨è¾…åŠ©æŸå¤±é¼“åŠ±ä¸“å®¶å‡åŒ€åˆ†å¸ƒ
- **å®¹é‡å› å­**ï¼šé™åˆ¶æ¯ä¸ªä¸“å®¶å¤„ç†çš„æœ€å¤§ Token æ•°
- **Token ä¸¢å¼ƒ**ï¼šè¶…å‡ºå®¹é‡æ—¶ä¸¢å¼ƒ Tokenï¼ˆæ¨ç†æ—¶ä¸æ¨èä½¿ç”¨ï¼‰

:::

---

## ä½¿ç”¨ vLLM éƒ¨ç½² MoE

### vLLM MoE æ”¯æŒç‰¹æ€§

vLLM ä¸º MoE æ¨¡å‹æä¾›ä»¥ä¸‹ä¼˜åŒ–ï¼š

- **Expert Parallelism**ï¼šå°†ä¸“å®¶åˆ†å¸ƒåˆ°å¤šä¸ª GPU
- **Tensor Parallelism**ï¼šåœ¨å±‚å†…æ‹†åˆ†å¼ é‡
- **PagedAttention**ï¼šé«˜æ•ˆ KV Cache ç®¡ç†
- **Continuous Batching**ï¼šåŠ¨æ€æ‰¹å¤„ç†

### Mixtral 8x7B éƒ¨ç½² YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x7b-vllm
  namespace: inference
  labels:
    app: mixtral-8x7b
    serving-engine: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x7b
  template:
    metadata:
      labels:
        app: mixtral-8x7b
        serving-engine: vllm
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: p4d.24xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.0
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: VLLM_ATTENTION_BACKEND
              value: "FLASH_ATTN"
          args:
            - "--model"
            - "mistralai/Mixtral-8x7B-Instruct-v0.1"
            - "--tensor-parallel-size"
            - "2"
            - "--max-model-len"
            - "32768"
            - "--gpu-memory-utilization"
            - "0.90"
            - "--enable-chunked-prefill"
            - "--max-num-batched-tokens"
            - "32768"
            - "--trust-remote-code"
            - "--dtype"
            - "bfloat16"
          resources:
            requests:
              nvidia.com/gpu: 2
              memory: "180Gi"
              cpu: "24"
            limits:
              nvidia.com/gpu: 2
              memory: "200Gi"
              cpu: "32"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 300
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 5
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      terminationGracePeriodSeconds: 120
```

### Mixtral 8x22B å¤§è§„æ¨¡éƒ¨ç½²ï¼ˆ4 GPUï¼‰

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x22b-vllm
  namespace: inference
  labels:
    app: mixtral-8x22b
    serving-engine: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x22b
  template:
    metadata:
      labels:
        app: mixtral-8x22b
        serving-engine: vllm
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: p5.48xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.0
          ports:
            - name: http
              containerPort: 8000
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: NCCL_DEBUG
              value: "INFO"
            - name: NCCL_IB_DISABLE
              value: "0"
          args:
            - "--model"
            - "mistralai/Mixtral-8x22B-Instruct-v0.1"
            - "--tensor-parallel-size"
            - "4"
            - "--max-model-len"
            - "65536"
            - "--gpu-memory-utilization"
            - "0.92"
            - "--enable-chunked-prefill"
            - "--max-num-batched-tokens"
            - "65536"
            - "--dtype"
            - "bfloat16"
            - "--enforce-eager"
          resources:
            requests:
              nvidia.com/gpu: 4
              memory: "400Gi"
              cpu: "48"
            limits:
              nvidia.com/gpu: 4
              memory: "500Gi"
              cpu: "64"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
```

### vLLM Tensor Parallelism é…ç½®

Tensor Parallelism å°†æ¨¡å‹çš„æ¯ä¸€å±‚æ‹†åˆ†åˆ°å¤šä¸ª GPU ä¸Šã€‚

<TensorParallelismConfig />

:::tip Tensor Parallelism ä¼˜åŒ–

- **NVLink åˆ©ç”¨**ï¼šä½¿ç”¨æ”¯æŒ NVLink çš„å®ä¾‹å®ç°é«˜é€Ÿ GPU é€šä¿¡
- **TP å¤§å°é€‰æ‹©**ï¼šæ ¹æ®æ¨¡å‹å¤§å°å’Œ GPU å†…å­˜é€‰æ‹©æœ€å° TP å¤§å°
- **é€šä¿¡å¼€é”€**ï¼šæ›´å¤§çš„ TP å¤§å°ä¼šå¢åŠ  All-Reduce é€šä¿¡é‡

:::

### vLLM Expert Parallelism é…ç½®

Expert Parallelism å°† MoE æ¨¡å‹çš„ä¸“å®¶åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šã€‚

```yaml
# å¯ç”¨ Expert Parallelism ç¤ºä¾‹
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  - "--tensor-parallel-size"
  - "2"
  # Expert Parallelism åœ¨ vLLM ä¸­ä¼šè‡ªåŠ¨ä¼˜åŒ–
  # ä¸“å®¶åœ¨ TP å†…åˆ†å¸ƒ
  - "--distributed-executor-backend"
  - "ray"  # æˆ– "mp"ï¼ˆmultiprocessingï¼‰
```

---

## ä½¿ç”¨ TGI éƒ¨ç½² MoE

### TGI MoE æ”¯æŒç‰¹æ€§

Text Generation Inferenceï¼ˆTGIï¼‰æ˜¯ Hugging Face å¼€å‘çš„é«˜æ€§èƒ½æ¨ç†æœåŠ¡å™¨ã€‚

- **Flash Attention 2**ï¼šå†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—
- **Paged Attention**ï¼šåŠ¨æ€ KV Cache ç®¡ç†
- **Tensor Parallelism**ï¼šå¤š GPU åˆ†å¸ƒå¼æ¨ç†
- **Quantization**ï¼šæ”¯æŒ AWQã€GPTQã€EETQ

### TGI Mixtral 8x7B éƒ¨ç½² YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x7b-tgi
  namespace: inference
  labels:
    app: mixtral-8x7b
    serving-engine: tgi
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x7b-tgi
  template:
    metadata:
      labels:
        app: mixtral-8x7b-tgi
        serving-engine: tgi
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: p4d.24xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: tgi
          image: ghcr.io/huggingface/text-generation-inference:2.3.0
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: MODEL_ID
              value: "mistralai/Mixtral-8x7B-Instruct-v0.1"
            - name: NUM_SHARD
              value: "2"
            - name: MAX_INPUT_LENGTH
              value: "8192"
            - name: MAX_TOTAL_TOKENS
              value: "32768"
            - name: MAX_BATCH_PREFILL_TOKENS
              value: "32768"
            - name: DTYPE
              value: "bfloat16"
            - name: QUANTIZE
              value: ""  # æˆ– "awq"ã€"gptq"
            - name: TRUST_REMOTE_CODE
              value: "true"
          resources:
            requests:
              nvidia.com/gpu: 2
              memory: "180Gi"
              cpu: "24"
            limits:
              nvidia.com/gpu: 2
              memory: "200Gi"
              cpu: "32"
          volumeMounts:
            - name: model-cache
              mountPath: /data
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 300
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 120
            periodSeconds: 10
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
```

### TGI é‡åŒ–éƒ¨ç½²ï¼ˆAWQï¼‰

ä½¿ç”¨ AWQ é‡åŒ–æ¨¡å‹æé«˜å†…å­˜æ•ˆç‡ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral-8x7b-tgi-awq
  namespace: inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral-8x7b-tgi-awq
  template:
    metadata:
      labels:
        app: mixtral-8x7b-tgi-awq
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: g5.48xlarge
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: tgi
          image: ghcr.io/huggingface/text-generation-inference:2.3.0
          ports:
            - name: http
              containerPort: 8080
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            - name: MODEL_ID
              value: "TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ"
            - name: NUM_SHARD
              value: "2"
            - name: MAX_INPUT_LENGTH
              value: "8192"
            - name: MAX_TOTAL_TOKENS
              value: "16384"
            - name: QUANTIZE
              value: "awq"
          resources:
            requests:
              nvidia.com/gpu: 2
              memory: "90Gi"
              cpu: "16"
            limits:
              nvidia.com/gpu: 2
              memory: "120Gi"
              cpu: "24"
```

### vLLM ä¸ TGI æ€§èƒ½å¯¹æ¯”

<VllmVsTgi />

:::tip æ¨ç†å¼•æ“é€‰æ‹©æŒ‡å—

- **vLLM**ï¼šéœ€è¦æœ€å¤§ååé‡æ—¶ï¼Œå¤§è§„æ¨¡æ‰¹å¤„ç†åœºæ™¯
- **TGI**ï¼šHugging Face ç”Ÿæ€ç³»ç»Ÿé›†æˆï¼Œæ˜“äºéƒ¨ç½²
- **ä¸¤è€…çš†å¯**ï¼šä¸¤ä¸ªå¼•æ“éƒ½èƒ½å¾ˆå¥½åœ°æ”¯æŒ MoE æ¨¡å‹ï¼Œæ ¹æ®å·¥ä½œè´Ÿè½½é€‰æ‹©

:::

---

## Service å’Œ Ingress é…ç½®

### MoE æ¨¡å‹ Service YAML

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mixtral-8x7b-service
  namespace: inference
  labels:
    app: mixtral-8x7b
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8000
      targetPort: 8000
      protocol: TCP
  selector:
    app: mixtral-8x7b
---
apiVersion: v1
kind: Service
metadata:
  name: mixtral-8x7b-tgi-service
  namespace: inference
  labels:
    app: mixtral-8x7b-tgi
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: 8080
      protocol: TCP
  selector:
    app: mixtral-8x7b-tgi
```

### Gateway API HTTPRoute é…ç½®

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: moe-model-route
  namespace: inference
spec:
  parentRefs:
    - name: inference-gateway
      namespace: gateway-system
  hostnames:
    - "inference.example.com"
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /v1/mixtral
      backendRefs:
        - name: mixtral-8x7b-service
          port: 8000
      filters:
        - type: URLRewrite
          urlRewrite:
            path:
              type: ReplacePrefixMatch
              replacePrefixMatch: /v1
    - matches:
        - path:
            type: PathPrefix
            value: /v1/mixtral-tgi
      backendRefs:
        - name: mixtral-8x7b-tgi-service
          port: 8080
```

---

## æ€§èƒ½ä¼˜åŒ–

### KV Cache ä¼˜åŒ–

KV Cache æ˜¯æ˜¾è‘—å½±å“æ¨ç†æ€§èƒ½çš„å…³é”®è¦ç´ ã€‚

```mermaid
flowchart LR
    subgraph "Traditional KV Cache"
        T1[Token 1 KV] --> T2[Token 2 KV] --> T3[Token 3 KV]
        T3 --> WASTE[Wasted<br/>Memory]
    end

    subgraph "PagedAttention (vLLM)"
        P1[Page 1<br/>Token 1-4]
        P2[Page 2<br/>Token 5-8]
        P3[Page 3<br/>Token 9-12]
        POOL[Memory Pool<br/>Dynamic Allocation]
        P1 & P2 & P3 --> POOL
    end
```

#### vLLM KV Cache é…ç½®

```yaml
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  # åˆ†é…ç»™ KV Cache çš„ GPU å†…å­˜æ¯”ä¾‹
  - "--gpu-memory-utilization"
  - "0.90"
  # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆå½±å“ KV Cache å¤§å°ï¼‰
  - "--max-model-len"
  - "32768"
  # ä½¿ç”¨åˆ†å—é¢„å¡«å……æé«˜å†…å­˜æ•ˆç‡
  - "--enable-chunked-prefill"
  # æ¯æ‰¹æ¬¡æœ€å¤§ Token æ•°
  - "--max-num-batched-tokens"
  - "32768"
```

<KvCacheConfig />

### æ¨æµ‹è§£ç 

æ¨æµ‹è§£ç ä½¿ç”¨å°å‹è‰ç¨¿æ¨¡å‹æ¥åŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚

```mermaid
sequenceDiagram
    participant Draft as Draft Model<br/>(Small)
    participant Target as Target Model<br/>(Mixtral 8x7B)
    participant Output as Output

    Note over Draft,Output: Speculative Decoding Process

    Draft->>Draft: Generate K tokens quickly
    Draft->>Target: Request verification of K tokens
    Target->>Target: Verify K tokens in parallel

    alt All tokens approved
        Target->>Output: Output K tokens
    else Some tokens rejected
        Target->>Output: Output approved tokens only
        Target->>Draft: Regenerate from rejection point
    end
```

#### vLLM æ¨æµ‹è§£ç é…ç½®

```yaml
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  - "--tensor-parallel-size"
  - "2"
  # å¯ç”¨æ¨æµ‹è§£ç 
  - "--speculative-model"
  - "mistralai/Mistral-7B-Instruct-v0.2"
  # è‰ç¨¿æ¨¡å‹ç”Ÿæˆçš„ Token æ•°
  - "--num-speculative-tokens"
  - "5"
  # è‰ç¨¿æ¨¡å‹çš„ Tensor Parallel å¤§å°
  - "--speculative-draft-tensor-parallel-size"
  - "1"
```

:::info æ¨æµ‹è§£ç æ•ˆæœ

- **é€Ÿåº¦æå‡**ï¼šååé‡æå‡ 1.5 å€ - 2.5 å€ï¼ˆå› å·¥ä½œè´Ÿè½½è€Œå¼‚ï¼‰
- **è´¨é‡ä¿æŒ**ï¼šè¾“å‡ºè´¨é‡ä¿æŒä¸å˜ï¼ˆé€šè¿‡éªŒè¯ä¿è¯ï¼‰
- **é¢å¤–å†…å­˜**ï¼šè‰ç¨¿æ¨¡å‹éœ€è¦é¢å¤–çš„ GPU å†…å­˜

:::

### æ‰¹å¤„ç†ä¼˜åŒ–

é«˜æ•ˆçš„æ‰¹å¤„ç†å¯æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡ã€‚

```yaml
args:
  - "--model"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"
  # è¿ç»­æ‰¹å¤„ç†è®¾ç½®
  - "--max-num-seqs"
  - "256"  # æœ€å¤§å¹¶å‘åºåˆ—æ•°
  - "--max-num-batched-tokens"
  - "32768"  # æ¯æ‰¹æ¬¡æœ€å¤§ Token æ•°
  # åˆ†ç¦»é¢„å¡«å……å’Œè§£ç 
  - "--enable-chunked-prefill"
  - "--max-num-batched-tokens"
  - "32768"
```

<BatchOptimization />

---

## ç›‘æ§ä¸å‘Šè­¦

### MoE æ¨¡å‹ç‰¹å®šæŒ‡æ ‡

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: moe-model-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: mixtral-8x7b
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
  namespaceSelector:
    matchNames:
      - inference
```

### å…³é”®ç›‘æ§æŒ‡æ ‡

<MonitoringMetrics />

### Prometheus å‘Šè­¦è§„åˆ™

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: moe-model-alerts
  namespace: monitoring
spec:
  groups:
    - name: moe-model-alerts
      rules:
        - alert: MoEModelHighLatency
          expr: |
            histogram_quantile(0.95,
              rate(vllm:e2e_request_latency_seconds_bucket[5m])
            ) > 30
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "MoE æ¨¡å‹å“åº”å»¶è¿Ÿï¼ˆP95 > 30sï¼‰"
            description: "{{ $labels.model_name }} çš„ P95 å»¶è¿Ÿè¶…è¿‡ 30 ç§’ã€‚"

        - alert: MoEModelKVCacheFull
          expr: vllm:gpu_cache_usage_perc > 0.95
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "KV Cache å®¹é‡ä¸è¶³"
            description: "KV Cache åˆ©ç”¨ç‡è¶…è¿‡ 95%ã€‚æ–°è¯·æ±‚å¯èƒ½è¢«æ‹’ç»ã€‚"

        - alert: MoEModelQueueBacklog
          expr: vllm:num_requests_waiting > 100
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "è¯·æ±‚é˜Ÿåˆ—ç§¯å‹å¢åŠ "
            description: "ç­‰å¾…è¯·æ±‚è¶…è¿‡ 100 ä¸ªã€‚è¯·è€ƒè™‘æ‰©å®¹ã€‚"
```

---

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### OOMï¼ˆå†…å­˜ä¸è¶³ï¼‰é”™è¯¯

```bash
# ç—‡çŠ¶ï¼šCUDA out of memory é”™è¯¯
# è§£å†³æ–¹æ¡ˆï¼š
# 1. é™ä½ gpu-memory-utilization å€¼
--gpu-memory-utilization 0.85

# 2. å‡å° max-model-len
--max-model-len 16384

# 3. å¢åŠ  tensor parallel sizeï¼ˆä½¿ç”¨æ›´å¤š GPUï¼‰
--tensor-parallel-size 4
```

#### æ¨¡å‹åŠ è½½ç¼“æ…¢

```bash
# ç—‡çŠ¶ï¼šæ¨¡å‹åŠ è½½è¶…è¿‡ 10 åˆ†é’Ÿ
# è§£å†³æ–¹æ¡ˆï¼š
# 1. ä½¿ç”¨æ¨¡å‹ç¼“å­˜ PVC
# 2. ä½¿ç”¨ FSx for Lustre å®ç°å¿«é€Ÿæ¨¡å‹åŠ è½½
# 3. é¢„ä¸‹è½½æ¨¡å‹
```

#### ä¸“å®¶è´Ÿè½½ä¸å‡è¡¡

```bash
# ç—‡çŠ¶ï¼šä»…ç‰¹å®š GPU æ˜¾ç¤ºé«˜åˆ©ç”¨ç‡
# è§£å†³æ–¹æ¡ˆï¼š
# 1. å¢åŠ æ‰¹å¤„ç†å¤§å°ä»¥æ”¹å–„ Token åˆ†å¸ƒ
--max-num-seqs 256

# 2. å¤šæ ·åŒ–è¾“å…¥ä»¥æ¿€æ´»ä¸åŒçš„ä¸“å®¶
```

:::warning è°ƒè¯•æŠ€å·§

- **è°ƒæ•´æ—¥å¿—çº§åˆ«**ï¼šä½¿ç”¨ `VLLM_LOGGING_LEVEL=DEBUG` ç¯å¢ƒå˜é‡è·å–è¯¦ç»†æ—¥å¿—
- **NCCL è°ƒè¯•**ï¼šä½¿ç”¨ `NCCL_DEBUG=INFO` è¯Šæ–­ GPU é€šä¿¡é—®é¢˜
- **å†…å­˜åˆ†æ**ï¼šä½¿ç”¨ `nvidia-smi dmon` è¿›è¡Œå®æ—¶ GPU å†…å­˜ç›‘æ§

:::

---

## æ€»ç»“

MoE æ¨¡å‹æœåŠ¡å®ç°äº†å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²ã€‚

### æ ¸å¿ƒè¦ç‚¹

1. **ç†è§£æ¶æ„**ï¼šå­¦ä¹ ä¸“å®¶ç½‘ç»œå’Œè·¯ç”±æœºåˆ¶çš„å·¥ä½œåŸç†
2. **è§„åˆ’å†…å­˜**ï¼šç¡®ä¿è¶³å¤Ÿçš„ GPU å†…å­˜æ¥åŠ è½½æ‰€æœ‰ä¸“å®¶
3. **åˆ†å¸ƒå¼éƒ¨ç½²**ï¼šé€‚å½“ç»„åˆ Tensor Parallelism å’Œ Expert Parallelism
4. **é€‰æ‹©å¼•æ“**ï¼šé€‰æ‹© vLLMï¼ˆé«˜ååé‡ï¼‰æˆ– TGIï¼ˆæ˜“äºéƒ¨ç½²ï¼‰
5. **ä¼˜åŒ–æ€§èƒ½**ï¼šåº”ç”¨ KV Cacheã€æ¨æµ‹è§£ç å’Œæ‰¹å¤„ç†ä¼˜åŒ–

### åç»­æ­¥éª¤

- [GPU èµ„æºç®¡ç†](./gpu-resource-management.md) - åŠ¨æ€ GPU é›†ç¾¤èµ„æºåˆ†é…
- [æ¨ç†ç½‘å…³è·¯ç”±](./inference-gateway-routing.md) - å¤šæ¨¡å‹è·¯ç”±ç­–ç•¥
- [Agentic AI å¹³å°æ¶æ„](./agentic-platform-architecture.md) - å®Œæ•´å¹³å°æ­å»º

---

## å‚è€ƒèµ„æ–™

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [TGI å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/text-generation-inference)
- [Mixtral æ¨¡å‹å¡](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
- [MoE æ¶æ„è®ºæ–‡](https://arxiv.org/abs/2101.03961)
- [PagedAttention è®ºæ–‡](https://arxiv.org/abs/2309.06180)
