---
title: "EKS Hybrid Nodes å®Œæ•´æŒ‡å—"
sidebar_label: "1. Hybrid Nodes Complete Guide"
description: "Amazon EKS Hybrid Nodes é‡‡ç”¨å®Œæ•´æŒ‡å—ï¼šæ¶æ„ã€é…ç½®ã€ç½‘ç»œã€DNSã€GPU æœåŠ¡å™¨ã€æˆæœ¬åˆ†æå’ŒåŠ¨æ€èµ„æºåˆ†é… (DRA)"
tags: [eks, hybrid-nodes, nodeadm, kubernetes, harbor, networking, dns, gpu, dra, cost-optimization, architecture]
category: "hybrid-multicloud"
sidebar_position: 1
last_update:
  date: 2026-02-14
  author: devfloor9
---

# EKS Hybrid Nodes å®Œæ•´æŒ‡å—

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2025-08-20 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 6 åˆ†é’Ÿ

## ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [å‰ææ¡ä»¶](#å‰ææ¡ä»¶)
3. [ç½‘ç»œä¸ DNS é…ç½®](#ç½‘ç»œä¸-dns-é…ç½®)
4. [Harbor ç§æœ‰é•œåƒä»“åº“å®‰è£…](#harbor-ç§æœ‰é•œåƒä»“åº“å®‰è£…)
5. [EKS Hybrid Nodes é…ç½®](#eks-hybrid-nodes-é…ç½®)
6. [Harbor ä¸ EKS é›†æˆ](#harbor-ä¸-eks-é›†æˆ)
7. [GPU æœåŠ¡å™¨é›†æˆ](#gpu-æœåŠ¡å™¨é›†æˆ)
8. [æˆæœ¬åˆ†æä¸ä¼˜åŒ–](#æˆæœ¬åˆ†æä¸ä¼˜åŒ–)
9. [åŠ¨æ€èµ„æºåˆ†é… (DRA)](#åŠ¨æ€èµ„æºåˆ†é…-dra)
10. [è¿ç»´ä¸ç»´æŠ¤](#è¿ç»´ä¸ç»´æŠ¤)

## æ¦‚è¿°

æœ¬æŒ‡å—æä¾›äº† Amazon EKS Hybrid Nodes çš„å®Œæ•´é‡‡ç”¨æ–¹æ³•ã€‚EKS Hybrid Nodes äº 2024 å¹´ 12 æœˆæ­£å¼å‘å¸ƒï¼Œæ”¯æŒå°†æœ¬åœ°åŸºç¡€è®¾æ–½ä¸ AWS EKS è¿›è¡Œç»Ÿä¸€ç®¡ç†ï¼Œå…è®¸åœ¨å•ä¸ª Kubernetes é›†ç¾¤å†…ç®¡ç†é«˜æ€§èƒ½ GPU æœåŠ¡å™¨å’Œäº‘èµ„æºã€‚

**ä¸»è¦ç‰¹æ€§ï¼š**

- æœ¬åœ°ä¸äº‘ç«¯ç»Ÿä¸€ç®¡ç†
- Harbor 2.13 ç§æœ‰é•œåƒä»“åº“é›†æˆ
- H100 GPU æœåŠ¡å™¨æ”¯æŒ
- åŠ¨æ€èµ„æºåˆ†é… (DRA)
- çµæ´»çš„å·¥ä½œè´Ÿè½½è°ƒåº¦

## å‰ææ¡ä»¶

### ç³»ç»Ÿè¦æ±‚

**æœ¬åœ°èŠ‚ç‚¹ï¼š**

- æ“ä½œç³»ç»Ÿï¼šUbuntu 20.04/22.04/24.04 LTS æˆ– RHEL 8/9
- Docker Engine 20.10.10+ï¼ˆç”¨äº Harborï¼‰
- å®¹å™¨è¿è¡Œæ—¶ï¼šcontainerd 1.6.x æˆ–æ›´é«˜ç‰ˆæœ¬
- æœ€ä½ç¡¬ä»¶è¦æ±‚ï¼š2 CPU æ ¸å¿ƒï¼Œ4GB RAM

**GPU æœåŠ¡å™¨ï¼ˆå¯é€‰ï¼‰ï¼š**

- NVIDIA é©±åŠ¨ 550.x æˆ–æ›´é«˜ç‰ˆæœ¬
- NVIDIA Container Toolkit
- H100/H200 GPU æ”¯æŒ

### ç½‘ç»œè¦æ±‚

| é¡¹ç›® | è¦æ±‚ |
|------|------|
| å¸¦å®½ | æœ€ä½ 10Gbpsï¼ˆDirect Connect æˆ– VPNï¼‰ |
| å»¶è¿Ÿ | å»ºè®® 5ms ä»¥ä¸‹ |
| MTU | å»ºè®® Jumbo Frame (9000) |

## ç½‘ç»œä¸ DNS é…ç½®

### æ‰€éœ€é˜²ç«å¢™è®¾ç½®

é…ç½®æœ¬åœ°ä¸ AWS ä¹‹é—´æ‰€éœ€çš„é˜²ç«å¢™ç«¯å£ï¼š

| åè®® | ç«¯å£ | æ–¹å‘ | ç”¨é€” |
|------|------|------|------|
| TCP | 443 | åŒå‘ | Kubernetes API æœåŠ¡å™¨é€šä¿¡ |
| TCP | 10250 | æœ¬åœ° â†’ AWS | Kubelet API |
| TCP/UDP | 53 | åŒå‘ | DNS æŸ¥è¯¢ |
| TCP | 6443 | æœ¬åœ° â†’ AWS | Kubernetes APIï¼ˆå¤‡é€‰ï¼‰ |

### Pod CIDR é˜²ç«å¢™é…ç½®

:::tip å»ºè®®
å»ºè®®åœ¨é˜²ç«å¢™ä¸­æ³¨å†Œæ•´ä¸ª Pod CIDR èŒƒå›´ã€‚
:::

**é…ç½®æ–¹æ³•ï¼š**

- **å®Œæ•´ CIDR æ³¨å†Œï¼ˆæ¨èï¼‰**ï¼šä¾‹å¦‚ `10.244.0.0/16`
  - çµæ´»é€‚åº”åŠ¨æ€ Pod IP åˆ†é…
  - Pod æ‰©å±•æ—¶æ— éœ€é¢å¤–é˜²ç«å¢™è®¾ç½®
  - é™ä½ç®¡ç†å¤æ‚åº¦

- **ä»…å›ºå®š IP Worker èŠ‚ç‚¹ï¼ˆä¸æ¨èï¼‰**ï¼š
  - æ¯å½“ Pod IP å˜æ›´æ—¶éœ€æ›´æ–°é˜²ç«å¢™è§„åˆ™
  - å¢åŠ è¿ç»´å¤æ‚åº¦
  - å¢åŠ æœåŠ¡ä¸­æ–­é£é™©

### Istio + Calico CNI æ··åˆæ¨¡å¼

åŒæ—¶ä½¿ç”¨ Istio æœåŠ¡ç½‘æ ¼å’Œ Calico CNI æ—¶çš„é¢å¤–ç«¯å£é…ç½®ï¼š

| ç»„ä»¶ | ç«¯å£ | ç”¨é€” |
|------|------|------|
| Envoy Proxy | 15001 | å‡ºç«™æµé‡ |
| Envoy Proxy | 15006 | å…¥ç«™æµé‡ |
| Pilot | 15010 | xDS æœåŠ¡å™¨ |
| Istio Telemetry | 15004 | Mixer ç­–ç•¥ |
| Calico BGP | 179 | BGP å¯¹ç­‰ |
| Calico Felix | 9099 | æŒ‡æ ‡ |

```bash
# é˜²ç«å¢™è§„åˆ™ç¤ºä¾‹ï¼ˆAWS Security Groupï¼‰
aws ec2 authorize-security-group-ingress \
  --group-id sg-hybrid-nodes \
  --protocol tcp \
  --port 15001 \
  --source-group sg-eks-cluster

aws ec2 authorize-security-group-ingress \
  --group-id sg-hybrid-nodes \
  --protocol tcp \
  --port 179 \
  --source-group sg-hybrid-nodes
```

### DNS é…ç½®

#### Route 53 Resolver Inbound Endpointï¼ˆæœ¬åœ° â†’ AWSï¼‰

**ç”¨é€”**ï¼šä½¿æœ¬åœ°æœåŠ¡å™¨èƒ½å¤ŸæŸ¥è¯¢ AWS å†…éƒ¨åŸŸå

```bash
# åˆ›å»º Route 53 Resolver Inbound Endpoint
aws route53resolver create-resolver-endpoint \
  --creator-request-id unique-id-123 \
  --name hybrid-inbound-endpoint \
  --security-group-ids sg-resolver-xxxxx \
  --direction INBOUND \
  --ip-addresses SubnetId=subnet-xxxxx,Ip=10.0.1.100 \
              SubnetId=subnet-yyyyy,Ip=10.0.2.100
```

**æœ¬åœ° DNS é…ç½®ï¼ˆç¤ºä¾‹ï¼šBINDï¼‰ï¼š**

```bash
# /etc/named.conf
zone "eks.amazonaws.com" {
    type forward;
    forward only;
    forwarders { 10.0.1.100; 10.0.2.100; };
};
```

#### Route 53 Resolver Outbound Endpointï¼ˆAWS â†’ æœ¬åœ°ï¼‰

**ç”¨é€”**ï¼šä½¿ AWS Worker èŠ‚ç‚¹èƒ½å¤ŸæŸ¥è¯¢æœ¬åœ°å†…éƒ¨åŸŸå

```bash
# åˆ›å»º Outbound Endpoint
aws route53resolver create-resolver-endpoint \
  --creator-request-id unique-id-456 \
  --name hybrid-outbound-endpoint \
  --security-group-ids sg-resolver-xxxxx \
  --direction OUTBOUND \
  --ip-addresses SubnetId=subnet-xxxxx \
              SubnetId=subnet-yyyyy

# åˆ›å»º Resolver è§„åˆ™
aws route53resolver create-resolver-rule \
  --creator-request-id unique-id-789 \
  --name on-prem-dns-rule \
  --rule-type FORWARD \
  --domain-name company.local \
  --target-ips Ip=192.168.1.53,Port=53 Ip=192.168.1.54,Port=53 \
  --resolver-endpoint-id rslvr-out-xxxxx
```

#### åŒå‘ DNS æŸ¥è¯¢éªŒè¯

```bash
# ä»æœ¬åœ°æŸ¥è¯¢ AWS åŸŸå
dig @10.0.1.100 my-service.eks.amazonaws.com

# ä» AWS æŸ¥è¯¢æœ¬åœ°åŸŸå
dig harbor.company.local
```

### CIDR è®¾è®¡

#### CIDR è®¾è®¡åŸåˆ™

**AWS VPC CIDRï¼š**

- ä¸»è¦ï¼š`10.0.0.0/16`ï¼ˆ65,536 ä¸ª IPï¼‰
- è¾…åŠ©ï¼ˆå¦‚éœ€è¦ï¼‰ï¼š`10.1.0.0/16`

**æœ¬åœ° CIDRï¼š**

- ç°æœ‰ç½‘ç»œï¼š`192.168.0.0/16`
- Pod CIDRï¼š`10.244.0.0/16`
- Service CIDRï¼š`10.96.0.0/16`

**é˜²æ­¢åœ°å€é‡å ï¼š**

```bash
# æ£€æŸ¥ CIDR é‡å 
aws ec2 describe-vpcs --query 'Vpcs[*].CidrBlock'
# æ£€æŸ¥æœ¬åœ°è·¯ç”±è¡¨
ip route show
```

#### è·¯ç”±é…ç½®

```bash
# åˆ›å»º AWS Transit Gateway
aws ec2 create-transit-gateway \
  --description "Hybrid connectivity" \
  --options AmazonSideAsn=64512,AutoAcceptSharedAttachments=enable

# åˆ›å»º VPN è¿æ¥
aws ec2 create-vpn-connection \
  --type ipsec.1 \
  --customer-gateway-id cgw-xxxxx \
  --transit-gateway-id tgw-xxxxx \
  --options TunnelInsideIpVersion=ipv4,TunnelOptions=[{TunnelInsideCidr=169.254.10.0/30}]
```

## Harbor ç§æœ‰é•œåƒä»“åº“å®‰è£…

### ä¸‹è½½ Harbor 2.13.2

```bash
# ä¸‹è½½ Harbor 2.13.2ï¼ˆæœ€æ–°ç¨³å®šç‰ˆï¼‰
wget https://github.com/goharbor/harbor/releases/download/v2.13.2/harbor-offline-installer-v2.13.2.tgz

# è§£å‹å½’æ¡£æ–‡ä»¶
tar xvf harbor-offline-installer-v2.13.2.tgz
cd harbor
```

### SSL/TLS è¯ä¹¦é…ç½®

#### ç”Ÿæˆè‡ªç­¾åè¯ä¹¦

```bash
# 1. ç”Ÿæˆ CA è¯ä¹¦
openssl genrsa -out ca.key 4096
openssl req -x509 -new -nodes -sha512 -days 3650 \
  -key ca.key \
  -out ca.crt \
  -subj "/C=US/ST=California/L=San Francisco/O=MyOrganization/CN=Harbor-CA"

# 2. ç”ŸæˆæœåŠ¡å™¨è¯ä¹¦
openssl genrsa -out harbor.key 4096
openssl req -new -sha512 \
  -key harbor.key \
  -out harbor.csr \
  -subj "/C=US/ST=California/L=San Francisco/O=MyOrganization/CN=harbor.yourdomain.com"

# 3. åˆ›å»º v3.ext æ–‡ä»¶ï¼ˆSAN é…ç½®ï¼‰
cat > v3.ext <<EOF
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names

[alt_names]
DNS.1=harbor.yourdomain.com
DNS.2=yourdomain.com
IP.1=192.168.1.100
EOF

# 4. ç­¾ç½²è¯ä¹¦
openssl x509 -req -sha512 -days 3650 \
  -extfile v3.ext \
  -CA ca.crt -CAkey ca.key -CAcreateserial \
  -in harbor.csr \
  -out harbor.crt

# 5. åˆ›å»ºè¯ä¹¦ç›®å½•å¹¶å¤åˆ¶æ–‡ä»¶
mkdir -p /data/cert
cp harbor.crt /data/cert/
cp harbor.key /data/cert/
```

### Harbor é…ç½®æ–‡ä»¶è®¾ç½®

```bash
# å¤åˆ¶å¹¶ç¼–è¾‘ harbor.yml æ–‡ä»¶
cp harbor.yml.tmpl harbor.yml
vi harbor.yml
```

å…³é”®é…ç½®å†…å®¹ï¼š

```yaml
# ä¸»æœºåè®¾ç½®
hostname: harbor.yourdomain.com

# HTTPS é…ç½®
https:
  port: 443
  certificate: /data/cert/harbor.crt
  private_key: /data/cert/harbor.key

# Harbor ç®¡ç†å‘˜å¯†ç 
harbor_admin_password: Harbor12345!

# æ•°æ®åº“é…ç½®
database:
  password: root123
  max_idle_conns: 100
  max_open_conns: 900
  conn_max_lifetime: 5m
  conn_max_idle_time: 0

# æ•°æ®å­˜å‚¨è·¯å¾„
data_volume: /data

# æ—¥å¿—é…ç½®
log:
  level: info
  local:
    rotate_count: 50
    rotate_size: 200M
    location: /var/log/harbor

# Trivy æ¼æ´æ‰«æå™¨é…ç½®
trivy:
  ignore_unfixed: false
  skip_update: false
  offline_scan: false
  insecure: false

# æŒ‡æ ‡é…ç½®
metric:
  enabled: true
  port: 9090
  path: /metrics
```

### Harbor å®‰è£…

```bash
# è¿è¡Œå®‰è£…å‡†å¤‡è„šæœ¬
sudo ./prepare

# å®‰è£… Harborï¼ˆåŒ…å« Trivyï¼‰
sudo ./install.sh --with-trivy

# éªŒè¯å®‰è£…
docker-compose ps
```

### åˆ›å»º Robot è´¦æˆ·

```bash
# é€šè¿‡ Harbor UI åˆ›å»ºæˆ–ä½¿ç”¨ API
curl -X POST "https://harbor.yourdomain.com/api/v2.0/robots" \
  -H "Content-Type: application/json" \
  -u "admin:Harbor12345!" \
  -d '{
    "name": "k8s-robot",
    "duration": 365,
    "description": "Robot account for Kubernetes",
    "disable": false,
    "level": "system",
    "permissions": [
      {
        "namespace": "*",
        "kind": "project",
        "access": [
          {
            "resource": "repository",
            "action": "pull"
          }
        ]
      }
    ]
  }'
```

## EKS Hybrid Nodes é…ç½®

### å®‰è£… nodeadm

```bash
# x86_64 æ¶æ„
curl -OL 'https://hybrid-assets.eks.amazonaws.com/releases/latest/bin/linux/amd64/nodeadm'

# ARM æ¶æ„ï¼ˆå¦‚éœ€è¦ï¼‰
# curl -OL 'https://hybrid-assets.eks.amazonaws.com/releases/latest/bin/linux/arm64/nodeadm'

# æˆäºˆæ‰§è¡Œæƒé™
chmod +x nodeadm
sudo mv nodeadm /usr/local/bin/

# éªŒè¯ç‰ˆæœ¬
nodeadm version
```

### å®‰è£…æ‰€éœ€ç»„ä»¶

```bash
# å®‰è£… Kubernetes 1.33 æ”¯æŒç»„ä»¶
sudo nodeadm install 1.33 --credential-provider ssm

# æˆ–ä½¿ç”¨ IAM Roles Anywhere æ—¶
# sudo nodeadm install 1.33 --credential-provider iam-ra
```

### åˆ›å»º NodeConfig æ–‡ä»¶

```yaml
# nodeconfig.yaml
apiVersion: node.eks.aws/v1alpha1
kind: NodeConfig
spec:
  cluster:
    name: my-hybrid-cluster
    region: ap-northeast-2

  # ä½¿ç”¨ SSM çš„æ··åˆèŠ‚ç‚¹é…ç½®
  hybrid:
    ssm:
      activationCode: "YOUR-ACTIVATION-CODE"
      activationId: "YOUR-ACTIVATION-ID"

  # Containerd é…ç½®ï¼ˆHarbor é•œåƒä»“åº“è®¾ç½®ï¼‰
  containerd:
    config: |
      version = 2

      [plugins."io.containerd.grpc.v1.cri"]
        [plugins."io.containerd.grpc.v1.cri".registry]
          config_path = "/etc/containerd/certs.d:/etc/docker/certs.d"

        [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
          [plugins."io.containerd.grpc.v1.cri".registry.mirrors."harbor.yourdomain.com"]
            endpoint = ["https://harbor.yourdomain.com"]

        [plugins."io.containerd.grpc.v1.cri".registry.configs]
          [plugins."io.containerd.grpc.v1.cri".registry.configs."harbor.yourdomain.com"]
            [plugins."io.containerd.grpc.v1.cri".registry.configs."harbor.yourdomain.com".auth]
              username = "robot$k8s-robot"
              password = "YOUR-ROBOT-TOKEN"

            [plugins."io.containerd.grpc.v1.cri".registry.configs."harbor.yourdomain.com".tls]
              ca_file = "/etc/ssl/certs/harbor-ca.crt"
              insecure_skip_verify = false

  # Kubelet é…ç½®
  kubelet:
    config:
      shutdownGracePeriod: 30s
      maxPods: 110
    flags:
      - --node-labels=node-type=hybrid,registry=harbor
```

### å®‰è£…è¯ä¹¦

```bash
# å°† CA è¯ä¹¦æ·»åŠ åˆ°ç³»ç»Ÿä¿¡ä»»å­˜å‚¨
sudo cp ca.crt /usr/local/share/ca-certificates/harbor-ca.crt
sudo update-ca-certificates

# ä¸º containerd åˆ›å»ºè¯ä¹¦ç›®å½•
sudo mkdir -p /etc/containerd/certs.d/harbor.yourdomain.com

# å¤åˆ¶è¯ä¹¦
sudo cp ca.crt /etc/containerd/certs.d/harbor.yourdomain.com/ca.crt

# é‡å¯ containerd
sudo systemctl restart containerd
```

### èŠ‚ç‚¹åˆå§‹åŒ–

```bash
# ä½¿ç”¨ NodeConfig åˆå§‹åŒ–èŠ‚ç‚¹
sudo nodeadm init --config-source file://nodeconfig.yaml

# éªŒè¯èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes
```

## Harbor ä¸ EKS é›†æˆ

### ç½‘ç»œé…ç½®

```bash
# å…è®¸ EKS èŠ‚ç‚¹è®¿é—® Harbor å®‰å…¨ç»„
aws ec2 authorize-security-group-ingress \
  --group-id sg-harbor-xxxxx \
  --protocol tcp \
  --port 443 \
  --source-group sg-eks-nodes-xxxxx \
  --region ap-northeast-2
```

### CoreDNS é…ç½®

```yaml
# ä¿®æ”¹ CoreDNS ConfigMap
kubectl edit configmap coredns -n kube-system

# æ·»åŠ ä»¥ä¸‹å†…å®¹
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        # æ·»åŠ  Harbor DNS
        hosts {
          192.168.1.100 harbor.yourdomain.com
          fallthrough
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
```

### åˆ›å»º Kubernetes Secret

```bash
# æµ‹è¯• docker ç™»å½•
docker login harbor.yourdomain.com
Username: robot$k8s-robot
Password: YOUR-ROBOT-TOKEN

# åˆ›å»º Kubernetes Secret
kubectl create secret docker-registry harbor-registry \
  --docker-server=harbor.yourdomain.com \
  --docker-username='robot$k8s-robot' \
  --docker-password='YOUR-ROBOT-TOKEN' \
  --docker-email=admin@yourdomain.com

# å°† Secret å¤åˆ¶åˆ°æ‰€æœ‰å‘½åç©ºé—´ï¼ˆå¯é€‰ï¼‰
for ns in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do
  kubectl get secret harbor-registry -o yaml | \
    sed "s/namespace: default/namespace: $ns/" | \
    kubectl apply -f -
done
```

### æµ‹è¯•ä¸éªŒè¯

```bash
# 1. éªŒè¯ç½‘ç»œè¿é€šæ€§
curl -k https://harbor.yourdomain.com/api/v2.0/health

# 2. ç›´æ¥ä»èŠ‚ç‚¹æµ‹è¯•é•œåƒæ‹‰å–
sudo crictl pull harbor.yourdomain.com/library/nginx:latest

# 3. æµ‹è¯• Kubernetes Pod éƒ¨ç½²
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: harbor-test
spec:
  containers:
  - name: nginx
    image: harbor.yourdomain.com/library/nginx:latest
  imagePullSecrets:
  - name: harbor-registry
EOF

# 4. éªŒè¯ Pod çŠ¶æ€
kubectl get pod harbor-test
kubectl describe pod harbor-test
```

## GPU æœåŠ¡å™¨é›†æˆ

### H100 GPU æœåŠ¡å™¨é›†æˆ

**éªŒè¯ç›®æ ‡**ï¼šå°† 10 å° H100 GPU æœåŠ¡å™¨ä½œä¸º EKS Hybrid Nodes æ¥å…¥

#### GPU èŠ‚ç‚¹é…ç½®

```yaml
# nodeconfig-gpu.yaml
apiVersion: node.eks.aws/v1alpha1
kind: NodeConfig
spec:
  cluster:
    name: gpu-hybrid-cluster
    region: ap-northeast-2

  hybrid:
    ssm:
      activationCode: "ACTIVATION-CODE"
      activationId: "ACTIVATION-ID"

  kubelet:
    config:
      maxPods: 110
      shutdownGracePeriod: 30s
    flags:
      - --node-labels=node-type=hybrid,gpu=h100,gpu-count=8
      - --register-with-taints=nvidia.com/gpu=present:NoSchedule

  containerd:
    config: |
      version = 2
      [plugins."io.containerd.grpc.v1.cri".containerd]
        default_runtime_name = "nvidia"
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia]
          privileged_without_host_devices = false
          runtime_type = "io.containerd.runc.v2"
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
            BinaryName = "/usr/bin/nvidia-container-runtime"
```

#### éƒ¨ç½² NVIDIA Device Plugin

```bash
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.5/nvidia-device-plugin.yml

# éªŒè¯ GPU èµ„æº
kubectl get nodes -o json | jq '.items[].status.allocatable."nvidia.com/gpu"'
```

#### GPU æµ‹è¯•

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test
spec:
  restartPolicy: Never
  containers:
  - name: cuda
    image: nvidia/cuda:12.3.0-base-ubuntu22.04
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: 1
  nodeSelector:
    gpu: h100
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
```

### æœ¬åœ°å­˜å‚¨è®¿é—®

#### NFS æŒ‚è½½æµ‹è¯•

```bash
# åœ¨ AWS Worker èŠ‚ç‚¹ä¸Šè¿è¡Œ
sudo mount -t nfs -o vers=4.1,rsize=1048576,wsize=1048576 \
  192.168.1.100:/export/data /mnt/onprem-storage

# æ€§èƒ½æµ‹é‡
dd if=/dev/zero of=/mnt/onprem-storage/testfile bs=1M count=1000 oflag=direct
```

#### PersistentVolume é…ç½®

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: onprem-storage-pv
spec:
  capacity:
    storage: 10Ti
  accessModes:
    - ReadWriteMany
  nfs:
    server: 192.168.1.100
    path: /export/data
  mountOptions:
    - vers=4.1
    - rsize=1048576
    - wsize=1048576
    - hard
    - timeo=600
    - retrans=2
```

## æˆæœ¬åˆ†æä¸ä¼˜åŒ–

### Hybrid Nodes å®šä»·ç»“æ„

**åŸºç¡€å®šä»·ï¼ˆ2025 å¹´ 2 æœˆï¼‰ï¼š**

- æ¯ vCPUï¼š$0.1099/å°æ—¶
- æŒ‰æ¯æœˆ 730 å°æ—¶è®¡ç®—ï¼šçº¦ $80.23/vCPU

### H100 GPU æœåŠ¡å™¨æˆæœ¬åˆ†æ

**H100 GPU æœåŠ¡å™¨è§„æ ¼ï¼ˆåŸºäº DGX H200ï¼‰ï¼š**

- CPUï¼š224 vCPUï¼ˆ2x Intel Xeon Platinum 8592+ï¼‰
- RAMï¼š2TB
- GPUï¼š8x H200ï¼ˆ141GB HBM3eï¼‰

**æœˆåº¦æˆæœ¬è®¡ç®—ï¼š**

```
å•èŠ‚ç‚¹ï¼š
- 224 vCPU Ã— $80.23 = $17,971.52/æœˆ

10 èŠ‚ç‚¹ï¼š
- $17,971.52 Ã— 10 = $179,715.20/æœˆ
```

:::warning éœ€è¦å®¡æŸ¥æˆæœ¬ä¼˜åŒ–
ç”±äº H100 GPU æœåŠ¡å™¨çš„ vCPU æ•°é‡è¾ƒå¤šï¼ŒHybrid Nodes æˆæœ¬ç›¸å½“å¯è§‚ã€‚è¯·å®¡æŸ¥ä»¥ä¸‹ä¼˜åŒ–æ–¹æ³•ï¼š

1. **é€‰æ‹©æ€§å·¥ä½œè´Ÿè½½è°ƒåº¦**ï¼šä»…å°† GPU å¯†é›†å‹å·¥ä½œè´Ÿè½½æ”¾ç½®åœ¨ Hybrid Nodes ä¸Š
2. **Spot å®ä¾‹æ··åˆä½¿ç”¨**ï¼šä¸º AWS Worker åˆ©ç”¨ Spot å®ä¾‹
3. **è‡ªåŠ¨æ‰©ç¼©å®¹**ï¼šåœ¨éä½¿ç”¨æ—¶æ®µç§»é™¤èŠ‚ç‚¹
4. **é¢„ç•™å®¹é‡**ï¼šé’ˆå¯¹é•¿æœŸä½¿ç”¨åå•†é¢„ç•™é€‰é¡¹
:::

### æˆæœ¬é™ä½ç­–ç•¥

#### 1. æ··åˆå·¥ä½œè´Ÿè½½åˆ†å¸ƒ

```yaml
# GPU å·¥ä½œè´Ÿè½½ â†’ æœ¬åœ°
apiVersion: v1
kind: Pod
metadata:
  name: gpu-training
spec:
  nodeSelector:
    gpu: h100
    node-type: hybrid
  containers:
  - name: training
    image: pytorch/pytorch:2.1-cuda12.1
    resources:
      limits:
        nvidia.com/gpu: 8

---
# CPU å·¥ä½œè´Ÿè½½ â†’ AWS EC2
apiVersion: v1
kind: Pod
metadata:
  name: web-api
spec:
  nodeSelector:
    eks.amazonaws.com/compute-type: ec2
  containers:
  - name: api
    image: nginx:latest
```

#### 2. Cluster Autoscaler é…ç½®

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  template:
    spec:
      containers:
      - name: cluster-autoscaler
        image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.0
        command:
          - ./cluster-autoscaler
          - --cloud-provider=aws
          - --skip-nodes-with-system-pods=false
          - --expander=least-waste
          - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
```

#### 3. æˆæœ¬ç›‘æ§

```bash
# ä½¿ç”¨ AWS Cost Explorer API è·Ÿè¸ª Hybrid Nodes æˆæœ¬
aws ce get-cost-and-usage \
  --time-period Start=2025-02-01,End=2025-02-28 \
  --granularity MONTHLY \
  --metrics BlendedCost \
  --filter file://filter.json

# filter.json
{
  "Dimensions": {
    "Key": "SERVICE",
    "Values": ["Amazon Elastic Kubernetes Service - Hybrid Nodes"]
  }
}
```

### å·¥ä½œè´Ÿè½½åˆ†å¸ƒç­–ç•¥

**æœ¬åœ° GPU Workerï¼š**

- AI/ML è®­ç»ƒå·¥ä½œè´Ÿè½½
- é«˜æ€§èƒ½æ¨ç†æœåŠ¡
- æ•°æ®å¯†é›†å‹å¤„ç†

**AWS CPU Workerï¼š**

- Web åº”ç”¨å’Œ API
- å¾®æœåŠ¡
- è½»é‡çº§æ‰¹å¤„ç†ä»»åŠ¡

## åŠ¨æ€èµ„æºåˆ†é… (DRA)

:::info ä»€ä¹ˆæ˜¯ DRAï¼Ÿ
åŠ¨æ€èµ„æºåˆ†é… (DRA) æ˜¯ Kubernetes 1.26 å¼•å…¥çš„åŠŸèƒ½ï¼Œä½¿ Pod èƒ½å¤Ÿåœ¨è¯·æ±‚æ—¶åŠ¨æ€åˆ†é… GPUã€NPU å’Œä¸“ç”¨åŠ é€Ÿå™¨ç­‰èµ„æºã€‚è¿™æ”¹è¿›äº†ä¼ ç»Ÿçš„é™æ€èµ„æºåˆ†é…æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„èµ„æºåˆ©ç”¨ã€‚
:::

### å¯ç”¨ DRA

```yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: hybrid-dra-cluster
  region: ap-northeast-2
  version: "1.30"

kubernetesNetworkConfig:
  serviceIPv4CIDR: 10.100.0.0/16

managedNodeGroups:
  - name: cpu-nodes
    instanceType: m5.xlarge
    desiredCapacity: 3
    minSize: 1
    maxSize: 10
    labels:
      node-type: cpu
      workload: general

  - name: gpu-nodes
    instanceType: g5.xlarge
    desiredCapacity: 2
    minSize: 1
    maxSize: 5
    labels:
      node-type: gpu
      workload: ml
    taints:
      - key: nvidia.com/gpu
        value: "true"
        effect: NoSchedule
```

### éƒ¨ç½² DRA é©±åŠ¨

```bash
# æ·»åŠ  DRA é©±åŠ¨ Helm ä»“åº“
helm repo add dra-driver https://charts.dra.io
helm repo update

# å®‰è£… DRA é©±åŠ¨
helm install dra-driver dra-driver/dra-driver \
  --namespace kube-system \
  --set driver.name=eks-hybrid-dra \
  --set driver.enableGPU=true \
  --set driver.enableCPU=true
```

### å®šä¹‰èµ„æºç±»

```yaml
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClass
metadata:
  name: gpu-compute-class
spec:
  driverName: eks-hybrid-dra
  suitableNodes:
    nodeSelectorTerms:
    - matchExpressions:
      - key: node-type
        operator: In
        values: ["gpu"]
  parametersRef:
    name: gpu-parameters
    namespace: kube-system
---
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClass
metadata:
  name: cpu-compute-class
spec:
  driverName: eks-hybrid-dra
  suitableNodes:
    nodeSelectorTerms:
    - matchExpressions:
      - key: node-type
        operator: In
        values: ["cpu"]
  parametersRef:
    name: cpu-parameters
    namespace: kube-system
```

### é…ç½®èµ„æºå£°æ˜

```yaml
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaimTemplate
metadata:
  name: ml-training-claim
  namespace: ml-workloads
spec:
  spec:
    resourceClassName: gpu-compute-class
    allocationMode: WaitForFirstConsumer
    parametersRef:
      name: ml-training-params
      namespace: ml-workloads
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-training-params
  namespace: ml-workloads
data:
  gpu-count: "1"
  gpu-memory: "16Gi"
  cuda-version: "12.2"
```

### ML è®­ç»ƒä½œä¸šç¤ºä¾‹

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-training-job
  namespace: ml-workloads
spec:
  template:
    metadata:
      labels:
        app: ml-training
    spec:
      resourceClaims:
      - name: gpu-resource
        source:
          resourceClaimTemplateName: ml-training-claim
      containers:
      - name: training
        image: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime
        command: ["python", "train.py"]
        resources:
          claims:
          - name: gpu-resource
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
      restartPolicy: OnFailure
      tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: "true"
        effect: NoSchedule
```

### DRA ç›‘æ§

:::tip å…³é”®æŒ‡æ ‡
ç›‘æ§ä»¥ä¸‹å…³é”®æŒ‡æ ‡ä»¥è¯„ä¼° DRA æ€§èƒ½ï¼š

- `dra_allocation_duration_seconds` - èµ„æºåˆ†é…è€—æ—¶
- `dra_allocation_errors_total` - åˆ†é…å¤±è´¥æ¬¡æ•°
- `dra_resource_utilization_ratio` - èµ„æºä½¿ç”¨æ•ˆç‡
- `dra_pending_claims_total` - æœªè°ƒåº¦çš„èµ„æºå£°æ˜
:::

## è¿ç»´ä¸ç»´æŠ¤

### å®‰å…¨åŠ å›º

```bash
# å¯ç”¨ Harbor æ¼æ´æ‰«æè‡ªåŠ¨åŒ–
curl -X PUT "https://harbor.yourdomain.com/api/v2.0/projects/1" \
  -H "Content-Type: application/json" \
  -u "admin:Harbor12345!" \
  -d '{
    "metadata": {
      "auto_scan": "true",
      "prevent_vul": "true",
      "severity": "high"
    }
  }'

# é…ç½®é•œåƒç­¾åç­–ç•¥ï¼ˆNotaryï¼‰
export DOCKER_CONTENT_TRUST=1
export DOCKER_CONTENT_TRUST_SERVER=https://harbor.yourdomain.com:4443
```

### å¤‡ä»½ä¸æ¢å¤

```bash
#!/bin/bash
# harbor-backup.sh

BACKUP_DIR="/backup/harbor-$(date +%Y%m%d-%H%M%S)"
mkdir -p $BACKUP_DIR

# 1. å¤‡ä»½ Harbor é…ç½®
cp -r /data/harbor $BACKUP_DIR/

# 2. å¤‡ä»½æ•°æ®åº“
docker exec harbor-db pg_dump -U postgres registry > $BACKUP_DIR/registry.sql

# 3. å¤‡ä»½é•œåƒä»“åº“æ•°æ®ï¼ˆå¯é€‰ï¼‰
tar -czf $BACKUP_DIR/registry-data.tar.gz /data/registry

echo "å¤‡ä»½å®Œæˆ: $BACKUP_DIR"
```

### ç›‘æ§

#### Prometheus æŒ‡æ ‡é‡‡é›†

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
    scrape_configs:
    - job_name: 'harbor'
      static_configs:
      - targets: ['harbor.yourdomain.com:9090']
      metrics_path: '/metrics'
```

#### å…³é”®ç›‘æ§æŒ‡æ ‡

- é•œåƒä»“åº“è¯·æ±‚é€Ÿç‡
- è®¤è¯å¤±è´¥æ¬¡æ•°
- å­˜å‚¨ä½¿ç”¨é‡
- æ•°æ®åº“è¿æ¥æ•°
- API å“åº”æ—¶é—´

### æ€§èƒ½éªŒè¯

#### Direct Connect æ€§èƒ½æµ‹è¯•

```bash
# 1. åŸºç¡€è¿é€šæ€§æµ‹è¯•
ping -c 100 <aws-endpoint>

# 2. æ£€æŸ¥ MTU ä¼˜åŒ–
ping -M do -s 8972 <aws-endpoint>

# 3. è·¯ç”±è·Ÿè¸ª
traceroute -n <aws-endpoint>

# 4. å¸¦å®½æµ‹é‡
iperf3 -c <aws-endpoint> -t 60 -P 10 -w 512K
```

#### æ€§èƒ½åŸºå‡†

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å‘Šè­¦å€¼ | ä¸¥é‡å€¼ |
|------|--------|--------|--------|
| å»¶è¿Ÿ | < 5ms | 5-10ms | > 10ms |
| æŠ–åŠ¨ | < 2ms | 2-5ms | > 5ms |
| ä¸¢åŒ…ç‡ | < 0.01% | 0.01-0.1% | > 0.1% |
| å¸¦å®½ | > 10Gbps | 5-10Gbps | < 5Gbps |

### æ•…éšœæ’é™¤

#### ImagePullBackOff é”™è¯¯

```bash
# è¯Šæ–­é—®é¢˜
kubectl describe pod <pod-name>
kubectl get events --field-selector involvedObject.name=<pod-name>

# æ£€æŸ¥ Secret
kubectl get secret harbor-registry -o jsonpath='{.data.\.dockerconfigjson}' | base64 -d
```

#### è¯ä¹¦é”™è¯¯

```bash
# åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šå®‰è£… CA è¯ä¹¦
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: harbor-ca-installer
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: harbor-ca-installer
  template:
    metadata:
      labels:
        name: harbor-ca-installer
    spec:
      hostNetwork: true
      hostPID: true
      containers:
      - name: installer
        image: busybox
        command: ['sh', '-c']
        args:
        - |
          echo "Installing Harbor CA certificate..."
          cp /ca-cert/ca.crt /host/usr/local/share/ca-certificates/harbor-ca.crt
          chroot /host update-ca-certificates
          chroot /host systemctl restart containerd
          sleep 3600
        volumeMounts:
        - name: ca-cert
          mountPath: /ca-cert
        - name: host
          mountPath: /host
        securityContext:
          privileged: true
      volumes:
      - name: ca-cert
        configMap:
          name: harbor-ca
      - name: host
        hostPath:
          path: /
EOF
```

#### DNS è§£æå¤±è´¥

```bash
# æµ‹è¯• DNS
kubectl run -it --rm debug --image=busybox --restart=Never -- nslookup harbor.yourdomain.com

# æ£€æŸ¥ CoreDNS æ—¥å¿—
kubectl logs -n kube-system -l k8s-app=kube-dns

# é‡å¯ CoreDNS
kubectl rollout restart deployment coredns -n kube-system
```

## ç»“è®º

EKS Hybrid Nodes æä¾›äº†è·¨æœ¬åœ°ä¸äº‘ç«¯çš„ç»Ÿä¸€ Kubernetes ç¯å¢ƒã€‚æœ¬æŒ‡å—æ¶µç›–çš„å…³é”®æˆåŠŸå› ç´ ï¼š

1. **æ­£ç¡®çš„ç½‘ç»œé…ç½®**ï¼šå®Œæ•´çš„ Pod CIDR é˜²ç«å¢™æ³¨å†Œå’ŒåŒå‘ DNS é…ç½®
2. **è¯ä¹¦ç®¡ç†**ï¼šä½¿ç”¨è‡ªç­¾åè¯ä¹¦æ—¶ï¼Œéœ€åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šå®‰è£… CA è¯ä¹¦
3. **æˆæœ¬ä¼˜åŒ–**ï¼šæ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹æ€§å»ºç«‹æ··åˆåˆ†å¸ƒç­–ç•¥
4. **åŠ¨æ€èµ„æºåˆ†é…**ï¼šä½¿ç”¨ DRA å®ç°é«˜æ•ˆçš„ GPU èµ„æºç®¡ç†
5. **æŒç»­éªŒè¯**ï¼šé€šè¿‡åˆ†æ­¥æµ‹è¯•è¿›è¡Œé…ç½®éªŒè¯

åœ¨é‡‡ç”¨å‰ï¼Œä¼˜å…ˆå®¡æŸ¥ä»¥ä¸‹äº‹é¡¹ï¼š

- é€šè¿‡ Direct Connect ç¡®ä¿å®‰å…¨çš„ä½å»¶è¿Ÿè¿æ¥
- H100 GPU æœåŠ¡å™¨é«˜ vCPU æˆæœ¬ä¼˜åŒ–ç­–ç•¥
- é€šè¿‡ PoC éªŒè¯å®é™…ç¯å¢ƒçš„æ€§èƒ½å’Œç¨³å®šæ€§

---

### å‚è€ƒèµ„æ–™

- [Amazon EKS Hybrid Nodes å®˜æ–¹æ–‡æ¡£](https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes.html)
- [EKS Hybrid Nodes å®šä»·](https://aws.amazon.com/eks/pricing/)
- [Harbor 2.13 å‘è¡Œè¯´æ˜](https://github.com/goharbor/harbor/releases/tag/v2.13.2)
- [Kubernetes DRA KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation)
- [NVIDIA GPU Operator æ–‡æ¡£](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html)
