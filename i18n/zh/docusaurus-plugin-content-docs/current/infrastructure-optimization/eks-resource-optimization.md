---
title: "EKS Pod èµ„æºä¼˜åŒ–æŒ‡å—"
sidebar_label: "5. Pod èµ„æºä¼˜åŒ–"
description: "Kubernetes Pod çš„ CPU/Memory èµ„æºè®¾ç½®ã€QoS ç±»åˆ«ã€VPA/HPA è‡ªåŠ¨æ‰©ç¼©å®¹ã€èµ„æº Right-Sizing ç­–ç•¥"
tags: [eks, kubernetes, resources, cpu, memory, qos, vpa, hpa, right-sizing, optimization]
category: "performance-networking"
last_update:
  date: 2026-02-14
  author: devfloor9
sidebar_position: 5
---

# EKS Pod èµ„æºä¼˜åŒ–æŒ‡å—

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2026-02-12 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 46 åˆ†é’Ÿ

> **ğŸ“Œ åŸºå‡†ç¯å¢ƒ**: EKS 1.30+, Kubernetes 1.30+, Metrics Server v0.7+

## æ¦‚è¿°

åœ¨ Kubernetes ç¯å¢ƒä¸­ï¼ŒPod èµ„æºè®¾ç½®ç›´æ¥å½±å“é›†ç¾¤æ•ˆç‡å’Œæˆæœ¬ã€‚**50% çš„å®¹å™¨ä»…ä½¿ç”¨äº†å…¶è¯·æ±‚ CPU çš„ 1/3**ï¼Œè¿™å¯¼è‡´å¹³å‡ 40-60% çš„èµ„æºæµªè´¹ã€‚æœ¬æŒ‡å—é€šè¿‡ Pod çº§åˆ«çš„èµ„æºä¼˜åŒ–ï¼Œæä¾›æœ€å¤§åŒ–é›†ç¾¤æ•ˆç‡å¹¶é™ä½ 30-50% æˆæœ¬çš„å®æˆ˜ç­–ç•¥ã€‚

:::info ä¸ç›¸å…³æ–‡æ¡£çš„åŒºåˆ«
- **[karpenter-autoscaling.md](/docs/infrastructure-optimization/karpenter-autoscaling)**: èŠ‚ç‚¹çº§åˆ«çš„è‡ªåŠ¨æ‰©ç¼©å®¹ï¼ˆæœ¬æ–‡æ¡£æ˜¯ Pod çº§åˆ«ï¼‰
- **[cost-management.md](/docs/infrastructure-optimization/cost-management)**: æ•´ä½“æˆæœ¬ç­–ç•¥ï¼ˆæœ¬æ–‡æ¡£ä¸“æ³¨äºèµ„æºè®¾ç½®ï¼‰
- **[eks-resiliency-guide.md](/docs/operations-observability/eks-resiliency-guide)**: ä»…å°†èµ„æºè®¾ç½®ä½œä¸ºæ£€æŸ¥æ¸…å•é¡¹
:::

### æ ¸å¿ƒå†…å®¹

- **Requests vs Limits æ·±å…¥ç†è§£**: CPU throttling å’Œ OOM Kill æœºåˆ¶
- **QoS ç±»åˆ«ç­–ç•¥**: Guaranteedã€Burstableã€BestEffort çš„å®æˆ˜åº”ç”¨
- **VPA å®Œæ•´æŒ‡å—**: è‡ªåŠ¨èµ„æºè°ƒæ•´ä¸ HPA å…±å­˜æ¨¡å¼
- **Right-Sizing æ–¹æ³•è®º**: åŸºäº P95 çš„èµ„æºä¼°ç®—åŠ Goldilocks ä½¿ç”¨
- **æˆæœ¬å½±å“åˆ†æ**: èµ„æºä¼˜åŒ–çš„å®é™…èŠ‚çœæ•ˆæœ

### å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æŒ‡å—åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

- ç†è§£ CPU å’Œ Memory requests/limits çš„ç²¾ç¡®å·¥ä½œåŸç†
- æ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹æ€§é€‰æ‹©åˆé€‚çš„ QoS ç±»åˆ«
- å®‰å…¨åœ°é…ç½® VPA å’Œ HPA å…±å­˜
- åŸºäºå®é™…ä½¿ç”¨é‡æ‰§è¡Œ Right-Sizing
- å°†èµ„æºæ•ˆç‡æå‡ 30% ä»¥ä¸Š

## å‰ç½®è¦æ±‚

### æ‰€éœ€å·¥å…·

| å·¥å…· | ç‰ˆæœ¬ | ç”¨é€” |
|------|------|------|
| kubectl | 1.28+ | Kubernetes é›†ç¾¤ç®¡ç† |
| helm | 3.12+ | VPAã€Goldilocks å®‰è£… |
| metrics-server | 0.7+ | èµ„æºæŒ‡æ ‡æ”¶é›† |
| kubectl-top | å†…ç½® | èµ„æºä½¿ç”¨é‡ç¡®è®¤ |

### æ‰€éœ€æƒé™

```bash
# RBAC æƒé™ç¡®è®¤
kubectl auth can-i get pods --all-namespaces
kubectl auth can-i get resourcequotas
kubectl auth can-i create verticalpodautoscaler
```

### å‰ç½®çŸ¥è¯†

- Kubernetes Podã€Deployment åŸºæœ¬æ¦‚å¿µ
- YAML æ¸…å•ç¼–å†™ç»éªŒ
- Linux cgroups åŸºæœ¬ç†è§£ï¼ˆæ¨èï¼‰
- Prometheus/Grafana åŸºæœ¬ä½¿ç”¨æ–¹æ³•ï¼ˆæ¨èï¼‰

## Resource Requests & Limits æ·±å…¥ç†è§£

### 2.1 Requests vs Limits çš„ç²¾ç¡®å«ä¹‰

Resource requests å’Œ limits æ˜¯ Kubernetes èµ„æºç®¡ç†çš„æ ¸å¿ƒæ¦‚å¿µã€‚

**Requestsï¼ˆè¯·æ±‚é‡ï¼‰**
- **å®šä¹‰**: è°ƒåº¦å™¨åœ¨ Pod æ”¾ç½®æ—¶ä¿è¯çš„æœ€å°èµ„æº
- **ä½œç”¨**: èŠ‚ç‚¹é€‰æ‹©ä¾æ®ã€QoS ç±»åˆ«å†³å®š
- **ä¿è¯**: kubelet å§‹ç»ˆç¡®ä¿æ­¤é‡çš„å¯ç”¨

**Limitsï¼ˆé™åˆ¶é‡ï¼‰**
- **å®šä¹‰**: kubelet å¼ºåˆ¶æ‰§è¡Œçš„æœ€å¤§èµ„æº
- **ä½œç”¨**: é˜²æ­¢èµ„æºè€—å°½ã€é™åˆ¶å˜ˆæ‚é‚»å±…ï¼ˆnoisy neighborï¼‰
- **å¼ºåˆ¶**: CPU ä½¿ç”¨ throttlingï¼ŒMemory ä½¿ç”¨ OOM Kill

```mermaid
graph TB
    subgraph "èµ„æºåˆ†é…æµç¨‹"
        A[Pod åˆ›å»ºè¯·æ±‚] --> B{Scheduler}
        B -->|æ£€æŸ¥ Requests| C[é€‰æ‹©åˆé€‚çš„èŠ‚ç‚¹]
        C --> D[kubelet]
        D -->|è®¾ç½® cgroups| E[Container Runtime]

        subgraph "è¿è¡Œæ—¶æ§åˆ¶"
            E --> F{å®é™…ä½¿ç”¨é‡}
            F -->|CPU > Limit| G[CPU Throttling]
            F -->|Memory > Limit| H[OOM Kill]
            F -->|æ­£å¸¸èŒƒå›´| I[æ­£å¸¸è¿è¡Œ]
        end
    end

    style G fill:#ff6b6b
    style H fill:#ff0000,color:#fff
    style I fill:#51cf66
```

**æ ¸å¿ƒå·®å¼‚**

| å±æ€§ | CPU | Memory |
|------|-----|--------|
| **è¶…è¿‡ Requests æ—¶** | å¦‚æœå…¶ä»– Pod æœªä½¿ç”¨åˆ™å¯ä»¥ä½¿ç”¨ | å¦‚æœå…¶ä»– Pod æœªä½¿ç”¨åˆ™å¯ä»¥ä½¿ç”¨ |
| **è¶…è¿‡ Limits æ—¶** | **Throttling**ï¼ˆè¿›ç¨‹é€Ÿåº¦é™ä½ï¼‰ | **OOM Kill**ï¼ˆè¿›ç¨‹å¼ºåˆ¶ç»ˆæ­¢ï¼‰ |
| **æ˜¯å¦å¯å‹ç¼©** | å¯å‹ç¼© (Compressible) | ä¸å¯å‹ç¼© (Incompressible) |
| **è¶…é¢ä½¿ç”¨é£é™©** | æ€§èƒ½ä¸‹é™ | æœåŠ¡ä¸­æ–­ |

### 2.2 CPU èµ„æºæ·±å…¥ç†è§£

#### CPU Millicore å•ä½

```yaml
# CPU è¡¨ç¤ºæ–¹æ³•
resources:
  requests:
    cpu: "500m"    # 500 millicore = 0.5 CPU core
    cpu: "1"       # 1000 millicore = 1 CPU core
    cpu: "2.5"     # 2500 millicore = 2.5 CPU cores
```

**1 CPU core = 1000 millicore**
- AWS vCPUã€Azure vCore å‡ç›¸åŒ
- åœ¨è¶…çº¿ç¨‹ç¯å¢ƒä¸­ä¹Ÿæ˜¯ä»¥é€»è¾‘æ ¸å¿ƒä¸ºåŸºå‡†

#### CFS Bandwidth Throttling

Linux CFS (Completely Fair Scheduler) å¼ºåˆ¶æ‰§è¡Œ CPU limitsï¼š

```bash
# cgroups v2 åŸºå‡†
/sys/fs/cgroup/cpu.max
# ç¤ºä¾‹: "100000 100000" = æ¯ 100ms å‘¨æœŸå¯ä½¿ç”¨ 100ms (100% = 1 CPU)
# ç¤ºä¾‹: "50000 100000" = æ¯ 100ms å‘¨æœŸå¯ä½¿ç”¨ 50ms (50% = 0.5 CPU)
```

**Throttling æœºåˆ¶**

```
æ—¶é—´å‘¨æœŸ: 100ms
CPU Limit: 500m (0.5 CPU)
â†’ 100ms ä¸­ä»…å¯ä½¿ç”¨ 50ms

å®é™…è¿è¡Œ:
[0-50ms] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (è¿è¡Œ)
[50-100ms] ...................... (throttled)
[100-150ms] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (è¿è¡Œ)
[150-200ms] ...................... (throttled)
```

:::warning ä¸è®¾ç½® CPU Limits çš„ç­–ç•¥
Googleã€Datadog ç­‰å¤§è§„æ¨¡é›†ç¾¤è¿è¥ç»„ç»‡ä¸è®¾ç½® CPU limitsï¼š

**åŸå› ï¼š**
- CPU æ˜¯å¯å‹ç¼©èµ„æºï¼ˆå…¶ä»– Pod éœ€è¦æ—¶ä¼šè‡ªåŠ¨è°ƒæ•´ï¼‰
- é¿å…å›  Throttling å¯¼è‡´çš„ä¸å¿…è¦æ€§èƒ½ä¸‹é™
- ä»…é€šè¿‡ Requests å³å¯å®ç°è°ƒåº¦å’Œ QoS æ§åˆ¶

**æ›¿ä»£æ¨èï¼š**
- CPU requests åŸºäº P95 ä½¿ç”¨é‡è®¾ç½®
- é€šè¿‡ HPA æŒ‰è´Ÿè½½æ°´å¹³æ‰©å±•
- åŠ å¼ºèŠ‚ç‚¹çº§åˆ«èµ„æºç›‘æ§

**ä¾‹å¤–ï¼ˆéœ€è¦è®¾ç½® Limitsï¼‰ï¼š**
- æ‰¹å¤„ç†ä½œä¸šï¼ˆé˜²æ­¢ CPU ç‹¬å ï¼‰
- ä¸å¯ä¿¡çš„å·¥ä½œè´Ÿè½½
- å¤šç§Ÿæˆ·ç¯å¢ƒ
:::

#### CPU èµ„æºè®¾ç½®ç¤ºä¾‹

```yaml
# æ¨¡å¼ 1: ä»…è®¾ç½® Requestsï¼ˆæ¨èï¼‰
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    resources:
      requests:
        cpu: "250m"       # åŸºäº P95 ä½¿ç”¨é‡
        memory: "128Mi"
      # çœç•¥ limits - åˆ©ç”¨ CPU å¯å‹ç¼©èµ„æºç‰¹æ€§

---
# æ¨¡å¼ 2: æ‰¹å¤„ç†ä½œä¸šï¼ˆè®¾ç½® Limitsï¼‰
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing
spec:
  template:
    spec:
      containers:
      - name: processor
        image: data-processor:v1
        resources:
          requests:
            cpu: "1000m"
          limits:
            cpu: "2000m"   # é˜²æ­¢ CPU ç‹¬å 
            memory: "4Gi"
      restartPolicy: OnFailure
```

### 2.3 Memory èµ„æºæ·±å…¥ç†è§£

#### Memory å•ä½

```yaml
# Memory è¡¨ç¤ºæ–¹æ³• (1024 åŸºå‡† vs 1000 åŸºå‡†)
resources:
  requests:
    memory: "128Mi"    # 128 * 1024^2 bytes = 134,217,728 bytes
    memory: "128M"     # 128 * 1000^2 bytes = 128,000,000 bytes
    memory: "1Gi"      # 1 * 1024^3 bytes = 1,073,741,824 bytes
    memory: "1G"       # 1 * 1000^3 bytes = 1,000,000,000 bytes
```

**æ¨è**: **ä½¿ç”¨ Miã€Gi**ï¼ˆ1024 åŸºå‡†ï¼ŒKubernetes æ ‡å‡†ï¼‰

#### OOM Kill æœºåˆ¶

å½“è¶…è¿‡ Memory limits æ—¶ï¼ŒLinux OOM Killer ä¼šå¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ï¼š

```
å®é™…ä½¿ç”¨é‡ > Memory Limit
â†’ è¶…è¿‡ cgroup memory.max
â†’ Kernel OOM Killer è§¦å‘
â†’ è¿›ç¨‹ SIGKILL
â†’ Pod çŠ¶æ€: OOMKilled
â†’ kubelet é‡å¯ Podï¼ˆéµå¾ª RestartPolicyï¼‰
```

**OOM Score è®¡ç®—**

```bash
# æŸ¥çœ‹æ¯ä¸ªè¿›ç¨‹çš„ OOM Score
cat /proc/<PID>/oom_score

# OOM Score è®¡ç®—å› ç´ 
# 1. å†…å­˜ä½¿ç”¨é‡ï¼ˆè¶Šé«˜åˆ†æ•°è¶Šé«˜ï¼‰
# 2. oom_score_adj å€¼ï¼ˆæ¯ä¸ª QoS ç±»åˆ«ä¸åŒï¼‰
# 3. root è¿›ç¨‹ä¿æŠ¤ï¼ˆ-1000 = ç»ä¸ Killï¼‰
```

:::danger Memory limits å¿…é¡»è®¾ç½®
Memory æ˜¯ä¸å¯å‹ç¼©èµ„æºï¼Œå› æ­¤**å¿…é¡»è®¾ç½® limits**ï¼š

**åŸå› ï¼š**
- Memory è€—å°½æ—¶æ•´ä¸ªèŠ‚ç‚¹ä¸ç¨³å®š
- å¯èƒ½å¯¼è‡´ Kernel Panic
- å½±å“å…¶ä»– Podï¼ˆèŠ‚ç‚¹ Evictionï¼‰

**æ¨èè®¾ç½®ï¼š**
- `requests = limits`ï¼ˆGuaranteed QoSï¼‰
- æˆ– `limits = requests * 1.5`ï¼ˆBurstable QoSï¼‰
- JVM åº”ç”¨ï¼šHeap å¤§å°è®¾ç½®ä¸º limits çš„ 75%
:::

#### Memory èµ„æºè®¾ç½®ç¤ºä¾‹

```yaml
# æ¨¡å¼ 1: Guaranteed QoSï¼ˆç¨³å®šæ€§ä¼˜å…ˆï¼‰
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: postgres
        image: postgres:16
        resources:
          requests:
            cpu: "2000m"
            memory: "4Gi"
          limits:
            cpu: "2000m"      # ä¸ requests ç›¸åŒ
            memory: "4Gi"     # ä¸ requests ç›¸åŒ (Guaranteed)

---
# æ¨¡å¼ 2: JVM åº”ç”¨
apiVersion: apps/v1
kind: Deployment
metadata:
  name: java-app
spec:
  template:
    spec:
      containers:
      - name: app
        image: java-app:v1
        env:
        - name: JAVA_OPTS
          value: "-Xmx3072m -Xms3072m"  # limits çš„ 75% (4Gi * 0.75 = 3Gi)
        resources:
          requests:
            memory: "4Gi"
          limits:
            memory: "4Gi"

---
# æ¨¡å¼ 3: Node.js åº”ç”¨
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-api
spec:
  template:
    spec:
      containers:
      - name: api
        image: nodejs-api:v2
        env:
        - name: NODE_OPTIONS
          value: "--max-old-space-size=896"  # limits çš„ 70% (1280Mi * 0.7 = 896Mi)
        resources:
          requests:
            memory: "1280Mi"
          limits:
            memory: "1280Mi"
```

### 2.4 Ephemeral Storage

å®¹å™¨æœ¬åœ°å­˜å‚¨ä¹Ÿå¯ä»¥ä½œä¸ºèµ„æºè¿›è¡Œç®¡ç†ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-demo
spec:
  containers:
  - name: app
    image: busybox
    resources:
      requests:
        ephemeral-storage: "2Gi"    # æœ€ä½ä¿è¯
      limits:
        ephemeral-storage: "4Gi"    # æœ€å¤§ä½¿ç”¨é‡
    volumeMounts:
    - name: cache
      mountPath: /cache
  volumes:
  - name: cache
    emptyDir:
      sizeLimit: "4Gi"
```

**Ephemeral Storage åŒ…å«é¡¹ï¼š**
- å®¹å™¨å±‚å†™å…¥
- æ—¥å¿—æ–‡ä»¶ï¼ˆ`/var/log`ï¼‰
- emptyDir å·
- ä¸´æ—¶æ–‡ä»¶

**èŠ‚ç‚¹ Eviction é˜ˆå€¼ï¼š**

```yaml
# kubelet è®¾ç½®
evictionHard:
  nodefs.available: "10%"      # èŠ‚ç‚¹æ•´ä½“ç£ç›˜ä½äº 10% æ—¶ eviction
  nodefs.inodesFree: "5%"      # inode ä½äº 5% æ—¶ eviction
  imagefs.available: "10%"     # é•œåƒæ–‡ä»¶ç³»ç»Ÿä½äº 10% æ—¶ eviction
```

### 2.5 EKS Auto Mode èµ„æºä¼˜åŒ–

EKS Auto Mode æ˜¯ä¸€ç§å®Œå…¨æ‰˜ç®¡çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæå¤§åœ°é™ä½ Kubernetes é›†ç¾¤è¿ç»´çš„å¤æ‚æ€§ã€‚å®ƒä»è®¡ç®—ã€å­˜å‚¨ã€ç½‘ç»œçš„èµ„æºé…ç½®åˆ°æŒç»­ç»´æŠ¤å…¨éƒ¨è‡ªåŠ¨åŒ–ï¼Œä½¿è¿ç»´å›¢é˜Ÿèƒ½å¤Ÿä¸“æ³¨äºåº”ç”¨å¼€å‘è€ŒéåŸºç¡€è®¾æ–½ç®¡ç†ã€‚

#### 2.5.1 Auto Mode æ¦‚è¿°

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
- **ä¸€é”®æ¿€æ´»**: åˆ›å»ºé›†ç¾¤æ—¶åªéœ€ `--compute-config autoMode` æ ‡å¿—å³å¯æ¿€æ´»
- **è‡ªåŠ¨åŸºç¡€è®¾æ–½é…ç½®**: æ ¹æ® Pod è°ƒåº¦éœ€æ±‚è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ä¾‹ç±»å‹
- **æŒç»­ç»´æŠ¤**: OS è¡¥ä¸ã€å®‰å…¨æ›´æ–°ã€æ ¸å¿ƒ Add-on ç®¡ç†è‡ªåŠ¨åŒ–
- **æˆæœ¬ä¼˜åŒ–**: è‡ªåŠ¨ä½¿ç”¨ Graviton å¤„ç†å™¨å’Œ Spot å®ä¾‹
- **é›†æˆå®‰å…¨**: AWS å®‰å…¨æœåŠ¡é»˜è®¤é›†æˆ

```bash
# Auto Mode é›†ç¾¤åˆ›å»º
aws eks create-cluster \
  --name my-auto-cluster \
  --compute-config autoMode=ENABLED \
  --kubernetes-network-config serviceIpv4Cidr=10.100.0.0/16 \
  --access-config bootstrapClusterCreatorAdminPermissions=true
```

:::info Auto Mode vs æ‰‹åŠ¨ç®¡ç†
Auto Mode å¹¶éå®Œå…¨æ›¿ä»£ç°æœ‰çš„æ‰‹åŠ¨ç®¡ç†æ–¹å¼ï¼Œè€Œæ˜¯ä¸ºå¸Œæœ›æœ€å°åŒ–è¿ç»´å¼€é”€çš„å›¢é˜Ÿæä¾›çš„**è¡¥å……é€‰æ‹©**ã€‚å¦‚æœéœ€è¦ç²¾ç»†æ§åˆ¶ï¼Œä»ç„¶å¯ä»¥é€‰æ‹©æ‰‹åŠ¨ç®¡ç†æ–¹å¼ã€‚
:::

#### 2.5.2 Auto Mode vs æ‰‹åŠ¨ç®¡ç†å¯¹æ¯”

| é¡¹ç›® | æ‰‹åŠ¨ç®¡ç† | Auto Mode |
|------|----------|-----------|
| **èŠ‚ç‚¹é…ç½®** | Managed Node Groupã€Self-managedã€Karpenter ç›´æ¥é…ç½® | è‡ªåŠ¨é…ç½®ï¼ˆåŸºäº EC2 Managed Instancesï¼‰ |
| **å®ä¾‹ç±»å‹é€‰æ‹©** | æ‰‹åŠ¨é€‰æ‹©å¹¶é…ç½® NodePool | åŸºäº Pod éœ€æ±‚è‡ªåŠ¨é€‰æ‹©ï¼ˆGraviton ä¼˜å…ˆï¼‰ |
| **VPA è®¾ç½®** | éœ€è¦æ‰‹åŠ¨å®‰è£…å’Œé…ç½® | ä¸éœ€è¦ï¼ˆè‡ªåŠ¨èµ„æºä¼˜åŒ–ï¼‰ |
| **HPA è®¾ç½®** | æ‰‹åŠ¨è®¾ç½®å’ŒæŒ‡æ ‡é…ç½® | å¯è‡ªåŠ¨é…ç½®ï¼ˆå¼€å‘è€…ä»…éœ€å£°æ˜ï¼‰ |
| **OS è¡¥ä¸** | æ‰‹åŠ¨æˆ–è‡ªåŠ¨åŒ–è„šæœ¬ | å®Œå…¨è‡ªåŠ¨ï¼ˆé›¶å®•æœºï¼‰ |
| **å®‰å…¨æ›´æ–°** | æ‰‹åŠ¨åº”ç”¨ | è‡ªåŠ¨åº”ç”¨ |
| **æ ¸å¿ƒ Add-on ç®¡ç†** | æ‰‹åŠ¨å‡çº§ (CoreDNS, kube-proxy, VPC CNI) | è‡ªåŠ¨å‡çº§ |
| **æˆæœ¬ä¼˜åŒ–** | æ‰‹åŠ¨é…ç½® Spotã€Graviton | è‡ªåŠ¨ä½¿ç”¨ï¼ˆæœ€é«˜èŠ‚çœ 90%ï¼‰ |
| **Request/Limit è®¾ç½®** | å¼€å‘è€…è´Ÿè´£ï¼ˆå¿…é¡»ï¼‰ | å¼€å‘è€…è´Ÿè´£ï¼ˆä»ç„¶å¿…é¡»ï¼‰ |
| **èµ„æºæ•ˆç‡** | VPA Off æ¨¡å¼ + æ‰‹åŠ¨åº”ç”¨ | è‡ªåŠ¨ Right-Sizingï¼ˆæŒç»­ï¼‰ |
| **å­¦ä¹ æ›²çº¿** | é«˜ï¼ˆéœ€è¦ Kubernetesã€AWS ä¸“ä¸šçŸ¥è¯†ï¼‰ | ä½ï¼ˆä»…éœ€ Kubernetes åŸºç¡€ï¼‰ |
| **è¿ç»´å¼€é”€** | é«˜ | æœ€å° |

:::warning Auto Mode ä¸­å¼€å‘è€…çš„è´£ä»»
Auto Mode è‡ªåŠ¨åŒ–äº†åŸºç¡€è®¾æ–½ï¼Œä½† **Pod çº§åˆ«çš„ requests/limits è®¾ç½®ä»ç„¶æ˜¯å¼€å‘è€…çš„è´£ä»»**ã€‚è¿™æ˜¯å› ä¸ºæœ€äº†è§£åº”ç”¨å®é™…èµ„æºéœ€æ±‚çš„äººæ˜¯å¼€å‘è€…ã€‚
:::

#### 2.5.3 Graviton + Spot ç»„åˆä¼˜åŒ–

Auto Mode æ™ºèƒ½åœ°ç»„åˆ AWS Graviton å¤„ç†å™¨å’Œ Spot å®ä¾‹ï¼Œæœ€å¤§åŒ–æˆæœ¬æ•ˆç‡ã€‚

**Graviton å¤„ç†å™¨çš„ä¼˜åŠ¿ï¼š**
- **æ€§ä»·æ¯”æå‡ 40%**ï¼ˆç›¸æ¯” x86ï¼‰
- æœ€é€‚åˆé€šç”¨å·¥ä½œè´Ÿè½½ã€Web æœåŠ¡å™¨ã€å®¹å™¨åŒ–å¾®æœåŠ¡
- æ”¯æŒ Arm64 æ¶æ„ï¼ˆå¤§å¤šæ•°å®¹å™¨é•œåƒå…¼å®¹ï¼‰

**Spot å®ä¾‹èŠ‚çœï¼š**
- **æœ€é«˜èŠ‚çœ 90% æˆæœ¬**ï¼ˆç›¸æ¯” On-Demandï¼‰
- Auto Mode è‡ªåŠ¨ç›‘æ§ Spot å¯ç”¨æ€§å¹¶å¤„ç† Fallback
- ä¸­æ–­å‰ 2 åˆ†é’Ÿé€šçŸ¥ï¼Œä¿è¯ Graceful Termination

```mermaid
graph TB
    subgraph "Auto Mode å®ä¾‹é€‰æ‹©é€»è¾‘"
        A[Pod è°ƒåº¦è¯·æ±‚] --> B{èµ„æºéœ€æ±‚åˆ†æ}
        B --> C[ä¼˜å…ˆå°è¯• Graviton Spot]
        C --> D{æ£€æŸ¥ Spot å¯ç”¨æ€§}
        D -->|å¯ç”¨| E[é…ç½® Graviton Spot å®ä¾‹]
        D -->|ä¸å¯ç”¨| F[å°è¯• Graviton On-Demand]
        F --> G{On-Demand å¯ç”¨æ€§}
        G -->|å¯ç”¨| H[é…ç½® Graviton On-Demand]
        G -->|ä¸å¯ç”¨| I[x86 Spot/On-Demand Fallback]

        E --> J[Pod æ”¾ç½®å®Œæˆ]
        H --> J
        I --> J
    end

    style E fill:#51cf66
    style H fill:#ffa94d
    style I fill:#ff6b6b
```

**NodePool YAML ç¤ºä¾‹ï¼ˆæ‰‹åŠ¨ç®¡ç†é›†ç¾¤ - åŸºäº Karpenterï¼‰ï¼š**

```yaml
# Auto Mode ä¼šè‡ªåŠ¨åˆ›å»ºæ­¤ç±» NodePoolï¼Œ
# ä»¥ä¸‹å±•ç¤ºæ‰‹åŠ¨è®¾ç½®æ—¶çš„ Graviton + Spot æ¨¡å¼ä¾›å‚è€ƒ
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: graviton-spot-pool
spec:
  template:
    spec:
      requirements:
      # Graviton å®ä¾‹ä¼˜å…ˆ
      - key: kubernetes.io/arch
        operator: In
        values: ["arm64"]

      # Spot ä¼˜å…ˆï¼ŒFallback åˆ° On-Demand
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot", "on-demand"]

      # é€šç”¨å·¥ä½œè´Ÿè½½å®ä¾‹æ—
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["m7g.medium", "m7g.large", "m7g.xlarge", "m7g.2xlarge"]

      nodeClassRef:
        name: default

  # Spot ä¸­æ–­å¤„ç†
  disruption:
    consolidationPolicy: WhenUnderutilized
    expireAfter: 720h

  # èµ„æºé™åˆ¶
  limits:
    cpu: "1000"
    memory: "1000Gi"

---
# Fallback: x86 On-Demandï¼ˆSpot ä¸å¯ç”¨æ—¶ï¼‰
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: x86-ondemand-fallback
spec:
  weight: 10  # ä½ä¼˜å…ˆçº§
  template:
    spec:
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]

      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand"]

      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["m6i.large", "m6i.xlarge", "m6i.2xlarge"]

      nodeClassRef:
        name: default
```

**Auto Mode ä¸­çš„è‡ªåŠ¨å¤„ç†ï¼š**

Auto Mode æ— éœ€æ‰‹åŠ¨ç¼–å†™ä¸Šè¿° NodePool é…ç½®ï¼Œå®ƒä¼šåˆ†æ Pod çš„èµ„æºéœ€æ±‚å’Œå·¥ä½œè´Ÿè½½ç‰¹æ€§ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ä¾‹ã€‚

```yaml
# Auto Mode ç¯å¢ƒä¸­å¼€å‘è€…ç¼–å†™çš„ Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: production
spec:
  replicas: 10
  template:
    spec:
      containers:
      - name: nginx
        image: nginx:1.25-arm64  # Graviton ç”¨é•œåƒ
        resources:
          requests:
            cpu: "250m"
            memory: "512Mi"
          limits:
            memory: "1Gi"

      # Auto Mode è‡ªåŠ¨æ‰§è¡Œï¼š
      # 1. å°è¯•é€‰æ‹© Graviton Spot å®ä¾‹
      # 2. Spot ä¸å¯ç”¨æ—¶ Fallback åˆ° Graviton On-Demand
      # 3. è‡ªåŠ¨é€‰æ‹©å®ä¾‹ç±»å‹ (m7g.large ç­‰)
      # 4. èŠ‚ç‚¹é…ç½®åŠ Pod æ”¾ç½®
```

:::tip Graviton é•œåƒå‡†å¤‡
è¦ä½¿ç”¨ Graviton å®ä¾‹ï¼Œéœ€è¦ **arm64 æ¶æ„çš„å®¹å™¨é•œåƒ**ã€‚å¤§å¤šæ•°å®˜æ–¹é•œåƒæ”¯æŒ multi-archï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨ç›¸åŒçš„é•œåƒæ ‡ç­¾åœ¨ Graviton å’Œ x86 ä¸Šè¿è¡Œã€‚

```bash
# æ£€æŸ¥ multi-arch é•œåƒ
docker manifest inspect nginx:1.25 | jq '.manifests[].platform'

# è¾“å‡ºç¤ºä¾‹ï¼š
# { "architecture": "amd64", "os": "linux" }
# { "architecture": "arm64", "os": "linux" }
```
:::

**å®é™…æˆæœ¬èŠ‚çœç¤ºä¾‹ï¼š**

| åœºæ™¯ | å®ä¾‹ç±»å‹ | æ¯å°æ—¶æˆæœ¬ | æœˆæˆæœ¬ (730å°æ—¶) | èŠ‚çœç‡ |
|---------|-------------|-----------|-------------------|--------|
| x86 On-Demand | m6i.2xlarge | $0.384 | $280.32 | - |
| Graviton On-Demand | m7g.2xlarge | $0.3264 | $238.27 | 15% |
| Graviton Spot | m7g.2xlarge | $0.0979 | $71.47 | 75% |

ä»¥ 10 ä¸ªèŠ‚ç‚¹ä¸ºåŸºå‡†ï¼š
- x86 On-Demand: $2,803/æœˆ
- Graviton On-Demand: $2,383/æœˆ (èŠ‚çœ 15%)
- **Graviton Spot: $715/æœˆ (èŠ‚çœ 75%)** â­

**Graviton4 ä¸“é¡¹ä¼˜åŒ–ï¼š**

Graviton4 (R8g, M8g, C8g) å®ä¾‹ç›¸æ¯” Graviton3 æä¾›äº† **30% çš„è®¡ç®—æ€§èƒ½æå‡**å’Œ **75% çš„å†…å­˜å¸¦å®½æå‡**ã€‚

| ä»£é™… | å®ä¾‹æ— | æ€§èƒ½æå‡ | ä¸»è¦å·¥ä½œè´Ÿè½½ |
|------|---------------|---------|-------------|
| Graviton3 | m7g, c7g, r7g | åŸºå‡† | é€šç”¨ Web/APIã€å®¹å™¨ |
| **Graviton4** | **m8g, c8g, r8g** | **+30% è®¡ç®—, +75% å†…å­˜** | **é«˜æ€§èƒ½æ•°æ®åº“ã€ML æ¨ç†ã€å®æ—¶åˆ†æ** |

**ARM64 Multi-Arch æ„å»ºæµæ°´çº¿ï¼š**

è¦å……åˆ†åˆ©ç”¨ Graviton å®ä¾‹ï¼Œéœ€è¦åŒæ—¶æ”¯æŒ ARM64 å’Œ AMD64 çš„ multi-arch å®¹å™¨é•œåƒã€‚

```dockerfile
# Multi-arch Dockerfile ç¤ºä¾‹
FROM --platform=$BUILDPLATFORM golang:1.22-alpine AS builder
ARG TARGETOS TARGETARCH

WORKDIR /app
COPY . .

# é’ˆå¯¹ç›®æ ‡æ¶æ„æ„å»º
RUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o app .

# è¿è¡Œæ—¶é•œåƒ
FROM alpine:3.19
COPY --from=builder /app/app /usr/local/bin/app
ENTRYPOINT ["/usr/local/bin/app"]
```

**GitHub Actions CI/CD ä¸­çš„ multi-arch æ„å»ºï¼š**

```yaml
# .github/workflows/build.yml
name: Build Multi-Arch Image
on:
  push:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push multi-arch
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64,linux/arm64  # åŒ…å« ARM64
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/myapp:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/myapp:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
```

**Graviton3 â†’ Graviton4 è¿ç§»åŸºå‡†æµ‹è¯•è¦ç‚¹ï¼š**

```yaml
# Graviton4 ä¼˜å…ˆ NodePool ç¤ºä¾‹ (Karpenter)
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: graviton4-spot-pool
spec:
  template:
    spec:
      requirements:
      # Graviton4 ä¼˜å…ˆï¼ŒGraviton3 Fallback
      - key: node.kubernetes.io/instance-type
        operator: In
        values:
          # Graviton4ï¼ˆæœ€ä¼˜å…ˆï¼‰
          - "m8g.medium"
          - "m8g.large"
          - "m8g.xlarge"
          - "m8g.2xlarge"
          # Graviton3ï¼ˆFallbackï¼‰
          - "m7g.medium"
          - "m7g.large"
          - "m7g.xlarge"
          - "m7g.2xlarge"

      - key: kubernetes.io/arch
        operator: In
        values: ["arm64"]

      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot", "on-demand"]

      nodeClassRef:
        name: default

  disruption:
    consolidationPolicy: WhenUnderutilized
    consolidateAfter: 30s

  limits:
    cpu: "1000"
    memory: "2000Gi"
```

**Graviton4 æ€§èƒ½åŸºå‡†æµ‹è¯•æ£€æŸ¥ç‚¹ï¼š**

è¿ç§»æ—¶ç›‘æ§ä»¥ä¸‹æŒ‡æ ‡ä»¥éªŒè¯æ€§èƒ½æå‡ï¼š

| æŒ‡æ ‡ | Graviton3 åŸºå‡† | Graviton4 ç›®æ ‡ | æµ‹é‡æ–¹æ³• |
|-------|--------------|--------------|---------|
| **P99 å“åº”æ—¶é—´** | 100ms | 70ms (-30%) | Prometheus `http_request_duration_seconds` |
| **ååé‡ (RPS)** | 1000 req/s | 1300 req/s (+30%) | è´Ÿè½½æµ‹è¯• (k6, Locust) |
| **å†…å­˜å¸¦å®½** | 205 GB/s | 358 GB/s (+75%) | `sysbench memory` |
| **CPU ä½¿ç”¨ç‡** | 60% | 45% (-25%) | `node_cpu_seconds_total` |

```bash
# Graviton4 æ€§èƒ½æµ‹è¯•è„šæœ¬
#!/bin/bash
# 1. å†…å­˜å¸¦å®½æµ‹è¯•
sysbench memory --memory-total-size=100G --memory-oper=write run

# 2. CPU åŸºå‡†æµ‹è¯•
sysbench cpu --cpu-max-prime=20000 --threads=8 run

# 3. åº”ç”¨è´Ÿè½½æµ‹è¯• (k6)
k6 run --vus 100 --duration 5m loadtest.js

# 4. Prometheus æŒ‡æ ‡æ”¶é›†
curl -s http://localhost:9090/api/v1/query?query=rate(http_request_duration_seconds_sum[5m]) | jq .
```

:::tip Graviton4 è¿ç§»æ£€æŸ¥æ¸…å•

- [ ] **å®¹å™¨é•œåƒ**: ç¡®è®¤ ARM64 æ”¯æŒ (`docker manifest inspect`)
- [ ] **ä¾èµ–åº“**: éªŒè¯ ARM64 å…¼å®¹æ€§
- [ ] **CI/CD æµæ°´çº¿**: å¯ç”¨ Multi-arch æ„å»º
- [ ] **NodePool ä¼˜å…ˆçº§**: è®¾ç½® Graviton4 â†’ Graviton3 â†’ x86 é¡ºåº
- [ ] **æ€§èƒ½åŸºå‡†æµ‹è¯•**: æµ‹é‡ P99 å»¶è¿Ÿã€ååé‡ã€CPU ä½¿ç”¨ç‡
- [ ] **æˆæœ¬åˆ†æ**: è®¡ç®—ç›¸æ¯” Graviton3 çš„æ€§ä»·æ¯”
:::

#### 2.5.4 Auto Mode ç¯å¢ƒçš„èµ„æºè®¾ç½®å»ºè®®

Auto Mode è™½ç„¶è‡ªåŠ¨åŒ–äº†å¾ˆå¤šéƒ¨åˆ†ï¼Œä½†å¼€å‘è€…ä»éœ€å‡†ç¡®è®¾ç½®åº”ç”¨çš„èµ„æºéœ€æ±‚ã€‚

**Auto Mode è‡ªåŠ¨å¤„ç†çš„é¡¹ç›®ï¼š**

| é¡¹ç›® | æ‰‹åŠ¨ç®¡ç† | Auto Mode |
|------|----------|-----------|
| èŠ‚ç‚¹é…ç½® | Karpenterã€Managed Node Group è®¾ç½® | è‡ªåŠ¨ |
| å®ä¾‹ç±»å‹é€‰æ‹© | åœ¨ NodePool ä¸­æ‰‹åŠ¨æŒ‡å®š | åŸºäº Pod requests è‡ªåŠ¨é€‰æ‹© |
| Spot/On-Demand åˆ‡æ¢ | æ‰‹åŠ¨æˆ– Karpenter è®¾ç½® | è‡ªåŠ¨ Fallback |
| èŠ‚ç‚¹æ‰©ç¼©å®¹ | HPA + Cluster Autoscaler/Karpenter | è‡ªåŠ¨ |
| OS è¡¥ä¸ | æ‰‹åŠ¨æˆ–è‡ªåŠ¨åŒ–è„šæœ¬ | è‡ªåŠ¨ï¼ˆé›¶å®•æœºï¼‰ |

**å¼€å‘è€…ä»éœ€è®¾ç½®çš„é¡¹ç›®ï¼š**

| é¡¹ç›® | åŸå›  | æ¨èæ–¹æ³• |
|------|------|----------|
| **CPU Requests** | è°ƒåº¦å†³ç­–ä¾æ® | P95 ä½¿ç”¨é‡ + 20% |
| **Memory Requests** | è°ƒåº¦åŠ OOM é˜²æ­¢ | P95 ä½¿ç”¨é‡ + 20% |
| **Memory Limits** | é˜²æ­¢ OOM Killï¼ˆå¿…é¡»ï¼‰ | Requests Ã— 1.5~2 |
| **CPU Limits** | é€šç”¨å·¥ä½œè´Ÿè½½å»ºè®®ä¸è®¾ç½® | ä»…æ‰¹å¤„ç†ä½œä¸šè®¾ç½® |
| **HPA æŒ‡æ ‡** | æ°´å¹³æ‰©å±•åŸºå‡† | CPU 70%, Custom Metrics |

**Auto Mode ç¯å¢ƒä¸­ VPA è§’è‰²çš„å˜åŒ–ï¼š**

```mermaid
graph TB
    subgraph "æ‰‹åŠ¨ç®¡ç†é›†ç¾¤"
        A1[VPA Recommender] --> A2[ç”Ÿæˆå»ºè®®]
        A2 --> A3[VPA Updater]
        A3 --> A4[é€šè¿‡ Pod é‡å¯æ›´æ”¹èµ„æº]
    end

    subgraph "Auto Mode é›†ç¾¤"
        B1[å†…ç½® Right-Sizing å¼•æ“] --> B2[æŒç»­ä½¿ç”¨é‡åˆ†æ]
        B2 --> B3[è‡ªåŠ¨èµ„æºä¼˜åŒ–]
        B3 --> B4[å‘å¼€å‘è€…æä¾›å»ºè®®]
        B4 --> B5[å¼€å‘è€…æ›´æ–° Deployment]
    end

    style A4 fill:#ffa94d
    style B3 fill:#51cf66
```

**åœ¨ Auto Mode ä¸­ VPAï¼š**
- æ— éœ€å•ç‹¬å®‰è£…
- å†…ç½® Right-Sizing å¼•æ“æŒç»­åˆ†æå·¥ä½œè´Ÿè½½
- å‘å¼€å‘è€…æä¾›å»ºè®®ï¼ˆè€Œéè‡ªåŠ¨åº”ç”¨ï¼‰
- å¼€å‘è€…å®¡æ ¸ååæ˜ åˆ° Deployment æ¸…å•ä¸­

**æ¨èå·¥ä½œæµç¨‹ï¼š**

```bash
# 1. éƒ¨ç½²åˆ° Auto Mode é›†ç¾¤
kubectl apply -f deployment.yaml

# 2. 7-14 å¤©ååœ¨ Auto Mode ä»ªè¡¨æ¿æŸ¥çœ‹å»ºè®®
# (AWS Console â†’ EKS â†’ Clusters â†’ <cluster-name> â†’ Insights)

# 3. å°†å»ºè®®åæ˜ åˆ° Deployment
kubectl set resources deployment web-app \
  --requests=cpu=300m,memory=512Mi \
  --limits=memory=1Gi

# 4. é€šè¿‡ GitOps æ›´æ–°æ¸…å•
git add deployment.yaml
git commit -m "chore: apply Auto Mode resource recommendations"
git push
```

:::tip Auto Mode æ¨èåœºæ™¯
Auto Mode åœ¨ä»¥ä¸‹æƒ…å†µä¸­ç‰¹åˆ«æœ‰ç”¨ï¼š

- **æ–°é›†ç¾¤**: æ— ç°æœ‰åŸºç¡€è®¾æ–½ï¼Œå¿«é€Ÿå¯åŠ¨
- **è¿ç»´èµ„æºä¸è¶³**: å°å›¢é˜Ÿæ—  Kubernetes ä¸“å®¶è¿ç»´
- **æˆæœ¬ä¼˜åŒ–ä¼˜å…ˆ**: è‡ªåŠ¨ä½¿ç”¨ Graviton + Spot ç«‹å³èŠ‚çœ
- **æ ‡å‡†åŒ–å·¥ä½œè´Ÿè½½**: ä¸€èˆ¬çš„ Web/API æœåŠ¡å™¨ã€å¾®æœåŠ¡

**æ¨èæ‰‹åŠ¨ç®¡ç†çš„åœºæ™¯ï¼š**
- **éœ€è¦ç²¾ç»†æ§åˆ¶**: ç‰¹å®šå®ä¾‹ç±»å‹ã€AZ æ”¾ç½®ã€ç½‘ç»œé…ç½®
- **ç°æœ‰ Karpenter æŠ•èµ„**: æ‹¥æœ‰é«˜åº¦å®šåˆ¶çš„ NodePool ç­–ç•¥
- **åˆè§„è¦æ±‚**: ç‰¹å®šç¡¬ä»¶ã€å®‰å…¨ç»„å¼ºåˆ¶
:::

**Auto Mode + æ‰‹åŠ¨ Right-Sizing å¯¹æ¯”ï¼š**

| é¡¹ç›® | æ‰‹åŠ¨ Right-Sizing (VPA Off) | Auto Mode |
|------|---------------------------|-----------|
| åˆå§‹è®¾ç½®å¤æ‚åº¦ | é«˜ï¼ˆVPA å®‰è£…ã€Prometheus é…ç½®ï¼‰ | ä½ï¼ˆåˆ›å»ºé›†ç¾¤æ—¶ä»…éœ€æ ‡å¿—ï¼‰ |
| å»ºè®®ç”Ÿæˆæ—¶é—´ | 7-14 å¤© | 7-14 å¤©ï¼ˆç›¸åŒï¼‰ |
| å»ºè®®å‡†ç¡®åº¦ | é«˜ï¼ˆåŸºäº Prometheusï¼‰ | é«˜ï¼ˆå†…ç½®åˆ†æå¼•æ“ï¼‰ |
| åº”ç”¨æ–¹å¼ | æ‰‹åŠ¨ï¼ˆå¼€å‘è€…ä¿®æ”¹æ¸…å•ï¼‰ | æ‰‹åŠ¨ï¼ˆå¼€å‘è€…ä¿®æ”¹æ¸…å•ï¼‰ |
| æŒç»­ç›‘æ§ | æ‰‹åŠ¨ï¼ˆå®šæœŸæ£€æŸ¥ VPAï¼‰ | è‡ªåŠ¨ï¼ˆä»ªè¡¨æ¿å‘Šè­¦ï¼‰ |
| åŸºç¡€è®¾æ–½ä¼˜åŒ– | æ‰‹åŠ¨ï¼ˆKarpenter è®¾ç½®ï¼‰ | è‡ªåŠ¨ï¼ˆGraviton + Spotï¼‰ |
| æ€»è¿ç»´å¼€é”€ | é«˜ | ä½ |

**ç»“è®ºï¼š**

Auto Mode **æ¶ˆé™¤äº†èµ„æºä¼˜åŒ–çš„å¤æ‚æ€§**ï¼Œä½†**ä¸æ¶ˆé™¤èµ„æºè®¾ç½®çš„è´£ä»»**ã€‚å¼€å‘è€…ä»éœ€è®¾ç½®åº”ç”¨çš„ requests/limitsï¼ŒAuto Mode åŸºäºæ­¤è‡ªåŠ¨é…ç½®æœ€ä¼˜åŸºç¡€è®¾æ–½ã€‚

è¿™é€šè¿‡**"å¼€å‘è€…å®šä¹‰åº”ç”¨éœ€æ±‚ï¼ŒAWS ç®¡ç†åŸºç¡€è®¾æ–½"**çš„æ˜ç¡®èŒè´£åˆ†ç¦»ï¼Œè®©åŒæ–¹éƒ½èƒ½ä¸“æ³¨äºå„è‡ªçš„ä¸“ä¸šé¢†åŸŸã€‚

## QoS (Quality of Service) ç±»åˆ«

### 3.1 ä¸‰ç§ QoS ç±»åˆ«

Kubernetes æ ¹æ®èµ„æºè®¾ç½®å°† Pod åˆ†ä¸º 3 ç§ QoS ç±»åˆ«ï¼š

#### Guaranteedï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰

**æ¡ä»¶ï¼š**
- æ‰€æœ‰å®¹å™¨éƒ½è®¾ç½®äº† CPU å’Œ Memory çš„ requests å’Œ limits
- **requests == limits**ï¼ˆç›¸åŒå€¼ï¼‰

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: guaranteed-pod
  labels:
    qos: guaranteed
spec:
  containers:
  - name: app
    image: nginx:1.25
    resources:
      requests:
        cpu: "500m"
        memory: "256Mi"
      limits:
        cpu: "500m"        # ä¸ requests ç›¸åŒ
        memory: "256Mi"    # ä¸ requests ç›¸åŒ
  - name: sidecar
    image: fluentd:v1
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "100m"
        memory: "128Mi"
```

**ç‰¹ç‚¹ï¼š**
- oom_score_adj: **-997**ï¼ˆæœ€ä½ï¼ŒOOM Kill ä¼˜å…ˆçº§æœ€ä½ï¼‰
- èŠ‚ç‚¹èµ„æºå‹åŠ›æ—¶æœ€åè¢« Eviction
- CPU è°ƒåº¦ä¼˜å…ˆçº§é«˜

#### Burstableï¼ˆä¸­ç­‰ä¼˜å…ˆçº§ï¼‰

**æ¡ä»¶ï¼š**
- è‡³å°‘ 1 ä¸ªå®¹å™¨è®¾ç½®äº† CPU æˆ– Memory requests
- ä¸æ»¡è¶³ Guaranteed æ¡ä»¶

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: burstable-pod
  labels:
    qos: burstable
spec:
  containers:
  - name: app
    image: web-app:v1
    resources:
      requests:
        cpu: "250m"
        memory: "512Mi"
      limits:
        cpu: "1000m"       # å¤§äº requests (Burstable)
        memory: "1Gi"      # å¤§äº requests

  - name: cache
    image: redis:7
    resources:
      requests:
        memory: "256Mi"    # æ—  CPU requests (Burstable)
      limits:
        memory: "512Mi"
```

**ç‰¹ç‚¹ï¼š**
- oom_score_adj: **min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)**
- æ ¹æ®ä½¿ç”¨é‡åŠ¨æ€è°ƒæ•´
- æœ‰ä½™é‡æ—¶å¯ burst

#### BestEffortï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰

**æ¡ä»¶ï¼š**
- æ‰€æœ‰å®¹å™¨éƒ½æœªè®¾ç½® requests å’Œ limits

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: besteffort-pod
  labels:
    qos: besteffort
spec:
  containers:
  - name: app
    image: test-app:latest
    # æ—  resources æ®µæˆ–ä¸ºç©º
```

**ç‰¹ç‚¹ï¼š**
- oom_score_adj: **1000**ï¼ˆæœ€é«˜ï¼ŒOOM Kill æœ€ä¼˜å…ˆï¼‰
- èŠ‚ç‚¹èµ„æºå‹åŠ›æ—¶æœ€å…ˆè¢« Eviction
- å»ºè®®ä»…åœ¨å¼€å‘/æµ‹è¯•ç¯å¢ƒä½¿ç”¨

### 3.2 QoS ä¸ Eviction ä¼˜å…ˆçº§

å½“èŠ‚ç‚¹èµ„æºå‹åŠ›æ—¶ï¼Œkubelet æŒ‰ä»¥ä¸‹é¡ºåº Evict Podï¼š

```mermaid
graph TB
    A[èŠ‚ç‚¹èµ„æºå‹åŠ›] --> B{Eviction å†³ç­–}

    B --> C[ç¬¬ 1 é˜¶æ®µ: BestEffort Pod]
    C --> D{èµ„æºæ¢å¤?}
    D -->|No| E[ç¬¬ 2 é˜¶æ®µ: Burstable Pod<br/>è¶…è¿‡ requests ä½¿ç”¨ä¸­]
    D -->|Yes| Z[Eviction åœæ­¢]

    E --> F{èµ„æºæ¢å¤?}
    F -->|No| G[ç¬¬ 3 é˜¶æ®µ: Burstable Pod<br/>ä½äº requests ä½¿ç”¨ä¸­]
    F -->|Yes| Z

    G --> H{èµ„æºæ¢å¤?}
    H -->|No| I[ç¬¬ 4 é˜¶æ®µ: Guaranteed Pod<br/>ä»…æ’é™¤å¿…è¦ç³»ç»Ÿ Pod]
    H -->|Yes| Z

    I --> Z

    style C fill:#ff6b6b
    style E fill:#ffa94d
    style G fill:#ffd43b
    style I fill:#ff0000,color:#fff
    style Z fill:#51cf66
```

**Eviction é¡ºåºæ‘˜è¦ï¼š**

| é¡ºä½ | QoS ç±»åˆ« | æ¡ä»¶ | oom_score_adj |
|------|-----------|------|---------------|
| 1 (æœ€ä¼˜å…ˆ) | BestEffort | æ‰€æœ‰ Pod | 1000 |
| 2 | Burstable | è¶…è¿‡ requests ä½¿ç”¨ä¸­ | 2-999 (ä¸ä½¿ç”¨é‡æˆæ¯”ä¾‹) |
| 3 | Burstable | ä½äº requests ä½¿ç”¨ä¸­ | 2-999 (ä¸ä½¿ç”¨é‡æˆæ¯”ä¾‹) |
| 4 (æœ€å) | Guaranteed | æ’é™¤ç³»ç»Ÿå…³é”® Pod | -997 |

**oom_score_adj æŸ¥çœ‹æ–¹æ³•ï¼š**

```bash
# æŸ¥æ‰¾ Pod çš„ä¸»å®¹å™¨è¿›ç¨‹
kubectl get pod <pod-name> -o jsonpath='{.status.containerStatuses[0].containerID}'

# åœ¨èŠ‚ç‚¹ä¸ŠæŸ¥çœ‹ oom_score_adj
docker inspect <container-id> | grep Pid
cat /proc/<pid>/oom_score_adj

# ç¤ºä¾‹è¾“å‡º
# BestEffort: 1000
# Burstable: 500 (æ ¹æ®ä½¿ç”¨é‡å˜åŠ¨)
# Guaranteed: -997
```

### 3.3 å®æˆ˜ QoS ç­–ç•¥

æ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹æ€§é€‰æ‹© QoS ç±»åˆ«çš„æŒ‡å—ï¼š

| å·¥ä½œè´Ÿè½½ç±»å‹ | æ¨è QoS | è®¾ç½®æ¨¡å¼ | åŸå›  |
|-------------|---------|----------|------|
| **ç”Ÿäº§ API** | Guaranteed | requests = limits | ç¨³å®šæ€§ä¼˜å…ˆï¼Œé˜²æ­¢ Eviction |
| **æ•°æ®åº“** | Guaranteed | requests = limits | å†…å­˜å‹åŠ›æ—¶ä¹Ÿå—ä¿æŠ¤ |
| **æ‰¹å¤„ç†ä½œä¸š** | Burstable | limits > requests | ç©ºé—²æ—¶åˆ©ç”¨èµ„æºï¼Œæˆæœ¬æ•ˆç‡é«˜ |
| **é˜Ÿåˆ— Worker** | Burstable | limits > requests | åº”å¯¹è´Ÿè½½æ³¢åŠ¨ |
| **å¼€å‘/æµ‹è¯•** | BestEffort | ä¸è®¾ç½® | èµ„æºæ•ˆç‡ï¼ˆç”Ÿäº§ç¯å¢ƒç¦æ­¢ï¼‰ |
| **ç›‘æ§ Agent** | Guaranteed | è®¾ç½®è¾ƒä½å€¼ | ç³»ç»Ÿç¨³å®šæ€§ |

**ç”Ÿäº§æ¨èè®¾ç½®ï¼š**

```yaml
# æ¨¡å¼ 1: å…³é”®ä»»åŠ¡æœåŠ¡ (Guaranteed)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-api
  namespace: production
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: payment-api
        tier: critical
    spec:
      containers:
      - name: api
        image: payment-api:v2.1
        resources:
          requests:
            cpu: "1000m"
            memory: "2Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
      priorityClassName: system-cluster-critical  # é¢å¤–ä¿æŠ¤

---
# æ¨¡å¼ 2: ä¸€èˆ¬ Web æœåŠ¡ (Burstable)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
  namespace: production
spec:
  replicas: 10
  template:
    spec:
      containers:
      - name: frontend
        image: web-frontend:v1.5
        resources:
          requests:
            cpu: "200m"       # P50 ä½¿ç”¨é‡
            memory: "256Mi"
          limits:
            cpu: "500m"       # P95 ä½¿ç”¨é‡
            memory: "512Mi"   # é˜²æ­¢ OOM

---
# æ¨¡å¼ 3: æ‰¹å¤„ç† Worker (Burstable)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-report
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: report-generator
            image: report-gen:v1
            resources:
              requests:
                cpu: "500m"
                memory: "1Gi"
              limits:
                cpu: "4000m"     # å¤œé—´æ—¶æ®µåˆ©ç”¨èµ„æº
                memory: "8Gi"
          restartPolicy: OnFailure
```

## VPA (Vertical Pod Autoscaler) è¯¦ç»†æŒ‡å—

### 4.1 VPA æ¶æ„

VPA ç”± 3 ä¸ªç»„ä»¶æ„æˆï¼š

```mermaid
graph TB
    subgraph "VPA æ¶æ„"
        subgraph "æŒ‡æ ‡æ”¶é›†"
            MS[Metrics Server] -->|èµ„æºæŒ‡æ ‡| PROM[Prometheus]
            PROM -->|æ—¶é—´åºåˆ—æ•°æ®| REC[VPA Recommender]
        end

        subgraph "VPA ç»„ä»¶"
            REC -->|è®¡ç®—æ¨èå€¼| VPA_OBJ[VPA CRD Object]
            VPA_OBJ -->|æ£€æŸ¥æ¨¡å¼| UPD[VPA Updater]
            VPA_OBJ -->|éªŒè¯æ–° Pod| ADM[VPA Admission Controller]

            UPD -->|é‡å¯ Pod| POD[Running Pods]
            ADM -->|æ³¨å…¥èµ„æº| NEW_POD[New Pods]
        end

        subgraph "å·¥ä½œè´Ÿè½½"
            POD -->|ä¸ŠæŠ¥ä½¿ç”¨é‡| MS
            NEW_POD -->|ä¸ŠæŠ¥ä½¿ç”¨é‡| MS
        end
    end

    style REC fill:#4dabf7
    style UPD fill:#ffa94d
    style ADM fill:#51cf66
```

**ç»„ä»¶è§’è‰²ï¼š**

| ç»„ä»¶ | è§’è‰² | æ•°æ®æº |
|---------|------|-----------|
| **Recommender** | åˆ†æå†å²ä½¿ç”¨é‡ï¼Œè®¡ç®—æ¨èå€¼ | Metrics Serverã€Prometheus |
| **Updater** | Auto æ¨¡å¼ä¸‹é‡å¯ Pod | VPA CRD çŠ¶æ€ |
| **Admission Controller** | è‡ªåŠ¨å‘æ–° Pod æ³¨å…¥èµ„æº | VPA CRD æ¨èå€¼ |

#### 4.1.4 VPA Recommender ML ç®—æ³•è¯¦è§£

VPA Recommender å¹¶éç®€å•çš„å¹³å‡å€¼è®¡ç®—ï¼Œè€Œæ˜¯åŸºäºæœºå™¨å­¦ä¹ çš„ç²¾ç»†ç®—æ³•æ¥è®¡ç®—èµ„æºæ¨èå€¼ã€‚

##### æŒ‡æ•°åŠ æƒç›´æ–¹å›¾ (Exponentially-weighted Histogram)

VPA Recommender çš„æ ¸å¿ƒæ˜¯éšæ—¶é—´è¡°å‡çš„åŠ æƒç›´æ–¹å›¾ï¼š

```
æœ€è¿‘æ•°æ® â†’ é«˜æƒé‡
å†å²æ•°æ® â†’ ä½æƒé‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
```

**ç®—æ³•å·¥ä½œæ–¹å¼ï¼š**

1. **æŒ‡æ ‡æ”¶é›†å‘¨æœŸ**ï¼šæ¯åˆ†é’Ÿæ”¶é›† Pod èµ„æºä½¿ç”¨é‡
2. **ç›´æ–¹å›¾æ›´æ–°**ï¼šå°†æ¯æ¬¡æµ‹é‡å€¼ç´¯ç§¯åˆ°ç›´æ–¹å›¾æ¡¶ä¸­
3. **æƒé‡åº”ç”¨**ï¼šå†å²æ•°æ®ä»¥ `e^(-t/decay_half_life)` æƒé‡è¡°å‡
4. **æ¨èå€¼è®¡ç®—**ï¼šåŸºäºç›´æ–¹å›¾ç™¾åˆ†ä½æ•°è®¡ç®—æ¨èå€¼

```mermaid
graph TB
    subgraph "VPA Recommender ç®—æ³•"
        A[Metrics Server] -->|æ¯åˆ†é’Ÿ| B[æ”¶é›†æŒ‡æ ‡]
        B --> C[æ›´æ–°ç›´æ–¹å›¾æ¡¶]
        C --> D[åº”ç”¨æŒ‡æ•°æƒé‡]
        D --> E[è®¡ç®—ç™¾åˆ†ä½æ•°]

        E --> F[Lower Bound<br/>P5]
        E --> G[Target<br/>P95]
        E --> H[Upper Bound<br/>P99]
        E --> I[Uncapped Target<br/>æ— çº¦æŸ P95]

        F --> J[æ›´æ–° VPA CRD]
        G --> J
        H --> J
        I --> J
    end

    style G fill:#51cf66
    style J fill:#4dabf7
```

##### å››ç§æ¨èå€¼è®¡ç®—æ–¹æ³•

| æ¨èå€¼ | è®¡ç®—æ–¹æ³• | å«ä¹‰ |
|--------|----------|------|
| **Lower Bound** | P5ï¼ˆç¬¬ 5 ç™¾åˆ†ä½æ•°ï¼‰ | æœ€ä½æ‰€éœ€èµ„æº - 95% æ—¶é—´å†…è¶³å¤Ÿ |
| **Target** | P95ï¼ˆç¬¬ 95 ç™¾åˆ†ä½æ•°ï¼‰ | **æ¨èè®¾ç½®å€¼** - åº”å¯¹ 5% å³°å€¼è´Ÿè½½ |
| **Upper Bound** | P99ï¼ˆç¬¬ 99 ç™¾åˆ†ä½æ•°ï¼‰ | æœ€å¤§è§‚å¯Ÿä½¿ç”¨é‡ - ç”¨äº Limits è®¾ç½®å‚è€ƒ |
| **Uncapped Target** | æ—  maxAllowed çº¦æŸè®¡ç®—çš„ P95 | ç”¨äºç¡®è®¤å®é™…éœ€æ±‚é‡ |

**ç™¾åˆ†ä½æ•°è®¡ç®—ç¤ºä¾‹ï¼š**

```python
# è™šæ‹Ÿ CPU ä½¿ç”¨é‡ç›´æ–¹å›¾ï¼ˆ1 å¤© = 1440 åˆ†é’Ÿï¼‰
cpu_samples = [100m, 150m, 200m, 250m, 300m, 350m, 400m, 450m, 500m, ...]

# åº”ç”¨æŒ‡æ•°æƒé‡ï¼ˆdecay_half_life = 24 å°æ—¶ï¼‰
weighted_samples = [
    (100m, weight=1.0),    # æœ€è¿‘ï¼ˆ1 å°æ—¶å‰ï¼‰
    (150m, weight=0.97),   # 2 å°æ—¶å‰
    (200m, weight=0.92),   # 5 å°æ—¶å‰
    (250m, weight=0.71),   # 12 å°æ—¶å‰
    (300m, weight=0.50),   # 24 å°æ—¶å‰ï¼ˆåŠè¡°æœŸï¼‰
    (350m, weight=0.25),   # 48 å°æ—¶å‰
    ...
]

# ç™¾åˆ†ä½æ•°è®¡ç®—
P5  = 150m  # Lower Bound
P95 = 450m  # Target â­
P99 = 500m  # Upper Bound
```

##### Confidence Multiplierï¼šåŸºäºç½®ä¿¡åº¦çš„è°ƒæ•´

æ•°æ®æ”¶é›†æœŸè¶ŠçŸ­ï¼Œæ¨èå€¼è¶Šä¿å®ˆï¼ˆè¶Šé«˜ï¼‰ï¼š

```
Confidence Multiplier = f(æ•°æ®æ”¶é›†æœŸ)

0-24 å°æ—¶ï¼šmultiplier = 1.5ï¼ˆ50% å®‰å…¨è£•é‡ï¼‰
1-3 å¤©ï¼š   multiplier = 1.3ï¼ˆ30% å®‰å…¨è£•é‡ï¼‰
3-7 å¤©ï¼š   multiplier = 1.1ï¼ˆ10% å®‰å…¨è£•é‡ï¼‰
7 å¤©ä»¥ä¸Šï¼š multiplier = 1.0ï¼ˆç½®ä¿¡åº¦è¶³å¤Ÿï¼‰
```

**å®é™…åº”ç”¨ç¤ºä¾‹ï¼š**

```yaml
# æ•°æ®æ”¶é›†ç¬¬ 2 å¤©
åŸå§‹ P95: 450m
Confidence Multiplier: 1.3
æœ€ç»ˆ Target: 450m Ã— 1.3 = 585m â‰ˆ 600m

# æ•°æ®æ”¶é›†ç¬¬ 10 å¤©
åŸå§‹ P95: 450m
Confidence Multiplier: 1.0
æœ€ç»ˆ Target: 450m Ã— 1.0 = 450m
```

:::info æ•°æ®æ”¶é›†æœŸçš„é‡è¦æ€§
VPA è¦æä¾›å‡†ç¡®çš„æ¨èå€¼ï¼Œè‡³å°‘éœ€è¦ **7 å¤©ï¼Œå»ºè®® 14 å¤©**çš„æ•°æ®æ”¶é›†æœŸã€‚è¦æ•æ‰å·¥ä½œæ—¥/å‘¨æœ«ç­‰å‘¨æœŸæ€§æ¨¡å¼ï¼Œè‡³å°‘éœ€è¦ 2 å‘¨ä»¥ä¸Šçš„è§‚å¯Ÿã€‚
:::

##### Memory æ¨èï¼šåŸºäº OOM äº‹ä»¶çš„ Bump-Up

Memory ä¸ CPU ä¸åŒï¼Œä¼šç‰¹åˆ«è€ƒè™‘ OOM Kill äº‹ä»¶ï¼š

**æ£€æµ‹åˆ° OOM äº‹ä»¶æ—¶ï¼š**

```
å½“å‰ Memory Target: 500Mi
OOM Kill å‘ç”Ÿæ—¶å†…å­˜: 600Mi
â†’ æ–° Target: 600Mi Ã— 1.2 = 720Miï¼ˆå¢åŠ  20% å®‰å…¨è£•é‡ï¼‰
```

**OOM Bump-Up é€»è¾‘ï¼š**

```python
if oom_kill_detected:
    oom_memory = get_memory_at_oom_time()
    new_target = max(
        current_target,
        oom_memory * 1.2  # 20% å®‰å…¨è£•é‡
    )

    # é˜²æ­¢çªç„¶å˜åŒ–ï¼ˆæœ€å¤§ 2 å€ï¼‰
    new_target = min(new_target, current_target * 2)
```

:::warning OOM Kill å³æ—¶ç”Ÿæ•ˆ
ä¸ CPU throttling ä¸åŒï¼ŒOOM Kill äº‹ä»¶ä¼š**ç«‹å³ä¸Šè°ƒ Memory Target**ã€‚è¿™æ˜¯é˜²æ­¢æœåŠ¡ä¸­æ–­çš„å®‰å…¨æœºåˆ¶ã€‚
:::

##### CPU æ¨èï¼šåŸºäº P95/P99 ä½¿ç”¨é‡

CPU æ˜¯å¯å‹ç¼©èµ„æºï¼Œå› æ­¤é‡‡å–ä¿å®ˆç­–ç•¥ï¼š

```
CPU Target = P95 ä½¿ç”¨é‡
CPU Upper Bound = P99 ä½¿ç”¨é‡

Throttling å‘ç”Ÿæ—¶ï¼š
â†’ ä¸æ”¹å˜æ¨èå€¼ï¼ˆå»ºè®®ä½¿ç”¨ HPA è§£å†³ï¼‰
```

**æ£€æµ‹åˆ° CPU Throttling æ—¶ï¼š**

```python
if cpu_throttling_detected:
    throttled_percentage = get_throttled_time_percentage()

    if throttled_percentage > 10:
        # VPA è‡ªèº«æ¨èå€¼ä¿æŒä¸å˜
        # è€Œæ˜¯å»ºè®®ä»¥ä¸‹æªæ–½ï¼š
        # 1. æ·»åŠ  HPA è¿›è¡Œæ°´å¹³æ‰©å±•
        # 2. ç§»é™¤ CPU limitsï¼ˆGoogleã€Datadog æ¨¡å¼ï¼‰
        # 3. æˆ–è€…æ‰‹åŠ¨å°† Target ä¸Šè°ƒè‡³ P99
        pass
```

:::tip CPU Throttling vs HPA
VPA æ£€æµ‹åˆ° CPU throttling æ—¶ä¸ä¼šå¤§å¹…æé«˜æ¨èå€¼ã€‚ç›¸åï¼Œ**ä½¿ç”¨ HPA è¿›è¡Œæ°´å¹³æ‰©å±•**æ‰æ˜¯ Kubernetes æœ€ä½³å®è·µã€‚
:::

##### VPA ä¸ Prometheus æ•°æ®æºé›†æˆ

VPA Recommender ä»…ä½¿ç”¨ Metrics Server å³å¯è¿è¡Œï¼Œä½†ä¸ Prometheus é›†æˆåèƒ½æä¾›æ›´ç²¾ç¡®çš„æ¨èï¼š

**Prometheus æŒ‡æ ‡ä½¿ç”¨ï¼š**

```yaml
# VPA Recommender çš„ Prometheus é›†æˆé…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: vpa-recommender-config
  namespace: vpa-system
data:
  recommender-config.yaml: |
    # å¯ç”¨ Prometheus æŒ‡æ ‡æº
    metrics-provider: prometheus
    prometheus-url: http://prometheus-server.monitoring.svc:9090

    # ç›´æ–¹å›¾è®¾ç½®
    histogram-decay-half-life: 24h
    histogram-bucket-size-growth: 1.05

    # CPU æ¨èè®¾ç½®
    cpu-histogram-decay-half-life: 24h
    memory-histogram-decay-half-life: 48h  # Memory éœ€è¦æ›´é•¿çš„è§‚å¯ŸæœŸ

    # OOM äº‹ä»¶å¤„ç†
    oom-min-bump-up: 1.2  # æœ€ä½ 20% å¢åŠ 
    oom-bump-up-ratio: 0.5  # 50% å®‰å…¨è£•é‡
```

**Prometheus Custom Metrics API é›†æˆï¼š**

```bash
# éƒ¨ç½² Custom Metrics API é€‚é…å™¨ï¼ˆPrometheus Adapterï¼‰
helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace monitoring \
  --set prometheus.url=http://prometheus-server.monitoring.svc \
  --set rules.default=true

# è®¾ç½® VPA ä½¿ç”¨ Custom Metrics API
kubectl edit deploy vpa-recommender -n vpa-system

# æ·»åŠ ç¯å¢ƒå˜é‡ï¼š
# - PROMETHEUS_ADDRESS=http://prometheus-server.monitoring.svc:9090
# - USE_CUSTOM_METRICS=true
```

**éªŒè¯é›†æˆï¼š**

```bash
# ç¡®è®¤ VPA Recommender æ­£åœ¨ä½¿ç”¨ Prometheus æŒ‡æ ‡
kubectl logs -n vpa-system deploy/vpa-recommender | grep prometheus

# é¢„æœŸè¾“å‡ºï¼š
# I0212 10:15:30.123456  1 metrics_client.go:45] Using Prometheus metrics provider
# I0212 10:15:31.234567  1 prometheus_client.go:78] Connected to Prometheus at http://prometheus-server.monitoring.svc:9090
```

##### VPA æ¨èè´¨é‡éªŒè¯æ–¹æ³•

ç”¨äºéªŒè¯æ¨èå€¼æ˜¯å¦åˆç†çš„ PromQL æŸ¥è¯¢ï¼š

**1. CPU æ¨èå€¼ vs å®é™…ä½¿ç”¨é‡æ¯”è¾ƒï¼š**

```promql
# VPA Target vs å®é™… P95 ä½¿ç”¨é‡æ¯”è¾ƒ
(
  kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="cpu"}
  -
  quantile_over_time(0.95,
    container_cpu_usage_seconds_total{pod=~"web-app-.*"}[7d]
  ) * 1000
) /
kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="cpu"} * 100

# è¾“å‡ºï¼šæ¨èå€¼ä¸å®é™… P95 å·®å¼‚ï¼ˆ%ï¼‰
# 10-20% èŒƒå›´ï¼šåˆé€‚ âœ…
# >30%ï¼šè¿‡åº¦é…ç½® âš ï¸
# <0%ï¼šé…ç½®ä¸è¶³ï¼ˆéœ€ç«‹å³è°ƒæ•´ï¼‰ğŸš¨
```

**2. Memory æ¨èå€¼éªŒè¯ï¼š**

```promql
# VPA Target vs å®é™… P99 ä½¿ç”¨é‡
(
  kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="memory"}
  -
  quantile_over_time(0.99,
    container_memory_working_set_bytes{pod=~"web-app-.*"}[7d]
  )
) /
kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="memory"} * 100

# 20-30% ä½™é‡ï¼šç†æƒ³ âœ…
# <10% ä½™é‡ï¼šOOM é£é™© ğŸš¨
```

**3. OOM Kill é¢‘ç‡ç›‘æ§ï¼š**

```promql
# æœ€è¿‘ 7 å¤© OOM Kill äº‹ä»¶æ•°
increase(
  kube_pod_container_status_terminated_reason{reason="OOMKilled"}[7d]
)

# 0 æ¬¡ï¼šVPA æ¨èå‡†ç¡® âœ…
# 1-2 æ¬¡ï¼šå¯æ¥å—ï¼ˆå³°å€¼è´Ÿè½½ï¼‰
# >3 æ¬¡ï¼šéœ€æ‰‹åŠ¨ä¸Šè°ƒ VPA Target ğŸš¨
```

**4. CPU Throttling æ¯”ç‡ï¼š**

```promql
# CPU Throttling æ—¶é—´æ¯”ç‡ï¼ˆ%ï¼‰
rate(container_cpu_cfs_throttled_seconds_total{pod=~"web-app-.*"}[5m])
/
rate(container_cpu_cfs_periods_total{pod=~"web-app-.*"}[5m]) * 100

# <5%ï¼šæ­£å¸¸ âœ…
# 5-10%ï¼šéœ€è¦ç›‘æ§ âš ï¸
# >10%ï¼šè€ƒè™‘æ·»åŠ  HPA æˆ–ç§»é™¤ CPU limits ğŸš¨
```

**Grafana ä»ªè¡¨æ¿ç¤ºä¾‹ï¼š**

```yaml
# VPA æ¨èè´¨é‡ç›‘æ§ä»ªè¡¨æ¿
apiVersion: v1
kind: ConfigMap
metadata:
  name: vpa-quality-dashboard
  namespace: monitoring
data:
  dashboard.json: |
    {
      "panels": [
        {
          "title": "CPU: VPA Target vs P95 å®é™…ä½¿ç”¨é‡",
          "targets": [
            {
              "expr": "kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource=\"cpu\"}",
              "legendFormat": "VPA Target"
            },
            {
              "expr": "quantile_over_time(0.95, container_cpu_usage_seconds_total[7d]) * 1000",
              "legendFormat": "å®é™… P95"
            }
          ]
        },
        {
          "title": "Memory: VPA Target vs P99 å®é™…ä½¿ç”¨é‡",
          "targets": [
            {
              "expr": "kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource=\"memory\"}",
              "legendFormat": "VPA Target"
            },
            {
              "expr": "quantile_over_time(0.99, container_memory_working_set_bytes[7d])",
              "legendFormat": "å®é™… P99"
            }
          ]
        },
        {
          "title": "OOM Kill äº‹ä»¶ï¼ˆ7 å¤©ï¼‰",
          "targets": [
            {
              "expr": "increase(kube_pod_container_status_terminated_reason{reason=\"OOMKilled\"}[7d])"
            }
          ]
        }
      ]
    }
```

:::tip VPA æ¨èçš„å±€é™æ€§
VPA åŸºäºå†å²æ•°æ®æ¨èï¼Œå› æ­¤åœ¨ä»¥ä¸‹åœºæ™¯å­˜åœ¨å±€é™ï¼š
- **çªç„¶çš„æµé‡æ¨¡å¼å˜åŒ–**ï¼šå†å²ä¸­ä¸å­˜åœ¨çš„å³°å€¼è´Ÿè½½
- **å­£èŠ‚æ€§å·¥ä½œè´Ÿè½½**ï¼šæœˆæœ«æ‰¹å¤„ç†ã€å¹´ç»ˆç»“ç®—ç­‰
- **åˆå§‹å¼•å¯¼é˜¶æ®µ**ï¼šåº”ç”¨å¯åŠ¨æ—¶çš„é«˜å†…å­˜ä½¿ç”¨

è¿™äº›æƒ…å†µä¸‹éœ€è¦**æ‰‹åŠ¨è°ƒæ•´**æˆ–**ä¸ HPA é…åˆä½¿ç”¨**ã€‚
:::

### 4.2 VPA å®‰è£…ä¸é…ç½®

#### é€šè¿‡ Helm å®‰è£…

```bash
# 1. å®‰è£… Metrics Serverï¼ˆå‰ç½®æ¡ä»¶ï¼‰
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# 2. ç¡®è®¤ Metrics Server
kubectl get deployment metrics-server -n kube-system
kubectl top nodes

# 3. æ·»åŠ  VPA Helm ä»“åº“
helm repo add fairwinds-stable https://charts.fairwinds.com/stable
helm repo update

# 4. å®‰è£… VPA
helm install vpa fairwinds-stable/vpa \
  --namespace vpa-system \
  --create-namespace \
  --set recommender.enabled=true \
  --set updater.enabled=true \
  --set admissionController.enabled=true

# 5. ç¡®è®¤å®‰è£…
kubectl get pods -n vpa-system
# é¢„æœŸè¾“å‡ºï¼š
# NAME                                      READY   STATUS    RESTARTS   AGE
# vpa-admission-controller-xxx              1/1     Running   0          1m
# vpa-recommender-xxx                       1/1     Running   0          1m
# vpa-updater-xxx                           1/1     Running   0          1m
```

#### æ‰‹åŠ¨å®‰è£…ï¼ˆå®˜æ–¹æ–¹æ³•ï¼‰

```bash
# å…‹éš† VPA å®˜æ–¹ä»“åº“
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler

# å®‰è£… VPA
./hack/vpa-up.sh

# ç¡®è®¤å®‰è£…
kubectl get crd | grep verticalpodautoscaler
```

### 4.3 VPA æ¨¡å¼

VPA æœ‰ 3 ç§è¿è¡Œæ¨¡å¼ï¼š

#### Off æ¨¡å¼ï¼ˆä»…æä¾›æ¨èå€¼ï¼‰

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"    # ä»…æ˜¾ç¤ºæ¨èå€¼ï¼Œä¸è‡ªåŠ¨åº”ç”¨
```

**é€‚ç”¨åœºæ™¯ï¼š**
- é¦–æ¬¡å¼•å…¥ VPA æ—¶
- åˆ†æç”Ÿäº§å·¥ä½œè´Ÿè½½
- éœ€è¦æ‰‹åŠ¨å®¡æ ¸åå†åº”ç”¨æ—¶

**æŸ¥çœ‹æ¨èå€¼ï¼š**

```bash
# æŸ¥çœ‹ VPA çŠ¶æ€
kubectl describe vpa web-app-vpa -n production

# è¾“å‡ºç¤ºä¾‹ï¼š
# Recommendation:
#   Container Recommendations:
#     Container Name: web-app
#     Lower Bound:
#       Cpu:     150m
#       Memory:  200Mi
#     Target:          # â† æ¨èä½¿ç”¨æ­¤å€¼
#       Cpu:     250m
#       Memory:  300Mi
#     Uncapped Target:
#       Cpu:     350m
#       Memory:  400Mi
#     Upper Bound:
#       Cpu:     500m
#       Memory:  600Mi
```

#### Initial æ¨¡å¼ï¼ˆä»…åœ¨ Pod åˆ›å»ºæ—¶åº”ç”¨ï¼‰

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: batch-worker-vpa
  namespace: batch
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: batch-worker
  updatePolicy:
    updateMode: "Initial"    # ä»…åœ¨ Pod åˆ›å»ºæ—¶è®¾ç½®èµ„æº
  resourcePolicy:
    containerPolicies:
    - containerName: worker
      minAllowed:
        cpu: "100m"
        memory: "128Mi"
      maxAllowed:
        cpu: "4000m"
        memory: "16Gi"
```

**é€‚ç”¨åœºæ™¯ï¼š**
- CronJobã€Job å·¥ä½œè´Ÿè½½
- ä¸å…è®¸é‡å¯çš„ StatefulSet
- éœ€è¦æ‰‹åŠ¨æ‰©ç¼©çš„åœºæ™¯

**å·¥ä½œæ–¹å¼ï¼š**
1. æ–° Pod åˆ›å»ºè¯·æ±‚
2. VPA Admission Controller æ³¨å…¥æ¨èèµ„æº
3. æ­£åœ¨è¿è¡Œçš„ Pod ä¿æŒä¸å˜

#### Auto æ¨¡å¼ï¼ˆå®Œå…¨è‡ªåŠ¨åŒ–ï¼‰

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
  namespace: development
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  updatePolicy:
    updateMode: "Auto"    # è‡ªåŠ¨é‡å¯ Pod å¹¶è°ƒæ•´èµ„æº
    minReplicas: 2        # è‡³å°‘ç»´æŒ 2 ä¸ª Pod
  resourcePolicy:
    containerPolicies:
    - containerName: api
      minAllowed:
        cpu: "200m"
        memory: "256Mi"
      maxAllowed:
        cpu: "2000m"
        memory: "4Gi"
      controlledResources:
      - cpu
      - memory
      controlledValues: RequestsAndLimits  # åŒæ—¶è°ƒæ•´ requests å’Œ limits
```

**é€‚ç”¨åœºæ™¯ï¼š**
- å¼€å‘/é¢„å‘å¸ƒç¯å¢ƒ
- Stateless åº”ç”¨
- å·²é…ç½® PodDisruptionBudget çš„å·¥ä½œè´Ÿè½½

:::warning Auto æ¨¡å¼æ³¨æ„äº‹é¡¹
Auto æ¨¡å¼ä¼š**é‡å¯ Pod**ï¼š
- é€šè¿‡ Eviction API é‡å¯
- å¯èƒ½å¯¼è‡´åœæœº
- å¿…é¡»é…ç½® PodDisruptionBudget (PDB)
- ç”Ÿäº§ç¯å¢ƒéœ€è°¨æ…ä½¿ç”¨

**å»ºè®®ï¼š** ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ **Off æˆ– Initial æ¨¡å¼**
:::

### 4.4 VPA + HPA å…±å­˜ç­–ç•¥

åŒæ—¶ä½¿ç”¨ VPA å’Œ HPA æ—¶å¿…é¡»é˜²æ­¢å†²çªã€‚

#### å†²çªåœºæ™¯ï¼ˆâŒ ç¦æ­¢ï¼‰

```yaml
# âŒ é”™è¯¯é…ç½®ï¼šVPA Auto + HPA CPU åŒæ—¶ä½¿ç”¨
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: bad-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Auto"    # âŒ Auto æ¨¡å¼
  resourcePolicy:
    containerPolicies:
    - containerName: app
      controlledResources:
      - cpu                # âŒ æ§åˆ¶ CPU
      - memory

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: bad-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu          # âŒ ä½¿ç”¨ CPU æŒ‡æ ‡
      target:
        type: Utilization
        averageUtilization: 70
```

**é—®é¢˜ï¼š**
- VPA æ›´æ”¹ CPU requests â†’ HPA çš„ CPU ä½¿ç”¨ç‡è®¡ç®—å‘ç”Ÿå˜åŒ–
- HPA è§¦å‘ Scale Out â†’ VPA å†æ¬¡è°ƒæ•´èµ„æº â†’ æ— é™å¾ªç¯

#### æ¨¡å¼ 1ï¼šVPA Off + HPAï¼ˆâœ… æ¨èï¼‰

```yaml
# âœ… æ­£ç¡®é…ç½®ï¼šVPA ä»…æä¾›æ¨èï¼ŒHPA è´Ÿè´£æ‰©ç¼©
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"    # âœ… ä»…æä¾›æ¨èå€¼
  resourcePolicy:
    containerPolicies:
    - containerName: app
      controlledResources:
      - cpu
      - memory

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

**è¿ç»´å·¥ä½œæµï¼š**
1. VPA ç”Ÿæˆæ¨èå€¼
2. å‘¨ä¼šå®¡æŸ¥ VPA æ¨èå€¼
3. æ‰‹åŠ¨æ›´æ–° Deployment æ¸…å•
4. HPA æ ¹æ®è´Ÿè½½è¿›è¡Œæ°´å¹³æ‰©å±•

#### æ¨¡å¼ 2ï¼šVPA Memory + HPA CPUï¼ˆâœ… æ¨èï¼‰

```yaml
# âœ… æŒ‡æ ‡åˆ†ç¦»ï¼šVPA ç®¡ç† Memoryï¼ŒHPA ç®¡ç† CPU
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  updatePolicy:
    updateMode: "Auto"    # ä»…è‡ªåŠ¨è°ƒæ•´ Memory
  resourcePolicy:
    containerPolicies:
    - containerName: api
      controlledResources:
      - memory            # âœ… ä»…æ§åˆ¶ Memory
      minAllowed:
        memory: "256Mi"
      maxAllowed:
        memory: "8Gi"

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 5
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu          # âœ… ä»…ä½¿ç”¨ CPU æŒ‡æ ‡
      target:
        type: Utilization
        averageUtilization: 60
```

**ä¼˜åŠ¿ï¼š**
- VPA ä¼˜åŒ– Memoryï¼ˆå‚ç›´ï¼‰
- HPA æ ¹æ®è´Ÿè½½æ°´å¹³æ‰©å±•ï¼ˆæ°´å¹³ï¼‰
- æ— å†²çª

#### æ¨¡å¼ 3ï¼šVPA + HPA + Custom Metricsï¼ˆâœ… é«˜çº§ï¼‰

```yaml
# âœ… HPA ä½¿ç”¨è‡ªå®šä¹‰æŒ‡æ ‡
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: worker-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: queue-worker
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: worker
      controlledResources:
      - cpu
      - memory

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: queue-worker
  minReplicas: 2
  maxReplicas: 50
  metrics:
  - type: External
    external:
      metric:
        name: sqs_queue_depth    # âœ… è‡ªå®šä¹‰æŒ‡æ ‡ï¼ˆé CPU/Memoryï¼‰
        selector:
          matchLabels:
            queue: "tasks"
      target:
        type: AverageValue
        averageValue: "30"
```

**é€‚ç”¨åœºæ™¯ï¼š**
- åŸºäºé˜Ÿåˆ—çš„å·¥ä½œè´Ÿè½½ï¼ˆSQSã€RabbitMQã€Kafkaï¼‰
- äº‹ä»¶é©±åŠ¨æ¶æ„
- åŸºäºä¸šåŠ¡æŒ‡æ ‡çš„æ‰©ç¼©

### 4.5 VPA é™åˆ¶ä¸æ³¨æ„äº‹é¡¹

:::danger VPA ä½¿ç”¨æ³¨æ„äº‹é¡¹

**1. éœ€è¦é‡å¯ Podï¼ˆAuto/Recreate æ¨¡å¼ï¼‰**
- VPA **æ— æ³•å°±åœ° (in-place) å˜æ›´**è¿è¡Œä¸­ Pod çš„èµ„æº
- é€šè¿‡ Evict Pod å¹¶é‡æ–°åˆ›å»ºï¼ˆå¯èƒ½å¯¼è‡´åœæœºï¼‰
- è§£å†³æ–¹æ¡ˆï¼šå¿…é¡»é…ç½® PodDisruptionBudget

**2. JVM å †å¤§å°ä¸åŒ¹é…**
```yaml
# é—®é¢˜åœºæ™¯
containers:
- name: java-app
  env:
  - name: JAVA_OPTS
    value: "-Xmx2g"    # å›ºå®šå€¼
  resources:
    requests:
      memory: "3Gi"    # VPA ä¹‹åå¯èƒ½å˜æ›´ä¸º 4Gi
    limits:
      memory: "3Gi"    # VPA ä¹‹åå¯èƒ½å˜æ›´ä¸º 4Gi

# VPA å°† memory å˜æ›´ä¸º 4Giï¼Œä½† JVM ä»ä½¿ç”¨ 2Gi å †
# â†’ èµ„æºæµªè´¹
```

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
containers:
- name: java-app
  env:
  - name: MEM_LIMIT
    valueFrom:
      resourceFieldRef:
        resource: limits.memory
  - name: JAVA_OPTS
    value: "-XX:MaxRAMPercentage=75.0"  # åŠ¨æ€è®¡ç®—
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"
```

**3. StatefulSet æ³¨æ„äº‹é¡¹**
- StatefulSet Pod æŒ‰é¡ºåºé‡å¯
- å­˜åœ¨æ•°æ®ä¸¢å¤±é£é™©
- æ¨èï¼š**ä»…ä½¿ç”¨ Initial æ¨¡å¼**

**4. Metrics Server ä¾èµ–**
- VPA å¿…é¡»ä¾èµ– Metrics Server
- Metrics Server æ•…éšœæ—¶æ¨èå€¼æ›´æ–°åœæ­¢

**5. æ¨èå€¼è®¡ç®—æ—¶é—´**
- è‡³å°‘éœ€è¦ 24 å°æ—¶æ•°æ®
- æµé‡æ¨¡å¼å˜åŒ–çš„åæ˜ éœ€è¦æ—¶é—´
:::

## HPA é«˜çº§æ¨¡å¼

### 5.1 HPA Behavior è®¾ç½®

HPA v2 å¯ä»¥ç²¾ç»†æ§åˆ¶æ‰©ç¼©è¡Œä¸ºï¼š

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: advanced-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 5
  maxReplicas: 100

  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0    # ç«‹å³æ‰©å®¹
      policies:
      - type: Percent
        value: 100                     # å…è®¸ 100% å¢åŠ ï¼ˆ2 å€ï¼‰
        periodSeconds: 15              # æ¯ 15 ç§’è¯„ä¼°
      - type: Pods
        value: 10                      # æˆ–å¢åŠ  10 ä¸ª Pod
        periodSeconds: 15
      selectPolicy: Max                # é€‰æ‹©è¾ƒå¤§å€¼

    scaleDown:
      stabilizationWindowSeconds: 300  # 5 åˆ†é’Ÿç¨³å®šæœŸï¼ˆé˜²æ­¢æ€¥å‰§ç¼©å‡ï¼‰
      policies:
      - type: Percent
        value: 10                      # 10% ç¼©å‡
        periodSeconds: 60              # æ¯åˆ†é’Ÿè¯„ä¼°
      - type: Pods
        value: 5                       # æˆ–å‡å°‘ 5 ä¸ª Pod
        periodSeconds: 60
      selectPolicy: Min                # é€‰æ‹©è¾ƒå°å€¼ï¼ˆä¿å®ˆï¼‰
```

**å‚æ•°è¯´æ˜ï¼š**

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|---------|------|--------|
| `stabilizationWindowSeconds` | æŒ‡æ ‡ç¨³å®šç­‰å¾…æ—¶é—´ | ScaleUp: 0-30sï¼ŒScaleDown: 300-600s |
| `type: Percent` | æŒ‰å½“å‰å‰¯æœ¬ç™¾åˆ†æ¯”å¢å‡ | ScaleUp: 100%ï¼ŒScaleDown: 10-25% |
| `type: Pods` | æŒ‰ç»å¯¹ Pod æ•°å¢å‡ | æ ¹æ®å·¥ä½œè´Ÿè½½è§„æ¨¡è°ƒæ•´ |
| `periodSeconds` | ç­–ç•¥è¯„ä¼°å‘¨æœŸ | 15-60 ç§’ |
| `selectPolicy` | Maxï¼ˆæ¿€è¿›ï¼‰ã€Minï¼ˆä¿å®ˆï¼‰ã€Disabled | ScaleUp: Maxï¼ŒScaleDown: Min |

:::info å‚è€ƒ karpenter-autoscaling.md
HPA ä¸ Karpenter é…åˆä½¿ç”¨çš„å®Œæ•´æ¶æ„ï¼Œè¯·å‚è€ƒ [Karpenter è‡ªåŠ¨æ‰©ç¼©æŒ‡å—](/docs/infrastructure-optimization/karpenter-autoscaling)ã€‚
:::

### 5.2 è‡ªå®šä¹‰æŒ‡æ ‡ HPA

#### Prometheus Adapter ä½¿ç”¨

```bash
# å®‰è£… Prometheus Adapter
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace monitoring \
  --set prometheus.url=http://prometheus-server.monitoring.svc \
  --set prometheus.port=80
```

**è‡ªå®šä¹‰æŒ‡æ ‡é…ç½®ï¼š**

```yaml
# values.yaml for prometheus-adapter
rules:
  default: false
  custom:
  - seriesQuery: 'http_requests_total{namespace!="",pod!=""}'
    resources:
      overrides:
        namespace: {resource: "namespace"}
        pod: {resource: "pod"}
    name:
      matches: "^(.*)_total$"
      as: "${1}_per_second"
    metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'
```

**HPA é…ç½®ï¼š**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: custom-metric-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"    # æ¯ Pod 1000 req/s
```

#### KEDA ScaledObject

```bash
# å®‰è£… KEDA
helm repo add kedacore https://kedacore.github.io/charts
helm install keda kedacore/keda --namespace keda --create-namespace
```

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prometheus-scaledobject
spec:
  scaleTargetRef:
    name: api-server
  minReplicaCount: 2
  maxReplicaCount: 100
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-server.monitoring.svc:80
      metricName: http_requests_per_second
      threshold: "1000"
      query: sum(rate(http_requests_total{app="api-server"}[2m]))
```

### 5.3 å¤šæŒ‡æ ‡ HPA

ç»„åˆå¤šä¸ªæŒ‡æ ‡è¿›è¡Œæ‰©ç¼©ï¼š

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: multi-metric-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 5
  maxReplicas: 100

  metrics:
  # 1. CPU æŒ‡æ ‡
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # 2. Memory æŒ‡æ ‡
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

  # 3. è‡ªå®šä¹‰æŒ‡æ ‡ - RPS
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"

  # 4. å¤–éƒ¨æŒ‡æ ‡ - ALB Target Response Time
  - type: External
    external:
      metric:
        name: alb_target_response_time
        selector:
          matchLabels:
            targetgroup: "web-app-tg"
      target:
        type: Value
        value: "100"    # 100ms

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 50
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

**å¤šæŒ‡æ ‡è¯„ä¼°ï¼š**
- HPA **ç‹¬ç«‹è¯„ä¼°æ¯ä¸ªæŒ‡æ ‡**
- é€‰æ‹©**æœ€é«˜çš„å‰¯æœ¬æ•°**ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
- ä¾‹å¦‚ï¼šCPU éœ€è¦ 10 ä¸ªï¼ŒMemory éœ€è¦ 15 ä¸ªï¼ŒRPS éœ€è¦ 20 ä¸ª â†’ **é€‰æ‹© 20 ä¸ª**

## Node Readiness Controller ä¸èµ„æºä¼˜åŒ–

### 5.3 æœªå°±ç»ªèŠ‚ç‚¹ä¸Šçš„èµ„æºæµªè´¹

åœ¨ Kubernetes é›†ç¾¤ä¸­ï¼Œå½“æ–°èŠ‚ç‚¹è¢«é…ç½®æ—¶ï¼ŒCNI æ’ä»¶ã€CSI é©±åŠ¨ã€GPU é©±åŠ¨ç­‰åŸºç¡€è®¾æ–½ç»„ä»¶æœªå°±ç»ªå‰ Pod å°±è¢«è°ƒåº¦çš„é—®é¢˜å¯èƒ½å‘ç”Ÿã€‚è¿™ä¼šå¯¼è‡´ä»¥ä¸‹èµ„æºæµªè´¹ï¼š

**èµ„æºæµªè´¹åœºæ™¯ï¼š**

1. **CrashLoopBackOff å¾ªç¯**
   - Pod è¢«è°ƒåº¦åˆ°æœªå°±ç»ªèŠ‚ç‚¹ â†’ å¤±è´¥ â†’ åå¤é‡å¯
   - ä¸å¿…è¦çš„ CPU/å†…å­˜ä½¿ç”¨å’Œå®¹å™¨é•œåƒé‡æ–°ä¸‹è½½

2. **ä¸å¿…è¦çš„èŠ‚ç‚¹é…ç½®**
   - Pod å¤„äº Pending çŠ¶æ€ â†’ Karpenter/Cluster Autoscaler åˆ›å»ºé¢å¤–èŠ‚ç‚¹
   - å®é™…ä¸Šç°æœ‰èŠ‚ç‚¹å°±ç»ªåå³å¯æ‰¿è½½

3. **é‡è°ƒåº¦å¼€é”€**
   - å°†å¤±è´¥çš„ Pod ç§»åŠ¨åˆ°å…¶ä»–èŠ‚ç‚¹ â†’ ç½‘ç»œ/å­˜å‚¨èµ„æºæµªè´¹
   - åº”ç”¨åˆå§‹åŒ–æˆæœ¬é‡å¤å‘ç”Ÿ

### 5.4 Node Readiness Controller (NRC) æ¦‚è¿°

Node Readiness Controller æ˜¯ Kubernetes 1.32 å¼•å…¥çš„åŠŸèƒ½ï¼Œåœ¨åŸºç¡€è®¾æ–½å°±ç»ªä¹‹å‰é˜»æ­¢ Pod è°ƒåº¦ï¼Œä»è€Œæé«˜èµ„æºæ•ˆç‡ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**

| åŠŸèƒ½ | è¯´æ˜ | èµ„æºä¼˜åŒ–æ•ˆæœ |
|------|------|-------------------|
| **Readiness Gate** | ç‰¹å®šæ¡ä»¶æ»¡è¶³å‰å°†èŠ‚ç‚¹ä¿æŒåœ¨ NotReady çŠ¶æ€ | é˜»æ­¢ Pod è°ƒåº¦é˜²æ­¢ CrashLoop |
| **Custom Taint** | è‡ªåŠ¨ä¸ºæœªå°±ç»ªèŠ‚ç‚¹æ·»åŠ  taint | é˜²æ­¢èµ„æºæµªè´¹ï¼ˆNoSchedule æ•ˆæœï¼‰|
| **Enforcement Mode** | é€‰æ‹© `bootstrap-only` æˆ– `continuous` æ¨¡å¼ | ä»…åˆå§‹å¼•å¯¼æˆ–æŒç»­éªŒè¯ |

**API ç»“æ„ï¼š**

```yaml
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
```

### 5.5 Karpenter è”åŠ¨ä¼˜åŒ–

å°† Karpenter ä¸ Node Readiness Controller é…åˆä½¿ç”¨å¯æ˜¾è‘—æé«˜èŠ‚ç‚¹é…ç½®æ•ˆç‡ã€‚

**ä¼˜åŒ–æ¨¡å¼ï¼š**

```mermaid
graph TB
    A[Karpenter: é…ç½®æ–°èŠ‚ç‚¹] --> B[NRC: è‡ªåŠ¨æ·»åŠ  Taint]
    B --> C{CNI/CSI å°±ç»ªï¼Ÿ}
    C -->|å¦| D[Pod ä¿æŒ Pending]
    D --> E[Karpenter: ä¸åˆ›å»ºé¢å¤–èŠ‚ç‚¹]
    C -->|æ˜¯| F[NRC: ç§»é™¤ Taint]
    F --> G[å¼€å§‹è°ƒåº¦ Pod]
    G --> H[èµ„æºé«˜æ•ˆåˆ†é…]

    style B fill:#a8dadc
    style E fill:#457b9d
    style H fill:#1d3557
```

**Karpenter NodePool ä¸ NRC è”åŠ¨ï¼š**

```yaml
# 1. CSI Driver å°±ç»ªç¡®è®¤ï¼ˆEBSï¼‰
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: ebs-csi-readiness
spec:
  conditions:
    - type: "ebs.csi.aws.com/driver-ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/storage-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "bootstrap-only"  # ä»…åˆå§‹å¼•å¯¼éªŒè¯

---
# 2. VPC CNI å°±ç»ªç¡®è®¤
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: vpc-cni-readiness
spec:
  conditions:
    - type: "vpc.amazonaws.com/cni-ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/network-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "bootstrap-only"

---
# 3. GPU Driver å°±ç»ªç¡®è®¤ï¼ˆGPU èŠ‚ç‚¹ä¸“ç”¨ï¼‰
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-driver-readiness
spec:
  conditions:
    - type: "nvidia.com/gpu-driver-ready"
      requiredStatus: "True"
    - type: "nvidia.com/cuda-ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/gpu-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "bootstrap-only"
  # GPU é©±åŠ¨åŠ è½½è€—æ—¶è¾ƒé•¿ï¼ˆ30-60 ç§’ï¼‰
  # NRC åœ¨æ­¤æœŸé—´é˜»æ­¢ Pod è°ƒåº¦
```

### 5.6 èµ„æºæ•ˆç‡æ”¹å–„æ•ˆæœ

Node Readiness Controller å®æ–½å‰åå¯¹æ¯”ï¼š

| æŒ‡æ ‡ | å®æ–½å‰ | å®æ–½å | æ”¹å–„ç‡ |
|------|---------|---------|--------|
| **CrashLoopBackOff å‘ç”Ÿç‡** | 15-20% | < 2% | 90% é™ä½ |
| **ä¸å¿…è¦çš„èŠ‚ç‚¹é…ç½®** | å¹³å‡ 2-3 ä¸ª/å°æ—¶ | < 0.5 ä¸ª/å°æ—¶ | 75% é™ä½ |
| **Pod å¯åŠ¨å¤±è´¥ç‡** | 8-12% | < 1% | 90% é™ä½ |
| **å®¹å™¨é•œåƒé‡æ–°ä¸‹è½½** | 100-200GB/å¤© | 20-30GB/å¤© | 80% é™ä½ |

**æˆæœ¬å½±å“ï¼ˆ100 èŠ‚ç‚¹é›†ç¾¤åŸºå‡†ï¼‰ï¼š**

```
å®æ–½å‰ï¼š
- ä¸å¿…è¦çš„èŠ‚ç‚¹é…ç½®ï¼šå¹³å‡ 3 ä¸ª Ã— $0.384/å°æ—¶ Ã— 24 å°æ—¶ Ã— 30 å¤© = $829/æœˆ
- é•œåƒé‡æ–°ä¸‹è½½æ•°æ®ä¼ è¾“è´¹ï¼š150GB/å¤© Ã— 30 å¤© Ã— $0.09/GB = $405/æœˆ
- æ€»æµªè´¹æˆæœ¬ï¼š$1,234/æœˆ

å®æ–½åï¼š
- ä¸å¿…è¦çš„èŠ‚ç‚¹é…ç½®ï¼šå¹³å‡ 0.5 ä¸ª Ã— $0.384/å°æ—¶ Ã— 24 å°æ—¶ Ã— 30 å¤© = $138/æœˆ
- é•œåƒé‡æ–°ä¸‹è½½æ•°æ®ä¼ è¾“è´¹ï¼š25GB/å¤© Ã— 30 å¤© Ã— $0.09/GB = $67.5/æœˆ
- æ€»æˆæœ¬ï¼š$205.5/æœˆ

èŠ‚çœé‡‘é¢ï¼š$1,234 - $205.5 = $1,028.5/æœˆï¼ˆ83% èŠ‚çœï¼‰
```

### 5.7 å®æˆ˜å®æ–½æŒ‡å—

#### Step 1ï¼šå¯ç”¨ Feature Gate

```bash
# åœ¨ EKS 1.32+ é›†ç¾¤ä¸­ç¡®è®¤ Feature Gate
kubectl get --raw /metrics | grep node_readiness_controller

# åœ¨ Karpenter é…ç½®ä¸­å¯ç”¨ Feature Gate
# values.yamlï¼ˆKarpenter Helm Chartï¼‰
controller:
  featureGates:
    NodeReadinessController: true
```

#### Step 2ï¼šåº”ç”¨ NodeReadinessRule

```yaml
# production-nrc.yaml
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: production-readiness
spec:
  # å¤šä¸ªæ¡ä»¶ä»¥ AND æ–¹å¼éªŒè¯
  conditions:
    - type: "ebs.csi.aws.com/driver-ready"
      requiredStatus: "True"
    - type: "vpc.amazonaws.com/cni-ready"
      requiredStatus: "True"

  taint:
    key: "readiness.k8s.io/not-ready"
    effect: "NoSchedule"
    value: "pending"

  # bootstrap-onlyï¼šä»…éªŒè¯èŠ‚ç‚¹åˆå§‹å¼•å¯¼
  # continuousï¼šæŒç»­éªŒè¯ï¼ˆé©±åŠ¨é‡å¯æ—¶ä¹Ÿç”Ÿæ•ˆï¼‰
  enforcementMode: "bootstrap-only"
```

```bash
kubectl apply -f production-nrc.yaml

# ç¡®è®¤åº”ç”¨
kubectl get nodereadinessrule
kubectl describe nodereadinessrule production-readiness
```

#### Step 3ï¼šç›‘æ§èŠ‚ç‚¹çŠ¶æ€

```bash
# æ–°èŠ‚ç‚¹é…ç½®åç¡®è®¤æ¡ä»¶
kubectl get nodes -o json | jq '.items[] | {
  name: .metadata.name,
  conditions: [.status.conditions[] | select(.type |
    test("ebs.csi.aws.com|vpc.amazonaws.com")) |
    {type: .type, status: .status}]
}'

# ç¡®è®¤ Taint çŠ¶æ€
kubectl get nodes -o json | jq '.items[] | {
  name: .metadata.name,
  taints: .spec.taints
}'
```

#### Step 4ï¼šKarpenter NodePool ä¼˜åŒ–

```yaml
# Karpenter NodePool with NRC
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: optimized-pool
spec:
  template:
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64", "arm64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]

      # NRC è‡ªåŠ¨ç®¡ç† taintï¼Œæ­¤å¤„æ— éœ€é…ç½®
      # taints: []  # NRC ç®¡ç†

      # å¢åŠ èŠ‚ç‚¹å¼•å¯¼å®Œæˆç­‰å¾…æ—¶é—´
      kubelet:
        maxPods: 110
        # ç”±äº NRC å¯¼è‡´èŠ‚ç‚¹ Ready æ—¶é—´å¢åŠ ï¼ˆ30 ç§’ â†’ 60 ç§’ï¼‰
        # è®¾ç½®é¿å… Karpenter è¿‡æ—©è¶…æ—¶
        systemReserved:
          cpu: 100m
          memory: 512Mi

  disruption:
    consolidationPolicy: WhenUnderutilized
    # ç”±äº NRC å¯¼è‡´èŠ‚ç‚¹å¯åŠ¨å˜æ…¢ï¼Œå¢åŠ  consolidation é—´éš”
    consolidateAfter: 60s  # é»˜è®¤ 30s â†’ 60s
```

:::warning GPU èŠ‚ç‚¹ç‰¹åˆ«æ³¨æ„äº‹é¡¹
GPU é©±åŠ¨åŠ è½½éœ€è¦ 30-60 ç§’ï¼Œå› æ­¤ GPU NodePool å¿…é¡»åº”ç”¨ NRCã€‚å¦åˆ™ Pod å°†åœ¨ GPU ä¸å¯ç”¨çŠ¶æ€ä¸‹è¢«è°ƒåº¦å¹¶æŒç»­å¤±è´¥ã€‚

```yaml
# GPU ä¸“ç”¨ NRC
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-readiness
spec:
  nodeSelector:
    matchExpressions:
      - key: nvidia.com/gpu
        operator: Exists
  conditions:
    - type: "nvidia.com/gpu-driver-ready"
      requiredStatus: "True"
  taint:
    key: "nvidia.com/gpu-not-ready"
    effect: "NoSchedule"
  enforcementMode: "bootstrap-only"
```
:::

### 5.8 é—®é¢˜æ’æŸ¥ä¸ç›‘æ§

#### å¸¸è§é—®é¢˜

**1. èŠ‚ç‚¹æŒç»­å¤„äº NotReady çŠ¶æ€ï¼š**

```bash
# æŸ¥çœ‹èŠ‚ç‚¹æ¡ä»¶è¯¦æƒ…
kubectl describe node <node-name> | grep -A 10 "Conditions:"

# æŸ¥çœ‹ NRC äº‹ä»¶
kubectl get events --all-namespaces --field-selector involvedObject.kind=Node,involvedObject.name=<node-name>

# æŸ¥çœ‹é©±åŠ¨ DaemonSet çŠ¶æ€
kubectl get pods -n kube-system | grep -E "aws-node|ebs-csi|nvidia"
```

**2. Taint æœªè¢«ç§»é™¤ï¼š**

```bash
# ç¡®è®¤ NRC æ˜¯å¦æ­£åœ¨è¿è¡Œ
kubectl logs -n kube-system -l app=karpenter -c controller | grep "NodeReadiness"

# æ‰‹åŠ¨ç§»é™¤ taintï¼ˆä¸´æ—¶è§£å†³ï¼‰
kubectl taint nodes <node-name> readiness.k8s.io/not-ready:NoSchedule-
```

#### Prometheus æŒ‡æ ‡

```yaml
# NRC æŒ‡æ ‡çš„ ServiceMonitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-readiness-controller
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: karpenter
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s

# ä¸»è¦æŒ‡æ ‡ï¼š
# - node_readiness_controller_reconcile_duration_seconds
# - node_readiness_controller_condition_evaluation_total
# - node_readiness_controller_taint_operations_total
```

:::tip å‚è€ƒèµ„æ–™
- **å®˜æ–¹åšå®¢**ï¼š[Introducing Node Readiness Controller](https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/)
- **KEP (Kubernetes Enhancement Proposal)**ï¼šKEP-4403
- **API æ–‡æ¡£**ï¼š`readiness.node.x-k8s.io/v1alpha1`
:::
