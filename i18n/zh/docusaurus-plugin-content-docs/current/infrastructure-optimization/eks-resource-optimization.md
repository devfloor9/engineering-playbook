---
title: "EKS Pod èµ„æºä¼˜åŒ–æŒ‡å—"
sidebar_label: "5. Pod èµ„æºä¼˜åŒ–"
description: "Kubernetes Pod çš„ CPU/Memory èµ„æºè®¾ç½®ã€QoS ç±»åˆ«ã€VPA/HPA è‡ªåŠ¨æ‰©ç¼©å®¹ã€èµ„æº Right-Sizing ç­–ç•¥"
tags: [eks, kubernetes, resources, cpu, memory, qos, vpa, hpa, right-sizing, optimization]
category: "performance-networking"
last_update:
  date: 2026-02-14
  author: devfloor9
sidebar_position: 5
---

# EKS Pod èµ„æºä¼˜åŒ–æŒ‡å—

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2026-02-12 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 46 åˆ†é’Ÿ

> **ğŸ“Œ åŸºå‡†ç¯å¢ƒ**: EKS 1.30+, Kubernetes 1.30+, Metrics Server v0.7+

## æ¦‚è¿°

åœ¨ Kubernetes ç¯å¢ƒä¸­ï¼ŒPod èµ„æºè®¾ç½®ç›´æ¥å½±å“é›†ç¾¤æ•ˆç‡å’Œæˆæœ¬ã€‚**50% çš„å®¹å™¨ä»…ä½¿ç”¨äº†å…¶è¯·æ±‚ CPU çš„ 1/3**ï¼Œè¿™å¯¼è‡´å¹³å‡ 40-60% çš„èµ„æºæµªè´¹ã€‚æœ¬æŒ‡å—é€šè¿‡ Pod çº§åˆ«çš„èµ„æºä¼˜åŒ–ï¼Œæä¾›æœ€å¤§åŒ–é›†ç¾¤æ•ˆç‡å¹¶é™ä½ 30-50% æˆæœ¬çš„å®æˆ˜ç­–ç•¥ã€‚

:::info ä¸ç›¸å…³æ–‡æ¡£çš„åŒºåˆ«
- **[karpenter-autoscaling.md](/docs/infrastructure-optimization/karpenter-autoscaling)**: èŠ‚ç‚¹çº§åˆ«çš„è‡ªåŠ¨æ‰©ç¼©å®¹ï¼ˆæœ¬æ–‡æ¡£æ˜¯ Pod çº§åˆ«ï¼‰
- **[cost-management.md](/docs/infrastructure-optimization/cost-management)**: æ•´ä½“æˆæœ¬ç­–ç•¥ï¼ˆæœ¬æ–‡æ¡£ä¸“æ³¨äºèµ„æºè®¾ç½®ï¼‰
- **[eks-resiliency-guide.md](/docs/operations-observability/eks-resiliency-guide)**: ä»…å°†èµ„æºè®¾ç½®ä½œä¸ºæ£€æŸ¥æ¸…å•é¡¹
:::

### æ ¸å¿ƒå†…å®¹

- **Requests vs Limits æ·±å…¥ç†è§£**: CPU throttling å’Œ OOM Kill æœºåˆ¶
- **QoS ç±»åˆ«ç­–ç•¥**: Guaranteedã€Burstableã€BestEffort çš„å®æˆ˜åº”ç”¨
- **VPA å®Œæ•´æŒ‡å—**: è‡ªåŠ¨èµ„æºè°ƒæ•´ä¸ HPA å…±å­˜æ¨¡å¼
- **Right-Sizing æ–¹æ³•è®º**: åŸºäº P95 çš„èµ„æºä¼°ç®—åŠ Goldilocks ä½¿ç”¨
- **æˆæœ¬å½±å“åˆ†æ**: èµ„æºä¼˜åŒ–çš„å®é™…èŠ‚çœæ•ˆæœ

### å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æŒ‡å—åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

- ç†è§£ CPU å’Œ Memory requests/limits çš„ç²¾ç¡®å·¥ä½œåŸç†
- æ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹æ€§é€‰æ‹©åˆé€‚çš„ QoS ç±»åˆ«
- å®‰å…¨åœ°é…ç½® VPA å’Œ HPA å…±å­˜
- åŸºäºå®é™…ä½¿ç”¨é‡æ‰§è¡Œ Right-Sizing
- å°†èµ„æºæ•ˆç‡æå‡ 30% ä»¥ä¸Š

## å‰ç½®è¦æ±‚

### æ‰€éœ€å·¥å…·

| å·¥å…· | ç‰ˆæœ¬ | ç”¨é€” |
|------|------|------|
| kubectl | 1.28+ | Kubernetes é›†ç¾¤ç®¡ç† |
| helm | 3.12+ | VPAã€Goldilocks å®‰è£… |
| metrics-server | 0.7+ | èµ„æºæŒ‡æ ‡æ”¶é›† |
| kubectl-top | å†…ç½® | èµ„æºä½¿ç”¨é‡ç¡®è®¤ |

### æ‰€éœ€æƒé™

```bash
# RBAC æƒé™ç¡®è®¤
kubectl auth can-i get pods --all-namespaces
kubectl auth can-i get resourcequotas
kubectl auth can-i create verticalpodautoscaler
```

### å‰ç½®çŸ¥è¯†

- Kubernetes Podã€Deployment åŸºæœ¬æ¦‚å¿µ
- YAML æ¸…å•ç¼–å†™ç»éªŒ
- Linux cgroups åŸºæœ¬ç†è§£ï¼ˆæ¨èï¼‰
- Prometheus/Grafana åŸºæœ¬ä½¿ç”¨æ–¹æ³•ï¼ˆæ¨èï¼‰

## Resource Requests & Limits æ·±å…¥ç†è§£

### 2.1 Requests vs Limits çš„ç²¾ç¡®å«ä¹‰

Resource requests å’Œ limits æ˜¯ Kubernetes èµ„æºç®¡ç†çš„æ ¸å¿ƒæ¦‚å¿µã€‚

**Requestsï¼ˆè¯·æ±‚é‡ï¼‰**
- **å®šä¹‰**: è°ƒåº¦å™¨åœ¨ Pod æ”¾ç½®æ—¶ä¿è¯çš„æœ€å°èµ„æº
- **ä½œç”¨**: èŠ‚ç‚¹é€‰æ‹©ä¾æ®ã€QoS ç±»åˆ«å†³å®š
- **ä¿è¯**: kubelet å§‹ç»ˆç¡®ä¿æ­¤é‡çš„å¯ç”¨

**Limitsï¼ˆé™åˆ¶é‡ï¼‰**
- **å®šä¹‰**: kubelet å¼ºåˆ¶æ‰§è¡Œçš„æœ€å¤§èµ„æº
- **ä½œç”¨**: é˜²æ­¢èµ„æºè€—å°½ã€é™åˆ¶å˜ˆæ‚é‚»å±…ï¼ˆnoisy neighborï¼‰
- **å¼ºåˆ¶**: CPU ä½¿ç”¨ throttlingï¼ŒMemory ä½¿ç”¨ OOM Kill

```mermaid
graph TB
    subgraph "èµ„æºåˆ†é…æµç¨‹"
        A[Pod åˆ›å»ºè¯·æ±‚] --> B{Scheduler}
        B -->|æ£€æŸ¥ Requests| C[é€‰æ‹©åˆé€‚çš„èŠ‚ç‚¹]
        C --> D[kubelet]
        D -->|è®¾ç½® cgroups| E[Container Runtime]

        subgraph "è¿è¡Œæ—¶æ§åˆ¶"
            E --> F{å®é™…ä½¿ç”¨é‡}
            F -->|CPU > Limit| G[CPU Throttling]
            F -->|Memory > Limit| H[OOM Kill]
            F -->|æ­£å¸¸èŒƒå›´| I[æ­£å¸¸è¿è¡Œ]
        end
    end

    style G fill:#ff6b6b
    style H fill:#ff0000,color:#fff
    style I fill:#51cf66
```

**æ ¸å¿ƒå·®å¼‚**

| å±æ€§ | CPU | Memory |
|------|-----|--------|
| **è¶…è¿‡ Requests æ—¶** | å¦‚æœå…¶ä»– Pod æœªä½¿ç”¨åˆ™å¯ä»¥ä½¿ç”¨ | å¦‚æœå…¶ä»– Pod æœªä½¿ç”¨åˆ™å¯ä»¥ä½¿ç”¨ |
| **è¶…è¿‡ Limits æ—¶** | **Throttling**ï¼ˆè¿›ç¨‹é€Ÿåº¦é™ä½ï¼‰ | **OOM Kill**ï¼ˆè¿›ç¨‹å¼ºåˆ¶ç»ˆæ­¢ï¼‰ |
| **æ˜¯å¦å¯å‹ç¼©** | å¯å‹ç¼© (Compressible) | ä¸å¯å‹ç¼© (Incompressible) |
| **è¶…é¢ä½¿ç”¨é£é™©** | æ€§èƒ½ä¸‹é™ | æœåŠ¡ä¸­æ–­ |

### 2.2 CPU èµ„æºæ·±å…¥ç†è§£

#### CPU Millicore å•ä½

```yaml
# CPU è¡¨ç¤ºæ–¹æ³•
resources:
  requests:
    cpu: "500m"    # 500 millicore = 0.5 CPU core
    cpu: "1"       # 1000 millicore = 1 CPU core
    cpu: "2.5"     # 2500 millicore = 2.5 CPU cores
```

**1 CPU core = 1000 millicore**
- AWS vCPUã€Azure vCore å‡ç›¸åŒ
- åœ¨è¶…çº¿ç¨‹ç¯å¢ƒä¸­ä¹Ÿæ˜¯ä»¥é€»è¾‘æ ¸å¿ƒä¸ºåŸºå‡†

#### CFS Bandwidth Throttling

Linux CFS (Completely Fair Scheduler) å¼ºåˆ¶æ‰§è¡Œ CPU limitsï¼š

```bash
# cgroups v2 åŸºå‡†
/sys/fs/cgroup/cpu.max
# ç¤ºä¾‹: "100000 100000" = æ¯ 100ms å‘¨æœŸå¯ä½¿ç”¨ 100ms (100% = 1 CPU)
# ç¤ºä¾‹: "50000 100000" = æ¯ 100ms å‘¨æœŸå¯ä½¿ç”¨ 50ms (50% = 0.5 CPU)
```

**Throttling æœºåˆ¶**

```
æ—¶é—´å‘¨æœŸ: 100ms
CPU Limit: 500m (0.5 CPU)
â†’ 100ms ä¸­ä»…å¯ä½¿ç”¨ 50ms

å®é™…è¿è¡Œ:
[0-50ms] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (è¿è¡Œ)
[50-100ms] ...................... (throttled)
[100-150ms] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (è¿è¡Œ)
[150-200ms] ...................... (throttled)
```

:::warning ä¸è®¾ç½® CPU Limits çš„ç­–ç•¥
Googleã€Datadog ç­‰å¤§è§„æ¨¡é›†ç¾¤è¿è¥ç»„ç»‡ä¸è®¾ç½® CPU limitsï¼š

**åŸå› ï¼š**
- CPU æ˜¯å¯å‹ç¼©èµ„æºï¼ˆå…¶ä»– Pod éœ€è¦æ—¶ä¼šè‡ªåŠ¨è°ƒæ•´ï¼‰
- é¿å…å›  Throttling å¯¼è‡´çš„ä¸å¿…è¦æ€§èƒ½ä¸‹é™
- ä»…é€šè¿‡ Requests å³å¯å®ç°è°ƒåº¦å’Œ QoS æ§åˆ¶

**æ›¿ä»£æ¨èï¼š**
- CPU requests åŸºäº P95 ä½¿ç”¨é‡è®¾ç½®
- é€šè¿‡ HPA æŒ‰è´Ÿè½½æ°´å¹³æ‰©å±•
- åŠ å¼ºèŠ‚ç‚¹çº§åˆ«èµ„æºç›‘æ§

**ä¾‹å¤–ï¼ˆéœ€è¦è®¾ç½® Limitsï¼‰ï¼š**
- æ‰¹å¤„ç†ä½œä¸šï¼ˆé˜²æ­¢ CPU ç‹¬å ï¼‰
- ä¸å¯ä¿¡çš„å·¥ä½œè´Ÿè½½
- å¤šç§Ÿæˆ·ç¯å¢ƒ
:::

#### CPU èµ„æºè®¾ç½®ç¤ºä¾‹

```yaml
# æ¨¡å¼ 1: ä»…è®¾ç½® Requestsï¼ˆæ¨èï¼‰
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    resources:
      requests:
        cpu: "250m"       # åŸºäº P95 ä½¿ç”¨é‡
        memory: "128Mi"
      # çœç•¥ limits - åˆ©ç”¨ CPU å¯å‹ç¼©èµ„æºç‰¹æ€§

---
# æ¨¡å¼ 2: æ‰¹å¤„ç†ä½œä¸šï¼ˆè®¾ç½® Limitsï¼‰
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing
spec:
  template:
    spec:
      containers:
      - name: processor
        image: data-processor:v1
        resources:
          requests:
            cpu: "1000m"
          limits:
            cpu: "2000m"   # é˜²æ­¢ CPU ç‹¬å 
            memory: "4Gi"
      restartPolicy: OnFailure
```

### 2.3 Memory èµ„æºæ·±å…¥ç†è§£

#### Memory å•ä½

```yaml
# Memory è¡¨ç¤ºæ–¹æ³• (1024 åŸºå‡† vs 1000 åŸºå‡†)
resources:
  requests:
    memory: "128Mi"    # 128 * 1024^2 bytes = 134,217,728 bytes
    memory: "128M"     # 128 * 1000^2 bytes = 128,000,000 bytes
    memory: "1Gi"      # 1 * 1024^3 bytes = 1,073,741,824 bytes
    memory: "1G"       # 1 * 1000^3 bytes = 1,000,000,000 bytes
```

**æ¨è**: **ä½¿ç”¨ Miã€Gi**ï¼ˆ1024 åŸºå‡†ï¼ŒKubernetes æ ‡å‡†ï¼‰

#### OOM Kill æœºåˆ¶

å½“è¶…è¿‡ Memory limits æ—¶ï¼ŒLinux OOM Killer ä¼šå¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ï¼š

```
å®é™…ä½¿ç”¨é‡ > Memory Limit
â†’ è¶…è¿‡ cgroup memory.max
â†’ Kernel OOM Killer è§¦å‘
â†’ è¿›ç¨‹ SIGKILL
â†’ Pod çŠ¶æ€: OOMKilled
â†’ kubelet é‡å¯ Podï¼ˆéµå¾ª RestartPolicyï¼‰
```

**OOM Score è®¡ç®—**

```bash
# æŸ¥çœ‹æ¯ä¸ªè¿›ç¨‹çš„ OOM Score
cat /proc/<PID>/oom_score

# OOM Score è®¡ç®—å› ç´ 
# 1. å†…å­˜ä½¿ç”¨é‡ï¼ˆè¶Šé«˜åˆ†æ•°è¶Šé«˜ï¼‰
# 2. oom_score_adj å€¼ï¼ˆæ¯ä¸ª QoS ç±»åˆ«ä¸åŒï¼‰
# 3. root è¿›ç¨‹ä¿æŠ¤ï¼ˆ-1000 = ç»ä¸ Killï¼‰
```

:::danger Memory limits å¿…é¡»è®¾ç½®
Memory æ˜¯ä¸å¯å‹ç¼©èµ„æºï¼Œå› æ­¤**å¿…é¡»è®¾ç½® limits**ï¼š

**åŸå› ï¼š**
- Memory è€—å°½æ—¶æ•´ä¸ªèŠ‚ç‚¹ä¸ç¨³å®š
- å¯èƒ½å¯¼è‡´ Kernel Panic
- å½±å“å…¶ä»– Podï¼ˆèŠ‚ç‚¹ Evictionï¼‰

**æ¨èè®¾ç½®ï¼š**
- `requests = limits`ï¼ˆGuaranteed QoSï¼‰
- æˆ– `limits = requests * 1.5`ï¼ˆBurstable QoSï¼‰
- JVM åº”ç”¨ï¼šHeap å¤§å°è®¾ç½®ä¸º limits çš„ 75%
:::

#### Memory èµ„æºè®¾ç½®ç¤ºä¾‹

```yaml
# æ¨¡å¼ 1: Guaranteed QoSï¼ˆç¨³å®šæ€§ä¼˜å…ˆï¼‰
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: postgres
        image: postgres:16
        resources:
          requests:
            cpu: "2000m"
            memory: "4Gi"
          limits:
            cpu: "2000m"      # ä¸ requests ç›¸åŒ
            memory: "4Gi"     # ä¸ requests ç›¸åŒ (Guaranteed)

---
# æ¨¡å¼ 2: JVM åº”ç”¨
apiVersion: apps/v1
kind: Deployment
metadata:
  name: java-app
spec:
  template:
    spec:
      containers:
      - name: app
        image: java-app:v1
        env:
        - name: JAVA_OPTS
          value: "-Xmx3072m -Xms3072m"  # limits çš„ 75% (4Gi * 0.75 = 3Gi)
        resources:
          requests:
            memory: "4Gi"
          limits:
            memory: "4Gi"

---
# æ¨¡å¼ 3: Node.js åº”ç”¨
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-api
spec:
  template:
    spec:
      containers:
      - name: api
        image: nodejs-api:v2
        env:
        - name: NODE_OPTIONS
          value: "--max-old-space-size=896"  # limits çš„ 70% (1280Mi * 0.7 = 896Mi)
        resources:
          requests:
            memory: "1280Mi"
          limits:
            memory: "1280Mi"
```

### 2.4 Ephemeral Storage

å®¹å™¨æœ¬åœ°å­˜å‚¨ä¹Ÿå¯ä»¥ä½œä¸ºèµ„æºè¿›è¡Œç®¡ç†ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-demo
spec:
  containers:
  - name: app
    image: busybox
    resources:
      requests:
        ephemeral-storage: "2Gi"    # æœ€ä½ä¿è¯
      limits:
        ephemeral-storage: "4Gi"    # æœ€å¤§ä½¿ç”¨é‡
    volumeMounts:
    - name: cache
      mountPath: /cache
  volumes:
  - name: cache
    emptyDir:
      sizeLimit: "4Gi"
```

**Ephemeral Storage åŒ…å«é¡¹ï¼š**
- å®¹å™¨å±‚å†™å…¥
- æ—¥å¿—æ–‡ä»¶ï¼ˆ`/var/log`ï¼‰
- emptyDir å·
- ä¸´æ—¶æ–‡ä»¶

**èŠ‚ç‚¹ Eviction é˜ˆå€¼ï¼š**

```yaml
# kubelet è®¾ç½®
evictionHard:
  nodefs.available: "10%"      # èŠ‚ç‚¹æ•´ä½“ç£ç›˜ä½äº 10% æ—¶ eviction
  nodefs.inodesFree: "5%"      # inode ä½äº 5% æ—¶ eviction
  imagefs.available: "10%"     # é•œåƒæ–‡ä»¶ç³»ç»Ÿä½äº 10% æ—¶ eviction
```

### 2.5 EKS Auto Mode èµ„æºä¼˜åŒ–

EKS Auto Mode æ˜¯ä¸€ç§å®Œå…¨æ‰˜ç®¡çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæå¤§åœ°é™ä½ Kubernetes é›†ç¾¤è¿ç»´çš„å¤æ‚æ€§ã€‚å®ƒä»è®¡ç®—ã€å­˜å‚¨ã€ç½‘ç»œçš„èµ„æºé…ç½®åˆ°æŒç»­ç»´æŠ¤å…¨éƒ¨è‡ªåŠ¨åŒ–ï¼Œä½¿è¿ç»´å›¢é˜Ÿèƒ½å¤Ÿä¸“æ³¨äºåº”ç”¨å¼€å‘è€ŒéåŸºç¡€è®¾æ–½ç®¡ç†ã€‚

#### 2.5.1 Auto Mode æ¦‚è¿°

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
- **ä¸€é”®æ¿€æ´»**: åˆ›å»ºé›†ç¾¤æ—¶åªéœ€ `--compute-config autoMode` æ ‡å¿—å³å¯æ¿€æ´»
- **è‡ªåŠ¨åŸºç¡€è®¾æ–½é…ç½®**: æ ¹æ® Pod è°ƒåº¦éœ€æ±‚è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ä¾‹ç±»å‹
- **æŒç»­ç»´æŠ¤**: OS è¡¥ä¸ã€å®‰å…¨æ›´æ–°ã€æ ¸å¿ƒ Add-on ç®¡ç†è‡ªåŠ¨åŒ–
- **æˆæœ¬ä¼˜åŒ–**: è‡ªåŠ¨ä½¿ç”¨ Graviton å¤„ç†å™¨å’Œ Spot å®ä¾‹
- **é›†æˆå®‰å…¨**: AWS å®‰å…¨æœåŠ¡é»˜è®¤é›†æˆ

```bash
# Auto Mode é›†ç¾¤åˆ›å»º
aws eks create-cluster \
  --name my-auto-cluster \
  --compute-config autoMode=ENABLED \
  --kubernetes-network-config serviceIpv4Cidr=10.100.0.0/16 \
  --access-config bootstrapClusterCreatorAdminPermissions=true
```

:::info Auto Mode vs æ‰‹åŠ¨ç®¡ç†
Auto Mode å¹¶éå®Œå…¨æ›¿ä»£ç°æœ‰çš„æ‰‹åŠ¨ç®¡ç†æ–¹å¼ï¼Œè€Œæ˜¯ä¸ºå¸Œæœ›æœ€å°åŒ–è¿ç»´å¼€é”€çš„å›¢é˜Ÿæä¾›çš„**è¡¥å……é€‰æ‹©**ã€‚å¦‚æœéœ€è¦ç²¾ç»†æ§åˆ¶ï¼Œä»ç„¶å¯ä»¥é€‰æ‹©æ‰‹åŠ¨ç®¡ç†æ–¹å¼ã€‚
:::

#### 2.5.2 Auto Mode vs æ‰‹åŠ¨ç®¡ç†å¯¹æ¯”

| é¡¹ç›® | æ‰‹åŠ¨ç®¡ç† | Auto Mode |
|------|----------|-----------|
| **èŠ‚ç‚¹é…ç½®** | Managed Node Groupã€Self-managedã€Karpenter ç›´æ¥é…ç½® | è‡ªåŠ¨é…ç½®ï¼ˆåŸºäº EC2 Managed Instancesï¼‰ |
| **å®ä¾‹ç±»å‹é€‰æ‹©** | æ‰‹åŠ¨é€‰æ‹©å¹¶é…ç½® NodePool | åŸºäº Pod éœ€æ±‚è‡ªåŠ¨é€‰æ‹©ï¼ˆGraviton ä¼˜å…ˆï¼‰ |
| **VPA è®¾ç½®** | éœ€è¦æ‰‹åŠ¨å®‰è£…å’Œé…ç½® | ä¸éœ€è¦ï¼ˆè‡ªåŠ¨èµ„æºä¼˜åŒ–ï¼‰ |
| **HPA è®¾ç½®** | æ‰‹åŠ¨è®¾ç½®å’ŒæŒ‡æ ‡é…ç½® | å¯è‡ªåŠ¨é…ç½®ï¼ˆå¼€å‘è€…ä»…éœ€å£°æ˜ï¼‰ |
| **OS è¡¥ä¸** | æ‰‹åŠ¨æˆ–è‡ªåŠ¨åŒ–è„šæœ¬ | å®Œå…¨è‡ªåŠ¨ï¼ˆé›¶å®•æœºï¼‰ |
| **å®‰å…¨æ›´æ–°** | æ‰‹åŠ¨åº”ç”¨ | è‡ªåŠ¨åº”ç”¨ |
| **æ ¸å¿ƒ Add-on ç®¡ç†** | æ‰‹åŠ¨å‡çº§ (CoreDNS, kube-proxy, VPC CNI) | è‡ªåŠ¨å‡çº§ |
| **æˆæœ¬ä¼˜åŒ–** | æ‰‹åŠ¨é…ç½® Spotã€Graviton | è‡ªåŠ¨ä½¿ç”¨ï¼ˆæœ€é«˜èŠ‚çœ 90%ï¼‰ |
| **Request/Limit è®¾ç½®** | å¼€å‘è€…è´Ÿè´£ï¼ˆå¿…é¡»ï¼‰ | å¼€å‘è€…è´Ÿè´£ï¼ˆä»ç„¶å¿…é¡»ï¼‰ |
| **èµ„æºæ•ˆç‡** | VPA Off æ¨¡å¼ + æ‰‹åŠ¨åº”ç”¨ | è‡ªåŠ¨ Right-Sizingï¼ˆæŒç»­ï¼‰ |
| **å­¦ä¹ æ›²çº¿** | é«˜ï¼ˆéœ€è¦ Kubernetesã€AWS ä¸“ä¸šçŸ¥è¯†ï¼‰ | ä½ï¼ˆä»…éœ€ Kubernetes åŸºç¡€ï¼‰ |
| **è¿ç»´å¼€é”€** | é«˜ | æœ€å° |

:::warning Auto Mode ä¸­å¼€å‘è€…çš„è´£ä»»
Auto Mode è‡ªåŠ¨åŒ–äº†åŸºç¡€è®¾æ–½ï¼Œä½† **Pod çº§åˆ«çš„ requests/limits è®¾ç½®ä»ç„¶æ˜¯å¼€å‘è€…çš„è´£ä»»**ã€‚è¿™æ˜¯å› ä¸ºæœ€äº†è§£åº”ç”¨å®é™…èµ„æºéœ€æ±‚çš„äººæ˜¯å¼€å‘è€…ã€‚
:::

#### 2.5.3 Graviton + Spot ç»„åˆä¼˜åŒ–

Auto Mode æ™ºèƒ½åœ°ç»„åˆ AWS Graviton å¤„ç†å™¨å’Œ Spot å®ä¾‹ï¼Œæœ€å¤§åŒ–æˆæœ¬æ•ˆç‡ã€‚

**Graviton å¤„ç†å™¨çš„ä¼˜åŠ¿ï¼š**
- **æ€§ä»·æ¯”æå‡ 40%**ï¼ˆç›¸æ¯” x86ï¼‰
- æœ€é€‚åˆé€šç”¨å·¥ä½œè´Ÿè½½ã€Web æœåŠ¡å™¨ã€å®¹å™¨åŒ–å¾®æœåŠ¡
- æ”¯æŒ Arm64 æ¶æ„ï¼ˆå¤§å¤šæ•°å®¹å™¨é•œåƒå…¼å®¹ï¼‰

**Spot å®ä¾‹èŠ‚çœï¼š**
- **æœ€é«˜èŠ‚çœ 90% æˆæœ¬**ï¼ˆç›¸æ¯” On-Demandï¼‰
- Auto Mode è‡ªåŠ¨ç›‘æ§ Spot å¯ç”¨æ€§å¹¶å¤„ç† Fallback
- ä¸­æ–­å‰ 2 åˆ†é’Ÿé€šçŸ¥ï¼Œä¿è¯ Graceful Termination

```mermaid
graph TB
    subgraph "Auto Mode å®ä¾‹é€‰æ‹©é€»è¾‘"
        A[Pod è°ƒåº¦è¯·æ±‚] --> B{èµ„æºéœ€æ±‚åˆ†æ}
        B --> C[ä¼˜å…ˆå°è¯• Graviton Spot]
        C --> D{æ£€æŸ¥ Spot å¯ç”¨æ€§}
        D -->|å¯ç”¨| E[é…ç½® Graviton Spot å®ä¾‹]
        D -->|ä¸å¯ç”¨| F[å°è¯• Graviton On-Demand]
        F --> G{On-Demand å¯ç”¨æ€§}
        G -->|å¯ç”¨| H[é…ç½® Graviton On-Demand]
        G -->|ä¸å¯ç”¨| I[x86 Spot/On-Demand Fallback]

        E --> J[Pod æ”¾ç½®å®Œæˆ]
        H --> J
        I --> J
    end

    style E fill:#51cf66
    style H fill:#ffa94d
    style I fill:#ff6b6b
```

**NodePool YAML ç¤ºä¾‹ï¼ˆæ‰‹åŠ¨ç®¡ç†é›†ç¾¤ - åŸºäº Karpenterï¼‰ï¼š**

```yaml
# Auto Mode ä¼šè‡ªåŠ¨åˆ›å»ºæ­¤ç±» NodePoolï¼Œ
# ä»¥ä¸‹å±•ç¤ºæ‰‹åŠ¨è®¾ç½®æ—¶çš„ Graviton + Spot æ¨¡å¼ä¾›å‚è€ƒ
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: graviton-spot-pool
spec:
  template:
    spec:
      requirements:
      # Graviton å®ä¾‹ä¼˜å…ˆ
      - key: kubernetes.io/arch
        operator: In
        values: ["arm64"]

      # Spot ä¼˜å…ˆï¼ŒFallback åˆ° On-Demand
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot", "on-demand"]

      # é€šç”¨å·¥ä½œè´Ÿè½½å®ä¾‹æ—
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["m7g.medium", "m7g.large", "m7g.xlarge", "m7g.2xlarge"]

      nodeClassRef:
        name: default

  # Spot ä¸­æ–­å¤„ç†
  disruption:
    consolidationPolicy: WhenUnderutilized
    expireAfter: 720h

  # èµ„æºé™åˆ¶
  limits:
    cpu: "1000"
    memory: "1000Gi"

---
# Fallback: x86 On-Demandï¼ˆSpot ä¸å¯ç”¨æ—¶ï¼‰
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: x86-ondemand-fallback
spec:
  weight: 10  # ä½ä¼˜å…ˆçº§
  template:
    spec:
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]

      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand"]

      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["m6i.large", "m6i.xlarge", "m6i.2xlarge"]

      nodeClassRef:
        name: default
```

**Auto Mode ä¸­çš„è‡ªåŠ¨å¤„ç†ï¼š**

Auto Mode æ— éœ€æ‰‹åŠ¨ç¼–å†™ä¸Šè¿° NodePool é…ç½®ï¼Œå®ƒä¼šåˆ†æ Pod çš„èµ„æºéœ€æ±‚å’Œå·¥ä½œè´Ÿè½½ç‰¹æ€§ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ä¾‹ã€‚

```yaml
# Auto Mode ç¯å¢ƒä¸­å¼€å‘è€…ç¼–å†™çš„ Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: production
spec:
  replicas: 10
  template:
    spec:
      containers:
      - name: nginx
        image: nginx:1.25-arm64  # Graviton ç”¨é•œåƒ
        resources:
          requests:
            cpu: "250m"
            memory: "512Mi"
          limits:
            memory: "1Gi"

      # Auto Mode è‡ªåŠ¨æ‰§è¡Œï¼š
      # 1. å°è¯•é€‰æ‹© Graviton Spot å®ä¾‹
      # 2. Spot ä¸å¯ç”¨æ—¶ Fallback åˆ° Graviton On-Demand
      # 3. è‡ªåŠ¨é€‰æ‹©å®ä¾‹ç±»å‹ (m7g.large ç­‰)
      # 4. èŠ‚ç‚¹é…ç½®åŠ Pod æ”¾ç½®
```

:::tip Graviton é•œåƒå‡†å¤‡
è¦ä½¿ç”¨ Graviton å®ä¾‹ï¼Œéœ€è¦ **arm64 æ¶æ„çš„å®¹å™¨é•œåƒ**ã€‚å¤§å¤šæ•°å®˜æ–¹é•œåƒæ”¯æŒ multi-archï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨ç›¸åŒçš„é•œåƒæ ‡ç­¾åœ¨ Graviton å’Œ x86 ä¸Šè¿è¡Œã€‚

```bash
# æ£€æŸ¥ multi-arch é•œåƒ
docker manifest inspect nginx:1.25 | jq '.manifests[].platform'

# è¾“å‡ºç¤ºä¾‹ï¼š
# { "architecture": "amd64", "os": "linux" }
# { "architecture": "arm64", "os": "linux" }
```
:::

**å®é™…æˆæœ¬èŠ‚çœç¤ºä¾‹ï¼š**

| åœºæ™¯ | å®ä¾‹ç±»å‹ | æ¯å°æ—¶æˆæœ¬ | æœˆæˆæœ¬ (730å°æ—¶) | èŠ‚çœç‡ |
|---------|-------------|-----------|-------------------|--------|
| x86 On-Demand | m6i.2xlarge | $0.384 | $280.32 | - |
| Graviton On-Demand | m7g.2xlarge | $0.3264 | $238.27 | 15% |
| Graviton Spot | m7g.2xlarge | $0.0979 | $71.47 | 75% |

ä»¥ 10 ä¸ªèŠ‚ç‚¹ä¸ºåŸºå‡†ï¼š
- x86 On-Demand: $2,803/æœˆ
- Graviton On-Demand: $2,383/æœˆ (èŠ‚çœ 15%)
- **Graviton Spot: $715/æœˆ (èŠ‚çœ 75%)** â­

**Graviton4 ä¸“é¡¹ä¼˜åŒ–ï¼š**

Graviton4 (R8g, M8g, C8g) å®ä¾‹ç›¸æ¯” Graviton3 æä¾›äº† **30% çš„è®¡ç®—æ€§èƒ½æå‡**å’Œ **75% çš„å†…å­˜å¸¦å®½æå‡**ã€‚

| ä»£é™… | å®ä¾‹æ— | æ€§èƒ½æå‡ | ä¸»è¦å·¥ä½œè´Ÿè½½ |
|------|---------------|---------|-------------|
| Graviton3 | m7g, c7g, r7g | åŸºå‡† | é€šç”¨ Web/APIã€å®¹å™¨ |
| **Graviton4** | **m8g, c8g, r8g** | **+30% è®¡ç®—, +75% å†…å­˜** | **é«˜æ€§èƒ½æ•°æ®åº“ã€ML æ¨ç†ã€å®æ—¶åˆ†æ** |

**ARM64 Multi-Arch æ„å»ºæµæ°´çº¿ï¼š**

è¦å……åˆ†åˆ©ç”¨ Graviton å®ä¾‹ï¼Œéœ€è¦åŒæ—¶æ”¯æŒ ARM64 å’Œ AMD64 çš„ multi-arch å®¹å™¨é•œåƒã€‚

```dockerfile
# Multi-arch Dockerfile ç¤ºä¾‹
FROM --platform=$BUILDPLATFORM golang:1.22-alpine AS builder
ARG TARGETOS TARGETARCH

WORKDIR /app
COPY . .

# é’ˆå¯¹ç›®æ ‡æ¶æ„æ„å»º
RUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o app .

# è¿è¡Œæ—¶é•œåƒ
FROM alpine:3.19
COPY --from=builder /app/app /usr/local/bin/app
ENTRYPOINT ["/usr/local/bin/app"]
```

**GitHub Actions CI/CD ä¸­çš„ multi-arch æ„å»ºï¼š**

```yaml
# .github/workflows/build.yml
name: Build Multi-Arch Image
on:
  push:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push multi-arch
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64,linux/arm64  # åŒ…å« ARM64
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/myapp:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/myapp:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
```

**Graviton3 â†’ Graviton4 è¿ç§»åŸºå‡†æµ‹è¯•è¦ç‚¹ï¼š**

```yaml
# Graviton4 ä¼˜å…ˆ NodePool ç¤ºä¾‹ (Karpenter)
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: graviton4-spot-pool
spec:
  template:
    spec:
      requirements:
      # Graviton4 ä¼˜å…ˆï¼ŒGraviton3 Fallback
      - key: node.kubernetes.io/instance-type
        operator: In
        values:
          # Graviton4ï¼ˆæœ€ä¼˜å…ˆï¼‰
          - "m8g.medium"
          - "m8g.large"
          - "m8g.xlarge"
          - "m8g.2xlarge"
          # Graviton3ï¼ˆFallbackï¼‰
          - "m7g.medium"
          - "m7g.large"
          - "m7g.xlarge"
          - "m7g.2xlarge"

      - key: kubernetes.io/arch
        operator: In
        values: ["arm64"]

      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot", "on-demand"]

      nodeClassRef:
        name: default

  disruption:
    consolidationPolicy: WhenUnderutilized
    consolidateAfter: 30s

  limits:
    cpu: "1000"
    memory: "2000Gi"
```

**Graviton4 æ€§èƒ½åŸºå‡†æµ‹è¯•æ£€æŸ¥ç‚¹ï¼š**

è¿ç§»æ—¶ç›‘æ§ä»¥ä¸‹æŒ‡æ ‡ä»¥éªŒè¯æ€§èƒ½æå‡ï¼š

| æŒ‡æ ‡ | Graviton3 åŸºå‡† | Graviton4 ç›®æ ‡ | æµ‹é‡æ–¹æ³• |
|-------|--------------|--------------|---------|
| **P99 å“åº”æ—¶é—´** | 100ms | 70ms (-30%) | Prometheus `http_request_duration_seconds` |
| **ååé‡ (RPS)** | 1000 req/s | 1300 req/s (+30%) | è´Ÿè½½æµ‹è¯• (k6, Locust) |
| **å†…å­˜å¸¦å®½** | 205 GB/s | 358 GB/s (+75%) | `sysbench memory` |
| **CPU ä½¿ç”¨ç‡** | 60% | 45% (-25%) | `node_cpu_seconds_total` |

```bash
# Graviton4 æ€§èƒ½æµ‹è¯•è„šæœ¬
#!/bin/bash
# 1. å†…å­˜å¸¦å®½æµ‹è¯•
sysbench memory --memory-total-size=100G --memory-oper=write run

# 2. CPU åŸºå‡†æµ‹è¯•
sysbench cpu --cpu-max-prime=20000 --threads=8 run

# 3. åº”ç”¨è´Ÿè½½æµ‹è¯• (k6)
k6 run --vus 100 --duration 5m loadtest.js

# 4. Prometheus æŒ‡æ ‡æ”¶é›†
curl -s http://localhost:9090/api/v1/query?query=rate(http_request_duration_seconds_sum[5m]) | jq .
```

:::tip Graviton4 è¿ç§»æ£€æŸ¥æ¸…å•

- [ ] **å®¹å™¨é•œåƒ**: ç¡®è®¤ ARM64 æ”¯æŒ (`docker manifest inspect`)
- [ ] **ä¾èµ–åº“**: éªŒè¯ ARM64 å…¼å®¹æ€§
- [ ] **CI/CD æµæ°´çº¿**: å¯ç”¨ Multi-arch æ„å»º
- [ ] **NodePool ä¼˜å…ˆçº§**: è®¾ç½® Graviton4 â†’ Graviton3 â†’ x86 é¡ºåº
- [ ] **æ€§èƒ½åŸºå‡†æµ‹è¯•**: æµ‹é‡ P99 å»¶è¿Ÿã€ååé‡ã€CPU ä½¿ç”¨ç‡
- [ ] **æˆæœ¬åˆ†æ**: è®¡ç®—ç›¸æ¯” Graviton3 çš„æ€§ä»·æ¯”
:::

#### 2.5.4 Auto Mode ç¯å¢ƒçš„èµ„æºè®¾ç½®å»ºè®®

Auto Mode è™½ç„¶è‡ªåŠ¨åŒ–äº†å¾ˆå¤šéƒ¨åˆ†ï¼Œä½†å¼€å‘è€…ä»éœ€å‡†ç¡®è®¾ç½®åº”ç”¨çš„èµ„æºéœ€æ±‚ã€‚

**Auto Mode è‡ªåŠ¨å¤„ç†çš„é¡¹ç›®ï¼š**

| é¡¹ç›® | æ‰‹åŠ¨ç®¡ç† | Auto Mode |
|------|----------|-----------|
| èŠ‚ç‚¹é…ç½® | Karpenterã€Managed Node Group è®¾ç½® | è‡ªåŠ¨ |
| å®ä¾‹ç±»å‹é€‰æ‹© | åœ¨ NodePool ä¸­æ‰‹åŠ¨æŒ‡å®š | åŸºäº Pod requests è‡ªåŠ¨é€‰æ‹© |
| Spot/On-Demand åˆ‡æ¢ | æ‰‹åŠ¨æˆ– Karpenter è®¾ç½® | è‡ªåŠ¨ Fallback |
| èŠ‚ç‚¹æ‰©ç¼©å®¹ | HPA + Cluster Autoscaler/Karpenter | è‡ªåŠ¨ |
| OS è¡¥ä¸ | æ‰‹åŠ¨æˆ–è‡ªåŠ¨åŒ–è„šæœ¬ | è‡ªåŠ¨ï¼ˆé›¶å®•æœºï¼‰ |

**å¼€å‘è€…ä»éœ€è®¾ç½®çš„é¡¹ç›®ï¼š**

| é¡¹ç›® | åŸå›  | æ¨èæ–¹æ³• |
|------|------|----------|
| **CPU Requests** | è°ƒåº¦å†³ç­–ä¾æ® | P95 ä½¿ç”¨é‡ + 20% |
| **Memory Requests** | è°ƒåº¦åŠ OOM é˜²æ­¢ | P95 ä½¿ç”¨é‡ + 20% |
| **Memory Limits** | é˜²æ­¢ OOM Killï¼ˆå¿…é¡»ï¼‰ | Requests Ã— 1.5~2 |
| **CPU Limits** | é€šç”¨å·¥ä½œè´Ÿè½½å»ºè®®ä¸è®¾ç½® | ä»…æ‰¹å¤„ç†ä½œä¸šè®¾ç½® |
| **HPA æŒ‡æ ‡** | æ°´å¹³æ‰©å±•åŸºå‡† | CPU 70%, Custom Metrics |

**Auto Mode ç¯å¢ƒä¸­ VPA è§’è‰²çš„å˜åŒ–ï¼š**

```mermaid
graph TB
    subgraph "æ‰‹åŠ¨ç®¡ç†é›†ç¾¤"
        A1[VPA Recommender] --> A2[ç”Ÿæˆå»ºè®®]
        A2 --> A3[VPA Updater]
        A3 --> A4[é€šè¿‡ Pod é‡å¯æ›´æ”¹èµ„æº]
    end

    subgraph "Auto Mode é›†ç¾¤"
        B1[å†…ç½® Right-Sizing å¼•æ“] --> B2[æŒç»­ä½¿ç”¨é‡åˆ†æ]
        B2 --> B3[è‡ªåŠ¨èµ„æºä¼˜åŒ–]
        B3 --> B4[å‘å¼€å‘è€…æä¾›å»ºè®®]
        B4 --> B5[å¼€å‘è€…æ›´æ–° Deployment]
    end

    style A4 fill:#ffa94d
    style B3 fill:#51cf66
```

**åœ¨ Auto Mode ä¸­ VPAï¼š**
- æ— éœ€å•ç‹¬å®‰è£…
- å†…ç½® Right-Sizing å¼•æ“æŒç»­åˆ†æå·¥ä½œè´Ÿè½½
- å‘å¼€å‘è€…æä¾›å»ºè®®ï¼ˆè€Œéè‡ªåŠ¨åº”ç”¨ï¼‰
- å¼€å‘è€…å®¡æ ¸ååæ˜ åˆ° Deployment æ¸…å•ä¸­

**æ¨èå·¥ä½œæµç¨‹ï¼š**

```bash
# 1. éƒ¨ç½²åˆ° Auto Mode é›†ç¾¤
kubectl apply -f deployment.yaml

# 2. 7-14 å¤©ååœ¨ Auto Mode ä»ªè¡¨æ¿æŸ¥çœ‹å»ºè®®
# (AWS Console â†’ EKS â†’ Clusters â†’ <cluster-name> â†’ Insights)

# 3. å°†å»ºè®®åæ˜ åˆ° Deployment
kubectl set resources deployment web-app \
  --requests=cpu=300m,memory=512Mi \
  --limits=memory=1Gi

# 4. é€šè¿‡ GitOps æ›´æ–°æ¸…å•
git add deployment.yaml
git commit -m "chore: apply Auto Mode resource recommendations"
git push
```

:::tip Auto Mode æ¨èåœºæ™¯
Auto Mode åœ¨ä»¥ä¸‹æƒ…å†µä¸­ç‰¹åˆ«æœ‰ç”¨ï¼š

- **æ–°é›†ç¾¤**: æ— ç°æœ‰åŸºç¡€è®¾æ–½ï¼Œå¿«é€Ÿå¯åŠ¨
- **è¿ç»´èµ„æºä¸è¶³**: å°å›¢é˜Ÿæ—  Kubernetes ä¸“å®¶è¿ç»´
- **æˆæœ¬ä¼˜åŒ–ä¼˜å…ˆ**: è‡ªåŠ¨ä½¿ç”¨ Graviton + Spot ç«‹å³èŠ‚çœ
- **æ ‡å‡†åŒ–å·¥ä½œè´Ÿè½½**: ä¸€èˆ¬çš„ Web/API æœåŠ¡å™¨ã€å¾®æœåŠ¡

**æ¨èæ‰‹åŠ¨ç®¡ç†çš„åœºæ™¯ï¼š**
- **éœ€è¦ç²¾ç»†æ§åˆ¶**: ç‰¹å®šå®ä¾‹ç±»å‹ã€AZ æ”¾ç½®ã€ç½‘ç»œé…ç½®
- **ç°æœ‰ Karpenter æŠ•èµ„**: æ‹¥æœ‰é«˜åº¦å®šåˆ¶çš„ NodePool ç­–ç•¥
- **åˆè§„è¦æ±‚**: ç‰¹å®šç¡¬ä»¶ã€å®‰å…¨ç»„å¼ºåˆ¶
:::

**Auto Mode + æ‰‹åŠ¨ Right-Sizing å¯¹æ¯”ï¼š**

| é¡¹ç›® | æ‰‹åŠ¨ Right-Sizing (VPA Off) | Auto Mode |
|------|---------------------------|-----------|
| åˆå§‹è®¾ç½®å¤æ‚åº¦ | é«˜ï¼ˆVPA å®‰è£…ã€Prometheus é…ç½®ï¼‰ | ä½ï¼ˆåˆ›å»ºé›†ç¾¤æ—¶ä»…éœ€æ ‡å¿—ï¼‰ |
| å»ºè®®ç”Ÿæˆæ—¶é—´ | 7-14 å¤© | 7-14 å¤©ï¼ˆç›¸åŒï¼‰ |
| å»ºè®®å‡†ç¡®åº¦ | é«˜ï¼ˆåŸºäº Prometheusï¼‰ | é«˜ï¼ˆå†…ç½®åˆ†æå¼•æ“ï¼‰ |
| åº”ç”¨æ–¹å¼ | æ‰‹åŠ¨ï¼ˆå¼€å‘è€…ä¿®æ”¹æ¸…å•ï¼‰ | æ‰‹åŠ¨ï¼ˆå¼€å‘è€…ä¿®æ”¹æ¸…å•ï¼‰ |
| æŒç»­ç›‘æ§ | æ‰‹åŠ¨ï¼ˆå®šæœŸæ£€æŸ¥ VPAï¼‰ | è‡ªåŠ¨ï¼ˆä»ªè¡¨æ¿å‘Šè­¦ï¼‰ |
| åŸºç¡€è®¾æ–½ä¼˜åŒ– | æ‰‹åŠ¨ï¼ˆKarpenter è®¾ç½®ï¼‰ | è‡ªåŠ¨ï¼ˆGraviton + Spotï¼‰ |
| æ€»è¿ç»´å¼€é”€ | é«˜ | ä½ |

**ç»“è®ºï¼š**

Auto Mode **æ¶ˆé™¤äº†èµ„æºä¼˜åŒ–çš„å¤æ‚æ€§**ï¼Œä½†**ä¸æ¶ˆé™¤èµ„æºè®¾ç½®çš„è´£ä»»**ã€‚å¼€å‘è€…ä»éœ€è®¾ç½®åº”ç”¨çš„ requests/limitsï¼ŒAuto Mode åŸºäºæ­¤è‡ªåŠ¨é…ç½®æœ€ä¼˜åŸºç¡€è®¾æ–½ã€‚

è¿™é€šè¿‡**"å¼€å‘è€…å®šä¹‰åº”ç”¨éœ€æ±‚ï¼ŒAWS ç®¡ç†åŸºç¡€è®¾æ–½"**çš„æ˜ç¡®èŒè´£åˆ†ç¦»ï¼Œè®©åŒæ–¹éƒ½èƒ½ä¸“æ³¨äºå„è‡ªçš„ä¸“ä¸šé¢†åŸŸã€‚

## QoS (Quality of Service) ç±»åˆ«

### 3.1 ä¸‰ç§ QoS ç±»åˆ«

Kubernetes æ ¹æ®èµ„æºè®¾ç½®å°† Pod åˆ†ä¸º 3 ç§ QoS ç±»åˆ«ï¼š

#### Guaranteedï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰

**æ¡ä»¶ï¼š**
- æ‰€æœ‰å®¹å™¨éƒ½è®¾ç½®äº† CPU å’Œ Memory çš„ requests å’Œ limits
- **requests == limits**ï¼ˆç›¸åŒå€¼ï¼‰

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: guaranteed-pod
  labels:
    qos: guaranteed
spec:
  containers:
  - name: app
    image: nginx:1.25
    resources:
      requests:
        cpu: "500m"
        memory: "256Mi"
      limits:
        cpu: "500m"        # ä¸ requests ç›¸åŒ
        memory: "256Mi"    # ä¸ requests ç›¸åŒ
  - name: sidecar
    image: fluentd:v1
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "100m"
        memory: "128Mi"
```

**ç‰¹ç‚¹ï¼š**
- oom_score_adj: **-997**ï¼ˆæœ€ä½ï¼ŒOOM Kill ä¼˜å…ˆçº§æœ€ä½ï¼‰
- èŠ‚ç‚¹èµ„æºå‹åŠ›æ—¶æœ€åè¢« Eviction
- CPU è°ƒåº¦ä¼˜å…ˆçº§é«˜

#### Burstableï¼ˆä¸­ç­‰ä¼˜å…ˆçº§ï¼‰

**æ¡ä»¶ï¼š**
- è‡³å°‘ 1 ä¸ªå®¹å™¨è®¾ç½®äº† CPU æˆ– Memory requests
- ä¸æ»¡è¶³ Guaranteed æ¡ä»¶

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: burstable-pod
  labels:
    qos: burstable
spec:
  containers:
  - name: app
    image: web-app:v1
    resources:
      requests:
        cpu: "250m"
        memory: "512Mi"
      limits:
        cpu: "1000m"       # å¤§äº requests (Burstable)
        memory: "1Gi"      # å¤§äº requests

  - name: cache
    image: redis:7
    resources:
      requests:
        memory: "256Mi"    # æ—  CPU requests (Burstable)
      limits:
        memory: "512Mi"
```

**ç‰¹ç‚¹ï¼š**
- oom_score_adj: **min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)**
- æ ¹æ®ä½¿ç”¨é‡åŠ¨æ€è°ƒæ•´
- æœ‰ä½™é‡æ—¶å¯ burst

#### BestEffortï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰

**æ¡ä»¶ï¼š**
- æ‰€æœ‰å®¹å™¨éƒ½æœªè®¾ç½® requests å’Œ limits

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: besteffort-pod
  labels:
    qos: besteffort
spec:
  containers:
  - name: app
    image: test-app:latest
    # æ—  resources æ®µæˆ–ä¸ºç©º
```

**ç‰¹ç‚¹ï¼š**
- oom_score_adj: **1000**ï¼ˆæœ€é«˜ï¼ŒOOM Kill æœ€ä¼˜å…ˆï¼‰
- èŠ‚ç‚¹èµ„æºå‹åŠ›æ—¶æœ€å…ˆè¢« Eviction
- å»ºè®®ä»…åœ¨å¼€å‘/æµ‹è¯•ç¯å¢ƒä½¿ç”¨

### 3.2 QoS ä¸ Eviction ä¼˜å…ˆçº§

å½“èŠ‚ç‚¹èµ„æºå‹åŠ›æ—¶ï¼Œkubelet æŒ‰ä»¥ä¸‹é¡ºåº Evict Podï¼š

```mermaid
graph TB
    A[èŠ‚ç‚¹èµ„æºå‹åŠ›] --> B{Eviction å†³ç­–}

    B --> C[ç¬¬ 1 é˜¶æ®µ: BestEffort Pod]
    C --> D{èµ„æºæ¢å¤?}
    D -->|No| E[ç¬¬ 2 é˜¶æ®µ: Burstable Pod<br/>è¶…è¿‡ requests ä½¿ç”¨ä¸­]
    D -->|Yes| Z[Eviction åœæ­¢]

    E --> F{èµ„æºæ¢å¤?}
    F -->|No| G[ç¬¬ 3 é˜¶æ®µ: Burstable Pod<br/>ä½äº requests ä½¿ç”¨ä¸­]
    F -->|Yes| Z

    G --> H{èµ„æºæ¢å¤?}
    H -->|No| I[ç¬¬ 4 é˜¶æ®µ: Guaranteed Pod<br/>ä»…æ’é™¤å¿…è¦ç³»ç»Ÿ Pod]
    H -->|Yes| Z

    I --> Z

    style C fill:#ff6b6b
    style E fill:#ffa94d
    style G fill:#ffd43b
    style I fill:#ff0000,color:#fff
    style Z fill:#51cf66
```

**Eviction é¡ºåºæ‘˜è¦ï¼š**

| é¡ºä½ | QoS ç±»åˆ« | æ¡ä»¶ | oom_score_adj |
|------|-----------|------|---------------|
| 1 (æœ€ä¼˜å…ˆ) | BestEffort | æ‰€æœ‰ Pod | 1000 |
| 2 | Burstable | è¶…è¿‡ requests ä½¿ç”¨ä¸­ | 2-999 (ä¸ä½¿ç”¨é‡æˆæ¯”ä¾‹) |
| 3 | Burstable | ä½äº requests ä½¿ç”¨ä¸­ | 2-999 (ä¸ä½¿ç”¨é‡æˆæ¯”ä¾‹) |
| 4 (æœ€å) | Guaranteed | æ’é™¤ç³»ç»Ÿå…³é”® Pod | -997 |

**oom_score_adj æŸ¥çœ‹æ–¹æ³•ï¼š**

```bash
# æŸ¥æ‰¾ Pod çš„ä¸»å®¹å™¨è¿›ç¨‹
kubectl get pod <pod-name> -o jsonpath='{.status.containerStatuses[0].containerID}'

# åœ¨èŠ‚ç‚¹ä¸ŠæŸ¥çœ‹ oom_score_adj
docker inspect <container-id> | grep Pid
cat /proc/<pid>/oom_score_adj

# ç¤ºä¾‹è¾“å‡º
# BestEffort: 1000
# Burstable: 500 (æ ¹æ®ä½¿ç”¨é‡å˜åŠ¨)
# Guaranteed: -997
```

### 3.3 å®æˆ˜ QoS ç­–ç•¥

æ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹æ€§é€‰æ‹© QoS ç±»åˆ«çš„æŒ‡å—ï¼š

| å·¥ä½œè´Ÿè½½ç±»å‹ | æ¨è QoS | è®¾ç½®æ¨¡å¼ | åŸå›  |
|-------------|---------|----------|------|
| **ç”Ÿäº§ API** | Guaranteed | requests = limits | ç¨³å®šæ€§ä¼˜å…ˆï¼Œé˜²æ­¢ Eviction |
| **æ•°æ®åº“** | Guaranteed | requests = limits | å†…å­˜å‹åŠ›æ—¶ä¹Ÿå—ä¿æŠ¤ |
| **æ‰¹å¤„ç†ä½œä¸š** | Burstable | limits > requests | ç©ºé—²æ—¶åˆ©ç”¨èµ„æºï¼Œæˆæœ¬æ•ˆç‡é«˜ |
| **é˜Ÿåˆ— Worker** | Burstable | limits > requests | åº”å¯¹è´Ÿè½½æ³¢åŠ¨ |
| **å¼€å‘/æµ‹è¯•** | BestEffort | ä¸è®¾ç½® | èµ„æºæ•ˆç‡ï¼ˆç”Ÿäº§ç¯å¢ƒç¦æ­¢ï¼‰ |
| **ç›‘æ§ Agent** | Guaranteed | è®¾ç½®è¾ƒä½å€¼ | ç³»ç»Ÿç¨³å®šæ€§ |

**ç”Ÿäº§æ¨èè®¾ç½®ï¼š**

```yaml
# æ¨¡å¼ 1: å…³é”®ä»»åŠ¡æœåŠ¡ (Guaranteed)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-api
  namespace: production
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: payment-api
        tier: critical
    spec:
      containers:
      - name: api
        image: payment-api:v2.1
        resources:
          requests:
            cpu: "1000m"
            memory: "2Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
      priorityClassName: system-cluster-critical  # é¢å¤–ä¿æŠ¤

---
# æ¨¡å¼ 2: ä¸€èˆ¬ Web æœåŠ¡ (Burstable)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
  namespace: production
spec:
  replicas: 10
  template:
    spec:
      containers:
      - name: frontend
        image: web-frontend:v1.5
        resources:
          requests:
            cpu: "200m"       # P50 ä½¿ç”¨é‡
            memory: "256Mi"
          limits:
            cpu: "500m"       # P95 ä½¿ç”¨é‡
            memory: "512Mi"   # é˜²æ­¢ OOM

---
# æ¨¡å¼ 3: æ‰¹å¤„ç† Worker (Burstable)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-report
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: report-generator
            image: report-gen:v1
            resources:
              requests:
                cpu: "500m"
                memory: "1Gi"
              limits:
                cpu: "4000m"     # å¤œé—´æ—¶æ®µåˆ©ç”¨èµ„æº
                memory: "8Gi"
          restartPolicy: OnFailure
```

## VPA (Vertical Pod Autoscaler) è¯¦ç»†æŒ‡å—

### 4.1 VPA æ¶æ„

VPA ç”± 3 ä¸ªç»„ä»¶æ„æˆï¼š

```mermaid
graph TB
    subgraph "VPA æ¶æ„"
        subgraph "æŒ‡æ ‡æ”¶é›†"
            MS[Metrics Server] -->|èµ„æºæŒ‡æ ‡| PROM[Prometheus]
            PROM -->|æ—¶é—´åºåˆ—æ•°æ®| REC[VPA Recommender]
        end

        subgraph "VPA ç»„ä»¶"
            REC -->|è®¡ç®—æ¨èå€¼| VPA_OBJ[VPA CRD Object]
            VPA_OBJ -->|æ£€æŸ¥æ¨¡å¼| UPD[VPA Updater]
            VPA_OBJ -->|éªŒè¯æ–° Pod| ADM[VPA Admission Controller]

            UPD -->|é‡å¯ Pod| POD[Running Pods]
            ADM -->|æ³¨å…¥èµ„æº| NEW_POD[New Pods]
        end

        subgraph "å·¥ä½œè´Ÿè½½"
            POD -->|ä¸ŠæŠ¥ä½¿ç”¨é‡| MS
            NEW_POD -->|ä¸ŠæŠ¥ä½¿ç”¨é‡| MS
        end
    end

    style REC fill:#4dabf7
    style UPD fill:#ffa94d
    style ADM fill:#51cf66
```

**ç»„ä»¶è§’è‰²ï¼š**

| ç»„ä»¶ | è§’è‰² | æ•°æ®æº |
|---------|------|-----------|
| **Recommender** | åˆ†æå†å²ä½¿ç”¨é‡ï¼Œè®¡ç®—æ¨èå€¼ | Metrics Serverã€Prometheus |
| **Updater** | Auto æ¨¡å¼ä¸‹é‡å¯ Pod | VPA CRD çŠ¶æ€ |
| **Admission Controller** | è‡ªåŠ¨å‘æ–° Pod æ³¨å…¥èµ„æº | VPA CRD æ¨èå€¼ |

#### 4.1.4 VPA Recommender ML ç®—æ³•è¯¦è§£

VPA Recommender å¹¶éç®€å•çš„å¹³å‡å€¼è®¡ç®—ï¼Œè€Œæ˜¯åŸºäºæœºå™¨å­¦ä¹ çš„ç²¾ç»†ç®—æ³•æ¥è®¡ç®—èµ„æºæ¨èå€¼ã€‚

##### æŒ‡æ•°åŠ æƒç›´æ–¹å›¾ (Exponentially-weighted Histogram)

VPA Recommender çš„æ ¸å¿ƒæ˜¯éšæ—¶é—´è¡°å‡çš„åŠ æƒç›´æ–¹å›¾ï¼š

```
æœ€è¿‘æ•°æ® â†’ é«˜æƒé‡
å†å²æ•°æ® â†’ ä½æƒé‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
```

**ç®—æ³•å·¥ä½œæ–¹å¼ï¼š**

1. **æŒ‡æ ‡æ”¶é›†å‘¨æœŸ**ï¼šæ¯åˆ†é’Ÿæ”¶é›† Pod èµ„æºä½¿ç”¨é‡
2. **ç›´æ–¹å›¾æ›´æ–°**ï¼šå°†æ¯æ¬¡æµ‹é‡å€¼ç´¯ç§¯åˆ°ç›´æ–¹å›¾æ¡¶ä¸­
3. **æƒé‡åº”ç”¨**ï¼šå†å²æ•°æ®ä»¥ `e^(-t/decay_half_life)` æƒé‡è¡°å‡
4. **æ¨èå€¼è®¡ç®—**ï¼šåŸºäºç›´æ–¹å›¾ç™¾åˆ†ä½æ•°è®¡ç®—æ¨èå€¼

```mermaid
graph TB
    subgraph "VPA Recommender ç®—æ³•"
        A[Metrics Server] -->|æ¯åˆ†é’Ÿ| B[æ”¶é›†æŒ‡æ ‡]
        B --> C[æ›´æ–°ç›´æ–¹å›¾æ¡¶]
        C --> D[åº”ç”¨æŒ‡æ•°æƒé‡]
        D --> E[è®¡ç®—ç™¾åˆ†ä½æ•°]

        E --> F[Lower Bound<br/>P5]
        E --> G[Target<br/>P95]
        E --> H[Upper Bound<br/>P99]
        E --> I[Uncapped Target<br/>æ— çº¦æŸ P95]

        F --> J[æ›´æ–° VPA CRD]
        G --> J
        H --> J
        I --> J
    end

    style G fill:#51cf66
    style J fill:#4dabf7
```

##### å››ç§æ¨èå€¼è®¡ç®—æ–¹æ³•

| æ¨èå€¼ | è®¡ç®—æ–¹æ³• | å«ä¹‰ |
|--------|----------|------|
| **Lower Bound** | P5ï¼ˆç¬¬ 5 ç™¾åˆ†ä½æ•°ï¼‰ | æœ€ä½æ‰€éœ€èµ„æº - 95% æ—¶é—´å†…è¶³å¤Ÿ |
| **Target** | P95ï¼ˆç¬¬ 95 ç™¾åˆ†ä½æ•°ï¼‰ | **æ¨èè®¾ç½®å€¼** - åº”å¯¹ 5% å³°å€¼è´Ÿè½½ |
| **Upper Bound** | P99ï¼ˆç¬¬ 99 ç™¾åˆ†ä½æ•°ï¼‰ | æœ€å¤§è§‚å¯Ÿä½¿ç”¨é‡ - ç”¨äº Limits è®¾ç½®å‚è€ƒ |
| **Uncapped Target** | æ—  maxAllowed çº¦æŸè®¡ç®—çš„ P95 | ç”¨äºç¡®è®¤å®é™…éœ€æ±‚é‡ |

**ç™¾åˆ†ä½æ•°è®¡ç®—ç¤ºä¾‹ï¼š**

```python
# è™šæ‹Ÿ CPU ä½¿ç”¨é‡ç›´æ–¹å›¾ï¼ˆ1 å¤© = 1440 åˆ†é’Ÿï¼‰
cpu_samples = [100m, 150m, 200m, 250m, 300m, 350m, 400m, 450m, 500m, ...]

# åº”ç”¨æŒ‡æ•°æƒé‡ï¼ˆdecay_half_life = 24 å°æ—¶ï¼‰
weighted_samples = [
    (100m, weight=1.0),    # æœ€è¿‘ï¼ˆ1 å°æ—¶å‰ï¼‰
    (150m, weight=0.97),   # 2 å°æ—¶å‰
    (200m, weight=0.92),   # 5 å°æ—¶å‰
    (250m, weight=0.71),   # 12 å°æ—¶å‰
    (300m, weight=0.50),   # 24 å°æ—¶å‰ï¼ˆåŠè¡°æœŸï¼‰
    (350m, weight=0.25),   # 48 å°æ—¶å‰
    ...
]

# ç™¾åˆ†ä½æ•°è®¡ç®—
P5  = 150m  # Lower Bound
P95 = 450m  # Target â­
P99 = 500m  # Upper Bound
```

##### Confidence Multiplierï¼šåŸºäºç½®ä¿¡åº¦çš„è°ƒæ•´

æ•°æ®æ”¶é›†æœŸè¶ŠçŸ­ï¼Œæ¨èå€¼è¶Šä¿å®ˆï¼ˆè¶Šé«˜ï¼‰ï¼š

```
Confidence Multiplier = f(æ•°æ®æ”¶é›†æœŸ)

0-24 å°æ—¶ï¼šmultiplier = 1.5ï¼ˆ50% å®‰å…¨è£•é‡ï¼‰
1-3 å¤©ï¼š   multiplier = 1.3ï¼ˆ30% å®‰å…¨è£•é‡ï¼‰
3-7 å¤©ï¼š   multiplier = 1.1ï¼ˆ10% å®‰å…¨è£•é‡ï¼‰
7 å¤©ä»¥ä¸Šï¼š multiplier = 1.0ï¼ˆç½®ä¿¡åº¦è¶³å¤Ÿï¼‰
```

**å®é™…åº”ç”¨ç¤ºä¾‹ï¼š**

```yaml
# æ•°æ®æ”¶é›†ç¬¬ 2 å¤©
åŸå§‹ P95: 450m
Confidence Multiplier: 1.3
æœ€ç»ˆ Target: 450m Ã— 1.3 = 585m â‰ˆ 600m

# æ•°æ®æ”¶é›†ç¬¬ 10 å¤©
åŸå§‹ P95: 450m
Confidence Multiplier: 1.0
æœ€ç»ˆ Target: 450m Ã— 1.0 = 450m
```

:::info æ•°æ®æ”¶é›†æœŸçš„é‡è¦æ€§
VPA è¦æä¾›å‡†ç¡®çš„æ¨èå€¼ï¼Œè‡³å°‘éœ€è¦ **7 å¤©ï¼Œå»ºè®® 14 å¤©**çš„æ•°æ®æ”¶é›†æœŸã€‚è¦æ•æ‰å·¥ä½œæ—¥/å‘¨æœ«ç­‰å‘¨æœŸæ€§æ¨¡å¼ï¼Œè‡³å°‘éœ€è¦ 2 å‘¨ä»¥ä¸Šçš„è§‚å¯Ÿã€‚
:::

##### Memory æ¨èï¼šåŸºäº OOM äº‹ä»¶çš„ Bump-Up

Memory ä¸ CPU ä¸åŒï¼Œä¼šç‰¹åˆ«è€ƒè™‘ OOM Kill äº‹ä»¶ï¼š

**æ£€æµ‹åˆ° OOM äº‹ä»¶æ—¶ï¼š**

```
å½“å‰ Memory Target: 500Mi
OOM Kill å‘ç”Ÿæ—¶å†…å­˜: 600Mi
â†’ æ–° Target: 600Mi Ã— 1.2 = 720Miï¼ˆå¢åŠ  20% å®‰å…¨è£•é‡ï¼‰
```

**OOM Bump-Up é€»è¾‘ï¼š**

```python
if oom_kill_detected:
    oom_memory = get_memory_at_oom_time()
    new_target = max(
        current_target,
        oom_memory * 1.2  # 20% å®‰å…¨è£•é‡
    )

    # é˜²æ­¢çªç„¶å˜åŒ–ï¼ˆæœ€å¤§ 2 å€ï¼‰
    new_target = min(new_target, current_target * 2)
```

:::warning OOM Kill å³æ—¶ç”Ÿæ•ˆ
ä¸ CPU throttling ä¸åŒï¼ŒOOM Kill äº‹ä»¶ä¼š**ç«‹å³ä¸Šè°ƒ Memory Target**ã€‚è¿™æ˜¯é˜²æ­¢æœåŠ¡ä¸­æ–­çš„å®‰å…¨æœºåˆ¶ã€‚
:::

##### CPU æ¨èï¼šåŸºäº P95/P99 ä½¿ç”¨é‡

CPU æ˜¯å¯å‹ç¼©èµ„æºï¼Œå› æ­¤é‡‡å–ä¿å®ˆç­–ç•¥ï¼š

```
CPU Target = P95 ä½¿ç”¨é‡
CPU Upper Bound = P99 ä½¿ç”¨é‡

Throttling å‘ç”Ÿæ—¶ï¼š
â†’ ä¸æ”¹å˜æ¨èå€¼ï¼ˆå»ºè®®ä½¿ç”¨ HPA è§£å†³ï¼‰
```

**æ£€æµ‹åˆ° CPU Throttling æ—¶ï¼š**

```python
if cpu_throttling_detected:
    throttled_percentage = get_throttled_time_percentage()

    if throttled_percentage > 10:
        # VPA è‡ªèº«æ¨èå€¼ä¿æŒä¸å˜
        # è€Œæ˜¯å»ºè®®ä»¥ä¸‹æªæ–½ï¼š
        # 1. æ·»åŠ  HPA è¿›è¡Œæ°´å¹³æ‰©å±•
        # 2. ç§»é™¤ CPU limitsï¼ˆGoogleã€Datadog æ¨¡å¼ï¼‰
        # 3. æˆ–è€…æ‰‹åŠ¨å°† Target ä¸Šè°ƒè‡³ P99
        pass
```

:::tip CPU Throttling vs HPA
VPA æ£€æµ‹åˆ° CPU throttling æ—¶ä¸ä¼šå¤§å¹…æé«˜æ¨èå€¼ã€‚ç›¸åï¼Œ**ä½¿ç”¨ HPA è¿›è¡Œæ°´å¹³æ‰©å±•**æ‰æ˜¯ Kubernetes æœ€ä½³å®è·µã€‚
:::

##### VPA ä¸ Prometheus æ•°æ®æºé›†æˆ

VPA Recommender ä»…ä½¿ç”¨ Metrics Server å³å¯è¿è¡Œï¼Œä½†ä¸ Prometheus é›†æˆåèƒ½æä¾›æ›´ç²¾ç¡®çš„æ¨èï¼š

**Prometheus æŒ‡æ ‡ä½¿ç”¨ï¼š**

```yaml
# VPA Recommender çš„ Prometheus é›†æˆé…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: vpa-recommender-config
  namespace: vpa-system
data:
  recommender-config.yaml: |
    # å¯ç”¨ Prometheus æŒ‡æ ‡æº
    metrics-provider: prometheus
    prometheus-url: http://prometheus-server.monitoring.svc:9090

    # ç›´æ–¹å›¾è®¾ç½®
    histogram-decay-half-life: 24h
    histogram-bucket-size-growth: 1.05

    # CPU æ¨èè®¾ç½®
    cpu-histogram-decay-half-life: 24h
    memory-histogram-decay-half-life: 48h  # Memory éœ€è¦æ›´é•¿çš„è§‚å¯ŸæœŸ

    # OOM äº‹ä»¶å¤„ç†
    oom-min-bump-up: 1.2  # æœ€ä½ 20% å¢åŠ 
    oom-bump-up-ratio: 0.5  # 50% å®‰å…¨è£•é‡
```

**Prometheus Custom Metrics API é›†æˆï¼š**

```bash
# éƒ¨ç½² Custom Metrics API é€‚é…å™¨ï¼ˆPrometheus Adapterï¼‰
helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace monitoring \
  --set prometheus.url=http://prometheus-server.monitoring.svc \
  --set rules.default=true

# è®¾ç½® VPA ä½¿ç”¨ Custom Metrics API
kubectl edit deploy vpa-recommender -n vpa-system

# æ·»åŠ ç¯å¢ƒå˜é‡ï¼š
# - PROMETHEUS_ADDRESS=http://prometheus-server.monitoring.svc:9090
# - USE_CUSTOM_METRICS=true
```

**éªŒè¯é›†æˆï¼š**

```bash
# ç¡®è®¤ VPA Recommender æ­£åœ¨ä½¿ç”¨ Prometheus æŒ‡æ ‡
kubectl logs -n vpa-system deploy/vpa-recommender | grep prometheus

# é¢„æœŸè¾“å‡ºï¼š
# I0212 10:15:30.123456  1 metrics_client.go:45] Using Prometheus metrics provider
# I0212 10:15:31.234567  1 prometheus_client.go:78] Connected to Prometheus at http://prometheus-server.monitoring.svc:9090
```

##### VPA æ¨èè´¨é‡éªŒè¯æ–¹æ³•

ç”¨äºéªŒè¯æ¨èå€¼æ˜¯å¦åˆç†çš„ PromQL æŸ¥è¯¢ï¼š

**1. CPU æ¨èå€¼ vs å®é™…ä½¿ç”¨é‡æ¯”è¾ƒï¼š**

```promql
# VPA Target vs å®é™… P95 ä½¿ç”¨é‡æ¯”è¾ƒ
(
  kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="cpu"}
  -
  quantile_over_time(0.95,
    container_cpu_usage_seconds_total{pod=~"web-app-.*"}[7d]
  ) * 1000
) /
kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="cpu"} * 100

# è¾“å‡ºï¼šæ¨èå€¼ä¸å®é™… P95 å·®å¼‚ï¼ˆ%ï¼‰
# 10-20% èŒƒå›´ï¼šåˆé€‚ âœ…
# >30%ï¼šè¿‡åº¦é…ç½® âš ï¸
# <0%ï¼šé…ç½®ä¸è¶³ï¼ˆéœ€ç«‹å³è°ƒæ•´ï¼‰ğŸš¨
```

**2. Memory æ¨èå€¼éªŒè¯ï¼š**

```promql
# VPA Target vs å®é™… P99 ä½¿ç”¨é‡
(
  kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="memory"}
  -
  quantile_over_time(0.99,
    container_memory_working_set_bytes{pod=~"web-app-.*"}[7d]
  )
) /
kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource="memory"} * 100

# 20-30% ä½™é‡ï¼šç†æƒ³ âœ…
# <10% ä½™é‡ï¼šOOM é£é™© ğŸš¨
```

**3. OOM Kill é¢‘ç‡ç›‘æ§ï¼š**

```promql
# æœ€è¿‘ 7 å¤© OOM Kill äº‹ä»¶æ•°
increase(
  kube_pod_container_status_terminated_reason{reason="OOMKilled"}[7d]
)

# 0 æ¬¡ï¼šVPA æ¨èå‡†ç¡® âœ…
# 1-2 æ¬¡ï¼šå¯æ¥å—ï¼ˆå³°å€¼è´Ÿè½½ï¼‰
# >3 æ¬¡ï¼šéœ€æ‰‹åŠ¨ä¸Šè°ƒ VPA Target ğŸš¨
```

**4. CPU Throttling æ¯”ç‡ï¼š**

```promql
# CPU Throttling æ—¶é—´æ¯”ç‡ï¼ˆ%ï¼‰
rate(container_cpu_cfs_throttled_seconds_total{pod=~"web-app-.*"}[5m])
/
rate(container_cpu_cfs_periods_total{pod=~"web-app-.*"}[5m]) * 100

# <5%ï¼šæ­£å¸¸ âœ…
# 5-10%ï¼šéœ€è¦ç›‘æ§ âš ï¸
# >10%ï¼šè€ƒè™‘æ·»åŠ  HPA æˆ–ç§»é™¤ CPU limits ğŸš¨
```

**Grafana ä»ªè¡¨æ¿ç¤ºä¾‹ï¼š**

```yaml
# VPA æ¨èè´¨é‡ç›‘æ§ä»ªè¡¨æ¿
apiVersion: v1
kind: ConfigMap
metadata:
  name: vpa-quality-dashboard
  namespace: monitoring
data:
  dashboard.json: |
    {
      "panels": [
        {
          "title": "CPU: VPA Target vs P95 å®é™…ä½¿ç”¨é‡",
          "targets": [
            {
              "expr": "kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource=\"cpu\"}",
              "legendFormat": "VPA Target"
            },
            {
              "expr": "quantile_over_time(0.95, container_cpu_usage_seconds_total[7d]) * 1000",
              "legendFormat": "å®é™… P95"
            }
          ]
        },
        {
          "title": "Memory: VPA Target vs P99 å®é™…ä½¿ç”¨é‡",
          "targets": [
            {
              "expr": "kube_verticalpodautoscaler_status_recommendation_containerrecommendations_target{resource=\"memory\"}",
              "legendFormat": "VPA Target"
            },
            {
              "expr": "quantile_over_time(0.99, container_memory_working_set_bytes[7d])",
              "legendFormat": "å®é™… P99"
            }
          ]
        },
        {
          "title": "OOM Kill äº‹ä»¶ï¼ˆ7 å¤©ï¼‰",
          "targets": [
            {
              "expr": "increase(kube_pod_container_status_terminated_reason{reason=\"OOMKilled\"}[7d])"
            }
          ]
        }
      ]
    }
```

:::tip VPA æ¨èçš„å±€é™æ€§
VPA åŸºäºå†å²æ•°æ®æ¨èï¼Œå› æ­¤åœ¨ä»¥ä¸‹åœºæ™¯å­˜åœ¨å±€é™ï¼š
- **çªç„¶çš„æµé‡æ¨¡å¼å˜åŒ–**ï¼šå†å²ä¸­ä¸å­˜åœ¨çš„å³°å€¼è´Ÿè½½
- **å­£èŠ‚æ€§å·¥ä½œè´Ÿè½½**ï¼šæœˆæœ«æ‰¹å¤„ç†ã€å¹´ç»ˆç»“ç®—ç­‰
- **åˆå§‹å¼•å¯¼é˜¶æ®µ**ï¼šåº”ç”¨å¯åŠ¨æ—¶çš„é«˜å†…å­˜ä½¿ç”¨

è¿™äº›æƒ…å†µä¸‹éœ€è¦**æ‰‹åŠ¨è°ƒæ•´**æˆ–**ä¸ HPA é…åˆä½¿ç”¨**ã€‚
:::

### 4.2 VPA å®‰è£…ä¸é…ç½®

#### é€šè¿‡ Helm å®‰è£…

```bash
# 1. å®‰è£… Metrics Serverï¼ˆå‰ç½®æ¡ä»¶ï¼‰
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# 2. ç¡®è®¤ Metrics Server
kubectl get deployment metrics-server -n kube-system
kubectl top nodes

# 3. æ·»åŠ  VPA Helm ä»“åº“
helm repo add fairwinds-stable https://charts.fairwinds.com/stable
helm repo update

# 4. å®‰è£… VPA
helm install vpa fairwinds-stable/vpa \
  --namespace vpa-system \
  --create-namespace \
  --set recommender.enabled=true \
  --set updater.enabled=true \
  --set admissionController.enabled=true

# 5. ç¡®è®¤å®‰è£…
kubectl get pods -n vpa-system
# é¢„æœŸè¾“å‡ºï¼š
# NAME                                      READY   STATUS    RESTARTS   AGE
# vpa-admission-controller-xxx              1/1     Running   0          1m
# vpa-recommender-xxx                       1/1     Running   0          1m
# vpa-updater-xxx                           1/1     Running   0          1m
```

#### æ‰‹åŠ¨å®‰è£…ï¼ˆå®˜æ–¹æ–¹æ³•ï¼‰

```bash
# å…‹éš† VPA å®˜æ–¹ä»“åº“
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler

# å®‰è£… VPA
./hack/vpa-up.sh

# ç¡®è®¤å®‰è£…
kubectl get crd | grep verticalpodautoscaler
```

### 4.3 VPA æ¨¡å¼

VPA æœ‰ 3 ç§è¿è¡Œæ¨¡å¼ï¼š

#### Off æ¨¡å¼ï¼ˆä»…æä¾›æ¨èå€¼ï¼‰

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"    # ä»…æ˜¾ç¤ºæ¨èå€¼ï¼Œä¸è‡ªåŠ¨åº”ç”¨
```

**é€‚ç”¨åœºæ™¯ï¼š**
- é¦–æ¬¡å¼•å…¥ VPA æ—¶
- åˆ†æç”Ÿäº§å·¥ä½œè´Ÿè½½
- éœ€è¦æ‰‹åŠ¨å®¡æ ¸åå†åº”ç”¨æ—¶

**æŸ¥çœ‹æ¨èå€¼ï¼š**

```bash
# æŸ¥çœ‹ VPA çŠ¶æ€
kubectl describe vpa web-app-vpa -n production

# è¾“å‡ºç¤ºä¾‹ï¼š
# Recommendation:
#   Container Recommendations:
#     Container Name: web-app
#     Lower Bound:
#       Cpu:     150m
#       Memory:  200Mi
#     Target:          # â† æ¨èä½¿ç”¨æ­¤å€¼
#       Cpu:     250m
#       Memory:  300Mi
#     Uncapped Target:
#       Cpu:     350m
#       Memory:  400Mi
#     Upper Bound:
#       Cpu:     500m
#       Memory:  600Mi
```

#### Initial æ¨¡å¼ï¼ˆä»…åœ¨ Pod åˆ›å»ºæ—¶åº”ç”¨ï¼‰

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: batch-worker-vpa
  namespace: batch
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: batch-worker
  updatePolicy:
    updateMode: "Initial"    # ä»…åœ¨ Pod åˆ›å»ºæ—¶è®¾ç½®èµ„æº
  resourcePolicy:
    containerPolicies:
    - containerName: worker
      minAllowed:
        cpu: "100m"
        memory: "128Mi"
      maxAllowed:
        cpu: "4000m"
        memory: "16Gi"
```

**é€‚ç”¨åœºæ™¯ï¼š**
- CronJobã€Job å·¥ä½œè´Ÿè½½
- ä¸å…è®¸é‡å¯çš„ StatefulSet
- éœ€è¦æ‰‹åŠ¨æ‰©ç¼©çš„åœºæ™¯

**å·¥ä½œæ–¹å¼ï¼š**
1. æ–° Pod åˆ›å»ºè¯·æ±‚
2. VPA Admission Controller æ³¨å…¥æ¨èèµ„æº
3. æ­£åœ¨è¿è¡Œçš„ Pod ä¿æŒä¸å˜

#### Auto æ¨¡å¼ï¼ˆå®Œå…¨è‡ªåŠ¨åŒ–ï¼‰

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
  namespace: development
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  updatePolicy:
    updateMode: "Auto"    # è‡ªåŠ¨é‡å¯ Pod å¹¶è°ƒæ•´èµ„æº
    minReplicas: 2        # è‡³å°‘ç»´æŒ 2 ä¸ª Pod
  resourcePolicy:
    containerPolicies:
    - containerName: api
      minAllowed:
        cpu: "200m"
        memory: "256Mi"
      maxAllowed:
        cpu: "2000m"
        memory: "4Gi"
      controlledResources:
      - cpu
      - memory
      controlledValues: RequestsAndLimits  # åŒæ—¶è°ƒæ•´ requests å’Œ limits
```

**é€‚ç”¨åœºæ™¯ï¼š**
- å¼€å‘/é¢„å‘å¸ƒç¯å¢ƒ
- Stateless åº”ç”¨
- å·²é…ç½® PodDisruptionBudget çš„å·¥ä½œè´Ÿè½½

:::warning Auto æ¨¡å¼æ³¨æ„äº‹é¡¹
Auto æ¨¡å¼ä¼š**é‡å¯ Pod**ï¼š
- é€šè¿‡ Eviction API é‡å¯
- å¯èƒ½å¯¼è‡´åœæœº
- å¿…é¡»é…ç½® PodDisruptionBudget (PDB)
- ç”Ÿäº§ç¯å¢ƒéœ€è°¨æ…ä½¿ç”¨

**å»ºè®®ï¼š** ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ **Off æˆ– Initial æ¨¡å¼**
:::

### 4.4 VPA + HPA å…±å­˜ç­–ç•¥

åŒæ—¶ä½¿ç”¨ VPA å’Œ HPA æ—¶å¿…é¡»é˜²æ­¢å†²çªã€‚

#### å†²çªåœºæ™¯ï¼ˆâŒ ç¦æ­¢ï¼‰

```yaml
# âŒ é”™è¯¯é…ç½®ï¼šVPA Auto + HPA CPU åŒæ—¶ä½¿ç”¨
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: bad-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Auto"    # âŒ Auto æ¨¡å¼
  resourcePolicy:
    containerPolicies:
    - containerName: app
      controlledResources:
      - cpu                # âŒ æ§åˆ¶ CPU
      - memory

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: bad-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu          # âŒ ä½¿ç”¨ CPU æŒ‡æ ‡
      target:
        type: Utilization
        averageUtilization: 70
```

**é—®é¢˜ï¼š**
- VPA æ›´æ”¹ CPU requests â†’ HPA çš„ CPU ä½¿ç”¨ç‡è®¡ç®—å‘ç”Ÿå˜åŒ–
- HPA è§¦å‘ Scale Out â†’ VPA å†æ¬¡è°ƒæ•´èµ„æº â†’ æ— é™å¾ªç¯

#### æ¨¡å¼ 1ï¼šVPA Off + HPAï¼ˆâœ… æ¨èï¼‰

```yaml
# âœ… æ­£ç¡®é…ç½®ï¼šVPA ä»…æä¾›æ¨èï¼ŒHPA è´Ÿè´£æ‰©ç¼©
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"    # âœ… ä»…æä¾›æ¨èå€¼
  resourcePolicy:
    containerPolicies:
    - containerName: app
      controlledResources:
      - cpu
      - memory

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

**è¿ç»´å·¥ä½œæµï¼š**
1. VPA ç”Ÿæˆæ¨èå€¼
2. å‘¨ä¼šå®¡æŸ¥ VPA æ¨èå€¼
3. æ‰‹åŠ¨æ›´æ–° Deployment æ¸…å•
4. HPA æ ¹æ®è´Ÿè½½è¿›è¡Œæ°´å¹³æ‰©å±•

#### æ¨¡å¼ 2ï¼šVPA Memory + HPA CPUï¼ˆâœ… æ¨èï¼‰

```yaml
# âœ… æŒ‡æ ‡åˆ†ç¦»ï¼šVPA ç®¡ç† Memoryï¼ŒHPA ç®¡ç† CPU
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  updatePolicy:
    updateMode: "Auto"    # ä»…è‡ªåŠ¨è°ƒæ•´ Memory
  resourcePolicy:
    containerPolicies:
    - containerName: api
      controlledResources:
      - memory            # âœ… ä»…æ§åˆ¶ Memory
      minAllowed:
        memory: "256Mi"
      maxAllowed:
        memory: "8Gi"

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 5
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu          # âœ… ä»…ä½¿ç”¨ CPU æŒ‡æ ‡
      target:
        type: Utilization
        averageUtilization: 60
```

**ä¼˜åŠ¿ï¼š**
- VPA ä¼˜åŒ– Memoryï¼ˆå‚ç›´ï¼‰
- HPA æ ¹æ®è´Ÿè½½æ°´å¹³æ‰©å±•ï¼ˆæ°´å¹³ï¼‰
- æ— å†²çª

#### æ¨¡å¼ 3ï¼šVPA + HPA + Custom Metricsï¼ˆâœ… é«˜çº§ï¼‰

```yaml
# âœ… HPA ä½¿ç”¨è‡ªå®šä¹‰æŒ‡æ ‡
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: worker-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: queue-worker
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: worker
      controlledResources:
      - cpu
      - memory

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: queue-worker
  minReplicas: 2
  maxReplicas: 50
  metrics:
  - type: External
    external:
      metric:
        name: sqs_queue_depth    # âœ… è‡ªå®šä¹‰æŒ‡æ ‡ï¼ˆé CPU/Memoryï¼‰
        selector:
          matchLabels:
            queue: "tasks"
      target:
        type: AverageValue
        averageValue: "30"
```

**é€‚ç”¨åœºæ™¯ï¼š**
- åŸºäºé˜Ÿåˆ—çš„å·¥ä½œè´Ÿè½½ï¼ˆSQSã€RabbitMQã€Kafkaï¼‰
- äº‹ä»¶é©±åŠ¨æ¶æ„
- åŸºäºä¸šåŠ¡æŒ‡æ ‡çš„æ‰©ç¼©

### 4.5 VPA é™åˆ¶ä¸æ³¨æ„äº‹é¡¹

:::danger VPA ä½¿ç”¨æ³¨æ„äº‹é¡¹

**1. éœ€è¦é‡å¯ Podï¼ˆAuto/Recreate æ¨¡å¼ï¼‰**
- VPA **æ— æ³•å°±åœ° (in-place) å˜æ›´**è¿è¡Œä¸­ Pod çš„èµ„æº
- é€šè¿‡ Evict Pod å¹¶é‡æ–°åˆ›å»ºï¼ˆå¯èƒ½å¯¼è‡´åœæœºï¼‰
- è§£å†³æ–¹æ¡ˆï¼šå¿…é¡»é…ç½® PodDisruptionBudget

**2. JVM å †å¤§å°ä¸åŒ¹é…**
```yaml
# é—®é¢˜åœºæ™¯
containers:
- name: java-app
  env:
  - name: JAVA_OPTS
    value: "-Xmx2g"    # å›ºå®šå€¼
  resources:
    requests:
      memory: "3Gi"    # VPA ä¹‹åå¯èƒ½å˜æ›´ä¸º 4Gi
    limits:
      memory: "3Gi"    # VPA ä¹‹åå¯èƒ½å˜æ›´ä¸º 4Gi

# VPA å°† memory å˜æ›´ä¸º 4Giï¼Œä½† JVM ä»ä½¿ç”¨ 2Gi å †
# â†’ èµ„æºæµªè´¹
```

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
containers:
- name: java-app
  env:
  - name: MEM_LIMIT
    valueFrom:
      resourceFieldRef:
        resource: limits.memory
  - name: JAVA_OPTS
    value: "-XX:MaxRAMPercentage=75.0"  # åŠ¨æ€è®¡ç®—
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"
```

**3. StatefulSet æ³¨æ„äº‹é¡¹**
- StatefulSet Pod æŒ‰é¡ºåºé‡å¯
- å­˜åœ¨æ•°æ®ä¸¢å¤±é£é™©
- æ¨èï¼š**ä»…ä½¿ç”¨ Initial æ¨¡å¼**

**4. Metrics Server ä¾èµ–**
- VPA å¿…é¡»ä¾èµ– Metrics Server
- Metrics Server æ•…éšœæ—¶æ¨èå€¼æ›´æ–°åœæ­¢

**5. æ¨èå€¼è®¡ç®—æ—¶é—´**
- è‡³å°‘éœ€è¦ 24 å°æ—¶æ•°æ®
- æµé‡æ¨¡å¼å˜åŒ–çš„åæ˜ éœ€è¦æ—¶é—´
:::

## HPA é«˜çº§æ¨¡å¼

### 5.1 HPA Behavior è®¾ç½®

HPA v2 å¯ä»¥ç²¾ç»†æ§åˆ¶æ‰©ç¼©è¡Œä¸ºï¼š

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: advanced-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 5
  maxReplicas: 100

  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0    # ç«‹å³æ‰©å®¹
      policies:
      - type: Percent
        value: 100                     # å…è®¸ 100% å¢åŠ ï¼ˆ2 å€ï¼‰
        periodSeconds: 15              # æ¯ 15 ç§’è¯„ä¼°
      - type: Pods
        value: 10                      # æˆ–å¢åŠ  10 ä¸ª Pod
        periodSeconds: 15
      selectPolicy: Max                # é€‰æ‹©è¾ƒå¤§å€¼

    scaleDown:
      stabilizationWindowSeconds: 300  # 5 åˆ†é’Ÿç¨³å®šæœŸï¼ˆé˜²æ­¢æ€¥å‰§ç¼©å‡ï¼‰
      policies:
      - type: Percent
        value: 10                      # 10% ç¼©å‡
        periodSeconds: 60              # æ¯åˆ†é’Ÿè¯„ä¼°
      - type: Pods
        value: 5                       # æˆ–å‡å°‘ 5 ä¸ª Pod
        periodSeconds: 60
      selectPolicy: Min                # é€‰æ‹©è¾ƒå°å€¼ï¼ˆä¿å®ˆï¼‰
```

**å‚æ•°è¯´æ˜ï¼š**

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|---------|------|--------|
| `stabilizationWindowSeconds` | æŒ‡æ ‡ç¨³å®šç­‰å¾…æ—¶é—´ | ScaleUp: 0-30sï¼ŒScaleDown: 300-600s |
| `type: Percent` | æŒ‰å½“å‰å‰¯æœ¬ç™¾åˆ†æ¯”å¢å‡ | ScaleUp: 100%ï¼ŒScaleDown: 10-25% |
| `type: Pods` | æŒ‰ç»å¯¹ Pod æ•°å¢å‡ | æ ¹æ®å·¥ä½œè´Ÿè½½è§„æ¨¡è°ƒæ•´ |
| `periodSeconds` | ç­–ç•¥è¯„ä¼°å‘¨æœŸ | 15-60 ç§’ |
| `selectPolicy` | Maxï¼ˆæ¿€è¿›ï¼‰ã€Minï¼ˆä¿å®ˆï¼‰ã€Disabled | ScaleUp: Maxï¼ŒScaleDown: Min |

:::info å‚è€ƒ karpenter-autoscaling.md
HPA ä¸ Karpenter é…åˆä½¿ç”¨çš„å®Œæ•´æ¶æ„ï¼Œè¯·å‚è€ƒ [Karpenter è‡ªåŠ¨æ‰©ç¼©æŒ‡å—](/docs/infrastructure-optimization/karpenter-autoscaling)ã€‚
:::

### 5.2 è‡ªå®šä¹‰æŒ‡æ ‡ HPA

#### Prometheus Adapter ä½¿ç”¨

```bash
# å®‰è£… Prometheus Adapter
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace monitoring \
  --set prometheus.url=http://prometheus-server.monitoring.svc \
  --set prometheus.port=80
```

**è‡ªå®šä¹‰æŒ‡æ ‡é…ç½®ï¼š**

```yaml
# values.yaml for prometheus-adapter
rules:
  default: false
  custom:
  - seriesQuery: 'http_requests_total{namespace!="",pod!=""}'
    resources:
      overrides:
        namespace: {resource: "namespace"}
        pod: {resource: "pod"}
    name:
      matches: "^(.*)_total$"
      as: "${1}_per_second"
    metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'
```

**HPA é…ç½®ï¼š**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: custom-metric-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"    # æ¯ Pod 1000 req/s
```

#### KEDA ScaledObject

```bash
# å®‰è£… KEDA
helm repo add kedacore https://kedacore.github.io/charts
helm install keda kedacore/keda --namespace keda --create-namespace
```

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prometheus-scaledobject
spec:
  scaleTargetRef:
    name: api-server
  minReplicaCount: 2
  maxReplicaCount: 100
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-server.monitoring.svc:80
      metricName: http_requests_per_second
      threshold: "1000"
      query: sum(rate(http_requests_total{app="api-server"}[2m]))
```

### 5.3 å¤šæŒ‡æ ‡ HPA

ç»„åˆå¤šä¸ªæŒ‡æ ‡è¿›è¡Œæ‰©ç¼©ï¼š

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: multi-metric-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 5
  maxReplicas: 100

  metrics:
  # 1. CPU æŒ‡æ ‡
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # 2. Memory æŒ‡æ ‡
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

  # 3. è‡ªå®šä¹‰æŒ‡æ ‡ - RPS
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"

  # 4. å¤–éƒ¨æŒ‡æ ‡ - ALB Target Response Time
  - type: External
    external:
      metric:
        name: alb_target_response_time
        selector:
          matchLabels:
            targetgroup: "web-app-tg"
      target:
        type: Value
        value: "100"    # 100ms

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 50
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

**å¤šæŒ‡æ ‡è¯„ä¼°ï¼š**
- HPA **ç‹¬ç«‹è¯„ä¼°æ¯ä¸ªæŒ‡æ ‡**
- é€‰æ‹©**æœ€é«˜çš„å‰¯æœ¬æ•°**ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
- ä¾‹å¦‚ï¼šCPU éœ€è¦ 10 ä¸ªï¼ŒMemory éœ€è¦ 15 ä¸ªï¼ŒRPS éœ€è¦ 20 ä¸ª â†’ **é€‰æ‹© 20 ä¸ª**

## Node Readiness Controller ä¸èµ„æºä¼˜åŒ–

### 5.3 æœªå°±ç»ªèŠ‚ç‚¹ä¸Šçš„èµ„æºæµªè´¹

åœ¨ Kubernetes é›†ç¾¤ä¸­ï¼Œå½“æ–°èŠ‚ç‚¹è¢«é…ç½®æ—¶ï¼ŒCNI æ’ä»¶ã€CSI é©±åŠ¨ã€GPU é©±åŠ¨ç­‰åŸºç¡€è®¾æ–½ç»„ä»¶æœªå°±ç»ªå‰ Pod å°±è¢«è°ƒåº¦çš„é—®é¢˜å¯èƒ½å‘ç”Ÿã€‚è¿™ä¼šå¯¼è‡´ä»¥ä¸‹èµ„æºæµªè´¹ï¼š

**èµ„æºæµªè´¹åœºæ™¯ï¼š**

1. **CrashLoopBackOff å¾ªç¯**
   - Pod è¢«è°ƒåº¦åˆ°æœªå°±ç»ªèŠ‚ç‚¹ â†’ å¤±è´¥ â†’ åå¤é‡å¯
   - ä¸å¿…è¦çš„ CPU/å†…å­˜ä½¿ç”¨å’Œå®¹å™¨é•œåƒé‡æ–°ä¸‹è½½

2. **ä¸å¿…è¦çš„èŠ‚ç‚¹é…ç½®**
   - Pod å¤„äº Pending çŠ¶æ€ â†’ Karpenter/Cluster Autoscaler åˆ›å»ºé¢å¤–èŠ‚ç‚¹
   - å®é™…ä¸Šç°æœ‰èŠ‚ç‚¹å°±ç»ªåå³å¯æ‰¿è½½

3. **é‡è°ƒåº¦å¼€é”€**
   - å°†å¤±è´¥çš„ Pod ç§»åŠ¨åˆ°å…¶ä»–èŠ‚ç‚¹ â†’ ç½‘ç»œ/å­˜å‚¨èµ„æºæµªè´¹
   - åº”ç”¨åˆå§‹åŒ–æˆæœ¬é‡å¤å‘ç”Ÿ

### 5.4 Node Readiness Controller (NRC) æ¦‚è¿°

Node Readiness Controller æ˜¯ Kubernetes 1.32 å¼•å…¥çš„åŠŸèƒ½ï¼Œåœ¨åŸºç¡€è®¾æ–½å°±ç»ªä¹‹å‰é˜»æ­¢ Pod è°ƒåº¦ï¼Œä»è€Œæé«˜èµ„æºæ•ˆç‡ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**

| åŠŸèƒ½ | è¯´æ˜ | èµ„æºä¼˜åŒ–æ•ˆæœ |
|------|------|-------------------|
| **Readiness Gate** | ç‰¹å®šæ¡ä»¶æ»¡è¶³å‰å°†èŠ‚ç‚¹ä¿æŒåœ¨ NotReady çŠ¶æ€ | é˜»æ­¢ Pod è°ƒåº¦é˜²æ­¢ CrashLoop |
| **Custom Taint** | è‡ªåŠ¨ä¸ºæœªå°±ç»ªèŠ‚ç‚¹æ·»åŠ  taint | é˜²æ­¢èµ„æºæµªè´¹ï¼ˆNoSchedule æ•ˆæœï¼‰|
| **Enforcement Mode** | é€‰æ‹© `bootstrap-only` æˆ– `continuous` æ¨¡å¼ | ä»…åˆå§‹å¼•å¯¼æˆ–æŒç»­éªŒè¯ |

**API ç»“æ„ï¼š**

```yaml
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
```

### 5.5 Karpenter è”åŠ¨ä¼˜åŒ–

å°† Karpenter ä¸ Node Readiness Controller é…åˆä½¿ç”¨å¯æ˜¾è‘—æé«˜èŠ‚ç‚¹é…ç½®æ•ˆç‡ã€‚

**ä¼˜åŒ–æ¨¡å¼ï¼š**

```mermaid
graph TB
    A[Karpenter: é…ç½®æ–°èŠ‚ç‚¹] --> B[NRC: è‡ªåŠ¨æ·»åŠ  Taint]
    B --> C{CNI/CSI å°±ç»ªï¼Ÿ}
    C -->|å¦| D[Pod ä¿æŒ Pending]
    D --> E[Karpenter: ä¸åˆ›å»ºé¢å¤–èŠ‚ç‚¹]
    C -->|æ˜¯| F[NRC: ç§»é™¤ Taint]
    F --> G[å¼€å§‹è°ƒåº¦ Pod]
    G --> H[èµ„æºé«˜æ•ˆåˆ†é…]

    style B fill:#a8dadc
    style E fill:#457b9d
    style H fill:#1d3557
```

**Karpenter NodePool ä¸ NRC è”åŠ¨ï¼š**

```yaml
# 1. CSI Driver å°±ç»ªç¡®è®¤ï¼ˆEBSï¼‰
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: ebs-csi-readiness
spec:
  conditions:
    - type: "ebs.csi.aws.com/driver-ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/storage-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "bootstrap-only"  # ä»…åˆå§‹å¼•å¯¼éªŒè¯

---
# 2. VPC CNI å°±ç»ªç¡®è®¤
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: vpc-cni-readiness
spec:
  conditions:
    - type: "vpc.amazonaws.com/cni-ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/network-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "bootstrap-only"

---
# 3. GPU Driver å°±ç»ªç¡®è®¤ï¼ˆGPU èŠ‚ç‚¹ä¸“ç”¨ï¼‰
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-driver-readiness
spec:
  conditions:
    - type: "nvidia.com/gpu-driver-ready"
      requiredStatus: "True"
    - type: "nvidia.com/cuda-ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/gpu-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "bootstrap-only"
  # GPU é©±åŠ¨åŠ è½½è€—æ—¶è¾ƒé•¿ï¼ˆ30-60 ç§’ï¼‰
  # NRC åœ¨æ­¤æœŸé—´é˜»æ­¢ Pod è°ƒåº¦
```

### 5.6 èµ„æºæ•ˆç‡æ”¹å–„æ•ˆæœ

Node Readiness Controller å®æ–½å‰åå¯¹æ¯”ï¼š

| æŒ‡æ ‡ | å®æ–½å‰ | å®æ–½å | æ”¹å–„ç‡ |
|------|---------|---------|--------|
| **CrashLoopBackOff å‘ç”Ÿç‡** | 15-20% | < 2% | 90% é™ä½ |
| **ä¸å¿…è¦çš„èŠ‚ç‚¹é…ç½®** | å¹³å‡ 2-3 ä¸ª/å°æ—¶ | < 0.5 ä¸ª/å°æ—¶ | 75% é™ä½ |
| **Pod å¯åŠ¨å¤±è´¥ç‡** | 8-12% | < 1% | 90% é™ä½ |
| **å®¹å™¨é•œåƒé‡æ–°ä¸‹è½½** | 100-200GB/å¤© | 20-30GB/å¤© | 80% é™ä½ |

**æˆæœ¬å½±å“ï¼ˆ100 èŠ‚ç‚¹é›†ç¾¤åŸºå‡†ï¼‰ï¼š**

```
å®æ–½å‰ï¼š
- ä¸å¿…è¦çš„èŠ‚ç‚¹é…ç½®ï¼šå¹³å‡ 3 ä¸ª Ã— $0.384/å°æ—¶ Ã— 24 å°æ—¶ Ã— 30 å¤© = $829/æœˆ
- é•œåƒé‡æ–°ä¸‹è½½æ•°æ®ä¼ è¾“è´¹ï¼š150GB/å¤© Ã— 30 å¤© Ã— $0.09/GB = $405/æœˆ
- æ€»æµªè´¹æˆæœ¬ï¼š$1,234/æœˆ

å®æ–½åï¼š
- ä¸å¿…è¦çš„èŠ‚ç‚¹é…ç½®ï¼šå¹³å‡ 0.5 ä¸ª Ã— $0.384/å°æ—¶ Ã— 24 å°æ—¶ Ã— 30 å¤© = $138/æœˆ
- é•œåƒé‡æ–°ä¸‹è½½æ•°æ®ä¼ è¾“è´¹ï¼š25GB/å¤© Ã— 30 å¤© Ã— $0.09/GB = $67.5/æœˆ
- æ€»æˆæœ¬ï¼š$205.5/æœˆ

èŠ‚çœé‡‘é¢ï¼š$1,234 - $205.5 = $1,028.5/æœˆï¼ˆ83% èŠ‚çœï¼‰
```

### 5.7 å®æˆ˜å®æ–½æŒ‡å—

#### Step 1ï¼šå¯ç”¨ Feature Gate

```bash
# åœ¨ EKS 1.32+ é›†ç¾¤ä¸­ç¡®è®¤ Feature Gate
kubectl get --raw /metrics | grep node_readiness_controller

# åœ¨ Karpenter é…ç½®ä¸­å¯ç”¨ Feature Gate
# values.yamlï¼ˆKarpenter Helm Chartï¼‰
controller:
  featureGates:
    NodeReadinessController: true
```

#### Step 2ï¼šåº”ç”¨ NodeReadinessRule

```yaml
# production-nrc.yaml
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: production-readiness
spec:
  # å¤šä¸ªæ¡ä»¶ä»¥ AND æ–¹å¼éªŒè¯
  conditions:
    - type: "ebs.csi.aws.com/driver-ready"
      requiredStatus: "True"
    - type: "vpc.amazonaws.com/cni-ready"
      requiredStatus: "True"

  taint:
    key: "readiness.k8s.io/not-ready"
    effect: "NoSchedule"
    value: "pending"

  # bootstrap-onlyï¼šä»…éªŒè¯èŠ‚ç‚¹åˆå§‹å¼•å¯¼
  # continuousï¼šæŒç»­éªŒè¯ï¼ˆé©±åŠ¨é‡å¯æ—¶ä¹Ÿç”Ÿæ•ˆï¼‰
  enforcementMode: "bootstrap-only"
```

```bash
kubectl apply -f production-nrc.yaml

# ç¡®è®¤åº”ç”¨
kubectl get nodereadinessrule
kubectl describe nodereadinessrule production-readiness
```

#### Step 3ï¼šç›‘æ§èŠ‚ç‚¹çŠ¶æ€

```bash
# æ–°èŠ‚ç‚¹é…ç½®åç¡®è®¤æ¡ä»¶
kubectl get nodes -o json | jq '.items[] | {
  name: .metadata.name,
  conditions: [.status.conditions[] | select(.type |
    test("ebs.csi.aws.com|vpc.amazonaws.com")) |
    {type: .type, status: .status}]
}'

# ç¡®è®¤ Taint çŠ¶æ€
kubectl get nodes -o json | jq '.items[] | {
  name: .metadata.name,
  taints: .spec.taints
}'
```

#### Step 4ï¼šKarpenter NodePool ä¼˜åŒ–

```yaml
# Karpenter NodePool with NRC
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: optimized-pool
spec:
  template:
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64", "arm64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]

      # NRC è‡ªåŠ¨ç®¡ç† taintï¼Œæ­¤å¤„æ— éœ€é…ç½®
      # taints: []  # NRC ç®¡ç†

      # å¢åŠ èŠ‚ç‚¹å¼•å¯¼å®Œæˆç­‰å¾…æ—¶é—´
      kubelet:
        maxPods: 110
        # ç”±äº NRC å¯¼è‡´èŠ‚ç‚¹ Ready æ—¶é—´å¢åŠ ï¼ˆ30 ç§’ â†’ 60 ç§’ï¼‰
        # è®¾ç½®é¿å… Karpenter è¿‡æ—©è¶…æ—¶
        systemReserved:
          cpu: 100m
          memory: 512Mi

  disruption:
    consolidationPolicy: WhenUnderutilized
    # ç”±äº NRC å¯¼è‡´èŠ‚ç‚¹å¯åŠ¨å˜æ…¢ï¼Œå¢åŠ  consolidation é—´éš”
    consolidateAfter: 60s  # é»˜è®¤ 30s â†’ 60s
```

:::warning GPU èŠ‚ç‚¹ç‰¹åˆ«æ³¨æ„äº‹é¡¹
GPU é©±åŠ¨åŠ è½½éœ€è¦ 30-60 ç§’ï¼Œå› æ­¤ GPU NodePool å¿…é¡»åº”ç”¨ NRCã€‚å¦åˆ™ Pod å°†åœ¨ GPU ä¸å¯ç”¨çŠ¶æ€ä¸‹è¢«è°ƒåº¦å¹¶æŒç»­å¤±è´¥ã€‚

```yaml
# GPU ä¸“ç”¨ NRC
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-readiness
spec:
  nodeSelector:
    matchExpressions:
      - key: nvidia.com/gpu
        operator: Exists
  conditions:
    - type: "nvidia.com/gpu-driver-ready"
      requiredStatus: "True"
  taint:
    key: "nvidia.com/gpu-not-ready"
    effect: "NoSchedule"
  enforcementMode: "bootstrap-only"
```
:::

### 5.8 é—®é¢˜æ’æŸ¥ä¸ç›‘æ§

#### å¸¸è§é—®é¢˜

**1. èŠ‚ç‚¹æŒç»­å¤„äº NotReady çŠ¶æ€ï¼š**

```bash
# æŸ¥çœ‹èŠ‚ç‚¹æ¡ä»¶è¯¦æƒ…
kubectl describe node <node-name> | grep -A 10 "Conditions:"

# æŸ¥çœ‹ NRC äº‹ä»¶
kubectl get events --all-namespaces --field-selector involvedObject.kind=Node,involvedObject.name=<node-name>

# æŸ¥çœ‹é©±åŠ¨ DaemonSet çŠ¶æ€
kubectl get pods -n kube-system | grep -E "aws-node|ebs-csi|nvidia"
```

**2. Taint æœªè¢«ç§»é™¤ï¼š**

```bash
# ç¡®è®¤ NRC æ˜¯å¦æ­£åœ¨è¿è¡Œ
kubectl logs -n kube-system -l app=karpenter -c controller | grep "NodeReadiness"

# æ‰‹åŠ¨ç§»é™¤ taintï¼ˆä¸´æ—¶è§£å†³ï¼‰
kubectl taint nodes <node-name> readiness.k8s.io/not-ready:NoSchedule-
```

#### Prometheus æŒ‡æ ‡

```yaml
# NRC æŒ‡æ ‡çš„ ServiceMonitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-readiness-controller
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: karpenter
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s

# ä¸»è¦æŒ‡æ ‡ï¼š
# - node_readiness_controller_reconcile_duration_seconds
# - node_readiness_controller_condition_evaluation_total
# - node_readiness_controller_taint_operations_total
```

:::tip å‚è€ƒèµ„æ–™
- **å®˜æ–¹åšå®¢**ï¼š[Introducing Node Readiness Controller](https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/)
- **KEP (Kubernetes Enhancement Proposal)**ï¼šKEP-4403
- **API æ–‡æ¡£**ï¼š`readiness.node.x-k8s.io/v1alpha1`
:::

## Right-Sizing æ–¹æ³•è®º

### 6.1 å½“å‰èµ„æºä½¿ç”¨é‡åˆ†æ

#### kubectl top ä½¿ç”¨

```bash
# æŒ‰èŠ‚ç‚¹æŸ¥çœ‹èµ„æºä½¿ç”¨é‡
kubectl top nodes

# æŒ‰å‘½åç©ºé—´æŸ¥çœ‹ Pod èµ„æºä½¿ç”¨é‡
kubectl top pods -n production --sort-by=cpu
kubectl top pods -n production --sort-by=memory

# æŸ¥çœ‹ç‰¹å®š Pod çš„å®¹å™¨çº§ä½¿ç”¨é‡
kubectl top pods <pod-name> --containers -n production
```

#### Metrics Server API ç›´æ¥æŸ¥è¯¢

```bash
# CPU ä½¿ç”¨é‡
kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/production/pods | jq '.items[] | {name: .metadata.name, cpu: .containers[0].usage.cpu}'

# Memory ä½¿ç”¨é‡
kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/production/pods | jq '.items[] | {name: .metadata.name, memory: .containers[0].usage.memory}'
```

#### Container Insights (AWS)

```bash
# CloudWatch Logs Insights æŸ¥è¯¢
fields @timestamp, PodName, ContainerName, pod_cpu_utilization, pod_memory_utilization
| filter Namespace = "production"
| stats avg(pod_cpu_utilization) as avg_cpu,
        max(pod_cpu_utilization) as max_cpu,
        avg(pod_memory_utilization) as avg_mem,
        max(pod_memory_utilization) as max_mem
  by PodName
| sort max_cpu desc
```

#### 6.1.5 åŸºäº CloudWatch Observability Operator çš„è‡ªåŠ¨åˆ†æ

AWS äº 2025 å¹´ 12 æœˆé€šè¿‡ **CloudWatch Observability Operator** æ·»åŠ äº† EKS Control Plane æŒ‡æ ‡ç›‘æ§åŠŸèƒ½ã€‚è¿™ä½¿å¾—å¯ä»¥å…ˆè¡Œæ£€æµ‹èµ„æºç“¶é¢ˆå¹¶å®ç°è‡ªåŠ¨åŒ–åˆ†æã€‚

**CloudWatch Observability Operator å®‰è£…ï¼š**

```bash
# 1. æ·»åŠ  Helm ä»“åº“
helm repo add eks https://aws.github.io/eks-charts
helm repo update

# 2. å®‰è£… Operatorï¼ˆAmazon CloudWatch Observability namespaceï¼‰
helm install amazon-cloudwatch-observability eks/amazon-cloudwatch-observability \
  --namespace amazon-cloudwatch \
  --create-namespace \
  --set clusterName=<cluster-name> \
  --set region=<region>

# 3. ç¡®è®¤å®‰è£…
kubectl get pods -n amazon-cloudwatch

# é¢„æœŸè¾“å‡ºï¼š
# NAME                                                     READY   STATUS    RESTARTS   AGE
# amazon-cloudwatch-observability-controller-manager-xxx   2/2     Running   0          2m
# cloudwatch-agent-xxx                                     1/1     Running   0          2m
# dcgm-exporter-xxx                                        1/1     Running   0          2m
# fluent-bit-xxx                                           1/1     Running   0          2m
```

**Container Insights Enhanced åŠŸèƒ½ï¼š**

CloudWatch Observability Operator æä¾›ä»¥ä¸‹é«˜çº§åˆ†æåŠŸèƒ½ï¼š

| åŠŸèƒ½ | è¯´æ˜ | ç”¨é€” |
|------|------|------|
| **å¼‚å¸¸æ£€æµ‹** | é€šè¿‡ CloudWatch Anomaly Detection è‡ªåŠ¨è¯†åˆ«å¼‚å¸¸æ¨¡å¼ | æå‰æ£€æµ‹ CPU/Memory å³°å€¼ |
| **å†…å­˜æ³„æ¼å¯è§†åŒ–** | åœ¨æ—¶åºå›¾ä¸­é«˜äº®æ˜¾ç¤ºæŒç»­å¢é•¿æ¨¡å¼ | æ—©æœŸå‘ç°å†…å­˜æ³„æ¼ |
| **ä¸‹é’»åˆ†æ** | Namespace â†’ Deployment â†’ Pod â†’ Container å±‚çº§æ¢ç´¢ | èµ„æºç“¶é¢ˆæ ¹å› åˆ†æ |
| **Control Plane æŒ‡æ ‡** | API Serverã€etcdã€Scheduler æ€§èƒ½æŒ‡æ ‡ | æå‰æ£€æµ‹é›†ç¾¤æ‰©ç¼©ç“¶é¢ˆ |
| **è‡ªåŠ¨åˆ›å»ºå‘Šè­¦** | åŸºäºæ¨èé˜ˆå€¼è‡ªåŠ¨é…ç½® CloudWatch å‘Šè­¦ | è¿ç»´è‡ªåŠ¨åŒ– |

**é€šè¿‡ EKS Control Plane æŒ‡æ ‡å…ˆè¡Œæ£€æµ‹èµ„æºç“¶é¢ˆï¼š**

é€šè¿‡ Control Plane æŒ‡æ ‡å¯ä»¥æå‰æ£€æµ‹ Pod è°ƒåº¦å»¶è¿Ÿã€API Server è¿‡è½½ç­‰å½±å“èµ„æºä¼˜åŒ–çš„é›†ç¾¤çº§é—®é¢˜ã€‚

```bash
# CloudWatch Insights æŸ¥è¯¢ - Control Plane API Server è´Ÿè½½åˆ†æ
fields @timestamp, apiserver_request_duration_seconds_sum, apiserver_request_total
| filter @logStream like /kube-apiserver/
| stats avg(apiserver_request_duration_seconds_sum) as avg_latency,
        max(apiserver_request_total) as max_requests
  by bin(5m)
| sort @timestamp desc
```

**ä¸»è¦ Control Plane æŒ‡æ ‡ï¼š**

| æŒ‡æ ‡ | å«ä¹‰ | é˜ˆå€¼ | åº”å¯¹æªæ–½ |
|--------|------|--------|------|
| `apiserver_request_duration_seconds` | API è¯·æ±‚å»¶è¿Ÿ | P95 > 1 ç§’ | è€ƒè™‘ Provisioned Control Plane |
| `etcd_request_duration_seconds` | etcd å“åº”æ—¶é—´ | P95 > 100ms | å‡å°‘èŠ‚ç‚¹/Pod æ•°é‡ |
| `scheduler_schedule_attempts_total` | è°ƒåº¦å°è¯•æ¬¡æ•° | å¤±è´¥ç‡ > 5% | èµ„æºä¸è¶³ï¼Œæ£€æŸ¥ Node Affinity |
| `workqueue_depth` | Control Plane å·¥ä½œé˜Ÿåˆ—æ·±åº¦ | > 100 | é›†ç¾¤è¿‡è½½ä¿¡å· |

**æ•°æ®é©±åŠ¨ä¼˜åŒ–çš„ 3 ç§æµªè´¹æ¨¡å¼ï¼ˆAWS å®˜æ–¹æŒ‡å—ï¼‰ï¼š**

AWS äº 2025 å¹´ 11 æœˆå‘å¸ƒçš„ [Data-driven Amazon EKS cost optimization](https://aws.amazon.com/blogs/containers/data-driven-amazon-eks-cost-optimization-a-practical-guide-to-workload-analysis/) æŒ‡å—ä¸­ï¼Œé€šè¿‡å®é™…æ•°æ®åˆ†æè¯†åˆ«äº†ä»¥ä¸‹ 3 ç§ä¸»è¦æµªè´¹æ¨¡å¼ï¼š

```mermaid
graph TB
    A[èµ„æºæµªè´¹æ¨¡å¼åˆ†æ] --> B[1. Greedy Workloads]
    A --> C[2. Pet Workloads]
    A --> D[3. Isolated Workloads]

    B --> B1[è¿‡åº¦çš„èµ„æºè¯·æ±‚]
    B1 --> B2[å®é™…ä½¿ç”¨é‡çš„ 3-5 å€ requests]
    B2 --> B3[å¯¼è‡´èŠ‚ç‚¹ç¢ç‰‡åŒ–]

    C --> C1[ä¸¥æ ¼çš„ PodDisruptionBudget]
    C1 --> C2[é˜»æ­¢èŠ‚ç‚¹é©±é€]
    C2 --> C3[é˜»ç¢é›†ç¾¤ç¼©å‡]

    D --> D1[å›ºå®šåœ¨ç‰¹å®šèŠ‚ç‚¹çš„å·¥ä½œè´Ÿè½½]
    D1 --> D2[è¿‡åº¦ä½¿ç”¨ Node Affinity/Selector]
    D2 --> D3[èŠ‚ç‚¹æ± ç¢ç‰‡åŒ–]

    style B fill:#ff6b6b
    style C fill:#ffa94d
    style D fill:#ffd43b
```

**1. Greedy Workloadsï¼ˆè´ªå©ªå·¥ä½œè´Ÿè½½ï¼‰ï¼š**

è¿‡åº¦è¯·æ±‚èµ„æºçš„ Pod å¯¼è‡´èŠ‚ç‚¹åˆ©ç”¨ç‡ä½ä¸‹çš„æ¨¡å¼ã€‚

```bash
# CloudWatch Insights æŸ¥è¯¢ - è¯†åˆ« Over-requesting å®¹å™¨
fields @timestamp, PodName, ContainerName, pod_cpu_request, pod_cpu_utilization_over_pod_limit
| filter Namespace = "production"
| stats avg(pod_cpu_request) as avg_requested,
        avg(pod_cpu_utilization_over_pod_limit) as avg_utilization
  by PodName
| filter avg_utilization < 30  # ä½¿ç”¨ä¸åˆ°è¯·æ±‚é‡çš„ 30%
| sort avg_requested desc
```

**è¯†åˆ«æ ‡å‡†ï¼š**
- CPU requests ä½¿ç”¨ä¸åˆ° 30%
- Memory requests ä½¿ç”¨ä¸åˆ° 50%
- æŒç»­æ—¶é—´ï¼š7 å¤©ä»¥ä¸Š

**åº”å¯¹æ–¹æ³•ï¼š**
```yaml
# Beforeï¼ˆGreedyï¼‰
resources:
  requests:
    cpu: "2000m"       # å®é™…ä½¿ç”¨é‡ï¼š400mï¼ˆ20%ï¼‰
    memory: "4Gi"      # å®é™…ä½¿ç”¨é‡ï¼š1Giï¼ˆ25%ï¼‰

# Afterï¼ˆRight-Sizedï¼‰
resources:
  requests:
    cpu: "500m"        # P95 400m + 20% = 480m â†’ 500m
    memory: "1280Mi"   # P95 1Gi + 20% = 1.2Gi â†’ 1280Mi
  limits:
    memory: "2Gi"
```

**2. Pet Workloadsï¼ˆå® ç‰©å·¥ä½œè´Ÿè½½ï¼‰ï¼š**

ç”±äºä¸¥æ ¼çš„ PodDisruptionBudget (PDB) å¯¼è‡´é›†ç¾¤æ— æ³•ç¼©å‡çš„æ¨¡å¼ã€‚

```bash
# ç¡®è®¤å›  PDB å¯¼è‡´çš„èŠ‚ç‚¹é©±é€å¤±è´¥
kubectl get events --all-namespaces \
  --field-selector reason=EvictionFailed \
  --sort-by='.lastTimestamp'

# é¢„æœŸè¾“å‡ºï¼š
# NAMESPACE   LAST SEEN   TYPE      REASON           MESSAGE
# production  5m          Warning   EvictionFailed   Cannot evict pod as it would violate the pod's disruption budget
```

**è¯†åˆ«æ ‡å‡†ï¼š**
- è®¾ç½®äº† `minAvailable: 100%` æˆ– `maxUnavailable: 0`
- å­˜åœ¨é•¿æœŸï¼ˆ>30 åˆ†é’Ÿï¼‰Pending çŠ¶æ€èŠ‚ç‚¹
- Karpenter/Cluster Autoscaler ç¼©å‡å¤±è´¥æ—¥å¿—

**åº”å¯¹æ–¹æ³•ï¼š**
```yaml
# Beforeï¼ˆPetï¼‰
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
spec:
  minAvailable: 100%  # ä¿æŠ¤æ‰€æœ‰ Pod â†’ æ— æ³•ç¼©å‡

# Afterï¼ˆBalancedï¼‰
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
spec:
  minAvailable: 80%   # ç•™ 20% ä½™é‡å…è®¸ç¼©å‡
  selector:
    matchLabels:
      app: critical-app
```

**3. Isolated Workloadsï¼ˆå­¤ç«‹å·¥ä½œè´Ÿè½½ï¼‰ï¼š**

è¿‡åº¦ä½¿ç”¨ Node Affinityã€Taints/Tolerations å¯¼è‡´èŠ‚ç‚¹æ± ç¢ç‰‡åŒ–çš„æ¨¡å¼ã€‚

```bash
# åˆ†ææ¯ä¸ªèŠ‚ç‚¹çš„ Pod æ•°å’Œåˆ©ç”¨ç‡
kubectl get nodes -o json | jq -r '
  .items[] |
  {
    name: .metadata.name,
    pods: (.status.allocatable.pods | tonumber),
    cpu_capacity: (.status.capacity.cpu | tonumber),
    cpu_allocatable: (.status.allocatable.cpu | tonumber)
  }
' | jq -s 'sort_by(.pods) | .[]'
```

**è¯†åˆ«æ ‡å‡†ï¼š**
- æ¯èŠ‚ç‚¹å¹³å‡ Pod æ•° < 10 ä¸ª
- èŠ‚ç‚¹æ•° > æ‰€éœ€å®¹é‡çš„ 150%
- NodeSelector/Affinity ä½¿ç”¨ç‡ > 50%

**åº”å¯¹æ–¹æ³•ï¼š**
```yaml
# Beforeï¼ˆIsolatedï¼‰
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: workload-type
          operator: In
          values:
          - api-server-v2  # è¿‡äºå…·ä½“ â†’ èŠ‚ç‚¹ç¢ç‰‡åŒ–

# Afterï¼ˆFlexibleï¼‰
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:  # required â†’ preferred
    - weight: 100
      preference:
        matchExpressions:
        - key: workload-class
          operator: In
          values:
          - compute-optimized  # æ›´å¹¿æ³›çš„ç±»åˆ«
```

**æ•°æ®é©±åŠ¨ä¼˜åŒ–æµç¨‹ï¼š**

```mermaid
graph LR
    A[1. æ•°æ®æ”¶é›†] --> B[2. æ¨¡å¼åˆ†æ]
    B --> C[3. æµªè´¹è¯†åˆ«]
    C --> D[4. ä¼˜åŒ–åº”ç”¨]
    D --> E[5. éªŒè¯]
    E --> F{è¾¾æˆç›®æ ‡ï¼Ÿ}
    F -->|æ˜¯| G[æŒç»­ç›‘æ§]
    F -->|å¦| B

    A1[CloudWatch Container Insights] --> A
    A2[Prometheus Metrics] --> A
    A3[Cost Explorer] --> A

    C1[Greedy Workloads] --> C
    C2[Pet Workloads] --> C
    C3[Isolated Workloads] --> C

    D1[Right-Sizing] --> D
    D2[PDB æ”¾å®½] --> D
    D3[Affinity ä¼˜åŒ–] --> D

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style G fill:#c8e6c9
```

**å®é™…æ•ˆæœæ¡ˆä¾‹ï¼ˆAWS å®˜æ–¹æŒ‡å—ï¼‰ï¼š**

| ç»„ç»‡ | æµªè´¹æ¨¡å¼ | é‡‡å–æªæ–½ | èŠ‚çœæ•ˆæœ |
|------|----------|----------|----------|
| é‡‘èç§‘æŠ€åˆåˆ›å…¬å¸ | Greedy Workloads 40% | åº”ç”¨ VPA æ¨èå€¼ | èŠ‚ç‚¹æ•°å‡å°‘ 35% |
| ç”µå•†ä¼ä¸š | Pet Workloads 25% | PDB minAvailable æ”¾å®½è‡³ 80% | ç¼©å‡é€Ÿåº¦æå‡ 3 å€ |
| SaaS å¹³å° | Isolated Workloads 30% | ç§»é™¤ NodeSelectorï¼Œä½¿ç”¨ Spot | æˆæœ¬èŠ‚çœ 45% |

:::tip è‡ªåŠ¨åŒ–æµªè´¹æ¨¡å¼æ£€æµ‹
ä½¿ç”¨ CloudWatch Contributor Insights å¯ä»¥åˆ›å»ºè‡ªåŠ¨æ£€æµ‹ä¸Šè¿° 3 ç§æ¨¡å¼çš„è§„åˆ™ï¼š

```bash
# åˆ›å»º Contributor Insights è§„åˆ™ï¼ˆGreedy Workloadsï¼‰
aws cloudwatch put-insight-rule \
  --rule-name "EKS-GreedyWorkloads" \
  --rule-definition file://greedy-workloads-rule.json
```

è§„åˆ™å®šä¹‰ç¤ºä¾‹ï¼š
```json
{
  "Schema": {
    "Name": "CloudWatchLogRule",
    "Version": 1
  },
  "LogGroupNames": ["/aws/containerinsights/<cluster-name>/performance"],
  "LogFormat": "JSON",
  "Contribution": {
    "Keys": ["PodName"],
    "Filters": [
      {
        "Match": "$.Type",
        "In": ["Pod"]
      },
      {
        "Match": "$.pod_cpu_utilization_over_pod_limit",
        "LessThan": 30
      }
    ],
    "ValueOf": "pod_cpu_request"
  },
  "AggregateOn": "Sum"
}
```
:::

#### Prometheus æŸ¥è¯¢

```promql
# CPU ä½¿ç”¨é‡ï¼ˆP95ï¼Œ7 å¤©ï¼‰
quantile_over_time(0.95,
  sum by (pod, namespace) (
    rate(container_cpu_usage_seconds_total{namespace="production"}[5m])
  )[7d:5m]
)

# Memory ä½¿ç”¨é‡ï¼ˆP95ï¼Œ7 å¤©ï¼‰
quantile_over_time(0.95,
  sum by (pod, namespace) (
    container_memory_working_set_bytes{namespace="production"}
  )[7d:5m]
)

# CPU Requests ä¸å®é™…ä½¿ç”¨é‡æ¯”è¾ƒ
sum by (pod) (rate(container_cpu_usage_seconds_total[5m]))
/
sum by (pod) (kube_pod_container_resource_requests{resource="cpu"})

# Memory Requests ä¸å®é™…ä½¿ç”¨é‡æ¯”è¾ƒ
sum by (pod) (container_memory_working_set_bytes)
/
sum by (pod) (kube_pod_container_resource_requests{resource="memory"})
```

### 6.2 ä½¿ç”¨ Goldilocks è‡ªåŠ¨ Right-Sizing

Goldilocks åŸºäº VPA Recommender æä¾›ä»ªè¡¨æ¿ã€‚

#### å®‰è£…

```bash
# é€šè¿‡ Helm å®‰è£…
helm repo add fairwinds-stable https://charts.fairwinds.com/stable
helm repo update

helm install goldilocks fairwinds-stable/goldilocks \
  --namespace goldilocks \
  --create-namespace \
  --set dashboard.service.type=LoadBalancer
```

#### å¯ç”¨å‘½åç©ºé—´

```bash
# ä¸ºå‘½åç©ºé—´æ·»åŠ æ ‡ç­¾
kubectl label namespace production goldilocks.fairwinds.com/enabled=true
kubectl label namespace staging goldilocks.fairwinds.com/enabled=true

# Goldilocks å°†è‡ªåŠ¨åˆ›å»º VPAï¼ˆOff æ¨¡å¼ï¼‰
kubectl get vpa -n production
```

#### è®¿é—®ä»ªè¡¨æ¿

```bash
# ç¡®è®¤ä»ªè¡¨æ¿ URL
kubectl get svc -n goldilocks goldilocks-dashboard

# ç«¯å£è½¬å‘
kubectl port-forward -n goldilocks svc/goldilocks-dashboard 8080:80

# åœ¨æµè§ˆå™¨ä¸­è®¿é—® http://localhost:8080
```

**ä»ªè¡¨æ¿åŠŸèƒ½ï¼š**
- æŒ‰å‘½åç©ºé—´æ˜¾ç¤ºèµ„æºæ¨èå€¼
- æ˜¾ç¤º VPA Lower Boundã€Targetã€Upper Bound
- å½“å‰é…ç½®ä¸æ¨èå€¼å¯¹æ¯”
- QoS ç±»åˆ«æ˜¾ç¤º

### 6.3 Container Insights Enhanced å¼‚å¸¸æ£€æµ‹

AWS Container Insights Enhanced æä¾›æ¯”æ ‡å‡† Container Insights æ›´å¢å¼ºçš„å¯è§‚æµ‹æ€§åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡**è‡ªåŠ¨å¼‚å¸¸æ£€æµ‹**å’Œ**ä¸‹é’»åˆ†æ**åŠŸèƒ½å¯ä»¥æå‰å‘ç°èµ„æºé—®é¢˜ã€‚

#### 6.3.1 Container Insights Enhanced æ¦‚è¿°

**ç›¸æ¯”æ ‡å‡† Container Insights çš„å¢å¼ºåŠŸèƒ½ï¼š**

| åŠŸèƒ½ | æ ‡å‡† Container Insights | Enhanced |
|------|------------------------|----------|
| **æŒ‡æ ‡æ”¶é›†** | Pod/Container çº§åˆ« | Pod/Container + ç½‘ç»œç»†ç²’åº¦ |
| **å¼‚å¸¸æ£€æµ‹** | æ‰‹åŠ¨ï¼ˆç”¨æˆ·è®¾ç½®é˜ˆå€¼ï¼‰ | **è‡ªåŠ¨ï¼ˆML åŸºç¡€ anomaly detectionï¼‰** |
| **ä¸‹é’»** | æœ‰é™ | **å®Œæ•´å±‚çº§ç»“æ„ï¼ˆCluster â†’ Node â†’ Pod â†’ Containerï¼‰** |
| **å†…å­˜æ³„æ¼æ£€æµ‹** | éœ€æ‰‹åŠ¨åˆ†æ | **è‡ªåŠ¨è¯†åˆ«è§†è§‰æ¨¡å¼** |
| **CPU Throttling** | ä»…æä¾›æŒ‡æ ‡ | **è‡ªåŠ¨å‘Šè­¦ + åŸå› åˆ†æ** |
| **ç½‘ç»œå¯è§‚æµ‹æ€§** | åŸºæœ¬ | **Pod-to-Pod æµé‡åˆ†æ** |

**å¯ç”¨æ–¹æ³•ï¼š**

```bash
# éƒ¨ç½² CloudWatch Observability Operator
kubectl apply -f https://raw.githubusercontent.com/aws-observability/aws-cloudwatch-observability-operator/main/deploy/operator.yaml

# å¯ç”¨ Container Insights Enhanced
cat <<EOF | kubectl apply -f -
apiVersion: cloudwatch.aws.amazon.com/v1alpha1
kind: CloudWatchObservability
metadata:
  name: cloudwatch-observability
spec:
  enableContainerInsights: true
  enableEnhancedContainerInsights: true  # å¯ç”¨ Enhanced
  enableAutoInstrumentation: true
EOF

# ç¡®è®¤å¯ç”¨
kubectl get cloudwatchobservability cloudwatch-observability -o yaml
```

#### 6.3.2 å†…å­˜æ³„æ¼è§†è§‰è¯†åˆ«æ¨¡å¼

Container Insights Enhanced å¯è‡ªåŠ¨æ£€æµ‹å†…å­˜ä½¿ç”¨é‡çš„**æ¸è¿›å¢é•¿æ¨¡å¼**ã€‚

**å†…å­˜æ³„æ¼æ£€æµ‹åœºæ™¯ï¼š**

```mermaid
graph TB
    subgraph "Container Insights Enhanced å†…å­˜æ³„æ¼æ£€æµ‹"
        A[æ”¶é›†å†…å­˜æŒ‡æ ‡] --> B[CloudWatch Anomaly Detection]
        B --> C{æ£€æµ‹åˆ°å¼‚å¸¸æ¨¡å¼ï¼Ÿ}

        C -->|æ­£å¸¸| D[ç»§ç»­æ­£å¸¸ç›‘æ§]
        C -->|å†…å­˜æ¸è¿›å¢é•¿| E[ç–‘ä¼¼å†…å­˜æ³„æ¼]

        E --> F[è‡ªåŠ¨å‘é€é€šçŸ¥<br/>SNS/Slack/PagerDuty]
        F --> G[å¼€å§‹ä¸‹é’»åˆ†æ]

        G --> H[ç¡®è®¤ Pod çº§æŒ‡æ ‡]
        H --> I[Container çº§è¯¦ç»†åˆ†æ]
        I --> J[è¯†åˆ«åŸå›  Container]

        J --> K[èµ„æº Right-Sizing æˆ–<br/>åº”ç”¨ä¿®å¤]
    end

    style E fill:#ff6b6b
    style J fill:#ffa94d
    style K fill:#51cf66
```

**åœ¨ CloudWatch Console ä¸­ç¡®è®¤å†…å­˜æ³„æ¼ï¼š**

1. **CloudWatch â†’ Container Insights â†’ Performance monitoring**
2. **View: EKS Pods** é€‰æ‹©
3. **æŒ‡æ ‡ï¼šMemory Utilization (%)** é€‰æ‹©
4. **å¯ç”¨ Anomaly Detection Band**

```
æ­£å¸¸æ¨¡å¼ï¼š
Memory (%) â–²
100% |                    â”Œâ”€â”€â”€â”€â”
     |        â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”˜    â””â”€â”€â”
 50% |   â”Œâ”€â”€â”€â”˜    â””â”€â”€â”˜           â””â”€â”€â”€â”
     |â”€â”€â”€â”˜                            â””â”€â”€â”€
  0% +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
     0h    6h   12h   18h   24h        Time

å†…å­˜æ³„æ¼æ¨¡å¼ï¼ˆğŸš¨ï¼‰ï¼š
Memory (%) â–²
100% |                          â”Œâ”€â”€â”€â”€OOM Kill
     |                    â”Œâ”€â”€â”€â”€â”¤
 50% |           â”Œâ”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
     |      â”Œâ”€â”€â”€â”€â”¤       â”‚     â”‚
  0% +â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
     0h    6h   12h   18h   24h        Time
     æ¸è¿›ä¸Šå‡ï¼ˆAnomaly Detection è‡ªåŠ¨æ£€æµ‹ï¼‰
```

**è‡ªåŠ¨å‘Šè­¦é…ç½®ç¤ºä¾‹ï¼š**

```yaml
# CloudWatch Alarm with Anomaly Detection
apiVersion: v1
kind: ConfigMap
metadata:
  name: memory-leak-alarm
data:
  alarm.json: |
    {
      "AlarmName": "EKS-MemoryLeak-Detection",
      "ComparisonOperator": "LessThanLowerOrGreaterThanUpperThreshold",
      "EvaluationPeriods": 3,
      "Metrics": [
        {
          "Id": "m1",
          "ReturnData": true,
          "MetricStat": {
            "Metric": {
              "Namespace": "ContainerInsights",
              "MetricName": "pod_memory_utilization",
              "Dimensions": [
                {
                  "Name": "ClusterName",
                  "Value": "production-eks"
                }
              ]
            },
            "Period": 300,
            "Stat": "Average"
          }
        },
        {
          "Id": "ad1",
          "Expression": "ANOMALY_DETECTION_BAND(m1, 2)",
          "Label": "MemoryUsage (Expected)"
        }
      ],
      "ThresholdMetricId": "ad1",
      "ActionsEnabled": true,
      "AlarmActions": [
        "arn:aws:sns:us-east-1:123456789012:ops-alerts"
      ]
    }
```

**é€šè¿‡ AWS CLI åˆ›å»ºå‘Šè­¦ï¼š**

```bash
# åŸºäº Anomaly Detection çš„å†…å­˜å‘Šè­¦
aws cloudwatch put-metric-alarm \
  --alarm-name eks-memory-leak-detection \
  --alarm-description "Detects memory leak patterns in EKS pods" \
  --comparison-operator LessThanLowerOrGreaterThanUpperThreshold \
  --evaluation-periods 3 \
  --metrics '[
    {
      "Id": "m1",
      "ReturnData": true,
      "MetricStat": {
        "Metric": {
          "Namespace": "ContainerInsights",
          "MetricName": "pod_memory_utilization",
          "Dimensions": [
            {"Name": "ClusterName", "Value": "production-eks"}
          ]
        },
        "Period": 300,
        "Stat": "Average"
      }
    },
    {
      "Id": "ad1",
      "Expression": "ANOMALY_DETECTION_BAND(m1, 2)"
    }
  ]' \
  --threshold-metric-id ad1 \
  --alarm-actions arn:aws:sns:us-east-1:123456789012:ops-alerts
```

#### 6.3.3 CPU Throttling è‡ªåŠ¨æ£€æµ‹

Container Insights Enhanced å¯è‡ªåŠ¨æ£€æµ‹ CPU throttlingï¼Œå¹¶å¯¹**è¿‡é«˜çš„ CPU limit è®¾ç½®**å‘å‡ºå‘Šè­¦ã€‚

**CPU Throttling æŒ‡æ ‡ï¼š**

```
throttled_time_percentage = (container_cpu_cfs_throttled_seconds_total / container_cpu_cfs_periods_total) * 100

æ­£å¸¸ï¼š<5%
æ³¨æ„ï¼š5-10% âš ï¸
ä¸¥é‡ï¼š>10% ğŸš¨ï¼ˆéœ€è¦ HPA æˆ–ç§»é™¤ CPU limitsï¼‰
```

**é€šè¿‡ CloudWatch Insights æŸ¥è¯¢åˆ†æ Throttlingï¼š**

```sql
# CloudWatch Logs Insights Query
fields @timestamp, kubernetes.pod_name, cpu_limit_millicores, cpu_usage_millicores, throttled_time_ms
| filter kubernetes.namespace_name = "production"
| filter throttled_time_ms > 100  # throttling è¶…è¿‡ 100ms
| stats
    avg(cpu_usage_millicores) as avg_cpu,
    max(cpu_usage_millicores) as max_cpu,
    avg(throttled_time_ms) as avg_throttled,
    count(*) as throttling_count
  by kubernetes.pod_name
| sort throttling_count desc
| limit 20

# ç»“æœç¤ºä¾‹ï¼š
# pod_name            avg_cpu  max_cpu  avg_throttled  throttling_count
# web-app-abc123      450m     800m     250ms          150
# api-server-def456   600m     1000m    180ms          120
```

**Throttling è‡ªåŠ¨å‘Šè­¦ CloudWatch Alarmï¼š**

```bash
aws cloudwatch put-metric-alarm \
  --alarm-name eks-cpu-throttling-high \
  --alarm-description "Alerts when CPU throttling exceeds 10%" \
  --namespace ContainerInsights \
  --metric-name pod_cpu_throttled_percentage \
  --dimensions Name=ClusterName,Value=production-eks \
  --statistic Average \
  --period 300 \
  --threshold 10 \
  --comparison-operator GreaterThanThreshold \
  --evaluation-periods 2 \
  --alarm-actions arn:aws:sns:us-east-1:123456789012:ops-alerts
```

#### 6.3.4 å¼‚å¸¸æ£€æµ‹å¸¦ (Anomaly Detection Band) è®¾ç½®

CloudWatch Anomaly Detection ä½¿ç”¨ ML æ¨¡å‹è‡ªåŠ¨å­¦ä¹ æ­£å¸¸èŒƒå›´ã€‚

**Anomaly Detection å·¥ä½œåŸç†ï¼š**

```
1. å­¦ä¹ æœŸï¼šè‡³å°‘æ”¶é›† 2 å‘¨æ•°æ®
2. ML æ¨¡å‹è®­ç»ƒï¼šå­¦ä¹ æ—¶æ®µã€æ˜ŸæœŸæ¨¡å¼
3. é¢„æµ‹èŒƒå›´ç”Ÿæˆï¼šè®¡ç®—é¢„æœŸä¸Šé™/ä¸‹é™
4. å®æ—¶æ¯”è¾ƒï¼šå®é™…å€¼è¶…å‡ºèŒƒå›´æ—¶å‘Šè­¦
```

**å¸¦å®½è°ƒæ•´ï¼ˆæ ‡å‡†å·®ï¼‰ï¼š**

```yaml
# 2 ä¸ªæ ‡å‡†å·®ï¼ˆé»˜è®¤ï¼Œ95% ç½®ä¿¡åŒºé—´ï¼‰
Expression: ANOMALY_DETECTION_BAND(m1, 2)

# 3 ä¸ªæ ‡å‡†å·®ï¼ˆ99.7% ç½®ä¿¡åŒºé—´ï¼Œæ›´ä¿å®ˆï¼‰
Expression: ANOMALY_DETECTION_BAND(m1, 3)

# 1 ä¸ªæ ‡å‡†å·®ï¼ˆ68% ç½®ä¿¡åŒºé—´ï¼Œçµæ•æ£€æµ‹ï¼‰
Expression: ANOMALY_DETECTION_BAND(m1, 1)
```

**å¯è§†åŒ–ç¤ºä¾‹ï¼š**

```
èµ„æºä½¿ç”¨é‡ â–²
              |     â”Œâ”€â”€â”€â”€ Upper Bandï¼ˆé¢„æµ‹ä¸Šé™ï¼‰
              |    /
         100% | â”€â”€â—â”€â”€â”€â”€  å®é™…ä½¿ç”¨é‡ï¼ˆæ— å¼‚å¸¸ï¼‰
              |  / â”‚
              | /  â”‚
          50% |â”€â”€â”€â”€â—â”€â”€â”€â”€  å®é™…ä½¿ç”¨é‡ï¼ˆæ­£å¸¸ï¼‰
              | \  â”‚
              |  \ â”‚
           0% | â”€â”€â—â”€â”€â”€â”€  Lower Bandï¼ˆé¢„æµ‹ä¸‹é™ï¼‰
              +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
              0h   6h   12h   18h   24h
```

#### 6.3.5 å®æˆ˜å·¥ä½œæµï¼šå¼‚å¸¸æ£€æµ‹ â†’ è°ƒæŸ¥ â†’ Right-Sizing

**Step 1ï¼šCloudWatch Alarm è§¦å‘**

```
[CloudWatch Alarm] â†’ [SNS Topic] â†’ [Slack Webhook]

é€šçŸ¥ç¤ºä¾‹ï¼š
ğŸš¨ EKS Memory Anomaly Detected
Cluster: production-eks
Pod: web-app-7d8c9f-abc123
Memory Usage: 1.8Gi (Expected: 1.2Gi Â± 200Mi)
Duration: 15 minutes
Action: Investigate memory leak
```

**Step 2ï¼šContainer Insights ä¸‹é’»åˆ†æ**

```bash
# 1. åœ¨ CloudWatch Console ä¸­é€‰æ‹©è¯¥ Pod
# 2. ç‚¹å‡» "View in Container Insights"
# 3. å±‚çº§ä¸‹é’»ï¼š
#    Cluster â†’ Node â†’ Pod â†’ Container

# æˆ–é€šè¿‡ AWS CLI æŸ¥è¯¢æŒ‡æ ‡ï¼š
aws cloudwatch get-metric-statistics \
  --namespace ContainerInsights \
  --metric-name pod_memory_utilization \
  --dimensions \
    Name=ClusterName,Value=production-eks \
    Name=Namespace,Value=production \
    Name=PodName,Value=web-app-7d8c9f-abc123 \
  --start-time 2026-02-12T00:00:00Z \
  --end-time 2026-02-12T23:59:59Z \
  --period 300 \
  --statistics Average,Maximum
```

**Step 3ï¼šè¯†åˆ«åŸå› **

```bash
# ç¡®è®¤å†…å­˜æ³„æ¼
kubectl top pod web-app-7d8c9f-abc123 -n production --containers

# æŸ¥çœ‹æ—¥å¿—ï¼ˆOOM è­¦å‘Šï¼‰
kubectl logs web-app-7d8c9f-abc123 -n production | grep -i "memory\|heap\|oom"

# åº”ç”¨æ€§èƒ½åˆ†æï¼ˆJava ç¤ºä¾‹ï¼‰
kubectl exec web-app-7d8c9f-abc123 -n production -- jmap -heap 1
```

**Step 4ï¼šåº”ç”¨ Right-Sizing**

```yaml
# ä½¿ç”¨ VPA Off æ¨¡å¼ç¡®è®¤æ¨èå€¼
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"

# ç¡®è®¤ VPA æ¨èå€¼åæ›´æ–° Deployment
resources:
  requests:
    memory: "2Gi"    # VPA Target 1.8Gi + 20% ç¼“å†²
  limits:
    memory: "3Gi"    # Upper Bound 2.5Gi + ä½™é‡
```

**Step 5ï¼šæŒç»­ç›‘æ§**

```bash
# ç¡®è®¤ CloudWatch Alarm çŠ¶æ€
aws cloudwatch describe-alarms \
  --alarm-names eks-memory-leak-detection \
  --query 'MetricAlarms[0].StateValue'

# è¾“å‡ºï¼š"OK"ï¼ˆæ­£å¸¸ï¼‰æˆ– "ALARM"ï¼ˆå¼‚å¸¸ï¼‰
```

:::tip Container Insights Enhanced vs Prometheus
Container Insights Enhanced çš„ä¼˜åŠ¿åœ¨äº **AWS åŸç”Ÿé›†æˆ**å’Œ**é›¶é…ç½®å¼‚å¸¸æ£€æµ‹**ã€‚Prometheus å¯ä»¥å®ç°æ›´ç²¾ç»†çš„è‡ªå®šä¹‰ï¼Œä½†éœ€è¦è‡ªè¡Œæ„å»ºå¼‚å¸¸æ£€æµ‹ ML æ¨¡å‹ã€‚ä¸¤ç§å·¥å…·å¹¶è¡Œä½¿ç”¨å¯è·å¾—æœ€ä½³å¯è§‚æµ‹æ€§ã€‚
:::

:::warning å¼‚å¸¸æ£€æµ‹çš„å±€é™æ€§
ML åŸºç¡€å¼‚å¸¸æ£€æµ‹å­¦ä¹ **å†å²æ¨¡å¼**ï¼Œå› æ­¤ä»¥ä¸‹æƒ…å†µå¯èƒ½äº§ç”Ÿè¯¯æŠ¥ï¼ˆFalse Positiveï¼‰ï¼š
- æ–°éƒ¨ç½²ä¹‹åï¼ˆå­¦ä¹ æ•°æ®ä¸è¶³ï¼‰
- è¥é”€æ´»åŠ¨ç­‰è®¡åˆ’æ€§æµé‡å¢é•¿
- å­£èŠ‚æ€§äº‹ä»¶ï¼ˆé»‘è‰²æ˜ŸæœŸäº”ã€å¹´ç»ˆç»“ç®—ç­‰ï¼‰

è¿™äº›æƒ…å†µä¸‹éœ€è¦**æš‚æ—¶é™éŸ³å‘Šè­¦**æˆ–**å°†é¢„æœŸäº‹ä»¶åæ˜ åˆ° Anomaly Detection æ¨¡å‹ä¸­**ã€‚
:::

### 6.4 Right-Sizing æµç¨‹

5 é˜¶æ®µç³»ç»ŸåŒ– Right-Sizing æµç¨‹ï¼š

```mermaid
graph TB
    A[1 é˜¶æ®µï¼šå»ºç«‹åŸºçº¿] --> B[2 é˜¶æ®µï¼šéƒ¨ç½² VPA Off æ¨¡å¼]
    B --> C[3 é˜¶æ®µï¼šæ”¶é›† 7-14 å¤©æ•°æ®]
    C --> D[4 é˜¶æ®µï¼šåˆ†ææ¨èå€¼]
    D --> E[5 é˜¶æ®µï¼šåˆ†é˜¶æ®µåº”ç”¨]

    E --> F{éªŒè¯}
    F -->|æ€§èƒ½é—®é¢˜| G[å›æ»š]
    F -->|æ­£å¸¸| H[ä¸‹ä¸€ä¸ªå·¥ä½œè´Ÿè½½]

    G --> D
    H --> I[æŒç»­ç›‘æ§]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#f3e5f5
    style H fill:#c8e6c9
```

#### 1 é˜¶æ®µï¼šå»ºç«‹åŸºçº¿

```bash
# å¤‡ä»½å½“å‰èµ„æºé…ç½®
kubectl get deploy -n production -o yaml > deployments-backup.yaml

# å½“å‰ä½¿ç”¨é‡å¿«ç…§
kubectl top pods -n production --containers > baseline-usage.txt
```

#### 2 é˜¶æ®µï¼šéƒ¨ç½² VPA Off æ¨¡å¼

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"
  resourcePolicy:
    containerPolicies:
    - containerName: '*'    # æ‰€æœ‰å®¹å™¨
      minAllowed:
        cpu: "50m"
        memory: "64Mi"
      maxAllowed:
        cpu: "8000m"
        memory: "32Gi"
```

#### 3 é˜¶æ®µï¼šæ”¶é›† 7-14 å¤©æ•°æ®

```bash
# ç›‘æ§ VPA çŠ¶æ€
watch kubectl describe vpa web-app-vpa -n production

# è‡³å°‘ç­‰å¾… 7 å¤©ï¼Œå»ºè®® 14 å¤©
# å¦‚æœæµé‡æ¨¡å¼å‘ˆå‘¨å‘¨æœŸï¼Œ14 å¤©ä¸ºå¿…é¡»
```

#### 4 é˜¶æ®µï¼šåˆ†ææ¨èå€¼

```bash
# æå– VPA æ¨èå€¼
kubectl get vpa web-app-vpa -n production -o jsonpath='{.status.recommendation.containerRecommendations[0]}' | jq .

# è¾“å‡ºç¤ºä¾‹ï¼š
# {
#   "containerName": "web-app",
#   "lowerBound": {
#     "cpu": "150m",
#     "memory": "200Mi"
#   },
#   "target": {
#     "cpu": "250m",
#     "memory": "350Mi"
#   },
#   "uncappedTarget": {
#     "cpu": "300m",
#     "memory": "400Mi"
#   },
#   "upperBound": {
#     "cpu": "500m",
#     "memory": "700Mi"
#   }
# }
```

**æ¨èå€¼è§£è¯»ï¼š**

| é¡¹ç›® | å«ä¹‰ | ä½¿ç”¨æ—¶æœº |
|------|------|----------|
| **Lower Bound** | æœ€ä½æ‰€éœ€èµ„æº | æç«¯æˆæœ¬èŠ‚çœï¼ˆé£é™©é«˜ï¼‰|
| **Target** | **æ¨èè®¾ç½®å€¼** | **é»˜è®¤ä½¿ç”¨** â­ |
| **Uncapped Target** | æ— çº¦æŸæ¨èå€¼ | maxAllowed è°ƒæ•´å‚è€ƒ |
| **Upper Bound** | æœ€å¤§è§‚å¯Ÿä½¿ç”¨é‡ | Limits è®¾ç½®å‚è€ƒ |

:::tip Requests è®¡ç®—å…¬å¼
**æ¨èå…¬å¼**ï¼š`Requests = VPA Target + 20% ç¼“å†²`

åŸå› ï¼š
- åŸºäº P95 çš„æ¨èå€¼ï¼ˆåº”å¯¹ 5% æµé‡å³°å€¼ï¼‰
- éƒ¨ç½²ã€åˆå§‹åŒ–ç­‰ä¸´æ—¶ä½¿ç”¨é‡å¢åŠ 
- æœ€å°åŒ– Throttlingã€OOM é£é™©

**ç¤ºä¾‹ï¼š**
```
VPA Target CPU: 250m
â†’ Requests: 250m * 1.2 = 300m

VPA Target Memory: 350Mi
â†’ Requests: 350Mi * 1.2 = 420Miï¼ˆå››èˆäº”å…¥ 512Miï¼‰
```
:::

#### 5 é˜¶æ®µï¼šåˆ†é˜¶æ®µåº”ç”¨

```yaml
# åŸå§‹é…ç½®
resources:
  requests:
    cpu: "1000m"       # è¿‡åº¦é…ç½®
    memory: "2Gi"
  limits:
    cpu: "2000m"
    memory: "2Gi"

# VPA Target: CPU 250m, Memory 350Mi

# Right-Sized é…ç½®
resources:
  requests:
    cpu: "300m"        # Target 250m + 20% = 300m
    memory: "512Mi"    # Target 350Mi + 20% â‰ˆ 420Mi â†’ 512Mi
  limits:
    # ç§»é™¤ CPU limitsï¼ˆå¯å‹ç¼©èµ„æºï¼‰
    memory: "1Gi"      # Upper Bound 700Mi + ä½™é‡ = 1Gi
```

**åº”ç”¨ç­–ç•¥ï¼š**

```bash
# 1. Canary éƒ¨ç½²ï¼ˆ10% æµé‡ï¼‰
kubectl patch deploy web-app -n production -p '
{
  "spec": {
    "strategy": {
      "type": "RollingUpdate",
      "rollingUpdate": {
        "maxSurge": 1,
        "maxUnavailable": 0
      }
    }
  }
}'

# 2. åº”ç”¨èµ„æºå˜æ›´
kubectl set resources deploy web-app -n production \
  --limits=memory=1Gi \
  --requests=cpu=300m,memory=512Mi

# 3. ç›‘æ§ï¼ˆ1-3 å¤©ï¼‰
kubectl top pods -n production -l app=web-app
kubectl get events -n production --field-selector involvedObject.name=web-app

# 4. æ— å¼‚å¸¸åˆ™å…¨é¢åº”ç”¨
# æœ‰å¼‚å¸¸åˆ™ç«‹å³å›æ»š
kubectl rollout undo deploy web-app -n production
```

### 6.5 AI é©±åŠ¨çš„èµ„æºæ¨èè‡ªåŠ¨åŒ–ï¼ˆé«˜çº§ï¼‰

åˆ©ç”¨ AI å’Œ LLM å¯ä»¥è‡ªåŠ¨åŒ–èµ„æºä¼˜åŒ–æµç¨‹ã€‚æœ¬èŠ‚ä»‹ç»ä½¿ç”¨ Amazon Bedrockã€Kiroã€Amazon Q Developer çš„æœ€æ–°æ¨¡å¼ã€‚

#### 6.5.1 Amazon Bedrock + Prometheus â†’ è‡ªåŠ¨ Right-Sizing PR ç”Ÿæˆ

å°†ä¼ ç»Ÿæ‰‹åŠ¨ Right-Sizing æµç¨‹é€šè¿‡ AI å®ç°ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–çš„å·¥ä½œæµã€‚

**æ¶æ„æ¦‚è¿°ï¼š**

```mermaid
graph TB
    subgraph "æ•°æ®æ”¶é›†"
        A[EKS Cluster] -->|æŒ‡æ ‡| B[Prometheus/AMP]
        A -->|VPA æ¨èå€¼| C[VPA Recommender]
    end

    subgraph "AI åˆ†æ"
        B --> D[Lambda Function]
        C --> D
        D -->|æŒ‡æ ‡æŸ¥è¯¢| E[Amazon Bedrock<br/>Claude/Titan]
        E -->|åˆ†æç»“æœ| F[Right-Sizing æ¨èå€¼]
    end

    subgraph "è‡ªåŠ¨åº”ç”¨"
        F --> G[GitHub API]
        G -->|åˆ›å»º Pull Request| H[GitHub Repository]
        H -->|è‡ªåŠ¨å®¡æ‰¹/åˆå¹¶| I[ArgoCD/Flux]
        I -->|GitOps éƒ¨ç½²| A
    end

    style E fill:#4dabf7
    style G fill:#51cf66
    style I fill:#ffa94d
```

**å®ç°ç¤ºä¾‹ï¼š**

```python
# Lambda Functionï¼šAI é©±åŠ¨çš„ Right-Sizing æ¨è
import boto3
import json
import requests
from datetime import datetime, timedelta

bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')
amp_query_url = "https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-xxx/api/v1/query"

def lambda_handler(event, context):
    # 1. æ”¶é›† Prometheus æŒ‡æ ‡ï¼ˆ7 å¤©ï¼‰
    metrics = collect_prometheus_metrics(
        namespace="production",
        deployment="web-app",
        period_days=7
    )

    # 2. æ”¶é›† VPA æ¨èå€¼
    vpa_recommendations = get_vpa_recommendations("web-app-vpa", "production")

    # 3. é€šè¿‡ Amazon Bedrock åˆ†æ
    analysis_prompt = f"""
    è¯·åˆ†æä»¥ä¸‹ Kubernetes Deployment çš„èµ„æºä¼˜åŒ–ï¼š

    å½“å‰é…ç½®ï¼š
    {json.dumps(metrics['current_resources'], indent=2)}

    7 å¤©å®é™…ä½¿ç”¨é‡ï¼ˆP50/P95/P99ï¼‰ï¼š
    CPU: {metrics['cpu_p50']}m / {metrics['cpu_p95']}m / {metrics['cpu_p99']}m
    Memory: {metrics['mem_p50']}Mi / {metrics['mem_p95']}Mi / {metrics['mem_p99']}Mi

    VPA æ¨èå€¼ï¼š
    {json.dumps(vpa_recommendations, indent=2)}

    è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š
    1. å½“å‰èµ„æºæµªè´¹æˆ–ä¸è¶³æƒ…å†µ
    2. æ¨èçš„ requests/limits å€¼ï¼ˆå…·ä½“æ•°å€¼ï¼‰
    3. é¢„è®¡æˆæœ¬èŠ‚çœé¢
    4. é£é™©å› ç´ å’Œæ³¨æ„äº‹é¡¹
    5. åˆ†é˜¶æ®µåº”ç”¨è®¡åˆ’
    """

    response = bedrock.invoke_model(
        modelId='anthropic.claude-3-sonnet-20240229-v1:0',
        contentType='application/json',
        accept='application/json',
        body=json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 2000,
            "messages": [{
                "role": "user",
                "content": analysis_prompt
            }]
        })
    )

    analysis = json.loads(response['body'].read())['content'][0]['text']

    # 4. åˆ›å»º GitHub Pull Request
    create_right_sizing_pr(
        deployment="web-app",
        namespace="production",
        analysis=analysis,
        recommended_resources=parse_recommendations(analysis)
    )

    return {
        'statusCode': 200,
        'body': json.dumps({'message': 'Right-sizing PR created', 'analysis': analysis})
    }

def collect_prometheus_metrics(namespace, deployment, period_days):
    """ä» Prometheus æ”¶é›†èµ„æºä½¿ç”¨é‡"""
    end_time = datetime.now()
    start_time = end_time - timedelta(days=period_days)

    queries = {
        'cpu_p50': f'quantile_over_time(0.50, container_cpu_usage_seconds_total{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) * 1000',
        'cpu_p95': f'quantile_over_time(0.95, container_cpu_usage_seconds_total{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) * 1000',
        'cpu_p99': f'quantile_over_time(0.99, container_cpu_usage_seconds_total{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) * 1000',
        'mem_p50': f'quantile_over_time(0.50, container_memory_working_set_bytes{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) / 1024 / 1024',
        'mem_p95': f'quantile_over_time(0.95, container_memory_working_set_bytes{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) / 1024 / 1024',
        'mem_p99': f'quantile_over_time(0.99, container_memory_working_set_bytes{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) / 1024 / 1024',
    }

    results = {}
    for key, query in queries.items():
        response = requests.get(amp_query_url, params={'query': query})
        results[key] = int(float(response.json()['data']['result'][0]['value'][1]))

    return results

def create_right_sizing_pr(deployment, namespace, analysis, recommended_resources):
    """åœ¨ GitHub åˆ›å»º Right-Sizing PR"""
    github_token = get_secret('github-token')
    repo_owner = "my-org"
    repo_name = "k8s-manifests"

    # ä¿®æ”¹ Deployment YAML
    updated_yaml = update_deployment_resources(
        deployment=deployment,
        namespace=namespace,
        resources=recommended_resources
    )

    # åˆ›å»º Pull Request
    pr_body = f"""
## ğŸ¤– AI é©±åŠ¨çš„èµ„æº Right-Sizing å»ºè®®

### åˆ†æç»“æœ
{analysis}

### å˜æ›´å†…å®¹
- Deployment: `{namespace}/{deployment}`
- æ›´æ–°èµ„æº requests/limits

### éªŒè¯æ£€æŸ¥æ¸…å•
- [ ] åœ¨ Staging ç¯å¢ƒå®Œæˆæµ‹è¯•
- [ ] ç¡®è®¤æ€§èƒ½æŒ‡æ ‡æ­£å¸¸
- [ ] éªŒè¯æˆæœ¬èŠ‚çœé¢

### è‡ªåŠ¨ç”Ÿæˆä¿¡æ¯
- Generator: Amazon Bedrock + VPA Analysis
- Timestamp: {datetime.now().isoformat()}
"""

    headers = {
        'Authorization': f'token {github_token}',
        'Accept': 'application/vnd.github.v3+json'
    }

    # åˆ›å»ºåˆ†æ”¯å¹¶æäº¤
    create_branch_and_commit(repo_owner, repo_name, updated_yaml, headers)

    # åˆ›å»º PR
    pr_data = {
        'title': f'[AI] Right-Size {namespace}/{deployment}',
        'head': f'right-size-{deployment}-{datetime.now().strftime("%Y%m%d")}',
        'base': 'main',
        'body': pr_body
    }

    response = requests.post(
        f'https://api.github.com/repos/{repo_owner}/{repo_name}/pulls',
        headers=headers,
        json=pr_data
    )

    return response.json()
```

**é€šè¿‡ EventBridge è°ƒåº¦è‡ªåŠ¨åŒ–ï¼š**

```yaml
# CloudFormation æ¨¡æ¿ç¤ºä¾‹
Resources:
  RightSizingSchedule:
    Type: AWS::Events::Rule
    Properties:
      Name: weekly-right-sizing-analysis
      Description: "Weekly AI-based right-sizing analysis"
      ScheduleExpression: "cron(0 9 ? * MON *)"  # æ¯å‘¨ä¸€ä¸Šåˆ 9 ç‚¹
      State: ENABLED
      Targets:
        - Arn: !GetAtt RightSizingLambda.Arn
          Id: RightSizingTarget
          Input: |
            {
              "namespaces": ["production", "staging"],
              "auto_create_pr": true,
              "require_approval": true
            }
```

#### 6.5.2 åˆ©ç”¨ Kiro + EKS MCP çš„èµ„æºä¼˜åŒ–

**Kiro** æ˜¯ AWS çš„ AI é©±åŠ¨äº‘è¿ç»´å·¥å…·ï¼Œå¯é€šè¿‡**è‡ªç„¶è¯­è¨€æŸ¥è¯¢**æ‰§è¡Œ EKS èµ„æºä¼˜åŒ–ã€‚

**Kiro å®‰è£…ä¸é…ç½®ï¼š**

```bash
# å®‰è£… Kiro CLI
curl -sL https://kiro.aws.dev/install.sh | bash

# è¿æ¥ EKS MCP (Model Context Protocol)
kiro mcp connect eks --cluster production-eks --region us-east-1

# ç¡®è®¤è¿æ¥
kiro mcp list
# è¾“å‡ºï¼š
# âœ“ eks-production (connected)
# âœ“ cloudwatch-insights (connected)
# âœ“ cost-explorer (connected)
```

**è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¤ºä¾‹ï¼š**

```bash
# 1. æŸ¥æ‰¾éœ€è¦èµ„æºä¼˜åŒ–çš„ Pod
kiro ask "åœ¨ production å‘½åç©ºé—´ä¸­æ‰¾å‡º CPU ä½¿ç”¨ç‡ä½äº 30% çš„ Podï¼Œå¹¶ç»™å‡º Right-Sizing æ¨èå€¼"

# Kiro å“åº”ç¤ºä¾‹ï¼š
# ğŸ“Š åˆ†æç»“æœï¼š12 ä¸ª Pod å¤„äºè¿‡åº¦é…ç½®çŠ¶æ€ã€‚
#
# å‰ 5 åï¼š
# 1. web-app-7d8c9fï¼ˆå½“å‰ï¼š2 CPU / å®é™… P95ï¼š0.4 CPUï¼‰â†’ æ¨èï¼š0.5 CPU
# 2. api-server-abc123ï¼ˆå½“å‰ï¼š4 CPU / å®é™… P95ï¼š0.8 CPUï¼‰â†’ æ¨èï¼š1 CPU
# 3. worker-def456ï¼ˆå½“å‰ï¼š1 CPU / å®é™… P95ï¼š0.2 CPUï¼‰â†’ æ¨èï¼š0.3 CPU
#
# ğŸ’° é¢„è®¡èŠ‚çœï¼š$450/æœˆï¼ˆ45% èµ„æºå‡å°‘ï¼‰
#
# æ˜¯å¦åº”ç”¨ï¼Ÿ(y/n)

# 2. è¯†åˆ«ç–‘ä¼¼å†…å­˜æ³„æ¼çš„ Pod
kiro ask "æ‰¾å‡ºè¿‡å» 7 å¤©å†…å­˜ä½¿ç”¨é‡æŒç»­å¢é•¿çš„ Pod"

# Kiro å“åº”ï¼š
# ğŸ” æ£€æµ‹åˆ°å†…å­˜å¢é•¿æ¨¡å¼ï¼š
#
# âš ï¸ cache-service-xyz789
# - èµ·å§‹ï¼š500Mi â†’ å½“å‰ï¼š1.8Giï¼ˆå¢é•¿ 260%ï¼‰
# - è¶‹åŠ¿ï¼šæ¯å¤©å¢é•¿ 150Mi
# - é¢„è®¡ OOM æ—¶é—´ï¼š3 å¤©å
# - å»ºè®®æªæ–½ï¼šè°ƒæŸ¥å†…å­˜æ³„æ¼ + ä¸´æ—¶å°† limits ä¸Šè°ƒè‡³ 2.5Gi
#
# ğŸ“‹ æ˜¯å¦ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Šï¼Ÿ(y/n)

# 3. æ•´ä¸ªé›†ç¾¤æ•ˆç‡åˆ†æ
kiro ask "åˆ†æ production é›†ç¾¤çš„èµ„æºæ•ˆç‡å¹¶ç»™å‡ºä¼˜åŒ–ä¼˜å…ˆçº§"

# Kiro å“åº”ï¼š
# ğŸ“ˆ é›†ç¾¤æ•ˆç‡æŠ¥å‘Š
#
# æ€»ä½“æ•ˆç‡ï¼š52%ï¼ˆè¡Œä¸šå¹³å‡ï¼š65%ï¼‰
#
# ä¼˜åŒ–ä¼˜å…ˆçº§ï¼š
# 1. ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å¤„ç†ï¼‰
#    - 10 ä¸ª Deployment æœ‰ 70% CPU æœªä½¿ç”¨
#    - é¢„è®¡èŠ‚çœï¼š$1,200/æœˆ
#
# 2. ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆ1 å‘¨å†…ï¼‰
#    - 5 ä¸ª StatefulSet çš„ PVC å¤§å°è¿‡å¤§
#    - é¢„è®¡èŠ‚çœï¼š$300/æœˆ
#
# 3. ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆè§„åˆ’é˜¶æ®µï¼‰
#    - 15 ä¸ª Deployment æœªé…ç½® HPA
#    - å»ºè®®åˆ†ææµé‡æ¨¡å¼ååº”ç”¨
#
# æ˜¯å¦åˆ›å»ºè‡ªåŠ¨ Right-Sizing PRï¼Ÿ(y/n)
```

**Kiro å·¥ä½œæµè‡ªåŠ¨åŒ–ï¼š**

```yaml
# kiro-workflow.yaml
apiVersion: kiro.aws.dev/v1alpha1
kind: Workflow
metadata:
  name: weekly-optimization
spec:
  schedule: "0 9 * * MON"  # æ¯å‘¨ä¸€ä¸Šåˆ 9 ç‚¹
  steps:
    - name: analyze-underutilized
      action: analyze
      query: "åˆ†ææ‰€æœ‰ CPU ä½¿ç”¨ç‡ä½äº 30% æˆ– Memory ä½¿ç”¨ç‡ä½äº 40% çš„ Pod"
      outputFormat: json

    - name: generate-recommendations
      action: recommend
      input: ${{ steps.analyze-underutilized.output }}
      includeVPA: true
      includePrometheus: true

    - name: create-pr
      action: github-pr
      repository: my-org/k8s-manifests
      branch: kiro-right-sizing-{{ date }}
      title: "[Kiro] Weekly Right-Sizing Recommendations"
      body: ${{ steps.generate-recommendations.output }}
      autoMerge: false  # éœ€è¦äººå·¥å®¡æ ¸

    - name: notify
      action: slack
      webhook: ${{ secrets.SLACK_WEBHOOK }}
      message: |
        ğŸ“Š æ¯å‘¨ Right-Sizing åˆ†æå®Œæˆ
        PR: ${{ steps.create-pr.pr_url }}
        é¢„è®¡èŠ‚çœï¼š${{ steps.generate-recommendations.estimated_savings }}
```

#### 6.5.3 åˆ©ç”¨ Amazon Q Developer çš„å¯¹è¯å¼ä¼˜åŒ–

Amazon Q Developer å¯åœ¨ IDE å’Œ CLI ä¸­ç›´æ¥æä¾›èµ„æºä¼˜åŒ–å»ºè®®ã€‚

**åœ¨ VS Code ä¸­ä½¿ç”¨ï¼š**

```yaml
# æ‰“å¼€ deployment.yaml å¹¶å‘ Q Developer æé—®
# /q optimize-resources

# Q Developer å“åº”ï¼š
# åˆ†æäº†å½“å‰ Deployment çš„èµ„æºé…ç½®ï¼š
#
# ğŸ” å‘ç°çš„é—®é¢˜ï¼š
# 1. CPU requests æ¯”å®é™…ä½¿ç”¨é‡é«˜ 3 å€ï¼ˆ1000m â†’ æ¨è 350mï¼‰
# 2. æœªè®¾ç½® Memory limitsï¼Œå­˜åœ¨ OOM é£é™©
# 3. QoS ç±»åˆ«ï¼šBurstableï¼ˆæ¨è Guaranteedï¼‰
#
# ğŸ’¡ ä¼˜åŒ–åçš„é…ç½®ï¼š
resources:
  requests:
    cpu: "350m"      # å®é™… P95 + 20% ç¼“å†²
    memory: "512Mi"  # å®é™… P95 400Mi + 20%
  limits:
    memory: "1Gi"    # Upper Bound + ä½™é‡
    # ç§»é™¤ CPU limitsï¼ˆGoogle/Datadog æ¨¡å¼ï¼‰
#
# æ˜¯å¦åº”ç”¨æ­¤å˜æ›´ï¼Ÿ(Apply / Dismiss)
```

**åœ¨ CLI ä¸­ä½¿ç”¨ï¼š**

```bash
# é€šè¿‡ Amazon Q CLI æŸ¥è¯¢
q ask "ä¼˜åŒ–è¿™ä¸ª Deployment çš„èµ„æº" --file deployment.yaml

# è¾“å‡ºï¼š
# åˆ†æä¸­... âœ“
#
# å½“å‰é…ç½®é—®é¢˜ï¼š
# - CPU over-provisioned by 65%
# - Memory under-provisioned (OOM risk)
#
# æ¨èå˜æ›´å·²ä¿å­˜åˆ° deployment-optimized.yamlã€‚
# æ˜¯å¦æŸ¥çœ‹å·®å¼‚ï¼Ÿ(y/n)

# è¾“å…¥ y æ—¶ï¼š
diff deployment.yaml deployment-optimized.yaml
```

#### 6.5.4 æ³¨æ„äº‹é¡¹ä¸å±€é™æ€§

AI é©±åŠ¨çš„èµ„æºæ¨èè™½ç„¶å¼ºå¤§ï¼Œä½†éœ€ç†è§£ä»¥ä¸‹å±€é™æ€§ï¼š

| å±€é™æ€§ | è¯´æ˜ | åº”å¯¹æ–¹æ³• |
|------|------|----------|
| **ä¾èµ–å†å²æ•°æ®** | æ— æ³•é¢„æµ‹è¿‡å»æœªå‡ºç°çš„æµé‡æ¨¡å¼ | å¹¶è¡Œä½¿ç”¨ HPAï¼Œä¿æŒç¼“å†²ä½™é‡ |
| **ç¼ºä¹ä¸Šä¸‹æ–‡** | æœªåæ˜ ä¸šåŠ¡éœ€æ±‚ï¼ˆSLAã€åˆè§„ï¼‰ | å¿…é¡»åŒ…å«äººå·¥å®¡æ ¸é˜¶æ®µ |
| **ä¸´æ—¶å³°å€¼** | æœªè€ƒè™‘è¥é”€æ´»åŠ¨ç­‰è®¡åˆ’æ€§è´Ÿè½½ | æ´»åŠ¨æœŸé—´æ‰‹åŠ¨æ‰©å®¹ |
| **æˆæœ¬ä¼˜åŒ–åå·®** | å¯èƒ½ä¼˜å…ˆè€ƒè™‘æˆæœ¬èŠ‚çœè€Œéç¨³å®šæ€§ | æ’é™¤ Critical å·¥ä½œè´Ÿè½½ |

:::warning AI æ¨èåº”ä½œä¸ºè¾…åŠ©å·¥å…·ä½¿ç”¨
AI é©±åŠ¨çš„èµ„æºæ¨èæ˜¯**è¾…åŠ©å·¥å…·è€Œéæœ€ç»ˆå†³ç­–å·¥å…·**ã€‚ç”Ÿäº§ç¯å¢ƒåº”ç”¨å‰å¿…é¡»ï¼š

1. **åœ¨ Staging ç¯å¢ƒéªŒè¯**ï¼ˆè‡³å°‘ 3 å¤©ï¼‰
2. **ç›‘æ§æ€§èƒ½æŒ‡æ ‡**ï¼ˆLatency P99ã€Error Rateï¼‰
3. **æ¸è¿›å¼å‘å¸ƒ**ï¼ˆCanary 10% â†’ 50% â†’ 100%ï¼‰
4. **åˆ¶å®šå›æ»šè®¡åˆ’**ï¼ˆ1 åˆ†é’Ÿå†…å¯æ¢å¤åˆ°ä¸Šä¸€ç‰ˆæœ¬ï¼‰

ç‰¹åˆ«æ˜¯ä»¥ä¸‹å·¥ä½œè´Ÿè½½**ä¸è¦åº”ç”¨ AI æ¨èï¼Œåº”æ‰‹åŠ¨ç®¡ç†**ï¼š
- é‡‘èäº¤æ˜“ç³»ç»Ÿ
- åŒ»ç–—ä¿¡æ¯ç³»ç»Ÿ
- å®æ—¶æµåª’ä½“æœåŠ¡
- Stateful æ•°æ®åº“
:::

**AI æ¨èéªŒè¯æ£€æŸ¥æ¸…å•ï¼š**

```yaml
# ç”Ÿäº§ç¯å¢ƒåº”ç”¨å‰å¿…é¡»éªŒè¯
ai_recommendation_validation:
  staging_test:
    duration_days: 3
    success_criteria:
      - p99_latency_increase: "<5%"
      - error_rate_increase: "<0.1%"
      - no_oom_kills: true
      - no_cpu_throttling: "<10%"

  canary_rollout:
    initial_percentage: 10
    increment_percentage: 20
    increment_interval_hours: 6
    auto_rollback_threshold:
      error_rate: 1.0  # é”™è¯¯ç‡è¶…è¿‡ 1% æ—¶è‡ªåŠ¨å›æ»š
      latency_p99_ms: 500  # P99 å»¶è¿Ÿè¶…è¿‡ 500ms æ—¶å›æ»š

  monitoring:
    dashboard_url: "https://grafana.example.com/d/right-sizing"
    alert_channels: ["slack://ops-team", "pagerduty://oncall"]
    review_required: true  # ç¦æ­¢è‡ªåŠ¨åˆå¹¶ï¼Œå¿…é¡»äººå·¥å®¡æ ¸
```

:::tip AI + Human æ··åˆæ–¹æ³•
æœ€ä½³æ•ˆæœæ¥è‡ª **AI æ¨è + äººç±»ä¸“å®¶å®¡æ ¸**çš„ç»„åˆï¼š

1. AI ä»æ•°åƒä¸ª Pod ä¸­ç­›é€‰ä¼˜åŒ–ç›®æ ‡ï¼ˆé€Ÿåº¦ï¼‰
2. äººç±»æ’é™¤ Critical å·¥ä½œè´Ÿè½½å¹¶éªŒè¯ï¼ˆå¯é æ€§ï¼‰
3. AI ç”Ÿæˆåˆå§‹ PRï¼ˆè‡ªåŠ¨åŒ–ï¼‰
4. äººç±»åœ¨ Staging æµ‹è¯•åå®¡æ‰¹ï¼ˆå®‰å…¨æ€§ï¼‰
5. GitOps æ¸è¿›å¼éƒ¨ç½²ï¼ˆè¿ç»´æ•ˆç‡ï¼‰

æ­¤æµç¨‹å¯å®ç°**æ¯”æ‰‹åŠ¨èŠ‚çœ 80% æ—¶é—´**ï¼Œ**ç¨³å®šæ€§ä¿æŒä¸å˜**ã€‚
:::

## Resource Quota ä¸ LimitRange

### 7.1 Namespace çº§åˆ«èµ„æºé™åˆ¶

é€šè¿‡ ResourceQuota é™åˆ¶å‘½åç©ºé—´çš„æ€»èµ„æºï¼š

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    # æ€»èµ„æºé™åˆ¶
    requests.cpu: "100"           # 100 CPU æ ¸
    requests.memory: "200Gi"      # 200GB RAM
    limits.cpu: "200"             # CPU limits åˆè®¡
    limits.memory: "400Gi"        # Memory limits åˆè®¡

    # å¯¹è±¡æ•°é‡é™åˆ¶
    pods: "500"                   # æœ€å¤š 500 ä¸ª Pod
    services: "50"                # æœ€å¤š 50 ä¸ª Service
    persistentvolumeclaims: "100" # æœ€å¤š 100 ä¸ª PVC

    # å­˜å‚¨é™åˆ¶
    requests.storage: "2Ti"       # æ€»å…± 2TB å­˜å‚¨

---
# æŒ‰ç¯å¢ƒé…ç½® Quota ç¤ºä¾‹
apiVersion: v1
kind: ResourceQuota
metadata:
  name: development-quota
  namespace: development
spec:
  hard:
    requests.cpu: "20"
    requests.memory: "40Gi"
    limits.cpu: "40"
    limits.memory: "80Gi"
    pods: "100"

---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: staging-quota
  namespace: staging
spec:
  hard:
    requests.cpu: "50"
    requests.memory: "100Gi"
    limits.cpu: "100"
    limits.memory: "200Gi"
    pods: "200"
```

**æŸ¥çœ‹ Quota ä½¿ç”¨é‡ï¼š**

```bash
# å½“å‰ Quota ä½¿ç”¨é‡
kubectl describe resourcequota production-quota -n production

# è¾“å‡ºç¤ºä¾‹ï¼š
# Name:            production-quota
# Namespace:       production
# Resource         Used   Hard
# --------         ----   ----
# limits.cpu       150    200
# limits.memory    300Gi  400Gi
# pods             342    500
# requests.cpu     75     100
# requests.memory  150Gi  200Gi
```

### 7.2 é€šè¿‡ LimitRange è®¾ç½®é»˜è®¤å€¼

é€šè¿‡ LimitRange è‡ªåŠ¨ä¸º Pod/Container æ³¨å…¥é»˜è®¤èµ„æºï¼š

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limitrange
  namespace: production
spec:
  limits:
  # Container çº§åˆ«çº¦æŸ
  - type: Container
    default:                    # æœªè®¾ç½® limits æ—¶çš„é»˜è®¤å€¼
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:             # æœªè®¾ç½® requests æ—¶çš„é»˜è®¤å€¼
      cpu: "100m"
      memory: "128Mi"
    max:                        # æœ€å¤§å…è®¸å€¼
      cpu: "4000m"
      memory: "8Gi"
    min:                        # æœ€å°è¦æ±‚å€¼
      cpu: "50m"
      memory: "64Mi"
    maxLimitRequestRatio:       # limits/requests æœ€å¤§æ¯”ç‡
      cpu: "4"                  # limits æœ€å¤šä¸º requests çš„ 4 å€
      memory: "2"               # limits æœ€å¤šä¸º requests çš„ 2 å€

  # Pod çº§åˆ«çº¦æŸ
  - type: Pod
    max:
      cpu: "8000m"
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"

  # PVC çº¦æŸ
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "1Gi"

---
# å¼€å‘ç¯å¢ƒ LimitRange
apiVersion: v1
kind: LimitRange
metadata:
  name: development-limitrange
  namespace: development
spec:
  limits:
  - type: Container
    default:
      cpu: "200m"
      memory: "256Mi"
    defaultRequest:
      cpu: "50m"
      memory: "64Mi"
    max:
      cpu: "2000m"
      memory: "4Gi"
```

**å·¥ä½œç¤ºä¾‹ï¼š**

```yaml
# å¼€å‘è€…ç¼–å†™çš„ YAMLï¼ˆæœªæŒ‡å®šèµ„æºï¼‰
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: production
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    # æ²¡æœ‰ resources éƒ¨åˆ†

# LimitRange è‡ªåŠ¨æ³¨å…¥åçš„ç»“æœ
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: production
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    resources:
      requests:           # åº”ç”¨ defaultRequest
        cpu: "100m"
        memory: "128Mi"
      limits:             # åº”ç”¨ default
        cpu: "500m"
        memory: "512Mi"
```

**éªŒè¯ï¼š**

```bash
# æŸ¥çœ‹ LimitRange
kubectl describe limitrange production-limitrange -n production

# æŸ¥çœ‹ Pod åº”ç”¨çš„èµ„æº
kubectl get pod test-pod -n production -o jsonpath='{.spec.containers[0].resources}' | jq .
```

### 7.3 DRA (Dynamic Resource Allocation) - GPU/ç‰¹æ®Šèµ„æºç®¡ç†

Kubernetes 1.31+ å¼•å…¥çš„ **DRA (Dynamic Resource Allocation)** æ˜¯ä¸€ç§å¯ä»¥æ›´çµæ´»åœ°åˆ†é… GPUã€FPGAã€NPU ç­‰ç‰¹æ®Šèµ„æºçš„æ–°æœºåˆ¶ã€‚

#### ç°æœ‰ Device Plugin vs DRA

| ç‰¹æ€§ | Device Pluginï¼ˆç°æœ‰ï¼‰ | DRA (K8s 1.31+) |
|------|---------------------|-----------------|
| **èµ„æºè¡¨ç¤º** | ç®€å•æ•°å­— (`nvidia.com/gpu: 1`) | ç»“æ„åŒ–å‚æ•°ï¼ˆå†…å­˜ã€è®¡ç®—æ¨¡å¼ï¼‰|
| **å…±äº«å¯èƒ½æ€§** | ä¸å¯èƒ½ï¼ˆ1 Pod = 1 GPUï¼‰| å¯èƒ½ï¼ˆæ—¶é—´åˆ†ç‰‡ã€MIG æ”¯æŒï¼‰|
| **åŠ¨æ€åˆ†é…** | è°ƒåº¦æ—¶å†³å®š | è¿è¡Œæ—¶åŠ¨æ€åˆ†é… |
| **å¤æ‚æ‹“æ‰‘** | æœ‰é™ | NUMAã€PCIe æ‹“æ‰‘æ„ŸçŸ¥ |
| **å¤šç§Ÿæˆ·** | å›°éš¾ | åŸç”Ÿæ”¯æŒ |

**DRA æ ¸å¿ƒæ¦‚å¿µï¼š**

```mermaid
graph LR
    A[Pod with ResourceClaim] --> B[Scheduler]
    B --> C[ResourceClass åŒ¹é…]
    C --> D[DRA Driver]
    D --> E[åˆ†é…ç‰©ç† GPU]
    E --> F[Pod è¿è¡Œ]

    style A fill:#4dabf7
    style E fill:#51cf66
```

#### DRA ç»„æˆéƒ¨åˆ†

**1. ResourceClassï¼ˆé›†ç¾¤çº§èµ„æºå®šä¹‰ï¼‰**

```yaml
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClass
metadata:
  name: nvidia-a100-gpu
spec:
  driverName: gpu.nvidia.com
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClassParameters
    name: a100-80gb
---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClassParameters
metadata:
  name: a100-80gb
spec:
  # GPU ç‰¹æ€§å®šä¹‰
  memory: "80Gi"
  computeCapability: "8.0"
  # MIG (Multi-Instance GPU) æ”¯æŒ
  migEnabled: true
  migProfile: "1g.10gb"  # 1/7 GPU åˆ‡ç‰‡
```

**2. ResourceClaimï¼ˆPod è¯·æ±‚çš„èµ„æºï¼‰**

```yaml
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaim
metadata:
  name: ml-training-gpu
  namespace: ml-team
spec:
  resourceClassName: nvidia-a100-gpu
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClaimParameters
    name: training-config
---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: training-config
spec:
  # è¯·æ±‚çš„ GPU è§„æ ¼
  count: 2  # è¯·æ±‚ 2 ä¸ª GPU
  sharing: "TimeSlicing"  # å…è®¸æ—¶é—´åˆ†ç‰‡å…±äº«
  selector:
    matchLabels:
      gpu.nvidia.com/memory: "80Gi"
```

**3. åœ¨ Pod ä¸­ä½¿ç”¨ ResourceClaim**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pytorch-training
  namespace: ml-team
spec:
  containers:
  - name: trainer
    image: pytorch/pytorch:2.1.0-cuda12.1
    command: ["python", "train.py"]
    resources:
      requests:
        cpu: "8"
        memory: "32Gi"
      limits:
        memory: "64Gi"

  # é€šè¿‡ DRA åˆ†é… GPU
  resourceClaims:
  - name: gpu
    source:
      resourceClaimName: ml-training-gpu

  # åœ¨å®¹å™¨ä¸­å¼•ç”¨ claim
  containers:
  - name: trainer
    # ...
    resources:
      claims:
      - name: gpu
```

#### åœ¨ EKS ä¸­å¯ç”¨ DRA åŠ GPU åˆ†é…ç¤ºä¾‹

**Step 1ï¼šåœ¨ EKS é›†ç¾¤ä¸­å¯ç”¨ DRA Feature Gate**

```bash
# åˆ›å»º EKS 1.31+ é›†ç¾¤æ—¶
eksctl create cluster \
  --name dra-enabled-cluster \
  --version 1.31 \
  --region us-west-2 \
  --nodegroup-name gpu-nodes \
  --node-type p4d.24xlarge \
  --nodes 2 \
  --kubernetes-feature-gates DynamicResourceAllocation=true
```

**Step 2ï¼šå®‰è£… NVIDIA GPU Operatorï¼ˆå« DRA é©±åŠ¨ï¼‰**

```bash
# é€šè¿‡ Helm å®‰è£… GPU Operatorï¼ˆDRA æ”¯æŒç‰ˆæœ¬ï¼‰
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update

helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator \
  --create-namespace \
  --set driver.enabled=true \
  --set toolkit.enabled=true \
  --set devicePlugin.enabled=false \  # ç¦ç”¨ç°æœ‰ device plugin
  --set dra.enabled=true \             # å¯ç”¨ DRA
  --set migManager.enabled=true        # MIG æ”¯æŒ
```

**Step 3ï¼šé€šè¿‡ ResourceClaimTemplate è‡ªåŠ¨åˆ›å»º Claim**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference
  namespace: ml-team
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: model-server
        image: tritonserver:24.01
        resources:
          requests:
            cpu: "4"
            memory: "16Gi"
          claims:
          - name: gpu

      # é€šè¿‡ ResourceClaimTemplate ä¸ºæ¯ä¸ª Pod è‡ªåŠ¨åˆ›å»º
      resourceClaims:
      - name: gpu
        source:
          resourceClaimTemplateName: shared-gpu-template

---
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaimTemplate
metadata:
  name: shared-gpu-template
  namespace: ml-team
spec:
  spec:
    resourceClassName: nvidia-a100-gpu
    parametersRef:
      apiGroup: gpu.nvidia.com
      kind: GpuClaimParameters
      name: shared-inference-config

---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: shared-inference-config
spec:
  count: 1
  sharing: "TimeSlicing"  # å¤šä¸ª Pod æ—¶é—´åˆ†ç‰‡å…±äº«
  requests:
    memory: "10Gi"        # ä»…è¯·æ±‚ 10GB GPU å†…å­˜
```

**DRA ä¼˜åŠ¿æ€»ç»“ï¼š**

1. **GPU å…±äº«**ï¼šé€šè¿‡ MIG æˆ– Time-Slicing è®©å¤šä¸ª Pod ä½¿ç”¨ 1 ä¸ª GPU
2. **ç²¾ç»†æ§åˆ¶**ï¼šå¯æŒ‡å®š GPU å†…å­˜ã€è®¡ç®—æ¨¡å¼ã€æ‹“æ‰‘
3. **åŠ¨æ€åˆ†é…**ï¼šPod åˆ›å»ºåä¹Ÿå¯æ·»åŠ /ç§»é™¤èµ„æº
4. **æˆæœ¬èŠ‚çœ**ï¼šæé«˜ GPU åˆ©ç”¨ç‡ï¼ˆç°æœ‰ 30-40% â†’ DRA 70-80%ï¼‰

:::warning EKS DRA æ”¯æŒçŠ¶æ€ï¼ˆ2026 å¹´ 2 æœˆåŸºå‡†ï¼‰
- Kubernetes 1.31+ ä»¥ alpha åŠŸèƒ½æä¾›
- EKS ä¸­éœ€æ‰‹åŠ¨å¯ç”¨ Feature Gate
- ç”Ÿäº§ä½¿ç”¨æ—¶ç¡®è®¤ NVIDIA GPU Operator æœ€æ–°ç‰ˆæœ¬ï¼ˆv24.9.0+ï¼‰
- MIG æ”¯æŒä»…åœ¨ A100/H100 GPU ä¸Šå¯ç”¨
:::

### 7.3.1 Setuï¼šKueue-Karpenter é›†æˆæ¶ˆé™¤ GPU ç©ºé—²æˆæœ¬

åœ¨ AI/ML å·¥ä½œè´Ÿè½½ä¸­ GPU æ˜¯æœ€æ˜‚è´µçš„èµ„æºï¼Œä½†ç°æœ‰ååº”å¼é…ç½®æ–¹å¼ä¼šå¯¼è‡´ä¸¥é‡æµªè´¹ã€‚**Setu** å°† Kueue çš„é…é¢ç®¡ç†ä¸ Karpenter çš„èŠ‚ç‚¹é…ç½®è¿æ¥èµ·æ¥ï¼Œå®ç°ä¸»åŠ¨å¼èµ„æºåˆ†é…ã€‚

#### ååº”å¼é…ç½®çš„èµ„æºæµªè´¹é—®é¢˜

**é—®é¢˜åœºæ™¯ï¼š**
1. 4-GPU è®­ç»ƒ Job è¿›å…¥ Queue
2. Karpenter é€ä¸ªé…ç½®èŠ‚ç‚¹ï¼ˆè€—æ—¶ 5-10 åˆ†é’Ÿï¼‰
3. ä»… 2 ä¸ªèŠ‚ç‚¹å°±ç»ªæ—¶ Pod å°è¯•è°ƒåº¦ â†’ å¤±è´¥
4. **2 ä¸ª GPU å¤„äºç©ºé—²ç­‰å¾…çŠ¶æ€äº§ç”Ÿæˆæœ¬**
5. å…¶ä½™èŠ‚ç‚¹å°±ç»ªåå·¥ä½œè´Ÿè½½æ‰å¼€å§‹

**æˆæœ¬å½±å“ï¼š**
- p4d.24xlarge (8x A100) = $32.77/å°æ—¶
- 10 åˆ†é’Ÿç©ºé—²ç­‰å¾… Ã— 2 èŠ‚ç‚¹ = **$10.92 æµªè´¹**
- æ¯æ—¥ 100 æ¬¡æ‰§è¡Œæ—¶æ¯æœˆ $32,760 ä¸å¿…è¦æˆæœ¬

#### Setu çš„ All-or-Nothing é…ç½®

```mermaid
graph LR
    A[Job æäº¤] --> B[Kueue: é…é¢éªŒè¯]
    B --> C[Setu: NodePool å®¹é‡é¢„æ£€]
    C -->|å……è¶³| D[æ‰€æœ‰èŠ‚ç‚¹åŒæ—¶é…ç½®]
    C -->|ä¸è¶³| E[ç«‹å³å¤±è´¥ - ç­‰å¾…æ—¶é—´ 0]
    D --> F[ç¡®è®¤æ‰€æœ‰èŠ‚ç‚¹ Ready]
    F --> G[Job æ‰§è¡Œ - æ— ç©ºé—²]

    style C fill:#4dabf7
    style G fill:#51cf66
    style E fill:#ff6b6b
```

**Setu å·¥ä½œæ–¹å¼ï¼š**

1. **é¢„æ£€å®¹é‡**ï¼šç¡®è®¤ Karpenter NodePool ä¸­æ˜¯å¦æœ‰è¶³å¤Ÿçš„èŠ‚ç‚¹å®¹é‡
2. **åŒæ—¶é…ç½®**ï¼šåŒæ—¶è¯·æ±‚æ‰€æœ‰èŠ‚ç‚¹ï¼ˆæ— é¡ºåºç­‰å¾…ï¼‰
3. **Gang Scheduling ä¿è¯**ï¼šæ‰€æœ‰èŠ‚ç‚¹ Ready åæ‰å¯åŠ¨å·¥ä½œè´Ÿè½½
4. **å¤±è´¥æ—¶ç«‹å³ç»ˆæ­¢**ï¼šå®¹é‡ä¸è¶³æ—¶ç«‹å³å¤±è´¥ï¼Œæ¶ˆé™¤æ— æ„ä¹‰ç­‰å¾…

#### Kueue ClusterQueue é›†æˆ

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: gpu-cluster-queue
spec:
  namespaceSelector: {}
  resourceGroups:
  - coveredResources: ["cpu", "memory", "nvidia.com/gpu"]
    flavors:
    - name: a100-spot
      resources:
      - name: "nvidia.com/gpu"
        nominalQuota: 32  # 4 ä¸ªèŠ‚ç‚¹ Ã— 8 GPU
      - name: "cpu"
        nominalQuota: 384
      - name: "memory"
        nominalQuota: 1536Gi
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: ml-team-queue
  namespace: ml-training
spec:
  clusterQueue: gpu-cluster-queue
---
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: a100-spot-pool
spec:
  template:
    spec:
      requirements:
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["p4d.24xlarge"]
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot", "on-demand"]
      nodeClassRef:
        name: a100-nodeclass
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m
  # Setu é¢„æ£€æ­¤ NodePool çš„å®¹é‡
  limits:
    cpu: "384"
    memory: "1536Gi"
```

**Setu Controller æ“ä½œï¼š**

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: llm-training
  namespace: ml-training
  labels:
    kueue.x-k8s.io/queue-name: ml-team-queue
    setu.io/enabled: "true"  # å¯ç”¨ Setu
spec:
  parallelism: 4  # éœ€è¦ 4 ä¸ªèŠ‚ç‚¹
  completions: 4
  template:
    spec:
      schedulerName: default-scheduler
      containers:
      - name: trainer
        image: pytorch/pytorch:2.1-cuda12.1
        resources:
          requests:
            nvidia.com/gpu: 8  # æ¯èŠ‚ç‚¹ 8 GPU
            memory: 384Gi
          limits:
            nvidia.com/gpu: 8
```

**Setu æ“ä½œæµç¨‹ï¼š**

1. Job è¿›å…¥ Kueue Queue
2. Kueue ç¡®è®¤é…é¢ï¼ˆ32 GPU ä¸­å¯ç”¨æ•°ç¡®è®¤ï¼‰
3. **Setu ä»‹å…¥**ï¼šéªŒè¯ Karpenter NodePool `a100-spot-pool` ä¸­æ˜¯å¦å¯é…ç½® 4 ä¸ª p4d.24xlarge èŠ‚ç‚¹
4. **å¯è¡Œæ—¶**ï¼šåŒæ—¶è¯·æ±‚é…ç½® 4 ä¸ªèŠ‚ç‚¹ + Job ç­‰å¾…
5. **ä¸å¯è¡Œæ—¶**ï¼šJob ç«‹å³å¤±è´¥ï¼ˆè·¯ç”±åˆ°å…¶ä»– Queue æˆ–é‡è¯•ï¼‰
6. æ‰€æœ‰èŠ‚ç‚¹ Ready åè°ƒåº¦ Job â†’ **ç©ºé—² GPU ä¸º 0**

#### èµ„æºæ•ˆç‡å¯¹æ¯”

| åœºæ™¯ | ç°æœ‰æ–¹å¼ | Setu æ–¹å¼ | èŠ‚çœæ•ˆæœ |
|------|----------|-----------|----------|
| **4-GPU Job å¯åŠ¨æ—¶é—´** | é€ä¸ªé…ç½®èŠ‚ç‚¹ï¼ˆ15 åˆ†é’Ÿï¼‰ | åŒæ—¶é…ç½®ï¼ˆ7 åˆ†é’Ÿï¼‰ | **ç¼©çŸ­ 53%** |
| **ç©ºé—² GPU æˆæœ¬** | 2 ä¸ªèŠ‚ç‚¹ Ã— 10 åˆ†é’Ÿç­‰å¾… = $10.92 | 0ï¼ˆåŒæ—¶å¯åŠ¨ï¼‰ | **èŠ‚çœ 100%** |
| **å®¹é‡ä¸è¶³æ—¶ç­‰å¾…** | ç­‰å¾… 10 åˆ†é’Ÿåå¤±è´¥ | ç«‹å³å¤±è´¥ï¼ˆ0 ç§’ï¼‰ | **æ¶ˆé™¤ç­‰å¾…æ—¶é—´** |
| **Spot ä¸­æ–­æ—¶é‡å¯** | éƒ¨åˆ†èŠ‚ç‚¹é‡å»º â†’ äº§ç”Ÿç©ºé—² | Gang ä¿è¯é‡é…ç½® | **æœ€å°åŒ–ä¸­æ–­æˆæœ¬** |

**æœˆåº¦æˆæœ¬èŠ‚çœï¼ˆ100 Job æ‰§è¡ŒåŸºå‡†ï¼‰ï¼š**
- ç©ºé—²æˆæœ¬èŠ‚çœï¼š**$32,760/æœˆ**
- Cold start æ¶ˆé™¤ï¼š**$16,380/æœˆ**ï¼ˆå¯åŠ¨æ—¶é—´ç¼©çŸ­ 53%ï¼‰
- **æ€»èŠ‚çœï¼š$49,140/æœˆ**

#### å¤šç§Ÿæˆ·ç¯å¢ƒä¸­çš„å…¬å¹³æ€§ + æ•ˆç‡

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: shared-gpu-queue
spec:
  preemption:
    withinClusterQueue: LowerPriority
    reclaimWithinCohort: Any
  resourceGroups:
  - coveredResources: ["nvidia.com/gpu"]
    flavors:
    - name: a100-80gb
      resources:
      - name: "nvidia.com/gpu"
        nominalQuota: 64
        borrowingLimit: 32  # å…¶ä»–å›¢é˜Ÿç©ºé—²æ—¶å¯é¢å¤–ä½¿ç”¨ 32 GPU
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: research-team
  namespace: research
spec:
  clusterQueue: shared-gpu-queue
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: production-team
  namespace: production
spec:
  clusterQueue: shared-gpu-queue
```

**Setu + Kueue é›†æˆä¼˜åŠ¿ï¼š**

1. **å…¬å¹³é…é¢ç®¡ç†**ï¼šKueue ç®¡ç†å›¢é˜Ÿçº§ GPU é…é¢
2. **é«˜æ•ˆé…ç½®**ï¼šSetu åŸºäº NodePool å®¹é‡è¿›è¡Œé¢„æ£€
3. **Borrowing ä¼˜åŒ–**ï¼šå…¶ä»–å›¢é˜Ÿä½¿ç”¨ç©ºé—² GPU æ—¶ä¹Ÿä¿è¯ Gang Scheduling
4. **Spot åˆ©ç”¨æœ€å¤§åŒ–**ï¼šé˜²æ­¢éƒ¨åˆ†åˆ†é…ï¼Œæœ€å°åŒ– Spot ä¸­æ–­å½±å“

:::tip Setu æ¨èåº”ç”¨åœºæ™¯
- **å¤§è§„æ¨¡ GPU å·¥ä½œè´Ÿè½½**ï¼šéœ€è¦ 4+ GPU æ—¶ç©ºé—²æˆæœ¬ä¸¥é‡
- **ä½¿ç”¨ Spot å®ä¾‹**ï¼šé€šè¿‡ gang scheduling æé«˜ Spot ä¸­æ–­åº”å¯¹èƒ½åŠ›
- **å¤šç§Ÿæˆ·ç¯å¢ƒ**ï¼šåŒæ—¶ç¡®ä¿ Kueue å…¬å¹³æ€§ + Karpenter æ•ˆç‡
- **æˆæœ¬æ•æ„Ÿ**ï¼šGPU ç©ºé—²æ—¶é—´æ¯æœˆé€ æˆæ•°åƒç¾å…ƒæˆæœ¬
:::

**å‚è€ƒèµ„æ–™ï¼š**
- [Setu GitHub Repository](https://github.com/sanjeevrg89/Setu)
- [Kueue å®˜æ–¹æ–‡æ¡£](https://kueue.sigs.k8s.io/)
- [Karpenter NodePool é…ç½®æŒ‡å—](https://karpenter.sh/)

### 7.4 é€šè¿‡ EKS Blueprints IaC æ¨¡å¼æ ‡å‡†åŒ–èµ„æºç­–ç•¥

ä½¿ç”¨ Terraform EKS Blueprints å¯ä»¥å°† ResourceQuotaã€LimitRangeã€Policy Enforcement ä»£ç åŒ–ï¼Œåœ¨æ‰€æœ‰é›†ç¾¤ä¸­ä¸€è‡´åº”ç”¨ã€‚

#### Terraform EKS Blueprints AddOn ç»“æ„

```hcl
# main.tf - é€šè¿‡ EKS Blueprints è‡ªåŠ¨éƒ¨ç½²èµ„æºç­–ç•¥
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 20.0"

  cluster_name    = "production-eks"
  cluster_version = "1.31"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  enable_irsa = true

  eks_managed_node_groups = {
    general = {
      desired_size = 3
      min_size     = 2
      max_size     = 10
      instance_types = ["m6i.xlarge"]
    }
  }
}

# é€šè¿‡ EKS Blueprints AddOns éƒ¨ç½²èµ„æºç­–ç•¥
module "eks_blueprints_addons" {
  source  = "aws-ia/eks-blueprints-addons/aws"
  version = "~> 1.16"

  cluster_name      = module.eks.cluster_name
  cluster_endpoint  = module.eks.cluster_endpoint
  cluster_version   = module.eks.cluster_version
  oidc_provider_arn = module.eks.oidc_provider_arn

  # Metrics Serverï¼ˆVPA å‰ç½®æ¡ä»¶ï¼‰
  enable_metrics_server = true

  # Karpenterï¼ˆèŠ‚ç‚¹è‡ªåŠ¨æ‰©ç¼©ï¼‰
  enable_karpenter = true
  karpenter = {
    repository_username = data.aws_ecrpublic_authorization_token.token.user_name
    repository_password = data.aws_ecrpublic_authorization_token.token.password
  }

  # Kyvernoï¼ˆèµ„æºç­–ç•¥å¼ºåˆ¶ï¼‰
  enable_kyverno = true
  kyverno = {
    values = [templatefile("${path.module}/kyverno-policies.yaml", {
      default_cpu_request    = "100m"
      default_memory_request = "128Mi"
      max_cpu_limit          = "4000m"
      max_memory_limit       = "8Gi"
    })]
  }
}

# é€šè¿‡ Helm Chart éƒ¨ç½² ResourceQuota
resource "helm_release" "resource_quotas" {
  name      = "resource-quotas"
  namespace = "kube-system"

  chart = "${path.module}/charts/resource-quotas"

  values = [
    yamlencode({
      quotas = {
        production = {
          cpu    = "100"
          memory = "200Gi"
          pods   = "500"
        }
        staging = {
          cpu    = "50"
          memory = "100Gi"
          pods   = "200"
        }
        development = {
          cpu    = "20"
          memory = "40Gi"
          pods   = "100"
        }
      }
    })
  ]
}
```

#### é€šè¿‡ Kyverno ç­–ç•¥å¼ºåˆ¶èµ„æºè¯·æ±‚

```yaml
# kyverno-policies.yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-resource-requests
  annotations:
    policies.kyverno.io/title: Require Resource Requests
    policies.kyverno.io/severity: medium
    policies.kyverno.io/description: |
      æ‰€æœ‰ Pod å¿…é¡»è®¾ç½® CPU å’Œ Memory requestsã€‚
spec:
  validationFailureAction: Enforce  # Auditï¼ˆä»…è­¦å‘Šï¼‰æˆ– Enforceï¼ˆé˜»æ­¢ï¼‰
  background: true
  rules:
  - name: check-cpu-memory-requests
    match:
      any:
      - resources:
          kinds:
          - Pod
    validate:
      message: "CPU å’Œ Memory requests æ˜¯å¿…éœ€çš„"
      pattern:
        spec:
          containers:
          - resources:
              requests:
                memory: "?*"
                cpu: "?*"

  - name: enforce-memory-limits
    match:
      any:
      - resources:
          kinds:
          - Pod
    validate:
      message: "Memory limits æ˜¯å¿…éœ€çš„ï¼ˆé˜²æ­¢ OOM Killï¼‰"
      pattern:
        spec:
          containers:
          - resources:
              limits:
                memory: "?*"

  - name: prevent-excessive-resources
    match:
      any:
      - resources:
          kinds:
          - Pod
    validate:
      message: "CPU æœ€å¤§ {{ max_cpu_limit }}ï¼ŒMemory æœ€å¤§ {{ max_memory_limit }}"
      deny:
        conditions:
          any:
          - key: "{{ request.object.spec.containers[].resources.requests.cpu }}"
            operator: GreaterThan
            value: "{{ max_cpu_limit }}"
          - key: "{{ request.object.spec.containers[].resources.requests.memory }}"
            operator: GreaterThan
            value: "{{ max_memory_limit }}"
```

#### OPA Gatekeeper ç­–ç•¥ç¤ºä¾‹ï¼ˆæ›¿ä»£æ–¹æ¡ˆï¼‰

```yaml
# ConstraintTemplate - å¼ºåˆ¶èµ„æºè¯·æ±‚
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequireresources
spec:
  crd:
    spec:
      names:
        kind: K8sRequireResources
      validation:
        openAPIV3Schema:
          type: object
          properties:
            exemptNamespaces:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequireresources

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.requests.cpu
          msg := sprintf("å®¹å™¨ %v æœªè®¾ç½® CPU requests", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.requests.memory
          msg := sprintf("å®¹å™¨ %v æœªè®¾ç½® Memory requests", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.limits.memory
          msg := sprintf("å®¹å™¨ %v æœªè®¾ç½® Memory limitsï¼ˆOOM é£é™©ï¼‰", [container.name])
        }

---
# Constraint - åº”ç”¨ ConstraintTemplate
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequireResources
metadata:
  name: require-resources-production
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces: ["production", "staging"]
  parameters:
    exemptNamespaces: ["kube-system", "kube-node-lease"]
```

#### åŸºäº GitOps çš„èµ„æºç­–ç•¥ç®¡ç†æ¨¡å¼

**é€šè¿‡ ArgoCD ApplicationSet æŒ‰ç¯å¢ƒéƒ¨ç½² ResourceQuotaï¼š**

```yaml
# argocd/applicationset-resource-policies.yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: resource-policies
  namespace: argocd
spec:
  generators:
  - list:
      elements:
      - env: production
        cpu: "100"
        memory: "200Gi"
        pods: "500"
      - env: staging
        cpu: "50"
        memory: "100Gi"
        pods: "200"
      - env: development
        cpu: "20"
        memory: "40Gi"
        pods: "100"

  template:
    metadata:
      name: "resource-quota-{{env}}"
    spec:
      project: platform
      source:
        repoURL: https://github.com/myorg/k8s-manifests
        targetRevision: main
        path: resource-policies/{{env}}
        helm:
          parameters:
          - name: quota.cpu
            value: "{{cpu}}"
          - name: quota.memory
            value: "{{memory}}"
          - name: quota.pods
            value: "{{pods}}"
      destination:
        server: https://kubernetes.default.svc
        namespace: "{{env}}"
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
```

**ä»“åº“ç»“æ„ï¼š**

```
k8s-manifests/
â”œâ”€â”€ resource-policies/
â”‚   â”œâ”€â”€ production/
â”‚   â”‚   â”œâ”€â”€ resource-quota.yaml
â”‚   â”‚   â”œâ”€â”€ limit-range.yaml
â”‚   â”‚   â””â”€â”€ kyverno-policies.yaml
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ development/
â”‚       â””â”€â”€ ...
â””â”€â”€ argocd/
    â””â”€â”€ applicationset-resource-policies.yaml
```

:::tip EKS Blueprints + GitOps æ¨èæ¨¡å¼
1. **é€šè¿‡ Terraform é…ç½®é›†ç¾¤**ï¼ˆVPCã€EKSã€AddOnsï¼‰
2. **é€šè¿‡ Kyverno/OPA å¼ºåˆ¶ç­–ç•¥**ï¼ˆèµ„æºè¯·æ±‚å¿…éœ€ã€é˜»æ­¢è¿‡åº¦åˆ†é…ï¼‰
3. **é€šè¿‡ ArgoCD ApplicationSet æŒ‰ç¯å¢ƒéƒ¨ç½²ç­–ç•¥**ï¼ˆGitOpsï¼‰
4. **é€šè¿‡ Prometheus + Grafana ç›‘æ§ç­–ç•¥åˆè§„ç‡**

æ­¤ç»„åˆå®ç° **"é›†ç¾¤ç”¨ Terraformï¼Œç­–ç•¥ç”¨ Git"** çš„ç®¡ç†æ–¹å¼ï¼Œè¾¾åˆ°åŸºç¡€è®¾æ–½æ ‡å‡†åŒ–å’Œè¿ç»´è‡ªåŠ¨åŒ–ã€‚
:::
kubectl top pods -n production --containers > baseline-usage.txt
```

#### ç¬¬2æ­¥ï¼šéƒ¨ç½² VPA Off æ¨¡å¼

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Off"
  resourcePolicy:
    containerPolicies:
    - containerName: '*'    # æ‰€æœ‰å®¹å™¨
      minAllowed:
        cpu: "50m"
        memory: "64Mi"
      maxAllowed:
        cpu: "8000m"
        memory: "32Gi"
```

#### ç¬¬3æ­¥ï¼šæ”¶é›† 7-14 å¤©æ•°æ®

```bash
# ç›‘æ§ VPA çŠ¶æ€
watch kubectl describe vpa web-app-vpa -n production

# è‡³å°‘ç­‰å¾… 7 å¤©ï¼Œå»ºè®® 14 å¤©
# å¦‚æœæµé‡æ¨¡å¼å…·æœ‰å‘¨é—´å‘¨æœŸï¼Œåˆ™å¿…é¡»ç­‰å¾… 14 å¤©
```

#### ç¬¬4æ­¥ï¼šåˆ†æå»ºè®®

```bash
# æå– VPA å»ºè®®
kubectl get vpa web-app-vpa -n production -o jsonpath='{.status.recommendation.containerRecommendations[0]}' | jq .

# è¾“å‡ºç¤ºä¾‹ï¼š
# {
#   "containerName": "web-app",
#   "lowerBound": {
#     "cpu": "150m",
#     "memory": "200Mi"
#   },
#   "target": {
#     "cpu": "250m",
#     "memory": "350Mi"
#   },
#   "uncappedTarget": {
#     "cpu": "300m",
#     "memory": "400Mi"
#   },
#   "upperBound": {
#     "cpu": "500m",
#     "memory": "700Mi"
#   }
# }
```

**å»ºè®®è§£è¯»ï¼š**

| é¡¹ç›® | å«ä¹‰ | ä½¿ç”¨æ—¶æœº |
|------|------|----------|
| **Lower Bound** | æœ€å°æ‰€éœ€èµ„æº | æç«¯æˆæœ¬å‰Šå‡ï¼ˆæœ‰é£é™©ï¼‰ |
| **Target** | **æ¨èè®¾å®šå€¼** | **é»˜è®¤ä½¿ç”¨** â­ |
| **Uncapped Target** | æ— çº¦æŸæ¨èå€¼ | è°ƒæ•´ maxAllowed å‚è€ƒ |
| **Upper Bound** | è§‚å¯Ÿåˆ°çš„æœ€å¤§ä½¿ç”¨é‡ | Limits è®¾ç½®å‚è€ƒ |

:::tip Requests è®¡ç®—å…¬å¼
**æ¨èå…¬å¼**ï¼š`Requests = VPA Target + 20% ç¼“å†²`

åŸå› ï¼š
- åŸºäº P95 çš„å»ºè®®ï¼ˆåº”å¯¹ 5% æµé‡å°–å³°ï¼‰
- åº”å¯¹éƒ¨ç½²ã€åˆå§‹åŒ–ç­‰ä¸´æ—¶ä½¿ç”¨é‡å¢åŠ 
- æœ€å°åŒ– Throttlingã€OOM é£é™©

**ç¤ºä¾‹ï¼š**
```
VPA Target CPU: 250m
â†’ Requests: 250m * 1.2 = 300m

VPA Target Memory: 350Mi
â†’ Requests: 350Mi * 1.2 = 420Miï¼ˆå››èˆäº”å…¥ä¸º 512Miï¼‰
```
:::

#### ç¬¬5æ­¥ï¼šåˆ†é˜¶æ®µåº”ç”¨

```yaml
# ç°æœ‰è®¾ç½®
resources:
  requests:
    cpu: "1000m"       # è¿‡åº¦é…ç½®
    memory: "2Gi"
  limits:
    cpu: "2000m"
    memory: "2Gi"

# VPA Target: CPU 250m, Memory 350Mi

# Right-Sized è®¾ç½®
resources:
  requests:
    cpu: "300m"        # Target 250m + 20% = 300m
    memory: "512Mi"    # Target 350Mi + 20% â‰ˆ 420Mi â†’ 512Mi
  limits:
    # ç§»é™¤ CPU limitsï¼ˆå¯å‹ç¼©èµ„æºï¼‰
    memory: "1Gi"      # Upper Bound 700Mi + ä½™é‡ = 1Gi
```

**åº”ç”¨ç­–ç•¥ï¼š**

```bash
# 1. Canary éƒ¨ç½²ï¼ˆ10% æµé‡ï¼‰
kubectl patch deploy web-app -n production -p '
{
  "spec": {
    "strategy": {
      "type": "RollingUpdate",
      "rollingUpdate": {
        "maxSurge": 1,
        "maxUnavailable": 0
      }
    }
  }
}'

# 2. åº”ç”¨èµ„æºå˜æ›´
kubectl set resources deploy web-app -n production \
  --limits=memory=1Gi \
  --requests=cpu=300m,memory=512Mi

# 3. ç›‘æ§ï¼ˆ1-3 å¤©ï¼‰
kubectl top pods -n production -l app=web-app
kubectl get events -n production --field-selector involvedObject.name=web-app

# 4. æ— å¼‚å¸¸åˆ™å…¨é¢åº”ç”¨
# æœ‰å¼‚å¸¸åˆ™ç«‹å³å›æ»š
kubectl rollout undo deploy web-app -n production
```

### 6.5 åŸºäº AI çš„èµ„æºæ¨èè‡ªåŠ¨åŒ–ï¼ˆé«˜çº§ï¼‰

åˆ©ç”¨ AI å’Œ LLM å¯ä»¥è‡ªåŠ¨åŒ–èµ„æºä¼˜åŒ–æµç¨‹ã€‚æœ¬èŠ‚ä»‹ç»ä½¿ç”¨ Amazon Bedrockã€Kiroã€Amazon Q Developer çš„æœ€æ–°æ¨¡å¼ã€‚

#### 6.5.1 Amazon Bedrock + Prometheus â†’ è‡ªåŠ¨ Right-Sizing PR ç”Ÿæˆ

å°†ä¼ ç»Ÿçš„æ‰‹åŠ¨ Right-Sizing æµç¨‹é€šè¿‡ AI å®ç°ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–çš„å·¥ä½œæµã€‚

**æ¶æ„æ¦‚è§ˆï¼š**

```mermaid
graph TB
    subgraph "æ•°æ®æ”¶é›†"
        A[EKS Cluster] -->|æŒ‡æ ‡| B[Prometheus/AMP]
        A -->|VPA å»ºè®®| C[VPA Recommender]
    end

    subgraph "AI åˆ†æ"
        B --> D[Lambda Function]
        C --> D
        D -->|æŒ‡æ ‡æŸ¥è¯¢| E[Amazon Bedrock<br/>Claude/Titan]
        E -->|åˆ†æç»“æœ| F[Right-Sizing å»ºè®®]
    end

    subgraph "è‡ªåŠ¨åº”ç”¨"
        F --> G[GitHub API]
        G -->|åˆ›å»º Pull Request| H[GitHub Repository]
        H -->|è‡ªåŠ¨å®¡æ‰¹/åˆå¹¶| I[ArgoCD/Flux]
        I -->|GitOps éƒ¨ç½²| A
    end

    style E fill:#4dabf7
    style G fill:#51cf66
    style I fill:#ffa94d
```

**å®ç°ç¤ºä¾‹ï¼š**

```python
# Lambda Functionï¼šåŸºäº AI çš„ Right-Sizing æ¨è
import boto3
import json
import requests
from datetime import datetime, timedelta

bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')
amp_query_url = "https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-xxx/api/v1/query"

def lambda_handler(event, context):
    # 1. æ”¶é›† Prometheus æŒ‡æ ‡ï¼ˆ7 å¤©ï¼‰
    metrics = collect_prometheus_metrics(
        namespace="production",
        deployment="web-app",
        period_days=7
    )

    # 2. æ”¶é›† VPA å»ºè®®
    vpa_recommendations = get_vpa_recommendations("web-app-vpa", "production")

    # 3. ä½¿ç”¨ Amazon Bedrock è¿›è¡Œåˆ†æ
    analysis_prompt = f"""
    è¯·åˆ†æä»¥ä¸‹ Kubernetes Deployment çš„èµ„æºä¼˜åŒ–ï¼š

    å½“å‰è®¾ç½®ï¼š
    {json.dumps(metrics['current_resources'], indent=2)}

    7 å¤©å®é™…ä½¿ç”¨é‡ï¼ˆP50/P95/P99ï¼‰ï¼š
    CPU: {metrics['cpu_p50']}m / {metrics['cpu_p95']}m / {metrics['cpu_p99']}m
    Memory: {metrics['mem_p50']}Mi / {metrics['mem_p95']}Mi / {metrics['mem_p99']}Mi

    VPA å»ºè®®ï¼š
    {json.dumps(vpa_recommendations, indent=2)}

    è¯·æä¾›åŒ…å«ä»¥ä¸‹å†…å®¹çš„åˆ†æï¼š
    1. å½“å‰èµ„æºæ˜¯å¦å­˜åœ¨æµªè´¹æˆ–ä¸è¶³
    2. æ¨èçš„ requests/limits å€¼ï¼ˆå…·ä½“æ•°å€¼ï¼‰
    3. é¢„è®¡æˆæœ¬èŠ‚çœé¢
    4. é£é™©å› ç´ åŠæ³¨æ„äº‹é¡¹
    5. åˆ†é˜¶æ®µåº”ç”¨è®¡åˆ’
    """

    response = bedrock.invoke_model(
        modelId='anthropic.claude-3-sonnet-20240229-v1:0',
        contentType='application/json',
        accept='application/json',
        body=json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 2000,
            "messages": [{
                "role": "user",
                "content": analysis_prompt
            }]
        })
    )

    analysis = json.loads(response['body'].read())['content'][0]['text']

    # 4. åˆ›å»º GitHub Pull Request
    create_right_sizing_pr(
        deployment="web-app",
        namespace="production",
        analysis=analysis,
        recommended_resources=parse_recommendations(analysis)
    )

    return {
        'statusCode': 200,
        'body': json.dumps({'message': 'Right-sizing PR created', 'analysis': analysis})
    }

def collect_prometheus_metrics(namespace, deployment, period_days):
    """ä» Prometheus æ”¶é›†èµ„æºä½¿ç”¨é‡"""
    end_time = datetime.now()
    start_time = end_time - timedelta(days=period_days)

    queries = {
        'cpu_p50': f'quantile_over_time(0.50, container_cpu_usage_seconds_total{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) * 1000',
        'cpu_p95': f'quantile_over_time(0.95, container_cpu_usage_seconds_total{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) * 1000',
        'cpu_p99': f'quantile_over_time(0.99, container_cpu_usage_seconds_total{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) * 1000',
        'mem_p50': f'quantile_over_time(0.50, container_memory_working_set_bytes{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) / 1024 / 1024',
        'mem_p95': f'quantile_over_time(0.95, container_memory_working_set_bytes{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) / 1024 / 1024',
        'mem_p99': f'quantile_over_time(0.99, container_memory_working_set_bytes{{namespace="{namespace}",pod=~"{deployment}-.*"}}[{period_days}d]) / 1024 / 1024',
    }

    results = {}
    for key, query in queries.items():
        response = requests.get(amp_query_url, params={'query': query})
        results[key] = int(float(response.json()['data']['result'][0]['value'][1]))

    return results

def create_right_sizing_pr(deployment, namespace, analysis, recommended_resources):
    """åœ¨ GitHub ä¸Šåˆ›å»º Right-Sizing PR"""
    github_token = get_secret('github-token')
    repo_owner = "my-org"
    repo_name = "k8s-manifests"

    # ä¿®æ”¹ Deployment YAML
    updated_yaml = update_deployment_resources(
        deployment=deployment,
        namespace=namespace,
        resources=recommended_resources
    )

    # åˆ›å»º Pull Request
    pr_body = f"""
## ğŸ¤– åŸºäº AI çš„èµ„æº Right-Sizing å»ºè®®

### åˆ†æç»“æœ
{analysis}

### å˜æ›´å†…å®¹
- Deploymentï¼š`{namespace}/{deployment}`
- æ›´æ–°èµ„æº requests/limits

### éªŒè¯æ¸…å•
- [ ] Staging ç¯å¢ƒæµ‹è¯•å®Œæˆ
- [ ] æ€§èƒ½æŒ‡æ ‡æ­£å¸¸ç¡®è®¤
- [ ] æˆæœ¬èŠ‚çœé¢éªŒè¯

### è‡ªåŠ¨ç”Ÿæˆä¿¡æ¯
- Generatorï¼šAmazon Bedrock + VPA Analysis
- Timestampï¼š{datetime.now().isoformat()}
"""

    headers = {
        'Authorization': f'token {github_token}',
        'Accept': 'application/vnd.github.v3+json'
    }

    # åˆ›å»ºåˆ†æ”¯å¹¶æäº¤
    create_branch_and_commit(repo_owner, repo_name, updated_yaml, headers)

    # åˆ›å»º PR
    pr_data = {
        'title': f'[AI] Right-Size {namespace}/{deployment}',
        'head': f'right-size-{deployment}-{datetime.now().strftime("%Y%m%d")}',
        'base': 'main',
        'body': pr_body
    }

    response = requests.post(
        f'https://api.github.com/repos/{repo_owner}/{repo_name}/pulls',
        headers=headers,
        json=pr_data
    )

    return response.json()
```

**é€šè¿‡ EventBridge è°ƒåº¦å®ç°è‡ªåŠ¨åŒ–ï¼š**

```yaml
# CloudFormation æ¨¡æ¿ç¤ºä¾‹
Resources:
  RightSizingSchedule:
    Type: AWS::Events::Rule
    Properties:
      Name: weekly-right-sizing-analysis
      Description: "Weekly AI-based right-sizing analysis"
      ScheduleExpression: "cron(0 9 ? * MON *)"  # æ¯å‘¨ä¸€ä¸Šåˆ 9 ç‚¹
      State: ENABLED
      Targets:
        - Arn: !GetAtt RightSizingLambda.Arn
          Id: RightSizingTarget
          Input: |
            {
              "namespaces": ["production", "staging"],
              "auto_create_pr": true,
              "require_approval": true
            }
```

#### 6.5.2 åˆ©ç”¨ Kiro + EKS MCP è¿›è¡Œèµ„æºä¼˜åŒ–

**Kiro** æ˜¯ AWS çš„åŸºäº AI çš„äº‘è¿ç»´å·¥å…·ï¼Œå¯ä»¥é€šè¿‡**è‡ªç„¶è¯­è¨€æŸ¥è¯¢**æ‰§è¡Œ EKS èµ„æºä¼˜åŒ–ã€‚

**Kiro å®‰è£…å’Œé…ç½®ï¼š**

```bash
# å®‰è£… Kiro CLI
curl -sL https://kiro.aws.dev/install.sh | bash

# è¿æ¥ EKS MCPï¼ˆModel Context Protocolï¼‰
kiro mcp connect eks --cluster production-eks --region us-east-1

# ç¡®è®¤è¿æ¥
kiro mcp list
# è¾“å‡ºï¼š
# âœ“ eks-production (connected)
# âœ“ cloudwatch-insights (connected)
# âœ“ cost-explorer (connected)
```

**è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¤ºä¾‹ï¼š**

```bash
# 1. æŸ¥æ‰¾éœ€è¦èµ„æºä¼˜åŒ–çš„ Pod
kiro ask "æ‰¾å‡º production å‘½åç©ºé—´ä¸­ CPU ä½¿ç”¨ç‡ä½äº 30% çš„ Podï¼Œå¹¶ç»™å‡º Right-Sizing å»ºè®®"

# Kiro å“åº”ç¤ºä¾‹ï¼š
# ğŸ“Š åˆ†æç»“æœï¼š12 ä¸ª Pod å¤„äºè¿‡åº¦é…ç½®çŠ¶æ€ã€‚
#
# å‰ 5 ä¸ªï¼š
# 1. web-app-7d8c9fï¼ˆå½“å‰ï¼š2 CPU / å®é™… P95ï¼š0.4 CPUï¼‰â†’ å»ºè®®ï¼š0.5 CPU
# 2. api-server-abc123ï¼ˆå½“å‰ï¼š4 CPU / å®é™… P95ï¼š0.8 CPUï¼‰â†’ å»ºè®®ï¼š1 CPU
# 3. worker-def456ï¼ˆå½“å‰ï¼š1 CPU / å®é™… P95ï¼š0.2 CPUï¼‰â†’ å»ºè®®ï¼š0.3 CPU
#
# ğŸ’° é¢„è®¡èŠ‚çœï¼š$450/æœˆï¼ˆ45% èµ„æºå‡å°‘ï¼‰
#
# æ˜¯å¦åº”ç”¨ï¼Ÿ(y/n)

# 2. è¯†åˆ«ç–‘ä¼¼å†…å­˜æ³„æ¼çš„ Pod
kiro ask "æ‰¾å‡ºè¿‡å» 7 å¤©å†…å­˜ä½¿ç”¨é‡æŒç»­å¢é•¿çš„ Pod"

# Kiro å“åº”ï¼š
# ğŸ” æ£€æµ‹åˆ°å†…å­˜å¢é•¿æ¨¡å¼ï¼š
#
# âš ï¸ cache-service-xyz789
# - èµ·å§‹ï¼š500Mi â†’ å½“å‰ï¼š1.8Giï¼ˆå¢é•¿ 260%ï¼‰
# - è¶‹åŠ¿ï¼šæ¯å¤©å¢é•¿ 150Mi
# - é¢„è®¡ OOM æ—¶é—´ï¼š3 å¤©
# - å»ºè®®æªæ–½ï¼šè°ƒæŸ¥å†…å­˜æ³„æ¼ + ä¸´æ—¶å°† limits æé«˜åˆ° 2.5Gi
#
# ğŸ“‹ æ˜¯å¦ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Šï¼Ÿ(y/n)

# 3. é›†ç¾¤æ•´ä½“æ•ˆç‡åˆ†æ
kiro ask "åˆ†æ production é›†ç¾¤çš„èµ„æºæ•ˆç‡å¹¶ç»™å‡ºä¼˜åŒ–ä¼˜å…ˆçº§"

# Kiro å“åº”ï¼š
# ğŸ“ˆ é›†ç¾¤æ•ˆç‡æŠ¥å‘Š
#
# æ•´ä½“æ•ˆç‡ï¼š52%ï¼ˆè¡Œä¸šå¹³å‡ï¼š65%ï¼‰
#
# ä¼˜åŒ–ä¼˜å…ˆçº§ï¼š
# 1. ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å¤„ç†ï¼‰
#    - 10 ä¸ª Deployment çš„ CPU æœ‰ 70% æœªä½¿ç”¨
#    - é¢„è®¡èŠ‚çœï¼š$1,200/æœˆ
#
# 2. ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆ1 å‘¨å†…ï¼‰
#    - 5 ä¸ª StatefulSet çš„ PVC å¤§å°è¿‡å¤§
#    - é¢„è®¡èŠ‚çœï¼š$300/æœˆ
#
# 3. ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆè§„åˆ’é˜¶æ®µï¼‰
#    - 15 ä¸ª Deployment æœªè®¾ç½® HPA
#    - å»ºè®®åˆ†ææµé‡æ¨¡å¼ååº”ç”¨
#
# æ˜¯å¦åˆ›å»ºè‡ªåŠ¨ Right-Sizing PRï¼Ÿ(y/n)
```

**Kiro å·¥ä½œæµè‡ªåŠ¨åŒ–ï¼š**

```yaml
# kiro-workflow.yaml
apiVersion: kiro.aws.dev/v1alpha1
kind: Workflow
metadata:
  name: weekly-optimization
spec:
  schedule: "0 9 * * MON"  # æ¯å‘¨ä¸€ä¸Šåˆ 9 ç‚¹
  steps:
    - name: analyze-underutilized
      action: analyze
      query: "åˆ†ææ‰€æœ‰ CPU ä½¿ç”¨ç‡ä½äº 30% æˆ– Memory ä½¿ç”¨ç‡ä½äº 40% çš„ Pod"
      outputFormat: json

    - name: generate-recommendations
      action: recommend
      input: ${{ steps.analyze-underutilized.output }}
      includeVPA: true
      includePrometheus: true

    - name: create-pr
      action: github-pr
      repository: my-org/k8s-manifests
      branch: kiro-right-sizing-{{ date }}
      title: "[Kiro] Weekly Right-Sizing Recommendations"
      body: ${{ steps.generate-recommendations.output }}
      autoMerge: false  # éœ€è¦æ‰‹åŠ¨å®¡æŸ¥

    - name: notify
      action: slack
      webhook: ${{ secrets.SLACK_WEBHOOK }}
      message: |
        ğŸ“Š æ¯å‘¨ Right-Sizing åˆ†æå®Œæˆ
        PR: ${{ steps.create-pr.pr_url }}
        é¢„è®¡èŠ‚çœ: ${{ steps.generate-recommendations.estimated_savings }}
```

#### 6.5.3 åˆ©ç”¨ Amazon Q Developer è¿›è¡Œäº¤äº’å¼ä¼˜åŒ–

Amazon Q Developer ç›´æ¥åœ¨ IDE å’Œ CLI ä¸­æä¾›èµ„æºä¼˜åŒ–å»ºè®®ã€‚

**åœ¨ VS Code ä¸­ä½¿ç”¨ï¼š**

```yaml
# æ‰“å¼€ deployment.yaml å¹¶å‘ Q Developer æé—®
# /q optimize-resources

# Q Developer å“åº”ï¼š
# å·²åˆ†æå½“å‰ Deployment çš„èµ„æºè®¾ç½®ï¼š
#
# ğŸ” å‘ç°çš„é—®é¢˜ï¼š
# 1. CPU requests æ¯”å®é™…ä½¿ç”¨é‡é«˜ 3 å€ï¼ˆ1000m â†’ å»ºè®® 350mï¼‰
# 2. æ²¡æœ‰ Memory limitsï¼Œå­˜åœ¨ OOM é£é™©
# 3. QoS ç±»åˆ«ï¼šBurstableï¼ˆå»ºè®® Guaranteedï¼‰
#
# ğŸ’¡ ä¼˜åŒ–åçš„è®¾ç½®ï¼š
resources:
  requests:
    cpu: "350m"      # å®é™… P95 + 20% ç¼“å†²
    memory: "512Mi"  # å®é™… P95 400Mi + 20%
  limits:
    memory: "1Gi"    # Upper Bound + ä½™é‡
    # ç§»é™¤ CPU limitsï¼ˆGoogle/Datadog æ¨¡å¼ï¼‰
#
# æ˜¯å¦åº”ç”¨æ­¤å˜æ›´ï¼Ÿ(Apply / Dismiss)
```

**åœ¨ CLI ä¸­ä½¿ç”¨ï¼š**

```bash
# é€šè¿‡ Amazon Q CLI æŸ¥è¯¢
q ask "ä¼˜åŒ–è¿™ä¸ª Deployment çš„èµ„æº" --file deployment.yaml

# è¾“å‡ºï¼š
# åˆ†æä¸­... âœ“
#
# å½“å‰è®¾ç½®é—®é¢˜ï¼š
# - CPU over-provisioned by 65%
# - Memory under-provisioned (OOM risk)
#
# æ¨èå˜æ›´å·²ä¿å­˜åˆ° deployment-optimized.yamlã€‚
# æ˜¯å¦æŸ¥çœ‹å·®å¼‚ï¼Ÿ(y/n)

# è¾“å…¥ y æ—¶ï¼š
diff deployment.yaml deployment-optimized.yaml
```

#### 6.5.4 æ³¨æ„äº‹é¡¹å’Œå±€é™æ€§

åŸºäº AI çš„èµ„æºæ¨èè™½ç„¶å¼ºå¤§ï¼Œä½†éœ€è¦äº†è§£ä»¥ä¸‹å±€é™æ€§ï¼š

| å±€é™ | è¯´æ˜ | åº”å¯¹æ–¹æ³• |
|------|------|----------|
| **ä¾èµ–å†å²æ•°æ®** | æ— æ³•é¢„æµ‹è¿‡å»ä¸å­˜åœ¨çš„æµé‡æ¨¡å¼ | é…åˆ HPAï¼Œç¡®ä¿å……è¶³ç¼“å†² |
| **ç¼ºä¹ä¸Šä¸‹æ–‡** | æœªåæ˜ ä¸šåŠ¡éœ€æ±‚ï¼ˆSLAã€æ³•è§„ï¼‰ | æ‰‹åŠ¨å®¡æŸ¥æ­¥éª¤å¿…ä¸å¯å°‘ |
| **ä¸´æ—¶å°–å³°** | æœªè€ƒè™‘è¥é”€æ´»åŠ¨ç­‰è®¡åˆ’æ€§è´Ÿè½½ | æ´»åŠ¨æœŸé—´æ‰‹åŠ¨æ‰©å®¹ |
| **æˆæœ¬ä¼˜åŒ–åå‘** | å¯èƒ½ä¼˜å…ˆè€ƒè™‘æˆæœ¬å‰Šå‡è€Œéç¨³å®šæ€§ | æ’é™¤å…³é”®å·¥ä½œè´Ÿè½½è®¾ç½® |

:::warning AI æ¨èä½œä¸ºè¾…åŠ©å·¥å…·ä½¿ç”¨
åŸºäº AI çš„èµ„æºæ¨èæ˜¯**è¾…åŠ©å·¥å…·è€Œéæœ€ç»ˆå†³ç­–å·¥å…·**ã€‚åœ¨åº”ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒä¹‹å‰åŠ¡å¿…ï¼š

1. **åœ¨ Staging ç¯å¢ƒä¸­éªŒè¯**ï¼ˆè‡³å°‘ 3 å¤©ï¼‰
2. **ç›‘æ§æ€§èƒ½æŒ‡æ ‡**ï¼ˆLatency P99ã€Error Rateï¼‰
3. **æ¸è¿›å¼å‘å¸ƒ**ï¼ˆCanary 10% â†’ 50% â†’ 100%ï¼‰
4. **åˆ¶å®šå›æ»šè®¡åˆ’**ï¼ˆ1 åˆ†é’Ÿå†…æ¢å¤åˆ°ä¹‹å‰ç‰ˆæœ¬ï¼‰

ç‰¹åˆ«æ˜¯ä»¥ä¸‹å·¥ä½œè´Ÿè½½**ä¸è¦åº”ç”¨ AI æ¨èï¼Œåº”æ‰‹åŠ¨ç®¡ç†**ï¼š
- é‡‘èäº¤æ˜“ç³»ç»Ÿ
- åŒ»ç–—ä¿¡æ¯ç³»ç»Ÿ
- å®æ—¶æµåª’ä½“æœåŠ¡
- Stateful æ•°æ®åº“
:::

**AI æ¨èéªŒè¯æ¸…å•ï¼š**

```yaml
# ç”Ÿäº§ç¯å¢ƒåº”ç”¨å‰å¿…é¡»éªŒè¯
ai_recommendation_validation:
  staging_test:
    duration_days: 3
    success_criteria:
      - p99_latency_increase: "<5%"
      - error_rate_increase: "<0.1%"
      - no_oom_kills: true
      - no_cpu_throttling: "<10%"

  canary_rollout:
    initial_percentage: 10
    increment_percentage: 20
    increment_interval_hours: 6
    auto_rollback_threshold:
      error_rate: 1.0  # é”™è¯¯ç‡è¶…è¿‡ 1% æ—¶è‡ªåŠ¨å›æ»š
      latency_p99_ms: 500  # P99 å»¶è¿Ÿè¶…è¿‡ 500ms æ—¶å›æ»š

  monitoring:
    dashboard_url: "https://grafana.example.com/d/right-sizing"
    alert_channels: ["slack://ops-team", "pagerduty://oncall"]
    review_required: true  # ç¦æ­¢è‡ªåŠ¨åˆå¹¶ï¼Œéœ€æ‰‹åŠ¨å®¡æŸ¥
```

:::tip AI + äººç±»æ··åˆæ–¹æ³•
æœ€ä½³ç»“æœæ¥è‡ª **AI æ¨è + äººç±»ä¸“å®¶å®¡æŸ¥**çš„ç»„åˆï¼š

1. AI ä»æ•°åƒä¸ª Pod ä¸­ç­›é€‰ä¼˜åŒ–ç›®æ ‡ï¼ˆé€Ÿåº¦ï¼‰
2. äººç±»æ’é™¤å…³é”®å·¥ä½œè´Ÿè½½å¹¶éªŒè¯ï¼ˆå¯é æ€§ï¼‰
3. AI ç”Ÿæˆåˆå§‹ PRï¼ˆè‡ªåŠ¨åŒ–ï¼‰
4. äººç±»åœ¨ Staging æµ‹è¯•åæ‰¹å‡†ï¼ˆå®‰å…¨æ€§ï¼‰
5. GitOps æ¸è¿›å¼éƒ¨ç½²ï¼ˆè¿ç»´æ•ˆç‡ï¼‰

é€šè¿‡æ­¤æµç¨‹å¯å®ç°**æ¯”æ‰‹åŠ¨èŠ‚çœ 80% æ—¶é—´**ï¼ŒåŒæ—¶**ä¿æŒç›¸åŒçš„ç¨³å®šæ€§**ã€‚
:::

## Resource Quota ä¸ LimitRange

### 7.1 Namespace çº§åˆ«èµ„æºé™åˆ¶

é€šè¿‡ ResourceQuota é™åˆ¶æ•´ä¸ªå‘½åç©ºé—´çš„èµ„æºï¼š

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    # æ€»èµ„æºé™åˆ¶
    requests.cpu: "100"           # 100 CPU cores
    requests.memory: "200Gi"      # 200GB RAM
    limits.cpu: "200"             # CPU limits æ€»è®¡
    limits.memory: "400Gi"        # Memory limits æ€»è®¡

    # å¯¹è±¡æ•°é‡é™åˆ¶
    pods: "500"                   # æœ€å¤š 500 ä¸ª Pod
    services: "50"                # æœ€å¤š 50 ä¸ª Service
    persistentvolumeclaims: "100" # æœ€å¤š 100 ä¸ª PVC

    # å­˜å‚¨é™åˆ¶
    requests.storage: "2Ti"       # æ€»è®¡ 2TB å­˜å‚¨

---
# å„ç¯å¢ƒé…é¢ç¤ºä¾‹
apiVersion: v1
kind: ResourceQuota
metadata:
  name: development-quota
  namespace: development
spec:
  hard:
    requests.cpu: "20"
    requests.memory: "40Gi"
    limits.cpu: "40"
    limits.memory: "80Gi"
    pods: "100"

---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: staging-quota
  namespace: staging
spec:
  hard:
    requests.cpu: "50"
    requests.memory: "100Gi"
    limits.cpu: "100"
    limits.memory: "200Gi"
    pods: "200"
```

**æŸ¥çœ‹é…é¢ä½¿ç”¨é‡ï¼š**

```bash
# å½“å‰é…é¢ä½¿ç”¨é‡
kubectl describe resourcequota production-quota -n production

# è¾“å‡ºç¤ºä¾‹ï¼š
# Name:            production-quota
# Namespace:       production
# Resource         Used   Hard
# --------         ----   ----
# limits.cpu       150    200
# limits.memory    300Gi  400Gi
# pods             342    500
# requests.cpu     75     100
# requests.memory  150Gi  200Gi
```

### 7.2 é€šè¿‡ LimitRange è®¾ç½®é»˜è®¤å€¼

é€šè¿‡ LimitRange è‡ªåŠ¨ä¸º Pod/Container æ³¨å…¥é»˜è®¤èµ„æºï¼š

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limitrange
  namespace: production
spec:
  limits:
  # Container çº§åˆ«çº¦æŸ
  - type: Container
    default:                    # æœªè®¾ç½® limits æ—¶çš„é»˜è®¤å€¼
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:             # æœªè®¾ç½® requests æ—¶çš„é»˜è®¤å€¼
      cpu: "100m"
      memory: "128Mi"
    max:                        # æœ€å¤§å…è®¸å€¼
      cpu: "4000m"
      memory: "8Gi"
    min:                        # æœ€å°è¦æ±‚å€¼
      cpu: "50m"
      memory: "64Mi"
    maxLimitRequestRatio:       # limits/requests æœ€å¤§æ¯”ç‡
      cpu: "4"                  # limits æœ€å¤šä¸º requests çš„ 4 å€
      memory: "2"               # limits æœ€å¤šä¸º requests çš„ 2 å€

  # Pod çº§åˆ«çº¦æŸ
  - type: Pod
    max:
      cpu: "8000m"
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"

  # PVC çº¦æŸ
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "1Gi"

---
# å¼€å‘ç¯å¢ƒ LimitRange
apiVersion: v1
kind: LimitRange
metadata:
  name: development-limitrange
  namespace: development
spec:
  limits:
  - type: Container
    default:
      cpu: "200m"
      memory: "256Mi"
    defaultRequest:
      cpu: "50m"
      memory: "64Mi"
    max:
      cpu: "2000m"
      memory: "4Gi"
```

**è¡Œä¸ºç¤ºä¾‹ï¼š**

```yaml
# å¼€å‘è€…ç¼–å†™çš„ YAMLï¼ˆæœªæŒ‡å®šèµ„æºï¼‰
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: production
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    # æ²¡æœ‰ resources éƒ¨åˆ†

# LimitRange è‡ªåŠ¨æ³¨å…¥åçš„ç»“æœ
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: production
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    resources:
      requests:           # åº”ç”¨ defaultRequest
        cpu: "100m"
        memory: "128Mi"
      limits:             # åº”ç”¨ default
        cpu: "500m"
        memory: "512Mi"
```

**éªŒè¯ï¼š**

```bash
# æŸ¥çœ‹ LimitRange
kubectl describe limitrange production-limitrange -n production

# æŸ¥çœ‹ Pod ä¸Šåº”ç”¨çš„èµ„æº
kubectl get pod test-pod -n production -o jsonpath='{.spec.containers[0].resources}' | jq .
```

### 7.3 DRAï¼ˆDynamic Resource Allocationï¼‰- GPU/ç‰¹æ®Šèµ„æºç®¡ç†

Kubernetes 1.31+ å¼•å…¥çš„ **DRAï¼ˆDynamic Resource Allocationï¼‰** æ˜¯ä¸€ç§æ–°æœºåˆ¶ï¼Œå¯ä»¥æ›´çµæ´»åœ°åˆ†é… GPUã€FPGAã€NPU ç­‰ç‰¹æ®Šèµ„æºã€‚

#### ç°æœ‰ Device Plugin vs DRA

| ç‰¹æ€§ | Device Pluginï¼ˆç°æœ‰ï¼‰ | DRAï¼ˆK8s 1.31+ï¼‰ |
|------|---------------------|-----------------|
| **èµ„æºè¡¨ç¤º** | ç®€å•æ•°å­—ï¼ˆ`nvidia.com/gpu: 1`ï¼‰ | ç»“æ„åŒ–å‚æ•°ï¼ˆå†…å­˜ã€è®¡ç®—æ¨¡å¼ï¼‰ |
| **å…±äº«èƒ½åŠ›** | ä¸å¯èƒ½ï¼ˆ1 Pod = 1 GPUï¼‰ | å¯èƒ½ï¼ˆæ—¶é—´åˆ†ç‰‡ã€MIG æ”¯æŒï¼‰ |
| **åŠ¨æ€åˆ†é…** | è°ƒåº¦æ—¶å†³å®š | è¿è¡Œæ—¶åŠ¨æ€åˆ†é… |
| **å¤æ‚æ‹“æ‰‘** | æœ‰é™ | è€ƒè™‘ NUMAã€PCIe æ‹“æ‰‘ |
| **å¤šç§Ÿæˆ·** | å›°éš¾ | åŸç”Ÿæ”¯æŒ |

**DRA çš„æ ¸å¿ƒæ¦‚å¿µï¼š**

```mermaid
graph LR
    A[Pod with ResourceClaim] --> B[Scheduler]
    B --> C[ResourceClass åŒ¹é…]
    C --> D[DRA Driver]
    D --> E[ç‰©ç† GPU åˆ†é…]
    E --> F[Pod è¿è¡Œ]

    style A fill:#4dabf7
    style E fill:#51cf66
```

#### DRA ç»„æˆéƒ¨åˆ†

**1. ResourceClassï¼ˆé›†ç¾¤çº§åˆ«èµ„æºå®šä¹‰ï¼‰**

```yaml
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClass
metadata:
  name: nvidia-a100-gpu
spec:
  driverName: gpu.nvidia.com
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClassParameters
    name: a100-80gb
---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClassParameters
metadata:
  name: a100-80gb
spec:
  # GPU ç‰¹æ€§å®šä¹‰
  memory: "80Gi"
  computeCapability: "8.0"
  # MIGï¼ˆMulti-Instance GPUï¼‰æ”¯æŒ
  migEnabled: true
  migProfile: "1g.10gb"  # 1/7 GPU åˆ‡ç‰‡
```

**2. ResourceClaimï¼ˆPod è¯·æ±‚çš„èµ„æºï¼‰**

```yaml
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaim
metadata:
  name: ml-training-gpu
  namespace: ml-team
spec:
  resourceClassName: nvidia-a100-gpu
  parametersRef:
    apiGroup: gpu.nvidia.com
    kind: GpuClaimParameters
    name: training-config
---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: training-config
spec:
  # è¯·æ±‚çš„ GPU è§„æ ¼
  count: 2  # è¯·æ±‚ 2 ä¸ª GPU
  sharing: "TimeSlicing"  # å…è®¸æ—¶é—´åˆ†ç‰‡å…±äº«
  selector:
    matchLabels:
      gpu.nvidia.com/memory: "80Gi"
```

**3. åœ¨ Pod ä¸­ä½¿ç”¨ ResourceClaim**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pytorch-training
  namespace: ml-team
spec:
  containers:
  - name: trainer
    image: pytorch/pytorch:2.1.0-cuda12.1
    command: ["python", "train.py"]
    resources:
      requests:
        cpu: "8"
        memory: "32Gi"
      limits:
        memory: "64Gi"

  # é€šè¿‡ DRA åˆ†é… GPU
  resourceClaims:
  - name: gpu
    source:
      resourceClaimName: ml-training-gpu

  # åœ¨å®¹å™¨ä¸­å¼•ç”¨ claim
  containers:
  - name: trainer
    # ...
    resources:
      claims:
      - name: gpu
```

#### åœ¨ EKS ä¸­å¯ç”¨ DRA å’Œ GPU åˆ†é…ç¤ºä¾‹

**Step 1ï¼šåœ¨ EKS é›†ç¾¤ä¸­å¯ç”¨ DRA Feature Gate**

```bash
# åˆ›å»º EKS 1.31+ é›†ç¾¤æ—¶
eksctl create cluster \
  --name dra-enabled-cluster \
  --version 1.31 \
  --region us-west-2 \
  --nodegroup-name gpu-nodes \
  --node-type p4d.24xlarge \
  --nodes 2 \
  --kubernetes-feature-gates DynamicResourceAllocation=true
```

**Step 2ï¼šå®‰è£… NVIDIA GPU Operatorï¼ˆåŒ…å« DRA é©±åŠ¨ï¼‰**

```bash
# é€šè¿‡ Helm å®‰è£… GPU Operatorï¼ˆDRA æ”¯æŒç‰ˆæœ¬ï¼‰
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update

helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator \
  --create-namespace \
  --set driver.enabled=true \
  --set toolkit.enabled=true \
  --set devicePlugin.enabled=false \  # ç¦ç”¨ç°æœ‰ device plugin
  --set dra.enabled=true \             # å¯ç”¨ DRA
  --set migManager.enabled=true        # MIG æ”¯æŒ
```

**Step 3ï¼šä½¿ç”¨ ResourceClaimTemplate è‡ªåŠ¨åˆ›å»º Claim**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference
  namespace: ml-team
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: model-server
        image: tritonserver:24.01
        resources:
          requests:
            cpu: "4"
            memory: "16Gi"
          claims:
          - name: gpu

      # é€šè¿‡ ResourceClaimTemplate ä¸ºæ¯ä¸ª Pod è‡ªåŠ¨åˆ›å»º
      resourceClaims:
      - name: gpu
        source:
          resourceClaimTemplateName: shared-gpu-template

---
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaimTemplate
metadata:
  name: shared-gpu-template
  namespace: ml-team
spec:
  spec:
    resourceClassName: nvidia-a100-gpu
    parametersRef:
      apiGroup: gpu.nvidia.com
      kind: GpuClaimParameters
      name: shared-inference-config

---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: shared-inference-config
spec:
  count: 1
  sharing: "TimeSlicing"  # å¤šä¸ª Pod é€šè¿‡æ—¶é—´åˆ†ç‰‡å…±äº«
  requests:
    memory: "10Gi"        # ä»…è¯·æ±‚ 10GB GPU å†…å­˜
```

**DRA ä¼˜åŠ¿æ€»ç»“ï¼š**

1. **GPU å…±äº«**ï¼šé€šè¿‡ MIG æˆ– Time-Slicingï¼Œå¤šä¸ª Pod ä½¿ç”¨ 1 ä¸ª GPU
2. **ç²¾ç»†æ§åˆ¶**ï¼šå¯æŒ‡å®š GPU å†…å­˜ã€è®¡ç®—æ¨¡å¼ã€æ‹“æ‰‘
3. **åŠ¨æ€åˆ†é…**ï¼šPod åˆ›å»ºåä¹Ÿå¯æ·»åŠ /ç§»é™¤èµ„æº
4. **æˆæœ¬é™ä½**ï¼šGPU åˆ©ç”¨ç‡æå‡ï¼ˆç°æœ‰ 30-40% â†’ DRA å¯è¾¾ 70-80%ï¼‰

:::warning EKS DRA æ”¯æŒçŠ¶æ€ï¼ˆ2026 å¹´ 2 æœˆåŸºå‡†ï¼‰
- åœ¨ Kubernetes 1.31+ ä¸­ä½œä¸º alpha åŠŸèƒ½æä¾›
- åœ¨ EKS ä¸­éœ€è¦æ‰‹åŠ¨å¯ç”¨ Feature Gate
- ç”Ÿäº§ä½¿ç”¨æ—¶è¯·ç¡®è®¤ NVIDIA GPU Operator æœ€æ–°ç‰ˆæœ¬ï¼ˆv24.9.0+ï¼‰
- MIG æ”¯æŒä»…åœ¨ A100/H100 GPU ä¸Šå¯ç”¨
:::
