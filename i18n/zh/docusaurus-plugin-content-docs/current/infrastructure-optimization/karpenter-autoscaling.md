---
title: "Karpenter è¶…å¿«é€Ÿè‡ªåŠ¨æ‰©ç¼©å®¹"
sidebar_label: "4. Karpenter Autoscaling"
description: "å¦‚ä½•åœ¨ Amazon EKS ä¸­ä½¿ç”¨ Karpenter å’Œé«˜åˆ†è¾¨ç‡æŒ‡æ ‡å®ç° 10 ç§’ä»¥å†…çš„è‡ªåŠ¨æ‰©ç¼©å®¹ã€‚åŒ…å« CloudWatch ä¸ Prometheus æ¶æ„å¯¹æ¯”ã€HPA é…ç½®åŠç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ"
tags: [eks, karpenter, autoscaling, performance, cloudwatch, prometheus, spot-instances]
category: "performance-networking"
sidebar_position: 4
last_update:
  date: 2026-02-13
  author: devfloor9
---

# Karpenter å®ç° EKS è¶…å¿«é€Ÿè‡ªåŠ¨æ‰©ç¼©å®¹

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2025-02-09 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-13 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 5 åˆ†é’Ÿ


> ğŸ“… **å‘å¸ƒæ—¥æœŸ**: 2025å¹´6æœˆ30æ—¥ | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 10 åˆ†é’Ÿ

## æ¦‚è¿°

åœ¨ç°ä»£äº‘åŸç”Ÿæ¶æ„ä¸­ï¼Œ10 ç§’ä¸ 3 åˆ†é’Ÿä¹‹é—´çš„å·®å¼‚å¯èƒ½æ„å‘³ç€æ•°åƒä¸ªå¤±è´¥è¯·æ±‚ã€é™çº§çš„ç”¨æˆ·ä½“éªŒå’Œæ”¶å…¥æŸå¤±ã€‚æœ¬æ–‡å°†å±•ç¤ºå¦‚ä½•åˆ©ç”¨ Karpenter é©å‘½æ€§çš„èŠ‚ç‚¹é…ç½®æ–¹æ³•ï¼Œç»“åˆç²¾å¿ƒå®æ–½çš„é«˜åˆ†è¾¨ç‡æŒ‡æ ‡ï¼Œåœ¨ Amazon EKS ä¸­å®ç°ä¸€è‡´çš„ 10 ç§’ä»¥å†…è‡ªåŠ¨æ‰©ç¼©å®¹ã€‚

æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä¸€ä¸ªç»è¿‡ç”Ÿäº§éªŒè¯çš„æ¶æ„ï¼Œè¯¥æ¶æ„å°†æ‰©ç¼©å®¹å»¶è¿Ÿä» 180 ç§’ä»¥ä¸Šé™ä½åˆ° 10 ç§’ä»¥å†…ï¼ŒåŒæ—¶ç®¡ç†è·¨å¤šä¸ªåŒºåŸŸï¼ˆ3 ä¸ªåŒºåŸŸã€28 ä¸ªé›†ç¾¤ï¼‰çš„ 15,000 å¤šä¸ª Podã€‚

## ä¸ºä»€ä¹ˆä¼ ç»Ÿè‡ªåŠ¨æ‰©ç¼©å®¹åœ¨é€Ÿåº¦ä¸Šä¸å°½äººæ„

åœ¨æ·±å…¥è§£å†³æ–¹æ¡ˆä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£ä¼ ç»Ÿæ–¹æ³•å¤±è´¥çš„åŸå› ï¼š

```mermaid
graph LR
    subgraph "Traditional Scaling Timeline (3+ minutes)"
        T1[Traffic Spike<br/>T+0s] --> T2[CPU Metrics Update<br/>T+60s]
        T2 --> T3[HPA Decision<br/>T+90s]
        T3 --> T4[ASG Scaling<br/>T+120s]
        T4 --> T5[Node Ready<br/>T+180s]
        T5 --> T6[Pod Scheduled<br/>T+210s]
    end

    subgraph "User Impact"
        I1[Timeouts Start<br/>T+5s]
        I2[Errors Spike<br/>T+30s]
        I3[Service Degraded<br/>T+60s]
    end

    T1 -.-> I1
    T2 -.-> I2
    T3 -.-> I3

    style I1 fill:#ff4444
    style I2 fill:#ff6666
    style I3 fill:#ff8888

```

æ ¹æœ¬é—®é¢˜åœ¨äºï¼šå½“ CPU æŒ‡æ ‡è§¦å‘æ‰©ç¼©å®¹æ—¶ï¼Œä¸€åˆ‡å·²ç»å¤ªè¿Ÿäº†ã€‚

**å½“å‰ç¯å¢ƒæŒ‘æˆ˜ï¼š**

- **å…¨çƒè§„æ¨¡**ï¼š3 ä¸ªåŒºåŸŸã€28 ä¸ª EKS é›†ç¾¤ã€15,000 å¤šä¸ª Pod æ­£åœ¨è¿è¡Œ
- **é«˜æµé‡è´Ÿè½½**ï¼šæ¯å¤©å¤„ç† 773,400 ä¸ªè¯·æ±‚
- **å»¶è¿Ÿé—®é¢˜**ï¼šå½“å‰ HPA + Karpenter ç»„åˆå­˜åœ¨ 1-3 åˆ†é’Ÿçš„æ‰©ç¼©å®¹å»¶è¿Ÿ
- **æŒ‡æ ‡é‡‡é›†å»¶è¿Ÿ**ï¼šCloudWatch æŒ‡æ ‡å­˜åœ¨ 1-3 åˆ†é’Ÿçš„å»¶è¿Ÿï¼Œæ— æ³•å®ç°å®æ—¶å“åº”

## Karpenter é©å‘½ï¼šç›´æ¥åˆ°åº•å±‚çš„é…ç½®

Karpenter æ¶ˆé™¤äº† Auto Scaling Groupï¼ˆASGï¼‰æŠ½è±¡å±‚ï¼Œæ ¹æ®å¾…è°ƒåº¦ Pod çš„éœ€æ±‚ç›´æ¥é…ç½® EC2 å®ä¾‹ï¼š

```mermaid
graph TB
    subgraph "Karpenter Architecture"
        PP[Pending Pods<br/>Detected]
        KL[Karpenter Logic]
        EC2[EC2 Fleet API]

        PP -->|Milliseconds| KL

        subgraph "Intelligent Decision Engine"
            IS[Instance Selection]
            SP[Spot/OD Mix]
            AZ[AZ Distribution]
            CP[Capacity Planning]
        end

        KL --> IS
        KL --> SP
        KL --> AZ
        KL --> CP

        IS --> EC2
        SP --> EC2
        AZ --> EC2
        CP --> EC2
    end

    subgraph "Traditional ASG"
        ASG[Auto Scaling Group]
        LT[Launch Template]
        ASGL[ASG Logic]

        ASG --> LT
        LT --> ASGL
        ASGL -->|2-3 min| EC2_OLD[EC2 API]
    end

    EC2 -->|30-45s| NODE[Node Ready]
    EC2_OLD -->|120-180s| NODE_OLD[Node Ready]

    style KL fill:#ff9900,stroke:#232f3e,stroke-width:3px
    style EC2 fill:#146eb4,stroke:#232f3e,stroke-width:2px
    style ASG fill:#cccccc,stroke:#999999

```

## é«˜é€ŸæŒ‡æ ‡æ¶æ„ï¼šä¸¤ç§æ–¹æ¡ˆ

å®ç° 10 ç§’ä»¥å†…çš„æ‰©ç¼©å®¹éœ€è¦å¿«é€Ÿæ„ŸçŸ¥ã€‚æˆ‘ä»¬å¯¹æ¯”ä¸¤ç§ç»è¿‡éªŒè¯çš„æ¶æ„ã€‚

### æ–¹æ¡ˆä¸€ï¼šCloudWatch é«˜åˆ†è¾¨ç‡é›†æˆ

åˆ©ç”¨ CloudWatch çš„é«˜åˆ†è¾¨ç‡æŒ‡æ ‡ï¼Œåœ¨ AWS åŸç”Ÿç¯å¢ƒä¸­å®ç°ä¼˜åŒ–çš„æ‰©ç¼©å®¹ã€‚

#### æ ¸å¿ƒç»„ä»¶

```mermaid
graph TB
    subgraph "Metric Sources"
        subgraph "Critical (1s)"
            RPS[Requests/sec]
            LAT[P99 Latency]
            ERR[Error Rate]
            QUEUE[Queue Depth]
        end

        subgraph "Standard (60s)"
            CPU[CPU Usage]
            MEM[Memory Usage]
            DISK[Disk I/O]
            NET[Network I/O]
        end
    end

    subgraph "Collection Pipeline"
        AGENT[ADOT Collector<br/>Batch: 1s]
        EMF[EMF Format<br/>Compression]
        CW[CloudWatch API<br/>PutMetricData]
    end

    subgraph "Decision Layer"
        API[Custom Metrics API]
        CACHE[In-Memory Cache<br/>TTL: 5s]
        HPA[HPA Controller]
    end

    RPS --> AGENT
    LAT --> AGENT
    ERR --> AGENT
    QUEUE --> AGENT

    CPU --> AGENT
    MEM --> AGENT

    AGENT --> EMF
    EMF --> CW
    CW --> API
    API --> CACHE
    CACHE --> HPA

    style RPS fill:#ff4444
    style LAT fill:#ff4444
    style ERR fill:#ff4444
    style QUEUE fill:#ff4444

```

#### æ‰©ç¼©å®¹æ—¶é—´çº¿ï¼ˆ15 ç§’ï¼‰

```mermaid
timeline
    title CloudWatch-Based Autoscaling Timeline

    T+0s  : Application generates metrics
    T+1s  : Asynchronous batch send to CloudWatch
    T+2s  : CloudWatch metric processing complete
    T+5s  : KEDA polling cycle execution
    T+6s  : KEDA makes scaling decision
    T+8s  : HPA update and pod creation request
    T+12s : Karpenter node provisioning
    T+14s : Pod scheduling complete
```

**ä¼˜åŠ¿ï¼š**

- âœ… **å¿«é€ŸæŒ‡æ ‡é‡‡é›†**ï¼š1-2 ç§’ä½å»¶è¿Ÿ
- âœ… **é…ç½®ç®€å•**ï¼šAWS åŸç”Ÿé›†æˆ
- âœ… **æ— è¿ç»´å¼€é”€**ï¼šæ— éœ€ç®¡ç†é¢å¤–åŸºç¡€è®¾æ–½

**åŠ£åŠ¿ï¼š**

- âŒ **ååé‡æœ‰é™**ï¼šæ¯ä¸ªè´¦æˆ· 1,000 TPS
- âŒ **Pod æ•°é‡é™åˆ¶**ï¼šæ¯ä¸ªé›†ç¾¤æœ€å¤š 5,000 ä¸ª Pod
- âŒ **æŒ‡æ ‡æˆæœ¬è¾ƒé«˜**ï¼šAWS CloudWatch æŒ‡æ ‡å®šä»·

### æ–¹æ¡ˆäºŒï¼šADOT + Prometheus æ¶æ„

åŸºäºå¼€æºåŸºç¡€ï¼Œç»“åˆ AWS Distro for OpenTelemetryï¼ˆADOTï¼‰å’Œ Prometheus æ„å»ºé«˜æ€§èƒ½æŒ‡æ ‡ç®¡é“ã€‚

#### æ ¸å¿ƒç»„ä»¶

- **ADOT Collector**ï¼šDaemonSet å’Œ Sidecar æ··åˆéƒ¨ç½²
- **Prometheus**ï¼šHA é«˜å¯ç”¨é…ç½®ï¼Œé›†æˆ Remote Storage
- **Thanos Query å±‚**ï¼šå¤šé›†ç¾¤å…¨å±€è§†å›¾
- **KEDA Prometheus Scaler**ï¼š2 ç§’é—´éš”çš„é«˜é€Ÿè½®è¯¢
- **Grafana Mimir**ï¼šé•¿æœŸå­˜å‚¨å’Œå¿«é€ŸæŸ¥è¯¢å¼•æ“

#### æ‰©ç¼©å®¹æ—¶é—´çº¿ï¼ˆ70 ç§’ï¼‰

```mermaid
timeline
    title ADOT + Prometheus Autoscaling Timeline (Optimized Environment)

    T+0s   : Application generates metrics
    T+15s  : ADOT collects metrics (optimized 15s scrape)
    T+16s  : Prometheus storage and indexing complete
    T+25s  : KEDA polling execution (10s interval optimized)
    T+26s  : Scaling decision (P95 metrics-based)
    T+41s  : HPA update (15s sync period)
    T+46s  : Pod creation request initiated
    T+51s  : Image pulling and container startup
    T+66s  : Pod Ready state - Autoscaling complete
```

**ä¼˜åŠ¿ï¼š**

- âœ… **é«˜ååé‡**ï¼šæ”¯æŒ 100,000+ TPS
- âœ… **å¯æ‰©å±•æ€§å¼º**ï¼šæ¯ä¸ªé›†ç¾¤æ”¯æŒ 20,000 ä¸ªä»¥ä¸Šçš„ Pod
- âœ… **æŒ‡æ ‡æˆæœ¬ä½**ï¼šä»…éœ€å­˜å‚¨æˆæœ¬ï¼ˆè‡ªä¸»ç®¡ç†ï¼‰
- âœ… **å®Œå…¨å¯æ§**ï¼šå®Œå…¨çš„é…ç½®å’Œä¼˜åŒ–è‡ªç”±åº¦

**åŠ£åŠ¿ï¼š**

- âŒ **é…ç½®å¤æ‚**ï¼šéœ€è¦ç®¡ç†é¢å¤–ç»„ä»¶
- âŒ **è¿ç»´å¤æ‚åº¦é«˜**ï¼šéœ€è¦ HA é…ç½®ã€å¤‡ä»½æ¢å¤ã€æ€§èƒ½è°ƒä¼˜
- âŒ **éœ€è¦ä¸“ä¸šçŸ¥è¯†**ï¼šPrometheus è¿ç»´ç»éªŒå¿…ä¸å¯å°‘

### æˆæœ¬ä¼˜åŒ–çš„æŒ‡æ ‡ç­–ç•¥

```mermaid
pie title "Monthly CloudWatch Costs per Cluster"
    "High-Res Metrics (10)" : 3
    "Standard Metrics (100)" : 10
    "API Calls" : 5
    "Total per Cluster" : 18

```

28 ä¸ªé›†ç¾¤çš„æ€»è´¹ç”¨ï¼šå…¨é¢ç›‘æ§çº¦ $500/æœˆï¼Œè€Œå°†æ‰€æœ‰æŒ‡æ ‡è®¾ä¸ºé«˜åˆ†è¾¨ç‡åˆ™éœ€è¦ $30,000 ä»¥ä¸Šã€‚

### æ¨èä½¿ç”¨åœºæ™¯

**CloudWatch é«˜åˆ†è¾¨ç‡æŒ‡æ ‡é€‚ç”¨äºï¼š**

- å°å‹åº”ç”¨ï¼ˆ5,000 ä¸ª Pod ä»¥ä¸‹ï¼‰
- ç®€å•çš„ç›‘æ§éœ€æ±‚
- åå¥½ AWS åŸç”Ÿè§£å†³æ–¹æ¡ˆ
- ä¼˜å…ˆè€ƒè™‘å¿«é€Ÿéƒ¨ç½²å’Œç¨³å®šè¿ç»´

**ADOT + Prometheus é€‚ç”¨äºï¼š**

- å¤§è§„æ¨¡é›†ç¾¤ï¼ˆ20,000 ä¸ªä»¥ä¸Šçš„ Podï¼‰
- é«˜æŒ‡æ ‡å¤„ç†ååé‡éœ€æ±‚
- ç²¾ç»†åŒ–ç›‘æ§å’Œè‡ªå®šä¹‰éœ€æ±‚
- å¯¹æ€§èƒ½å’Œå¯æ‰©å±•æ€§æœ‰æœ€é«˜è¦æ±‚

## 10 ç§’æ¶æ„ï¼šé€å±‚åˆ†æ

å®ç° 10 ç§’ä»¥å†…çš„æ‰©ç¼©å®¹éœ€è¦åœ¨æ¯ä¸€å±‚è¿›è¡Œä¼˜åŒ–ï¼š

```mermaid
graph TB
    subgraph "Layer 1: Ultra-Fast Metrics [1-2s]"
        ALB[ALB Metrics]
        APP[App Metrics]
        PROM[Prometheus<br/>Scrape: 1s]

        ALB -->|1s| PROM
        APP -->|1s| PROM
    end

    subgraph "Layer 2: Instant Decisions [2-3s]"
        MA[Metrics API]
        HPA[HPA Controller<br/>Sync: 5s]
        VPA[VPA Recommender]

        PROM --> MA
        MA --> HPA
        MA --> VPA
    end

    subgraph "Layer 3: Rapid Provisioning [30-45s]"
        KARP[Karpenter<br/>Provisioner]
        SPOT[Spot Fleet]
        OD[On-Demand]

        HPA --> KARP
        KARP --> SPOT
        KARP --> OD
    end

    subgraph "Layer 4: Instant Scheduling [2-5s]"
        SCHED[Scheduler]
        NODE[Available Nodes]
        POD[New Pods]

        SPOT --> NODE
        OD --> NODE
        NODE --> SCHED
        SCHED --> POD
    end

    subgraph "Total Timeline"
        TOTAL[Total: 35-55s<br/>P95: Sub-10s for pods<br/>P95: Under 60s for nodes]
    end

    style KARP fill:#ff9900,stroke:#232f3e,stroke-width:3px
    style HPA fill:#146eb4,stroke:#232f3e,stroke-width:2px
    style TOTAL fill:#48C9B0,stroke:#232f3e,stroke-width:3px

```

## å…³é”®é…ç½®ï¼šKarpenter Provisioner

å®ç° 60 ç§’ä»¥å†…èŠ‚ç‚¹é…ç½®çš„å…³é”®åœ¨äº Karpenter çš„æœ€ä¼˜é…ç½®ï¼š

```mermaid
graph LR
    subgraph "Provisioner Strategy"
        subgraph "Instance Selection"
            IT[Instance Types<br/>c6i.xlarge â†’ c6i.8xlarge<br/>c7i.xlarge â†’ c7i.8xlarge<br/>c6a.xlarge â†’ c6a.8xlarge]
            FLEX[Flexibility = Speed<br/>15+ instance types]
        end

        subgraph "Capacity Mix"
            SPOT[Spot: 70-80%<br/>Diverse instance pools]
            OD[On-Demand: 20-30%<br/>Critical workloads]
            INT[Interruption Handling<br/>30s grace period]
        end

        subgraph "Speed Optimizations"
            TTL[ttlSecondsAfterEmpty: 30<br/>Fast deprovisioning]
            CONS[Consolidation: true<br/>Continuous optimization]
            LIMITS[Soft limits only<br/>No hard constraints]
        end
    end

    IT --> RESULT[45-60s provisioning]
    SPOT --> RESULT
    TTL --> RESULT

    style RESULT fill:#48C9B0,stroke:#232f3e,stroke-width:3px

```

### Karpenter Provisioner YAML

```yaml
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: fast-scaling
spec:
  # é€Ÿåº¦ä¼˜åŒ–é…ç½®
  ttlSecondsAfterEmpty: 30
  ttlSecondsUntilExpired: 604800  # 7 å¤©

  # æœ€å¤§çµæ´»æ€§ä»¥æå‡é€Ÿåº¦
  requirements:
    - key: karpenter.sh/capacity-type
      operator: In
      values: ["spot", "on-demand"]
    - key: kubernetes.io/arch
      operator: In
      values: ["amd64"]
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
        # è®¡ç®—ä¼˜åŒ–å‹ - é¦–é€‰
        - c6i.xlarge
        - c6i.2xlarge
        - c6i.4xlarge
        - c6i.8xlarge
        - c7i.xlarge
        - c7i.2xlarge
        - c7i.4xlarge
        - c7i.8xlarge
        # AMD æ›¿ä»£æ–¹æ¡ˆ - æ›´å¥½çš„å¯ç”¨æ€§
        - c6a.xlarge
        - c6a.2xlarge
        - c6a.4xlarge
        - c6a.8xlarge
        # å†…å­˜ä¼˜åŒ–å‹ - ç”¨äºç‰¹å®šå·¥ä½œè´Ÿè½½
        - m6i.xlarge
        - m6i.2xlarge
        - m6i.4xlarge

  # ç¡®ä¿å¿«é€Ÿé…ç½®
  limits:
    resources:
      cpu: 100000  # ä»…è½¯é™åˆ¶
      memory: 400000Gi

  # æ•´åˆä¼˜åŒ–ä»¥æé«˜æ•ˆç‡
  consolidation:
    enabled: true

  # AWS ç‰¹å®šä¼˜åŒ–
  providerRef:
    name: fast-nodepool
---
apiVersion: karpenter.k8s.aws/v1alpha1
kind: AWSNodeInstanceProfile
metadata:
  name: fast-nodepool
spec:
  subnetSelector:
    karpenter.sh/discovery: "${CLUSTER_NAME}"
  securityGroupSelector:
    karpenter.sh/discovery: "${CLUSTER_NAME}"

  # é€Ÿåº¦ä¼˜åŒ–
  userData: |
    #!/bin/bash
    # ä¼˜åŒ–èŠ‚ç‚¹å¯åŠ¨æ—¶é—´
    /etc/eks/bootstrap.sh ${CLUSTER_NAME} \
      --b64-cluster-ca ${B64_CLUSTER_CA} \
      --apiserver-endpoint ${API_SERVER_URL} \
      --container-runtime containerd \
      --node-labels=karpenter.sh/fast-scaling=true \
      --max-pods=110

    # é¢„æ‹‰å–å…³é”®é•œåƒ
    ctr -n k8s.io images pull k8s.gcr.io/pause:3.9 &
    ctr -n k8s.io images pull public.ecr.aws/eks-distro/kubernetes/pause:3.9 &

```

## å®æ—¶æ‰©ç¼©å®¹å·¥ä½œæµ

ä»¥ä¸‹æ˜¯æ‰€æœ‰ç»„ä»¶å¦‚ä½•ååŒå·¥ä½œä»¥å®ç° 10 ç§’ä»¥å†…æ‰©ç¼©å®¹çš„å®Œæ•´æµç¨‹ï¼š

```mermaid
sequenceDiagram
    participant User
    participant ALB
    participant Pod
    participant Metrics
    participant HPA
    participant Karpenter
    participant EC2
    participant Node

    User->>ALB: Traffic spike begins
    ALB->>Pod: Forward requests
    Pod->>Pod: Queue building

    Note over Metrics: 1s collection interval
    Pod->>Metrics: Queue depth > threshold
    Metrics->>HPA: Metric update (2s)

    HPA->>HPA: Calculate new replicas
    HPA->>Pod: Create new pods

    Note over Karpenter: Detect unschedulable pods
    Pod->>Karpenter: Pending pods signal
    Karpenter->>Karpenter: Select optimal instances<br/>(200ms)

    Karpenter->>EC2: Launch instances<br/>(Fleet API)
    EC2->>Node: Provision nodes<br/>(30-45s)

    Node->>Node: Join cluster<br/>(10-15s)
    Node->>Pod: Schedule pods
    Pod->>ALB: Ready to serve

    Note over User,ALB: Total time: Under 60s for new capacity

```

## ç§¯ææ‰©ç¼©å®¹çš„ HPA é…ç½®

HorizontalPodAutoscaler å¿…é¡»é…ç½®ä¸ºå³æ—¶å“åº”ï¼š

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ultra-fast-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 10
  maxReplicas: 1000

  metrics:
  # ä¸»è¦æŒ‡æ ‡ - é˜Ÿåˆ—æ·±åº¦
  - type: External
    external:
      metric:
        name: sqs_queue_depth
        selector:
          matchLabels:
            queue: "web-requests"
      target:
        type: AverageValue
        averageValue: "10"

  # æ¬¡è¦æŒ‡æ ‡ - è¯·æ±‚é€Ÿç‡
  - type: External
    external:
      metric:
        name: alb_request_rate
        selector:
          matchLabels:
            targetgroup: "web-tg"
      target:
        type: AverageValue
        averageValue: "100"

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0  # æ— å»¶è¿Ÿï¼
      policies:
      - type: Percent
        value: 100
        periodSeconds: 10
      - type: Pods
        value: 100
        periodSeconds: 10
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 åˆ†é’Ÿå†·å´æœŸ
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60

```

## KEDA çš„é€‚ç”¨åœºæ™¯ï¼šäº‹ä»¶é©±åŠ¨åœºæ™¯

Karpenter è´Ÿè´£åŸºç¡€è®¾æ–½æ‰©ç¼©å®¹ï¼Œè€Œ KEDA åœ¨ç‰¹å®šçš„äº‹ä»¶é©±åŠ¨åœºæ™¯ä¸­è¡¨ç°å“è¶Šï¼š

```mermaid
graph LR
    subgraph "Use Karpenter + HPA"
        WEB[Web Traffic]
        API[API Requests]
        SYNC[Synchronous Workloads]
        USER[User-Facing Services]
    end

    subgraph "Use KEDA"
        QUEUE[Queue Processing<br/>SQS, Kafka]
        BATCH[Batch Jobs<br/>Scheduled Tasks]
        ASYNC[Async Processing]
        DEV[Dev/Test Envs<br/>Scale to Zero]
    end

    WEB --> DECISION{Scaling<br/>Strategy}
    API --> DECISION
    SYNC --> DECISION
    USER --> DECISION

    QUEUE --> DECISION
    BATCH --> DECISION
    ASYNC --> DECISION
    DEV --> DECISION

    DECISION -->|Karpenter| FAST[Under 60s<br/>Node Scaling]
    DECISION -->|KEDA| EVENT[Event-Driven<br/>Pod Scaling]

    style FAST fill:#ff9900
    style EVENT fill:#76c5d5

```

## ç”Ÿäº§ç¯å¢ƒæ€§èƒ½æŒ‡æ ‡

ä»¥ä¸‹æ˜¯å¤„ç†æ¯æ—¥ 750,000 ä»¥ä¸Šè¯·æ±‚çš„éƒ¨ç½²çš„å®é™…ç»“æœï¼š

```mermaid
graph TB
    subgraph "Before Optimization"
        B1[Scaling Trigger<br/>60-90s delay]
        B2[Node Provisioning<br/>3-5 minutes]
        B3[Total Response<br/>4-6 minutes]
        B4[User Impact<br/>Timeouts & Errors]
    end

    subgraph "After Karpenter + High-Res"
        A1[Scaling Trigger<br/>2-5s delay]
        A2[Node Provisioning<br/>45-60s]
        A3[Total Response<br/>Under 60s]
        A4[User Impact<br/>None]
    end

    subgraph "Improvements"
        I1[95% faster detection]
        I2[75% faster provisioning]
        I3[80% faster overall]
        I4[100% availability maintained]
    end

    B1 --> I1
    B2 --> I2
    B3 --> I3
    B4 --> I4

    I1 --> A1
    I2 --> A2
    I3 --> A3
    I4 --> A4

    style A3 fill:#48C9B0
    style I3 fill:#ff9900

```

## å¤šåŒºåŸŸæ³¨æ„äº‹é¡¹

å¯¹äºåœ¨å¤šä¸ªåŒºåŸŸè¿è¡Œçš„ç»„ç»‡ï¼Œè¦å®ç°ä¸€è‡´çš„ 10 ç§’ä»¥å†…æ‰©ç¼©å®¹ï¼Œéœ€è¦é’ˆå¯¹å„åŒºåŸŸè¿›è¡Œç‰¹å®šä¼˜åŒ–ï¼š

```mermaid
graph TB
    subgraph "Global Architecture"
        subgraph "US Region (40% traffic)"
            US_KARP[Karpenter US]
            US_TYPES[c6i, c7i priority]
            US_SPOT[80% Spot]
        end

        subgraph "EU Region (35% traffic)"
            EU_KARP[Karpenter EU]
            EU_TYPES[c6a, c7a priority]
            EU_SPOT[75% Spot]
        end

        subgraph "AP Region (25% traffic)"
            AP_KARP[Karpenter AP]
            AP_TYPES[c5, m5 included]
            AP_SPOT[70% Spot]
        end
    end

    subgraph "Cross-Region Metrics"
        GLOBAL[Global Metrics<br/>Aggregator]
        REGIONAL[Regional<br/>Decision Making]
    end

    US_KARP --> REGIONAL
    EU_KARP --> REGIONAL
    AP_KARP --> REGIONAL

    REGIONAL --> GLOBAL

```

## 10 ç§’ä»¥å†…æ‰©ç¼©å®¹æœ€ä½³å®è·µ

### 1. æŒ‡æ ‡é€‰æ‹©

- ä½¿ç”¨é¢†å…ˆæŒ‡æ ‡ï¼ˆé˜Ÿåˆ—æ·±åº¦ã€è¿æ¥æ•°ï¼‰è€Œéæ»åæŒ‡æ ‡ï¼ˆCPUï¼‰
- æ¯ä¸ªé›†ç¾¤çš„é«˜åˆ†è¾¨ç‡æŒ‡æ ‡æ§åˆ¶åœ¨ 10-15 ä¸ªä»¥å†…
- æ‰¹é‡æäº¤æŒ‡æ ‡ä»¥é¿å… API é™æµ

### 2. Karpenter ä¼˜åŒ–

- æä¾›æœ€å¤§çš„å®ä¾‹ç±»å‹çµæ´»æ€§
- ç§¯æä½¿ç”¨ Spot å®ä¾‹å¹¶é…åˆé€‚å½“çš„ä¸­æ–­å¤„ç†
- å¯ç”¨æ•´åˆï¼ˆConsolidationï¼‰ä»¥æé«˜æˆæœ¬æ•ˆç‡
- è®¾ç½®åˆé€‚çš„ ttlSecondsAfterEmptyï¼ˆ30-60 ç§’ï¼‰

### 3. HPA è°ƒä¼˜

- æ‰©å®¹æ—¶ç¨³å®šçª—å£è®¾ä¸ºé›¶
- ç§¯æçš„æ‰©ç¼©å®¹ç­–ç•¥ï¼ˆå…è®¸ 100% å¢å¹…ï¼‰
- å¤šæŒ‡æ ‡é…åˆé€‚å½“æƒé‡
- ç¼©å®¹æ—¶è®¾ç½®é€‚å½“çš„å†·å´æœŸ

### 4. ç›‘æ§

- å°† P95 æ‰©ç¼©å®¹å»¶è¿Ÿä½œä¸ºä¸»è¦ KPI è¿›è¡Œè·Ÿè¸ª
- æ‰©ç¼©å®¹å¤±è´¥æˆ–å»¶è¿Ÿè¶…è¿‡ 15 ç§’æ—¶è§¦å‘å‘Šè­¦
- ç›‘æ§ Spot ä¸­æ–­ç‡
- è·Ÿè¸ªæ¯ä¸ªæ‰©ç¼©å®¹ Pod çš„æˆæœ¬

## å¸¸è§é—®é¢˜æ’æŸ¥

```mermaid
graph LR
    subgraph "Symptom"
        SLOW[Scaling over 10s]
    end

    subgraph "Diagnosis"
        D1[Check Metric Lag]
        D2[Verify HPA Config]
        D3[Review Instance Types]
        D4[Analyze Subnet Capacity]
    end

    subgraph "Solution"
        S1[Reduce Collection Interval]
        S2[Remove Stabilization Window]
        S3[Add More Instance Types]
        S4[Expand Subnet CIDR]
    end

    SLOW --> D1 --> S1
    SLOW --> D2 --> S2
    SLOW --> D3 --> S3
    SLOW --> D4 --> S4

```

## æ··åˆæ–¹æ¡ˆï¼ˆæ¨èï¼‰

åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬æ¨èç»“åˆä¸¤ç§æ–¹æ³•çš„æ··åˆæ–¹æ¡ˆï¼š

1. **å…³é”®ä»»åŠ¡æœåŠ¡**ï¼šä½¿ç”¨ ADOT + Prometheus å®ç° 10-13 ç§’æ‰©ç¼©å®¹
2. **é€šç”¨æœåŠ¡**ï¼šä½¿ç”¨ CloudWatch Direct å®ç° 12-15 ç§’æ‰©ç¼©å®¹ï¼Œå¹¶ç®€åŒ–è¿ç»´
3. **æ¸è¿›å¼è¿ç§»**ï¼šä» CloudWatch å¼€å§‹ï¼Œæ ¹æ®éœ€è¦è¿‡æ¸¡åˆ° ADOT

## æ€»ç»“

åœ¨ EKS ä¸­å®ç° 10 ç§’ä»¥å†…çš„è‡ªåŠ¨æ‰©ç¼©å®¹ä¸ä»…æ˜¯å¯èƒ½çš„ï¼Œå¯¹äºç°ä»£åº”ç”¨æ¥è¯´æ›´æ˜¯å¿…ä¸å¯å°‘çš„ã€‚Karpenter çš„æ™ºèƒ½é…ç½®ã€å…³é”®æŒ‡æ ‡çš„é«˜åˆ†è¾¨ç‡ç›‘æ§ä»¥åŠç²¾å¿ƒè°ƒä¼˜çš„ HPA é…ç½®ä¸‰è€…ç»“åˆï¼Œæ„å»ºå‡ºä¸€ä¸ªèƒ½å¤Ÿè¿‘ä¹å®æ—¶å“åº”éœ€æ±‚çš„ç³»ç»Ÿã€‚

**å…³é”®è¦ç‚¹ï¼š**

- **Karpenter æ˜¯åŸºç¡€** - ç›´æ¥ EC2 é…ç½®å°†æ‰©ç¼©å®¹æ—¶é—´ç¼©çŸ­æ•°åˆ†é’Ÿ
- **é€‰æ‹©æ€§é«˜åˆ†è¾¨ç‡æŒ‡æ ‡** - ä»¥ 1-5 ç§’é—´éš”ç›‘æ§å…³é”®æŒ‡æ ‡
- **ç§¯æçš„ HPA é…ç½®** - æ¶ˆé™¤æ‰©ç¼©å®¹å†³ç­–ä¸­çš„äººä¸ºå»¶è¿Ÿ
- **é€šè¿‡æ™ºèƒ½åŒ–ä¼˜åŒ–æˆæœ¬** - å¿«é€Ÿæ‰©ç¼©å®¹å‡å°‘è¿‡åº¦é…ç½®
- **æ¶æ„é€‰æ‹©** - æ ¹æ®è§„æ¨¡å’Œéœ€æ±‚é€‰æ‹© CloudWatch æˆ– Prometheus

æœ¬æ–‡å±•ç¤ºçš„æ¶æ„å·²åœ¨æ¯å¤©å¤„ç†æ•°ç™¾ä¸‡è¯·æ±‚çš„ç”Ÿäº§ç¯å¢ƒä¸­å¾—åˆ°éªŒè¯ã€‚é€šè¿‡å®æ–½è¿™äº›æ¨¡å¼ï¼Œæ‚¨å¯ä»¥ç¡®ä¿ EKS é›†ç¾¤ä»¥ä¸šåŠ¡éœ€æ±‚çš„é€Ÿåº¦è¿›è¡Œæ‰©ç¼©å®¹â€”â€”ä»¥ç§’ä¸ºå•ä½è€Œéåˆ†é’Ÿã€‚

è¯·è®°ä½ï¼šåœ¨äº‘åŸç”Ÿä¸–ç•Œä¸­ï¼Œé€Ÿåº¦ä¸ä»…ä»…æ˜¯ä¸€ä¸ªåŠŸèƒ½ç‰¹æ€§â€”â€”å®ƒæ˜¯å¯é æ€§ã€æ•ˆç‡å’Œç”¨æˆ·æ»¡æ„åº¦çš„åŸºæœ¬è¦æ±‚ã€‚
