---
title: "åŸºäº Karpenter çš„ EKS æ‰©ç¼©å®¹ç­–ç•¥ç»¼åˆæŒ‡å—"
sidebar_label: "4. Karpenter æ‰©ç¼©å®¹ç­–ç•¥"
description: "åœ¨ Amazon EKS ä¸­åˆ©ç”¨ Karpenter çš„æ‰©ç¼©å®¹ç­–ç•¥ç»¼åˆæŒ‡å—ã€‚å“åº”å¼/é¢„æµ‹å¼/æ¶æ„å¼¹æ€§æ–¹æ³•å¯¹æ¯”ã€CloudWatch ä¸ Prometheus æ¶æ„å¯¹æ¯”ã€HPA é…ç½®ã€ç”Ÿäº§ç¯å¢ƒæ¨¡å¼"
tags: [eks, karpenter, autoscaling, performance, cloudwatch, prometheus, spot-instances]
category: "performance-networking"
last_update:
  date: 2026-02-13
  author: devfloor9
sidebar_position: 4
---

import { ScalingLatencyBreakdown, ControlPlaneComparison, WarmPoolCostAnalysis, AutoModeComparison, ScalingBenchmark, PracticalGuide } from '@site/src/components/KarpenterTables';

# åŸºäº Karpenter çš„ EKS æ‰©ç¼©å®¹ç­–ç•¥ç»¼åˆæŒ‡å—

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2025-02-09 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-18 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 28 åˆ†é’Ÿ

## æ¦‚è¿°

åœ¨ç°ä»£äº‘åŸç”Ÿåº”ç”¨ä¸­ï¼Œç¡®ä¿æµé‡æ¿€å¢æ—¶ç”¨æˆ·ä¸ä¼šé‡åˆ°é”™è¯¯æ˜¯æ ¸å¿ƒå·¥ç¨‹æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¡£æ¶µç›–äº†åœ¨ Amazon EKS ä¸­åˆ©ç”¨ Karpenter çš„**ç»¼åˆæ‰©ç¼©å®¹ç­–ç•¥**ï¼Œä»å“åº”å¼æ‰©ç¼©å®¹ä¼˜åŒ–åˆ°é¢„æµ‹å¼æ‰©ç¼©å®¹ã€æ¶æ„å¼¹æ€§ç­‰å†…å®¹ã€‚

:::caution ç°å®çš„ä¼˜åŒ–é¢„æœŸ
æœ¬æ–‡æ¡£ä¸­æ¶‰åŠçš„"è¶…å¿«é€Ÿæ‰©ç¼©å®¹"ä»¥ **Warm Poolï¼ˆé¢„åˆ†é…èŠ‚ç‚¹ï¼‰**ä¸ºå‰æã€‚E2E è‡ªåŠ¨æ‰©ç¼©å®¹ç®¡é“ï¼ˆæŒ‡æ ‡æ£€æµ‹ â†’ å†³ç­– â†’ Pod åˆ›å»º â†’ å®¹å™¨å¯åŠ¨ï¼‰çš„ç‰©ç†æœ€çŸ­æ—¶é—´ä¸º **6-11 ç§’**ï¼Œå¦‚æœéœ€è¦æ–°èŠ‚ç‚¹ä¾›åº”ï¼Œåˆ™éœ€é¢å¤– **45-90 ç§’**ã€‚

å°†æ‰©ç¼©å®¹é€Ÿåº¦æ¨åˆ°æé™å¹¶éå”¯ä¸€ç­–ç•¥ã€‚**æ¶æ„å¼¹æ€§**ï¼ˆåŸºäºé˜Ÿåˆ—çš„ç¼“å†²ã€Circuit Breakerï¼‰å’Œ**é¢„æµ‹å¼æ‰©ç¼©å®¹**ï¼ˆåŸºäºæ¨¡å¼çš„é¢„æ‰©å±•ï¼‰åœ¨å¤§å¤šæ•°å·¥ä½œè´Ÿè½½ä¸­æ›´å…·æˆæœ¬æ•ˆç›Šã€‚æœ¬æ–‡æ¡£å°†è¿™äº›æ–¹æ³•ä¸€å¹¶è®¨è®ºã€‚
:::

åœ¨å…¨çƒè§„æ¨¡çš„ EKS ç¯å¢ƒï¼ˆ3 ä¸ªåŒºåŸŸã€28 ä¸ªé›†ç¾¤ã€15,000+ Podï¼‰ä¸­ï¼Œå°†æ‰©ç¼©å®¹å»¶è¿Ÿä» 180 ç§’ä»¥ä¸Šç¼©çŸ­åˆ° 45 ç§’ä»¥ä¸‹ï¼Œå¹¶åœ¨ä½¿ç”¨ Warm Pool æ—¶è¾¾åˆ° 5-10 ç§’çš„ç”Ÿäº§éªŒè¯æ¶æ„ã€‚

## æ‰©ç¼©å®¹ç­–ç•¥å†³ç­–æ¡†æ¶

åœ¨ä¼˜åŒ–æ‰©ç¼©å®¹ä¹‹å‰ï¼Œåº”é¦–å…ˆåˆ¤æ–­ **"æˆ‘ä»¬çš„å·¥ä½œè´Ÿè½½æ˜¯å¦çœŸçš„éœ€è¦è¶…å¿«é€Ÿå“åº”å¼æ‰©ç¼©å®¹ï¼Ÿ"**ã€‚è§£å†³"æµé‡æ¿€å¢æ—¶é˜²æ­¢ç”¨æˆ·é”™è¯¯"è¿™ä¸€ç›¸åŒä¸šåŠ¡é—®é¢˜æœ‰ 4 ç§æ–¹æ³•ï¼Œåœ¨å¤§å¤šæ•°å·¥ä½œè´Ÿè½½ä¸­ï¼Œæ–¹æ³• 2-4 æ›´å…·æˆæœ¬æ•ˆç›Šã€‚

```mermaid
graph TB
    START[æµé‡æ¿€å¢æ—¶<br/>ç”¨æˆ·å‡ºç°é”™è¯¯] --> Q1{æµé‡æ¨¡å¼<br/>æ˜¯å¦å¯é¢„æµ‹ï¼Ÿ}

    Q1 -->|Yes| PRED[æ–¹æ³• 2ï¼šé¢„æµ‹å¼æ‰©ç¼©å®¹<br/>CronHPA + Predictive Scaling]
    Q1 -->|No| Q2{è¯·æ±‚æ˜¯å¦éœ€è¦<br/>ç«‹å³å¤„ç†ï¼Ÿ}

    Q2 -->|å¯ç­‰å¾…| ARCH[æ–¹æ³• 3ï¼šæ¶æ„å¼¹æ€§<br/>åŸºäºé˜Ÿåˆ—çš„ç¼“å†² + Rate Limiting]
    Q2 -->|å¿…é¡»ç«‹å³å¤„ç†| Q3{æ˜¯å¦å¯ä»¥å¢åŠ <br/>åŸºç¡€å®¹é‡ï¼Ÿ}

    Q3 -->|Yes| BASE[æ–¹æ³• 4ï¼šé€‚å½“çš„åŸºç¡€å®¹é‡<br/>ä»¥å³°å€¼ 70-80% ä¸ºåŸºç¡€è¿è¥]
    Q3 -->|æˆæœ¬é™åˆ¶| REACTIVE[æ–¹æ³• 1ï¼šå“åº”å¼æ‰©ç¼©å®¹åŠ é€Ÿ<br/>Karpenter + KEDA + Warm Pool]

    PRED --> COMBINE[å®è·µï¼šç»„åˆåº”ç”¨ 2-3 ç§æ–¹æ³•]
    ARCH --> COMBINE
    BASE --> COMBINE
    REACTIVE --> COMBINE

    style PRED fill:#059669,stroke:#232f3e,stroke-width:2px
    style ARCH fill:#3b82f6,stroke:#232f3e,stroke-width:2px
    style BASE fill:#8b5cf6,stroke:#232f3e,stroke-width:2px
    style REACTIVE fill:#f59e0b,stroke:#232f3e,stroke-width:2px
    style COMBINE fill:#1f2937,color:#fff,stroke:#232f3e,stroke-width:2px
```

### å„æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | æ ¸å¿ƒç­–ç•¥ | E2E æ‰©ç¼©å®¹æ—¶é—´ | æœˆé¢å¤–æˆæœ¬ï¼ˆ28 ä¸ªé›†ç¾¤ï¼‰ | å¤æ‚åº¦ | é€‚åˆçš„å·¥ä½œè´Ÿè½½ |
|--------|-----------|-------------------|---------------------------|--------|---------------|
| **1. å“åº”å¼åŠ é€Ÿ** | Karpenter + KEDA + Warm Pool | 5-45 ç§’ | $40K-190K | éå¸¸é«˜ | æå°‘æ•°å…³é”®ä»»åŠ¡ |
| **2. é¢„æµ‹å¼æ‰©ç¼©å®¹** | CronHPA + Predictive Scaling | é¢„æ‰©å±•ï¼ˆ0 ç§’ï¼‰ | $2K-5K | ä½ | æœ‰æ¨¡å¼çš„å¤§å¤šæ•°æœåŠ¡ |
| **3. æ¶æ„å¼¹æ€§** | SQS/Kafka + Circuit Breaker | å…è®¸æ‰©ç¼©å®¹å»¶è¿Ÿ | $1K-3K | ä¸­ç­‰ | å¯å¼‚æ­¥å¤„ç†çš„æœåŠ¡ |
| **4. é€‚å½“çš„åŸºç¡€å®¹é‡** | åŸºç¡€ replica å¢åŠ  20-30% | ä¸éœ€è¦ï¼ˆå·²è¶³å¤Ÿï¼‰ | $5K-15K | éå¸¸ä½ | ç¨³å®šæµé‡ |

### å„æ–¹æ³•æˆæœ¬ç»“æ„å¯¹æ¯”

ä»¥ä¸‹æ˜¯**ä¸­ç­‰è§„æ¨¡ 10 ä¸ªé›†ç¾¤åŸºå‡†**çš„æœˆé¢„ä¼°æˆæœ¬ã€‚å®é™…æˆæœ¬å› å·¥ä½œè´Ÿè½½å’Œå®ä¾‹ç±»å‹è€Œå¼‚ã€‚

```mermaid
graph LR
    subgraph "æ–¹æ³• 1ï¼šå“åº”å¼åŠ é€Ÿ"
        R1["Warm Pool ç»´æŠ¤<br/>$10,800/æœˆ"]
        R2["Provisioned CP<br/>$3,500/æœˆ"]
        R3["KEDA/ADOT è¿ç»´<br/>$500/æœˆ"]
        R4["Spot å®ä¾‹<br/>æŒ‰ç”¨é‡è®¡è´¹"]
        RT["åˆè®¡ï¼š$14,800+/æœˆ"]
        R1 --> RT
        R2 --> RT
        R3 --> RT
        R4 --> RT
    end

    subgraph "æ–¹æ³• 2ï¼šé¢„æµ‹å¼æ‰©ç¼©å®¹"
        P1["CronHPA é…ç½®<br/>$0 - k8s å†…ç½®"]
        P2["é«˜å³°æ—¶æ®µé¢å¤–å®¹é‡<br/>~$2,000/æœˆ"]
        P3["ç›‘æ§å·¥å…·<br/>$500/æœˆ"]
        PT["åˆè®¡ï¼š~$2,500/æœˆ"]
        P1 --> PT
        P2 --> PT
        P3 --> PT
    end

    subgraph "æ–¹æ³• 3ï¼šæ¶æ„å¼¹æ€§"
        A1["SQS/Kafka<br/>$300/æœˆ"]
        A2["Istio/Envoy<br/>$500/æœˆ"]
        A3["é¢å¤–å¼€å‘æˆæœ¬<br/>ä¸€æ¬¡æ€§"]
        AT["åˆè®¡ï¼š~$800/æœˆ"]
        A1 --> AT
        A2 --> AT
        A3 --> AT
    end

    subgraph "æ–¹æ³• 4ï¼šåŸºç¡€å®¹é‡å¢åŠ "
        B1["é¢å¤– replica 30%<br/>~$4,500/æœˆ"]
        B2["è¿ç»´æˆæœ¬<br/>$0 é¢å¤–"]
        BT["åˆè®¡ï¼š~$4,500/æœˆ"]
        B1 --> BT
        B2 --> BT
    end

    style RT fill:#ef4444,color:#fff
    style PT fill:#059669,color:#fff
    style AT fill:#3b82f6,color:#fff
    style BT fill:#8b5cf6,color:#fff
```

| æ–¹æ³• | æœˆæˆæœ¬ï¼ˆ10 ä¸ªé›†ç¾¤ï¼‰ | åˆå§‹å»ºè®¾æˆæœ¬ | è¿ç»´äººå‘˜éœ€æ±‚ | ROI è¾¾æˆæ¡ä»¶ |
|--------|----------------------|---------------|---------------|-------------|
| **1. å“åº”å¼åŠ é€Ÿ** | $14,800+ | é«˜ï¼ˆ2-4 å‘¨ï¼‰ | ä¸“èŒ 1-2 äºº | SLA è¿çº¦ç½šé‡‘ > $15K/æœˆ |
| **2. é¢„æµ‹å¼æ‰©ç¼©å®¹** | ~$2,500 | ä½ï¼ˆ2-3 å¤©ï¼‰ | ç°æœ‰äººå‘˜ | æµé‡æ¨¡å¼é¢„æµ‹ç‡ > 70% |
| **3. æ¶æ„å¼¹æ€§** | ~$800 | ä¸­ç­‰ï¼ˆ1-2 å‘¨ï¼‰ | ç°æœ‰äººå‘˜ | å…è®¸å¼‚æ­¥å¤„ç†çš„æœåŠ¡ |
| **4. åŸºç¡€å®¹é‡å¢åŠ ** | ~$4,500 | æ— ï¼ˆç«‹å³ï¼‰ | æ—  | å³°å€¼ 30% ç¼“å†²å³è¶³å¤Ÿ |

:::tip æ¨èï¼šæ–¹æ³•ç»„åˆ
åœ¨å¤§å¤šæ•°ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œ**æ–¹æ³• 2 + 4ï¼ˆé¢„æµ‹å¼ + åŸºç¡€å®¹é‡ï¼‰**å¯ä»¥è¦†ç›– 90% ä»¥ä¸Šçš„æµé‡æ¿€å¢ï¼Œå‰©ä½™ 10% ä½¿ç”¨**æ–¹æ³• 1ï¼ˆå“åº”å¼ Karpenterï¼‰**å¤„ç†ï¼Œè¿™ç§ç»„åˆæœ€å…·æˆæœ¬æ•ˆç›Šã€‚

æ–¹æ³• 3ï¼ˆæ¶æ„å¼¹æ€§ï¼‰æ˜¯è®¾è®¡æ–°æœåŠ¡æ—¶å¿…é¡»è€ƒè™‘çš„åŸºæœ¬æ¨¡å¼ã€‚
:::

### æ–¹æ³• 2ï¼šé¢„æµ‹å¼æ‰©ç¼©å®¹

å¤§å¤šæ•°ç”Ÿäº§æµé‡éƒ½æœ‰æ¨¡å¼ï¼ˆä¸Šç­æ—¶é—´ã€åˆé¤ã€æ´»åŠ¨ï¼‰ã€‚åœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œé¢„æµ‹å¼é¢„æ‰©å±•æ¯”å“åº”å¼æ‰©ç¼©å®¹æ›´æœ‰æ•ˆã€‚

```yaml
# CronHPAï¼šæŒ‰æ—¶é—´æ®µé¢„æ‰©ç¼©å®¹
apiVersion: autoscaling.k8s.io/v1alpha1
kind: CronHPA
metadata:
  name: traffic-pattern-scaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  jobs:
  - name: morning-peak
    schedule: "0 8 * * 1-5"    # å·¥ä½œæ—¥ä¸Šåˆ 8 ç‚¹
    targetSize: 50              # é¢„æ‰©å±•è‡³å³°å€¼æ°´å¹³
    completionPolicy:
      type: Never
  - name: lunch-peak
    schedule: "30 11 * * 1-5"   # å·¥ä½œæ—¥ä¸Šåˆ 11:30
    targetSize: 80
    completionPolicy:
      type: Never
  - name: off-peak
    schedule: "0 22 * * *"      # æ¯å¤©ä¸‹åˆ 10 ç‚¹
    targetSize: 10              # å¤œé—´ç¼©å‡
    completionPolicy:
      type: Never
```

### æ–¹æ³• 3ï¼šæ¶æ„å¼¹æ€§

ä¸å…¶å°†æ‰©ç¼©å®¹æ—¶é—´é™è‡³ 0ï¼Œä¸å¦‚è®¾è®¡æˆ**è®©æ‰©ç¼©å®¹å»¶è¿Ÿå¯¹ç”¨æˆ·ä¸å¯è§**æ›´ä¸ºç°å®ã€‚

**åŸºäºé˜Ÿåˆ—çš„ç¼“å†²**ï¼šå°†è¯·æ±‚æ”¾å…¥ SQS/Kafkaï¼Œæ‰©ç¼©å®¹å»¶è¿Ÿä»"å¤±è´¥"å˜ä¸º"ç­‰å¾…"ã€‚

```yaml
# KEDA SQS åŸºäºé˜Ÿåˆ—çš„æ‰©ç¼©å®¹ - è¯·æ±‚åœ¨é˜Ÿåˆ—ä¸­å®‰å…¨ç­‰å¾…
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: queue-worker
spec:
  scaleTargetRef:
    name: order-processor
  minReplicaCount: 2
  maxReplicaCount: 100
  triggers:
  - type: aws-sqs-queue
    metadata:
      queueURL: https://sqs.us-east-1.amazonaws.com/123456789/orders
      queueLength: "5"         # æ¯ 5 æ¡é˜Ÿåˆ—æ¶ˆæ¯å¯¹åº” 1 ä¸ª Pod
      awsRegion: us-east-1
```

**Circuit Breaker + Rate Limiting**ï¼šä½¿ç”¨ Istio/Envoy åœ¨è¿‡è½½æ—¶è¿›è¡Œä¼˜é›…é™çº§

```yaml
# Istio Circuit Breaker - é˜²æ­¢æ‰©ç¼©å®¹æœŸé—´è¿‡è½½
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: web-app-circuit-breaker
spec:
  host: web-app
  trafficPolicy:
    connectionPool:
      http:
        h2UpgradePolicy: DEFAULT
        http1MaxPendingRequests: 100    # é™åˆ¶ç­‰å¾…è¯·æ±‚
        http2MaxRequests: 1000          # é™åˆ¶å¹¶å‘è¯·æ±‚
    outlierDetection:
      consecutive5xxErrors: 5            # 5xx å‡ºç° 5 æ¬¡æ—¶éš”ç¦»
      interval: 10s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
```

### æ–¹æ³• 4ï¼šé€‚å½“çš„åŸºç¡€å®¹é‡

ä¸å…¶åœ¨ Warm Pool ä¸Šæ¯æœˆèŠ±è´¹ $1,080-$5,400ï¼Œä¸å¦‚å°†åŸºç¡€ replica å¢åŠ  20-30%ï¼Œæ— éœ€å¤æ‚åŸºç¡€è®¾æ–½å³å¯è·å¾—ç›¸åŒæ•ˆæœã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  # é¢„æœŸæ‰€éœ€ Podï¼š20 ä¸ª â†’ åŸºç¡€è¿è¡Œ 25 ä¸ªï¼ˆ25% ä½™é‡ï¼‰
  replicas: 25
  # HPA è´Ÿè´£é«˜å³°æ—¶çš„é¢å¤–æ‰©å±•
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 25     # ä¿è¯åŸºç¡€å®¹é‡
  maxReplicas: 100    # åº”å¯¹æç«¯æƒ…å†µ
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60   # å®½è£•çš„ç›®æ ‡ï¼ˆ70 â†’ 60ï¼‰
```

---

ä»¥ä¸‹ç« èŠ‚å°†è¯¦ç»†ä»‹ç»**æ–¹æ³• 1ï¼šå“åº”å¼æ‰©ç¼©å®¹åŠ é€Ÿ**çš„å®ç°ã€‚åœ¨æ£€æŸ¥å®Œä¸Šè¿°æ–¹æ³• 2-4 åï¼Œå¯¹äºéœ€è¦é¢å¤–ä¼˜åŒ–çš„å·¥ä½œè´Ÿè½½ï¼Œè¯·åº”ç”¨ä»¥ä¸‹å†…å®¹ã€‚

---

## ç°æœ‰è‡ªåŠ¨æ‰©ç¼©å®¹çš„é—®é¢˜

åœ¨ä¼˜åŒ–å“åº”å¼æ‰©ç¼©å®¹ä¹‹å‰ï¼Œéœ€è¦äº†è§£ç°æœ‰æ–¹æ³•çš„ç“¶é¢ˆï¼š

```mermaid
graph LR
    subgraph "ç°æœ‰æ‰©ç¼©å®¹æ—¶é—´çº¿ï¼ˆ3 åˆ†é’Ÿä»¥ä¸Šï¼‰"
        T1[æµé‡æ¿€å¢<br/>T+0s] --> T2[CPU æŒ‡æ ‡æ›´æ–°<br/>T+60s]
        T2 --> T3[HPA å†³ç­–<br/>T+90s]
        T3 --> T4[ASG æ‰©ç¼©å®¹<br/>T+120s]
        T4 --> T5[èŠ‚ç‚¹å°±ç»ª<br/>T+180s]
        T5 --> T6[Pod è°ƒåº¦<br/>T+210s]
    end

    subgraph "ç”¨æˆ·å½±å“"
        I1[è¶…æ—¶å¼€å§‹<br/>T+5s]
        I2[é”™è¯¯æ¿€å¢<br/>T+30s]
        I3[æœåŠ¡é™çº§<br/>T+60s]
    end

    T1 -.-> I1
    T2 -.-> I2
    T3 -.-> I3

    style I1 fill:#ff4444
    style I2 fill:#ff6666
    style I3 fill:#ff8888

```

æ ¹æœ¬é—®é¢˜ï¼šå½“ CPU æŒ‡æ ‡è§¦å‘æ‰©ç¼©å®¹æ—¶ï¼Œå·²ç»å¤ªæ™šäº†ã€‚

**å½“å‰ç¯å¢ƒçš„æŒ‘æˆ˜ï¼š**

- **å…¨çƒè§„æ¨¡**ï¼š3 ä¸ªåŒºåŸŸã€28 ä¸ª EKS é›†ç¾¤ã€15,000 ä¸ª Pod è¿è¡Œ
- **å¤§æµé‡**ï¼šæ—¥å¤„ç† 773.4K è¯·æ±‚
- **å»¶è¿Ÿé—®é¢˜**ï¼šHPA + Karpenter ç»„åˆå¯¼è‡´ 1-3 åˆ†é’Ÿæ‰©ç¼©å®¹å»¶è¿Ÿ
- **æŒ‡æ ‡æ”¶é›†å»¶è¿Ÿ**ï¼šCloudWatch æŒ‡æ ‡ 1-3 åˆ†é’Ÿå»¶è¿Ÿå¯¼è‡´æ— æ³•å®æ—¶å“åº”

## Karpenter é©å‘½ï¼šDirect-to-Metal ä¾›åº”

Karpenter ç§»é™¤äº† Auto Scaling Group (ASG) æŠ½è±¡å±‚ï¼ŒåŸºäºå¾…è°ƒåº¦ Pod çš„éœ€æ±‚ç›´æ¥ä¾›åº” EC2 å®ä¾‹ã€‚Karpenter v1.x é€šè¿‡ **Drift Detection** åŠŸèƒ½ï¼Œåœ¨ NodePool è§„æ ¼å˜æ›´æ—¶è‡ªåŠ¨æ›¿æ¢ç°æœ‰èŠ‚ç‚¹ã€‚AMI æ›´æ–°ã€å®‰å…¨è¡¥ä¸åº”ç”¨ç­‰å‡å¯è‡ªåŠ¨åŒ–ã€‚

```mermaid
graph TB
    subgraph "Karpenter æ¶æ„"
        PP[å¾…è°ƒåº¦ Pod<br/>å·²æ£€æµ‹]
        KL[Karpenter é€»è¾‘]
        EC2[EC2 Fleet API]

        PP -->|æ¯«ç§’| KL

        subgraph "æ™ºèƒ½å†³ç­–å¼•æ“"
            IS[å®ä¾‹é€‰æ‹©]
            SP[Spot/OD ç»„åˆ]
            AZ[AZ åˆ†å¸ƒ]
            CP[å®¹é‡è§„åˆ’]
        end

        KL --> IS
        KL --> SP
        KL --> AZ
        KL --> CP

        IS --> EC2
        SP --> EC2
        AZ --> EC2
        CP --> EC2
    end

    subgraph "ä¼ ç»Ÿ ASG"
        ASG[Auto Scaling Group]
        LT[Launch Template]
        ASGL[ASG é€»è¾‘]

        ASG --> LT
        LT --> ASGL
        ASGL -->|2-3 åˆ†é’Ÿ| EC2_OLD[EC2 API]
    end

    EC2 -->|30-45 ç§’| NODE[èŠ‚ç‚¹å°±ç»ª]
    EC2_OLD -->|120-180 ç§’| NODE_OLD[èŠ‚ç‚¹å°±ç»ª]

    style KL fill:#ff9900,stroke:#232f3e,stroke-width:3px
    style EC2 fill:#146eb4,stroke:#232f3e,stroke-width:2px
    style ASG fill:#cccccc,stroke:#999999

```

## é«˜é€ŸæŒ‡æ ‡æ¶æ„ï¼šä¸¤ç§æ–¹æ³•

è¦æœ€å°åŒ–æ‰©ç¼©å®¹å“åº”æ—¶é—´ï¼Œéœ€è¦å¿«é€Ÿæ£€æµ‹ç³»ç»Ÿã€‚æˆ‘ä»¬å¯¹æ¯”ä¸¤ç§ç»è¿‡éªŒè¯çš„æ¶æ„ã€‚

### æ–¹å¼ 1ï¼šCloudWatch High-Resolution Integration

åœ¨ AWS åŸç”Ÿç¯å¢ƒä¸­åˆ©ç”¨ CloudWatch çš„é«˜åˆ†è¾¨ç‡æŒ‡æ ‡ã€‚

#### ä¸»è¦ç»„ä»¶

```mermaid
graph TB
    subgraph "æŒ‡æ ‡æ¥æº"
        subgraph "å…³é”®æŒ‡æ ‡ï¼ˆ1 ç§’ï¼‰"
            RPS[æ¯ç§’è¯·æ±‚æ•°]
            LAT[P99 å»¶è¿Ÿ]
            ERR[é”™è¯¯ç‡]
            QUEUE[é˜Ÿåˆ—æ·±åº¦]
        end

        subgraph "æ ‡å‡†æŒ‡æ ‡ï¼ˆ60 ç§’ï¼‰"
            CPU[CPU ä½¿ç”¨ç‡]
            MEM[å†…å­˜ä½¿ç”¨ç‡]
            DISK[ç£ç›˜ I/O]
            NET[ç½‘ç»œ I/O]
        end
    end

    subgraph "æ”¶é›†ç®¡é“"
        AGENT[ADOT Collector<br/>æ‰¹æ¬¡ï¼š1 ç§’]
        EMF[EMF æ ¼å¼<br/>å‹ç¼©]
        CW[CloudWatch API<br/>PutMetricData]
    end

    subgraph "å†³ç­–å±‚"
        API[Custom Metrics API]
        CACHE[å†…å­˜ç¼“å­˜<br/>TTLï¼š5 ç§’]
        HPA[HPA Controller]
    end

    RPS --> AGENT
    LAT --> AGENT
    ERR --> AGENT
    QUEUE --> AGENT

    CPU --> AGENT
    MEM --> AGENT

    AGENT --> EMF
    EMF --> CW
    CW --> API
    API --> CACHE
    CACHE --> HPA

    style RPS fill:#ff4444
    style LAT fill:#ff4444
    style ERR fill:#ff4444
    style QUEUE fill:#ff4444

```

#### æ‰©ç¼©å®¹æ—¶é—´çº¿

```mermaid
timeline
    title åŸºäº CloudWatch çš„è‡ªåŠ¨æ‰©ç¼©å®¹æ—¶é—´çº¿

    section æŒ‡æ ‡ç®¡é“ï¼ˆ~8 ç§’ï¼‰
        T+0s  : åº”ç”¨äº§ç”ŸæŒ‡æ ‡
        T+1s  : å¼‚æ­¥æ‰¹é‡å‘é€è‡³ CloudWatch
        T+2s  : CloudWatch æŒ‡æ ‡å¤„ç†å®Œæˆ
        T+5s  : KEDA è½®è¯¢å‘¨æœŸæ‰§è¡Œ
        T+6s  : KEDA ä½œå‡ºæ‰©ç¼©å®¹å†³ç­–
        T+8s  : HPA æ›´æ–°åŠ Pod åˆ›å»ºè¯·æ±‚

    section èŠ‚ç‚¹å·²å­˜åœ¨æ—¶ï¼ˆ+5 ç§’ï¼‰
        T+10s : Pod è°ƒåº¦è‡³ç°æœ‰èŠ‚ç‚¹
        T+13s : å®¹å™¨å¯åŠ¨å¹¶ Ready

    section éœ€è¦æ–°èŠ‚ç‚¹æ—¶ï¼ˆ+40-50 ç§’ï¼‰
        T+10s : Karpenter é€‰æ‹©å®ä¾‹
        T+40s : EC2 å®ä¾‹å¯åŠ¨å®Œæˆ
        T+48s : åŠ å…¥é›†ç¾¤å¹¶è°ƒåº¦ Pod
        T+53s : å®¹å™¨å¯åŠ¨å¹¶ Ready
```

:::info æ—¶é—´çº¿è§£è¯»
- **èŠ‚ç‚¹å·²å­˜åœ¨çš„æƒ…å†µ**ï¼ˆWarm Pool æˆ–ç°æœ‰ç©ºé—²èŠ‚ç‚¹ï¼‰ï¼šE2E **~13 ç§’**
- **éœ€è¦æ–°èŠ‚ç‚¹ä¾›åº”çš„æƒ…å†µ**ï¼šE2E **~53 ç§’**
- EC2 å®ä¾‹å¯åŠ¨ï¼ˆ30-40 ç§’ï¼‰æ˜¯ç‰©ç†é™åˆ¶ï¼Œä»…é æŒ‡æ ‡ç®¡é“ä¼˜åŒ–æ— æ³•æ¶ˆé™¤ã€‚
:::

**ä¼˜ç‚¹ï¼š**

- âœ… **å¿«é€ŸæŒ‡æ ‡æ”¶é›†**ï¼š1-2 ç§’çš„ä½å»¶è¿Ÿ
- âœ… **ç®€å•é…ç½®**ï¼šAWS åŸç”Ÿé›†æˆ
- âœ… **æ— ç®¡ç†å¼€é”€**ï¼šä¸éœ€è¦é¢å¤–åŸºç¡€è®¾æ–½ç®¡ç†

**ç¼ºç‚¹ï¼š**

- âŒ **æœ‰é™çš„ååé‡**ï¼šæ¯è´¦æˆ· 500 TPSï¼ˆPutMetricData åŒºåŸŸé™åˆ¶ï¼‰
- âŒ **Pod é™åˆ¶**ï¼šæ¯é›†ç¾¤æœ€å¤š 5,000 ä¸ª
- âŒ **é«˜æŒ‡æ ‡æˆæœ¬**ï¼šAWS CloudWatch æŒ‡æ ‡è´¹ç”¨

### æ–¹å¼ 2ï¼šåŸºäº ADOT + Prometheus çš„æ¶æ„

ç»“åˆ AWS Distro for OpenTelemetry (ADOT) å’Œ Prometheus çš„å¼€æºé«˜æ€§èƒ½ç®¡é“ã€‚

#### ä¸»è¦ç»„ä»¶

- **ADOT Collector**ï¼šDaemonSet å’Œ Sidecar æ··åˆéƒ¨ç½²
- **Prometheus**ï¼šHA é…ç½®åŠ Remote Storage é›†æˆ
- **Thanos Query Layer**ï¼šæä¾›å¤šé›†ç¾¤å…¨å±€è§†å›¾
- **KEDA Prometheus Scaler**ï¼š2 ç§’é—´éš”é«˜é€Ÿè½®è¯¢
- **Grafana Mimir**ï¼šé•¿æœŸå­˜å‚¨åŠé«˜é€ŸæŸ¥è¯¢å¼•æ“

#### æ‰©ç¼©å®¹æ—¶é—´çº¿ï¼ˆ~66 ç§’ï¼‰

```mermaid
timeline
    title ADOT + Prometheus è‡ªåŠ¨æ‰©ç¼©å®¹æ—¶é—´çº¿ï¼ˆä¼˜åŒ–ç¯å¢ƒï¼Œ~66 ç§’ï¼‰

    T+0s   : åº”ç”¨äº§ç”ŸæŒ‡æ ‡
    T+15s  : ADOT æ”¶é›†ï¼ˆ15 ç§’ä¼˜åŒ–æŠ“å–ï¼‰
    T+16s  : Prometheus å­˜å‚¨åŠç´¢å¼•å®Œæˆ
    T+25s  : KEDA è½®è¯¢æ‰§è¡Œï¼ˆ10 ç§’é—´éš”ä¼˜åŒ–ï¼‰
    T+26s  : æ‰©ç¼©å®¹å†³ç­–ï¼ˆåŸºäº P95 æŒ‡æ ‡ï¼‰
    T+41s  : HPA æ›´æ–°ï¼ˆ15 ç§’åŒæ­¥å‘¨æœŸï¼‰
    T+46s  : Pod åˆ›å»ºè¯·æ±‚å¼€å§‹
    T+51s  : é•œåƒæ‹‰å–åŠå®¹å™¨å¯åŠ¨
    T+66s  : Pod Ready çŠ¶æ€åŠæ‰©ç¼©å®¹å®Œæˆ
```

**ä¼˜ç‚¹ï¼š**

- âœ… **é«˜ååé‡**ï¼šæ”¯æŒ 100,000+ TPS
- âœ… **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒæ¯é›†ç¾¤ 20,000+ Pod
- âœ… **ä½æŒ‡æ ‡æˆæœ¬**ï¼šä»…äº§ç”Ÿå­˜å‚¨æˆæœ¬ï¼ˆè‡ªç®¡ç†ï¼‰
- âœ… **å®Œå…¨æ§åˆ¶**ï¼šé…ç½®å’Œä¼˜åŒ–è‡ªç”±åº¦é«˜

**ç¼ºç‚¹ï¼š**

- âŒ **å¤æ‚é…ç½®**ï¼šéœ€è¦é¢å¤–ç»„ä»¶ç®¡ç†
- âŒ **é«˜è¿ç»´å¤æ‚åº¦**ï¼šéœ€è¦ HA é…ç½®ã€å¤‡ä»½/æ¢å¤ã€æ€§èƒ½è°ƒä¼˜
- âŒ **éœ€è¦ä¸“ä¸šäººå‘˜**ï¼šéœ€è¦ Prometheus è¿ç»´ç»éªŒ

### æˆæœ¬ä¼˜åŒ–æŒ‡æ ‡ç­–ç•¥

```mermaid
pie title "æ¯é›†ç¾¤æ¯æœˆ CloudWatch æˆæœ¬ï¼ˆ$18ï¼‰"
    "é«˜åˆ†è¾¨ç‡æŒ‡æ ‡ï¼ˆ10 ä¸ªï¼‰" : 3
    "æ ‡å‡†æŒ‡æ ‡ï¼ˆ100 ä¸ªï¼‰" : 10
    "API è°ƒç”¨" : 5

```

28 ä¸ªé›†ç¾¤åŸºå‡†ï¼šç»¼åˆç›‘æ§æ¯æœˆçº¦ $500 vs æ‰€æœ‰æŒ‡æ ‡ä½¿ç”¨é«˜åˆ†è¾¨ç‡æ”¶é›†æ—¶ $30,000+

### æ¨èä½¿ç”¨åœºæ™¯

**CloudWatch High Resolution Metric é€‚åˆçš„æƒ…å†µï¼š**

- å°è§„æ¨¡åº”ç”¨ï¼ˆPod 5,000 ä¸ªä»¥ä¸‹ï¼‰
- ç®€å•çš„ç›‘æ§éœ€æ±‚
- åå¥½ AWS åŸç”Ÿè§£å†³æ–¹æ¡ˆ
- å¿«é€Ÿæ„å»ºå’Œç¨³å®šè¿ç»´ä¼˜å…ˆ

**ADOT + Prometheus é€‚åˆçš„æƒ…å†µï¼š**

- å¤§è§„æ¨¡é›†ç¾¤ï¼ˆPod 20,000 ä¸ªä»¥ä¸Šï¼‰
- é«˜æŒ‡æ ‡å¤„ç†ååé‡éœ€æ±‚
- éœ€è¦ç²¾ç»†ç›‘æ§å’Œè‡ªå®šä¹‰
- éœ€è¦æœ€é«˜æ°´å¹³çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§

## æ‰©ç¼©å®¹ä¼˜åŒ–æ¶æ„ï¼šé€å±‚åˆ†æ

è¦æœ€å°åŒ–æ‰©ç¼©å®¹å“åº”æ—¶é—´ï¼Œéœ€è¦åœ¨æ‰€æœ‰å±‚è¿›è¡Œä¼˜åŒ–ï¼š

```mermaid
graph TB
    subgraph "ç¬¬ 1 å±‚ï¼šè¶…å¿«é€ŸæŒ‡æ ‡ [1-2 ç§’]"
        ALB[ALB æŒ‡æ ‡]
        APP[åº”ç”¨æŒ‡æ ‡]
        PROM[Prometheus<br/>æŠ“å–ï¼š1 ç§’]

        ALB -->|1 ç§’| PROM
        APP -->|1 ç§’| PROM
    end

    subgraph "ç¬¬ 2 å±‚ï¼šå³æ—¶å†³ç­– [2-3 ç§’]"
        MA[Metrics API]
        HPA[HPA Controller<br/>åŒæ­¥ï¼š5 ç§’]
        VPA[VPA Recommender]

        PROM --> MA
        MA --> HPA
        MA --> VPA
    end

    subgraph "ç¬¬ 3 å±‚ï¼šå¿«é€Ÿä¾›åº” [30-45 ç§’]"
        KARP[Karpenter<br/>Provisioner]
        SPOT[Spot Fleet]
        OD[On-Demand]

        HPA --> KARP
        KARP --> SPOT
        KARP --> OD
    end

    subgraph "ç¬¬ 4 å±‚ï¼šå³æ—¶è°ƒåº¦ [2-5 ç§’]"
        SCHED[Scheduler]
        NODE[å¯ç”¨èŠ‚ç‚¹]
        POD[æ–° Pod]

        SPOT --> NODE
        OD --> NODE
        NODE --> SCHED
        SCHED --> POD
    end

    subgraph "æ€»æ—¶é—´çº¿"
        TOTAL[æ€»æ—¶é—´ï¼š35-55 ç§’<br/>P95ï¼šç°æœ‰èŠ‚ç‚¹ Pod æ”¾ç½® ~10 ç§’<br/>P95ï¼šå«æ–°èŠ‚ç‚¹ ~60 ç§’]
    end

    style KARP fill:#ff9900,stroke:#232f3e,stroke-width:3px
    style HPA fill:#146eb4,stroke:#232f3e,stroke-width:2px
    style TOTAL fill:#48C9B0,stroke:#232f3e,stroke-width:3px

```

## Karpenter æ ¸å¿ƒé…ç½®

60 ç§’ä»¥å†…èŠ‚ç‚¹ä¾›åº”çš„å…³é”®åœ¨äºæœ€ä¼˜çš„ Karpenter é…ç½®ï¼š

```mermaid
graph LR
    subgraph "Provisioner ç­–ç•¥"
        subgraph "å®ä¾‹é€‰æ‹©"
            IT[å®ä¾‹ç±»å‹<br/>c6i.xlarge â†’ c6i.8xlarge<br/>c7i.xlarge â†’ c7i.8xlarge<br/>c6a.xlarge â†’ c6a.8xlarge]
            FLEX[çµæ´»æ€§ = é€Ÿåº¦<br/>15+ å®ä¾‹ç±»å‹]
        end

        subgraph "å®¹é‡ç»„åˆ"
            SPOT[Spotï¼š70-80%<br/>å¤šæ ·åŒ–å®ä¾‹æ± ]
            OD[On-Demandï¼š20-30%<br/>å…³é”®å·¥ä½œè´Ÿè½½]
            INT[ä¸­æ–­å¤„ç†<br/>30 ç§’å®½é™æœŸ]
        end

        subgraph "é€Ÿåº¦ä¼˜åŒ–"
            TTL[ttlSecondsAfterEmpty: 30<br/>å¿«é€Ÿå–æ¶ˆä¾›åº”]
            CONS[Consolidation: true<br/>æŒç»­ä¼˜åŒ–]
            LIMITS[ä»…è½¯é™åˆ¶<br/>æ— ç¡¬çº¦æŸ]
        end
    end

    IT --> RESULT[45-60 ç§’ä¾›åº”]
    SPOT --> RESULT
    TTL --> RESULT

    style RESULT fill:#48C9B0,stroke:#232f3e,stroke-width:3px

```

### Karpenter NodePool YAML

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: fast-scaling
spec:
  # é€Ÿåº¦ä¼˜åŒ–é…ç½®
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
    budgets:
    - nodes: "10%"

  # ä¸ºé€Ÿåº¦æä¾›æœ€å¤§çµæ´»æ€§
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            # è®¡ç®—ä¼˜åŒ– - é»˜è®¤é€‰æ‹©
            - c6i.xlarge
            - c6i.2xlarge
            - c6i.4xlarge
            - c6i.8xlarge
            - c7i.xlarge
            - c7i.2xlarge
            - c7i.4xlarge
            - c7i.8xlarge
            # AMD æ›¿ä»£ - æ›´å¥½çš„å¯ç”¨æ€§
            - c6a.xlarge
            - c6a.2xlarge
            - c6a.4xlarge
            - c6a.8xlarge
            # å†…å­˜ä¼˜åŒ– - ç‰¹å®šå·¥ä½œè´Ÿè½½
            - m6i.xlarge
            - m6i.2xlarge
            - m6i.4xlarge

      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: fast-nodepool

  # ä¿è¯å¿«é€Ÿä¾›åº”
  limits:
    cpu: 100000  # ä»…è½¯é™åˆ¶
    memory: 400000Gi
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: fast-nodepool
spec:
  amiSelectorTerms:
    - alias: al2023@latest

  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: "${CLUSTER_NAME}"

  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: "${CLUSTER_NAME}"

  role: "KarpenterNodeRole-${CLUSTER_NAME}"

  # é€Ÿåº¦ä¼˜åŒ–
  userData: |
    #!/bin/bash
    # èŠ‚ç‚¹å¯åŠ¨æ—¶é—´ä¼˜åŒ–
    /etc/eks/bootstrap.sh ${CLUSTER_NAME} \
      --b64-cluster-ca ${B64_CLUSTER_CA} \
      --apiserver-endpoint ${API_SERVER_URL} \
      --kubelet-extra-args '--node-labels=karpenter.sh/fast-scaling=true --max-pods=110'

    # å…³é”®é•œåƒé¢„æ‹‰å–ï¼ˆregistry.k8s.io æ›¿ä»£ k8s.gcr.ioï¼‰
    ctr -n k8s.io images pull registry.k8s.io/pause:3.10 &
    ctr -n k8s.io images pull public.ecr.aws/eks-distro/kubernetes/pause:3.10 &

```

## å®æ—¶æ‰©ç¼©å®¹å·¥ä½œæµ

æ‰€æœ‰ç»„ä»¶ååŒå·¥ä½œä»¥å®ç°æœ€ä¼˜æ‰©ç¼©å®¹æ€§èƒ½ï¼š

```mermaid
sequenceDiagram
    participant User
    participant ALB
    participant Pod
    participant Metrics
    participant HPA
    participant Karpenter
    participant EC2
    participant Node

    User->>ALB: æµé‡æ¿€å¢å¼€å§‹
    ALB->>Pod: è½¬å‘è¯·æ±‚
    Pod->>Pod: é˜Ÿåˆ—å¢é•¿

    Note over Metrics: 1 ç§’æ”¶é›†é—´éš”
    Pod->>Metrics: é˜Ÿåˆ—æ·±åº¦ > é˜ˆå€¼
    Metrics->>HPA: æŒ‡æ ‡æ›´æ–°ï¼ˆ2 ç§’ï¼‰

    HPA->>HPA: è®¡ç®—æ–°å‰¯æœ¬æ•°
    HPA->>Pod: åˆ›å»ºæ–° Pod

    Note over Karpenter: æ£€æµ‹åˆ°ä¸å¯è°ƒåº¦çš„ Pod
    Pod->>Karpenter: å¾…è°ƒåº¦ Pod ä¿¡å·
    Karpenter->>Karpenter: é€‰æ‹©æœ€ä¼˜å®ä¾‹<br/>(200ms)

    Karpenter->>EC2: å¯åŠ¨å®ä¾‹<br/>(Fleet API)
    EC2->>Node: èŠ‚ç‚¹ä¾›åº”<br/>(30-45 ç§’)

    Node->>Node: åŠ å…¥é›†ç¾¤<br/>(10-15 ç§’)
    Node->>Pod: Pod è°ƒåº¦
    Pod->>ALB: æœåŠ¡å°±ç»ª

    Note over User,ALB: æ€»æ—¶é—´ï¼š60 ç§’ä»¥å†…ï¼ˆæ–°å®¹é‡ï¼‰

```

## ç”¨äºæ¿€è¿›æ‰©ç¼©å®¹çš„ HPA é…ç½®

HorizontalPodAutoscaler å¿…é¡»é…ç½®ä¸ºå³æ—¶å“åº”ï¼š

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ultra-fast-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 10
  maxReplicas: 1000

  metrics:
  # ä¸»è¦æŒ‡æ ‡ - é˜Ÿåˆ—æ·±åº¦
  - type: External
    external:
      metric:
        name: sqs_queue_depth
        selector:
          matchLabels:
            queue: "web-requests"
      target:
        type: AverageValue
        averageValue: "10"

  # è¾…åŠ©æŒ‡æ ‡ - è¯·æ±‚é€Ÿç‡
  - type: External
    external:
      metric:
        name: alb_request_rate
        selector:
          matchLabels:
            targetgroup: "web-tg"
      target:
        type: AverageValue
        averageValue: "100"

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0  # æ— å»¶è¿Ÿï¼
      policies:
      - type: Percent
        value: 100
        periodSeconds: 10
      - type: Pods
        value: 100
        periodSeconds: 10
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 åˆ†é’Ÿå†·å´
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60

```

## KEDA ä½¿ç”¨æ—¶æœºï¼šäº‹ä»¶é©±åŠ¨åœºæ™¯

Karpenter å¤„ç†åŸºç¡€è®¾æ–½æ‰©ç¼©å®¹ï¼Œè€Œ KEDA åœ¨ç‰¹å®šäº‹ä»¶é©±åŠ¨åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼š

```mermaid
graph LR
    subgraph "Karpenter + HPA ä½¿ç”¨"
        WEB[Web æµé‡]
        API[API è¯·æ±‚]
        SYNC[åŒæ­¥å·¥ä½œè´Ÿè½½]
        USER[é¢å‘ç”¨æˆ·çš„æœåŠ¡]
    end

    subgraph "KEDA ä½¿ç”¨"
        QUEUE[é˜Ÿåˆ—å¤„ç†<br/>SQS, Kafka]
        BATCH[æ‰¹å¤„ç†ä½œä¸š<br/>è®¡åˆ’ä»»åŠ¡]
        ASYNC[å¼‚æ­¥å¤„ç†]
        DEV[å¼€å‘/æµ‹è¯•ç¯å¢ƒ<br/>é›¶æ‰©ç¼©å®¹]
    end

    WEB --> DECISION{æ‰©ç¼©å®¹<br/>ç­–ç•¥}
    API --> DECISION
    SYNC --> DECISION
    USER --> DECISION

    QUEUE --> DECISION
    BATCH --> DECISION
    ASYNC --> DECISION
    DEV --> DECISION

    DECISION -->|Karpenter| FAST[60 ç§’ä»¥å†…<br/>èŠ‚ç‚¹æ‰©ç¼©å®¹]
    DECISION -->|KEDA| EVENT[äº‹ä»¶é©±åŠ¨<br/>Pod æ‰©ç¼©å®¹]

    style FAST fill:#ff9900
    style EVENT fill:#76c5d5

```

## ç”Ÿäº§ç¯å¢ƒæ€§èƒ½æŒ‡æ ‡

å¤„ç†æ—¥å‡ 750K+ è¯·æ±‚çš„éƒ¨ç½²å®é™…ç»“æœï¼š

```mermaid
graph TB
    subgraph "ä¼˜åŒ–å‰"
        B1[æ‰©ç¼©å®¹è§¦å‘<br/>60-90 ç§’å»¶è¿Ÿ]
        B2[èŠ‚ç‚¹ä¾›åº”<br/>3-5 åˆ†é’Ÿ]
        B3[æ€»ä½“å“åº”<br/>4-6 åˆ†é’Ÿ]
        B4[ç”¨æˆ·å½±å“<br/>è¶…æ—¶å’Œé”™è¯¯]
    end

    subgraph "Karpenter + é«˜åˆ†è¾¨ç‡ä¹‹å"
        A1[æ‰©ç¼©å®¹è§¦å‘<br/>2-5 ç§’å»¶è¿Ÿ]
        A2[èŠ‚ç‚¹ä¾›åº”<br/>45-60 ç§’]
        A3[æ€»ä½“å“åº”<br/>60 ç§’ä»¥å†…]
        A4[ç”¨æˆ·å½±å“<br/>æ— ]
    end

    subgraph "æ”¹å–„é¡¹"
        I1[æ£€æµ‹é€Ÿåº¦æå‡ 95%]
        I2[ä¾›åº”é€Ÿåº¦æå‡ 75%]
        I3[æ€»ä½“é€Ÿåº¦æå‡ 80%]
        I4[ä¿æŒ 100% å¯ç”¨æ€§]
    end

    B1 --> I1
    B2 --> I2
    B3 --> I3
    B4 --> I4

    I1 --> A1
    I2 --> A2
    I3 --> A3
    I4 --> A4

    style A3 fill:#48C9B0
    style I3 fill:#ff9900

```

## å¤šåŒºåŸŸè€ƒè™‘

å¯¹äºåœ¨å¤šä¸ªåŒºåŸŸè¿è¥çš„ç»„ç»‡ï¼Œä¸ºå®ç°ä¸€è‡´çš„é«˜é€Ÿæ‰©ç¼©å®¹ï¼Œéœ€è¦æŒ‰åŒºåŸŸè¿›è¡Œä¼˜åŒ–ï¼š

```mermaid
graph TB
    subgraph "å…¨çƒæ¶æ„"
        subgraph "ç¾å›½åŒºåŸŸï¼ˆ40% æµé‡ï¼‰"
            US_KARP[Karpenter US]
            US_TYPES[c6i, c7i ä¼˜å…ˆ]
            US_SPOT[80% Spot]
        end

        subgraph "æ¬§æ´²åŒºåŸŸï¼ˆ35% æµé‡ï¼‰"
            EU_KARP[Karpenter EU]
            EU_TYPES[c6a, c7a ä¼˜å…ˆ]
            EU_SPOT[75% Spot]
        end

        subgraph "äºšå¤ªåŒºåŸŸï¼ˆ25% æµé‡ï¼‰"
            AP_KARP[Karpenter AP]
            AP_TYPES[c5, m5 åŒ…å«]
            AP_SPOT[70% Spot]
        end
    end

    subgraph "è·¨åŒºåŸŸæŒ‡æ ‡"
        GLOBAL[å…¨çƒæŒ‡æ ‡<br/>èšåˆå™¨]
        REGIONAL[åŒºåŸŸ<br/>å†³ç­–]
    end

    US_KARP --> REGIONAL
    EU_KARP --> REGIONAL
    AP_KARP --> REGIONAL

    REGIONAL --> GLOBAL

```

## æ‰©ç¼©å®¹ä¼˜åŒ–æœ€ä½³å®è·µ

### 1. æŒ‡æ ‡é€‰æ‹©

- ä½¿ç”¨å…ˆè¡ŒæŒ‡æ ‡ï¼ˆé˜Ÿåˆ—æ·±åº¦ã€è¿æ¥æ•°ï¼‰ï¼Œè€Œéæ»åæŒ‡æ ‡ï¼ˆCPUï¼‰
- æ¯é›†ç¾¤é«˜åˆ†è¾¨ç‡æŒ‡æ ‡ä¿æŒåœ¨ 10-15 ä¸ªä»¥ä¸‹
- æ‰¹é‡æäº¤æŒ‡æ ‡ä»¥é˜²æ­¢ API é™æµ

### 2. Karpenter ä¼˜åŒ–

- æä¾›æœ€å¤§å®ä¾‹ç±»å‹çµæ´»æ€§
- ç§¯æä½¿ç”¨ Spot å®ä¾‹å¹¶é…åˆé€‚å½“çš„ä¸­æ–­å¤„ç†
- å¯ç”¨æ•´åˆä»¥æé«˜æˆæœ¬æ•ˆç‡
- è®¾ç½®é€‚å½“çš„ ttlSecondsAfterEmptyï¼ˆ30-60 ç§’ï¼‰

### 3. HPA è°ƒä¼˜

- æ‰©å®¹ä½¿ç”¨é›¶ç¨³å®šçª—å£
- æ¿€è¿›çš„æ‰©ç¼©å®¹ç­–ç•¥ï¼ˆå…è®¸ 100% å¢é•¿ï¼‰
- å…·æœ‰é€‚å½“æƒé‡çš„å¤šæŒ‡æ ‡
- ç¼©å®¹ä½¿ç”¨é€‚å½“çš„å†·å´æœŸ

### 4. ç›‘æ§

- å°† P95 æ‰©ç¼©å®¹å»¶è¿Ÿä½œä¸ºåŸºæœ¬ KPI è·Ÿè¸ª
- å¯¹è¶…è¿‡ 15 ç§’çš„æ‰©ç¼©å®¹å¤±è´¥æˆ–å»¶è¿Ÿè®¾ç½®å‘Šè­¦
- ç›‘æ§ Spot ä¸­æ–­ç‡
- è·Ÿè¸ªæ¯ä¸ªæ‰©ç¼©å®¹ Pod çš„æˆæœ¬

## å¸¸è§é—®é¢˜æ’æŸ¥

```mermaid
graph LR
    subgraph "ç—‡çŠ¶"
        SLOW[æ‰©ç¼©å®¹è¶…è¿‡ 10 ç§’]
    end

    subgraph "è¯Šæ–­"
        D1[æ£€æŸ¥æŒ‡æ ‡å»¶è¿Ÿ]
        D2[éªŒè¯ HPA é…ç½®]
        D3[å®¡æŸ¥å®ä¾‹ç±»å‹]
        D4[åˆ†æå­ç½‘å®¹é‡]
    end

    subgraph "è§£å†³æ–¹æ¡ˆ"
        S1[ç¼©çŸ­æ”¶é›†é—´éš”]
        S2[ç§»é™¤ç¨³å®šçª—å£]
        S3[æ·»åŠ æ›´å¤šå®ä¾‹ç±»å‹]
        S4[æ‰©å±•å­ç½‘ CIDR]
    end

    SLOW --> D1 --> S1
    SLOW --> D2 --> S2
    SLOW --> D3 --> S3
    SLOW --> D4 --> S4

```

## æ··åˆæ–¹æ³•ï¼ˆæ¨èï¼‰

åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ¨èä½¿ç”¨ä¸¤ç§æ–¹å¼æ··åˆçš„æ··åˆæ–¹æ³•ï¼š

1. **å…³é”®ä»»åŠ¡æœåŠ¡**ï¼šä½¿ç”¨ ADOT + Prometheus å®ç° 10-13 ç§’æ‰©ç¼©å®¹
2. **ä¸€èˆ¬æœåŠ¡**ï¼šä½¿ç”¨ CloudWatch Direct å®ç° 12-15 ç§’æ‰©ç¼©å®¹å¹¶ç®€åŒ–è¿ç»´
3. **æ¸è¿›å¼è¿ç§»**ï¼šä» CloudWatch å¼€å§‹ï¼ŒæŒ‰éœ€åˆ‡æ¢åˆ° ADOT

## EKS Auto Mode vs Self-managed Karpenter

EKS Auto Modeï¼ˆ2025 GAï¼‰å†…ç½® Karpenter å¹¶è‡ªåŠ¨ç®¡ç†ï¼š

| é¡¹ç›® | Self-managed Karpenter | EKS Auto Mode |
|------|----------------------|---------------|
| å®‰è£…/å‡çº§ | è‡ªè¡Œç®¡ç†ï¼ˆHelmï¼‰ | AWS è‡ªåŠ¨ç®¡ç† |
| NodePool è®¾ç½® | å®Œå…¨è‡ªå®šä¹‰ | æœ‰é™é…ç½® |
| æˆæœ¬ä¼˜åŒ– | ç²¾ç»†æ§åˆ¶å¯èƒ½ | è‡ªåŠ¨ä¼˜åŒ– |
| OS è¡¥ä¸ | è‡ªè¡Œç®¡ç† | è‡ªåŠ¨è¡¥ä¸ |
| é€‚åˆçš„ç¯å¢ƒ | éœ€è¦é«˜çº§è‡ªå®šä¹‰ | æœ€å°åŒ–è¿ç»´è´Ÿæ‹… |

**æ¨è**ï¼šå¦‚æœ‰å¤æ‚çš„è°ƒåº¦éœ€æ±‚é€‰æ‹© Self-managedï¼Œå¦‚ç›®æ ‡æ˜¯ç®€åŒ–è¿ç»´åˆ™é€‰æ‹© EKS Auto Modeã€‚

## P1ï¼šè¶…å¿«é€Ÿæ‰©ç¼©å®¹æ¶æ„ï¼ˆCriticalï¼‰

### æ‰©ç¼©å®¹å»¶è¿Ÿæ—¶é—´åˆ†è§£åˆ†æ

ä¸ºä¼˜åŒ–æ‰©ç¼©å®¹å“åº”æ—¶é—´ï¼Œé¦–å…ˆéœ€è¦ç²¾ç»†åˆ†è§£æ•´ä¸ªæ‰©ç¼©å®¹é“¾ä¸­äº§ç”Ÿçš„å»¶è¿Ÿæ—¶é—´ã€‚

```mermaid
graph TB
    subgraph "æ‰©ç¼©å®¹å»¶è¿Ÿæ—¶é—´åˆ†è§£ï¼ˆä¼ ç»Ÿç¯å¢ƒï¼‰"
        M[æŒ‡æ ‡æ”¶é›†<br/>15-70 ç§’]
        H[HPA å†³ç­–<br/>15 ç§’]
        N[èŠ‚ç‚¹ä¾›åº”<br/>30-120 ç§’]
        C[å®¹å™¨å¯åŠ¨<br/>5-30 ç§’]

        M -->|ç´¯è®¡| H
        H -->|ç´¯è®¡| N
        N -->|ç´¯è®¡| C

        TOTAL[æ€»å»¶è¿Ÿï¼š65-235 ç§’]
        C --> TOTAL
    end

    subgraph "å„é˜¶æ®µç“¶é¢ˆå› ç´ "
        M1[æŒ‡æ ‡æ”¶é›†å»¶è¿Ÿ<br/>- CloudWatch èšåˆï¼š60 ç§’<br/>- Prometheus æŠ“å–ï¼š15 ç§’<br/>- API è½®è¯¢ï¼š10-30 ç§’]

        H1[HPA ç“¶é¢ˆ<br/>- åŒæ­¥å‘¨æœŸï¼š15 ç§’<br/>- ç¨³å®šçª—å£ï¼š0-300 ç§’<br/>- Metrics API å»¶è¿Ÿï¼š2-5 ç§’]

        N1[ä¾›åº”å»¶è¿Ÿ<br/>- ASG æ‰©ç¼©å®¹ï¼š60-90 ç§’<br/>- EC2 å¯åŠ¨ï¼š30-60 ç§’<br/>- é›†ç¾¤åŠ å…¥ï¼š15-30 ç§’]

        C1[å®¹å™¨ç“¶é¢ˆ<br/>- é•œåƒæ‹‰å–ï¼š5-20 ç§’<br/>- åˆå§‹åŒ–ï¼š2-10 ç§’<br/>- Readiness æ¢é’ˆï¼š5-15 ç§’]
    end

    M -.-> M1
    H -.-> H1
    N -.-> N1
    C -.-> C1

    style TOTAL fill:#ff4444,stroke:#232f3e,stroke-width:3px
    style M1 fill:#ffcccc
    style H1 fill:#ffcccc
    style N1 fill:#ffcccc
    style C1 fill:#ffcccc
```

<ScalingLatencyBreakdown />

:::danger ç»“æœ
æµé‡æ¿€å¢æ—¶**ç”¨æˆ·ä½“éªŒè¶…è¿‡ 5 åˆ†é’Ÿçš„é”™è¯¯** â€” èŠ‚ç‚¹ä¾›åº”å æ€»å»¶è¿Ÿçš„ 60% ä»¥ä¸Š
:::

### å¤šå±‚æ‰©ç¼©å®¹ç­–ç•¥

è¶…å¿«é€Ÿæ‰©ç¼©å®¹ä¸æ˜¯å•ä¸€ä¼˜åŒ–ï¼Œè€Œæ˜¯é€šè¿‡**3 å±‚å›é€€ç­–ç•¥**å®ç°ã€‚

```mermaid
graph TB
    subgraph "Layer 1: Warm Poolï¼ˆE2E 5-10 ç§’ï¼‰"
        WP1[Pause Pod Overprovisioning]
        WP2[é¢„ä¾›åº”çš„èŠ‚ç‚¹]
        WP3[é€šè¿‡ Preemption å³æ—¶è°ƒåº¦]
        WP4[å®¹é‡ï¼šé¢„æœŸå³°å€¼çš„ 10-20%]

        WP1 --> WP2 --> WP3 --> WP4

        WP_RESULT[E2Eï¼š5-10 ç§’ â€»å«æŒ‡æ ‡æ£€æµ‹+Pod å¯åŠ¨<br/>Pod è°ƒåº¦ï¼š0-2 ç§’<br/>æˆæœ¬ï¼šé«˜ Â· å¯é æ€§ï¼š99.9%]
        WP4 --> WP_RESULT
    end

    subgraph "Layer 2: Fast Provisioningï¼ˆE2E 42-65 ç§’ï¼‰"
        FP1[Karpenter ç›´æ¥ä¾›åº”]
        FP2[Spot Fleet å¤šå®ä¾‹ç±»å‹]
        FP3[Provisioned EKS Control Plane]
        FP4[å®¹é‡ï¼šæ— é™æ‰©å±•]

        FP1 --> FP2 --> FP3 --> FP4

        FP_RESULT[E2Eï¼š42-65 ç§’ â€»æ–°èŠ‚ç‚¹ä¾›åº”<br/>èŠ‚ç‚¹ä¾›åº”ï¼š30-45 ç§’<br/>æˆæœ¬ï¼šä¸­ç­‰ Â· å¯é æ€§ï¼š99%]
        FP4 --> FP_RESULT
    end

    subgraph "Layer 3: On-Demand Fallbackï¼ˆE2E 60-90 ç§’ï¼‰"
        OD1[On-Demand å®ä¾‹ä¿è¯]
        OD2[åˆ©ç”¨å®¹é‡é¢„ç•™]
        OD3[æœ€ç»ˆå®‰å…¨ç½‘]
        OD4[å®¹é‡ï¼šæœ‰ä¿è¯]

        OD1 --> OD2 --> OD3 --> OD4

        OD_RESULT[E2Eï¼š60-90 ç§’ â€»Spot ä¸å¯ç”¨æ—¶<br/>On-Demand ä¾›åº”ï¼š45-60 ç§’<br/>æˆæœ¬ï¼šæœ€é«˜ Â· å¯é æ€§ï¼š100%]
        OD4 --> OD_RESULT
    end

    TRAFFIC[æµé‡æ¿€å¢] --> DECISION{æ‰€éœ€å®¹é‡}
    DECISION -->|å³°å€¼ 20% ä»¥å†…| WP_RESULT
    DECISION -->|å³°å€¼ 20-200%| FP_RESULT
    DECISION -->|æç«¯çªå‘| OD_RESULT

    WP_RESULT -->|å®¹é‡ä¸è¶³| FP_RESULT
    FP_RESULT -->|Spot ä¸å¯ç”¨| OD_RESULT

    style WP_RESULT fill:#48C9B0,stroke:#232f3e,stroke-width:2px
    style FP_RESULT fill:#3498DB,stroke:#232f3e,stroke-width:2px
    style OD_RESULT fill:#F39C12,stroke:#232f3e,stroke-width:2px
```

### å„å±‚æ‰©ç¼©å®¹æ—¶é—´çº¿å¯¹æ¯”

```mermaid
timeline
    title å¤šå±‚æ‰©ç¼©å®¹æ—¶é—´çº¿ï¼ˆå®é™…æµ‹é‡å€¼ï¼‰

    section Layer 1 - Warm Pool
        T+0s : æ£€æµ‹åˆ°æµé‡æ¿€å¢
        T+0.5s : Pause Pod Preemption å¼€å§‹
        T+1s : å®é™… Pod è°ƒåº¦å®Œæˆ
        T+2s : å¼€å§‹æä¾›æœåŠ¡

    section Layer 2 - Fast Provisioning
        T+0s : æ£€æµ‹åˆ°ä¸å¯è°ƒåº¦çš„ Pod
        T+0.2s : Karpenter é€‰æ‹©æœ€ä¼˜å®ä¾‹
        T+2s : EC2 Fleet API è°ƒç”¨
        T+8s : å®ä¾‹å¯åŠ¨å®Œæˆ
        T+12s : åŠ å…¥é›†ç¾¤å¹¶è°ƒåº¦ Pod
        T+15s : å¼€å§‹æä¾›æœåŠ¡

    section Layer 3 - On-Demand Fallback
        T+0s : æ£€æµ‹åˆ° Spot å®¹é‡ä¸è¶³
        T+1s : On-Demand å®ä¾‹è¯·æ±‚
        T+10s : å®¹é‡é¢„ç•™æ¿€æ´»
        T+20s : å®ä¾‹å¯åŠ¨å®Œæˆ
        T+28s : åŠ å…¥é›†ç¾¤
        T+30s : å¼€å§‹æä¾›æœåŠ¡
```

:::tip å±‚é€‰æ‹©æ ‡å‡†
**Layer 1ï¼ˆWarm Poolï¼‰** â€” é¢„åˆ†é…ç­–ç•¥ï¼š
- **æœ¬è´¨**ï¼šä¸æ˜¯è‡ªåŠ¨æ‰©ç¼©å®¹è€Œæ˜¯**è¿‡åº¦ä¾›åº”**ã€‚é€šè¿‡ Pause Pod é¢„å…ˆç¡®ä¿èŠ‚ç‚¹
- E2E 5-10 ç§’ï¼ˆæŒ‡æ ‡æ£€æµ‹ + Preemption + å®¹å™¨å¯åŠ¨ï¼‰
- **æˆæœ¬**ï¼š24 å°æ—¶ç»´æŠ¤é¢„æœŸå³°å€¼å®¹é‡çš„ 10-20%ï¼ˆæ¯æœˆ $720-$5,400ï¼‰
- **è€ƒé‡**ï¼šç”¨ç›¸åŒæˆæœ¬å¢åŠ åŸºç¡€ replica å¯èƒ½æ›´ç®€å•

**Layer 2ï¼ˆFast Provisioningï¼‰** â€” å¤§å¤šæ•°åœºæ™¯çš„é»˜è®¤ç­–ç•¥ï¼š
- Karpenter + Spot å®ä¾‹è¿›è¡Œå®é™…èŠ‚ç‚¹ä¾›åº”
- E2E 42-65 ç§’ï¼ˆæŒ‡æ ‡æ£€æµ‹ + EC2 å¯åŠ¨ + å®¹å™¨å¯åŠ¨ï¼‰
- **æˆæœ¬**ï¼šä¸å®é™…ä½¿ç”¨é‡æˆæ¯”ä¾‹ï¼ˆSpot 70-80% æŠ˜æ‰£ï¼‰
- **è€ƒé‡**ï¼šä¸æ¶æ„å¼¹æ€§ï¼ˆåŸºäºé˜Ÿåˆ—ï¼‰ç»„åˆä½¿ç”¨æ—¶ï¼Œæ­¤æ—¶é—´ä¸ä¼šæš´éœ²ç»™ç”¨æˆ·

**Layer 3ï¼ˆOn-Demand Fallbackï¼‰** â€” å¿…è¦ä¿é™©ï¼š
- Spot å®¹é‡ä¸è¶³æ—¶çš„æœ€ç»ˆå®‰å…¨ç½‘
- E2E 60-90 ç§’ï¼ˆOn-Demand ä¾›åº”å¯èƒ½æ¯” Spot æ…¢ï¼‰
- **æˆæœ¬**ï¼šOn-Demand ä»·æ ¼ï¼ˆæœ€å°‘ä½¿ç”¨ï¼‰
:::

## P2: é€šè¿‡ Provisioned EKS Control Plane æ¶ˆé™¤ API ç“¶é¢ˆ

### Provisioned Control Plane æ¦‚è¿°

2025å¹´11æœˆï¼ŒAWS å‘å¸ƒäº† **EKS Provisioned Control Plane**ã€‚é€šè¿‡æ¶ˆé™¤ä¼ ç»Ÿ Standard Control Plane çš„ API é™æµé™åˆ¶ï¼Œåœ¨å¤§è§„æ¨¡çªå‘åœºæ™¯ä¸­å¤§å¹…æå‡äº†æ‰©ç¼©å®¹é€Ÿåº¦ã€‚

```mermaid
graph LR
    subgraph "Standard Control Plane é™åˆ¶"
        STD_API[API Server<br/>å…±äº«å®¹é‡]
        STD_THROTTLE[é™æµ<br/>- ListPods: 20 TPS<br/>- CreatePod: 10 TPS<br/>- UpdateNode: 5 TPS]
        STD_DELAY[æ‰©ç¼©å®¹å»¶è¿Ÿ<br/>100 Pod åˆ›å»º: 10-30ç§’]

        STD_API --> STD_THROTTLE --> STD_DELAY
    end

    subgraph "Provisioned Control Plane æ€§èƒ½"
        PROV_SIZE{é€‰æ‹©è§„æ ¼}
        PROV_XL[XL: 10x å®¹é‡<br/>200 TPS]
        PROV_2XL[2XL: 20x å®¹é‡<br/>400 TPS]
        PROV_4XL[4XL: 40x å®¹é‡<br/>800 TPS]
        PROV_RESULT[æ‰©ç¼©å®¹é€Ÿåº¦<br/>100 Pod åˆ›å»º: 2-5ç§’]

        PROV_SIZE --> PROV_XL
        PROV_SIZE --> PROV_2XL
        PROV_SIZE --> PROV_4XL

        PROV_XL --> PROV_RESULT
        PROV_2XL --> PROV_RESULT
        PROV_4XL --> PROV_RESULT
    end

    style STD_DELAY fill:#ff4444,stroke:#232f3e,stroke-width:2px
    style PROV_RESULT fill:#48C9B0,stroke:#232f3e,stroke-width:2px
```

### Standard vs Provisioned å¯¹æ¯”

<ControlPlaneComparison />

:::warning Provisioned Control Plane é€‰æ‹©æ ‡å‡†
**éœ€è¦å‡çº§åˆ° Provisioned çš„ä¿¡å·ï¼š**

1. **API é™æµé”™è¯¯é¢‘ç¹**: `kubectl` å‘½ä»¤ç»å¸¸å¤±è´¥æˆ–é‡è¯•
2. **å¤§è§„æ¨¡éƒ¨ç½²å»¶è¿Ÿ**: 100+ Pod éƒ¨ç½²è€—æ—¶è¶…è¿‡5åˆ†é’Ÿ
3. **Karpenter èŠ‚ç‚¹é…ç½®å¤±è´¥**: `too many requests` é”™è¯¯
4. **HPA æ‰©ç¼©å®¹å»¶è¿Ÿ**: Pod åˆ›å»ºè¯·æ±‚åœ¨é˜Ÿåˆ—ä¸­å †ç§¯
5. **é›†ç¾¤è§„æ¨¡**: å¸¸æ€1,000 Pod ä»¥ä¸Šæˆ–å³°å€¼3,000 Pod ä»¥ä¸Š

**æˆæœ¬ vs æ€§èƒ½æƒè¡¡ï¼š**
- **Standard â†’ XL**: æœˆå¢ $350 æˆæœ¬è·å¾— **10å€ API æ€§èƒ½**ï¼ˆROIï¼šé˜²æ­¢10åˆ†é’Ÿåœæœºå³å¯æŠµæ¶ˆï¼‰
- **XL â†’ 2XL**: ä»…è¶…å¤§è§„æ¨¡é›†ç¾¤(10,000+ Pod)éœ€è¦
- **4XL**: æé™è§„æ¨¡(50,000+ Pod)æˆ–å¤šç§Ÿæˆ·å¹³å°ä½¿ç”¨
:::

### Provisioned Control Plane é…ç½®

#### ä½¿ç”¨ AWS CLI åˆ›å»ºæ–°é›†ç¾¤

```bash
aws eks create-cluster \
  --name ultra-fast-cluster \
  --region us-east-1 \
  --role-arn arn:aws:iam::123456789012:role/EKSClusterRole \
  --resources-vpc-config subnetIds=subnet-xxx,subnet-yyy,securityGroupIds=sg-xxx \
  --kubernetes-version 1.32 \
  --compute-config enabled=true,nodePools=system,nodeRoleArn=arn:aws:iam::123456789012:role/EKSNodeRole \
  --kubernetes-network-config elasticLoadBalancing=disabled \
  --access-config authenticationMode=API \
  --upgrade-policy supportType=EXTENDED \
  --zonal-shift-config enabled=true \
  --compute-config enabled=true \
  --control-plane-placement groupName=my-placement-group,clusterTenancy=dedicated \
  --control-plane-provisioning mode=PROVISIONED,size=XL
```

#### å‡çº§ç°æœ‰é›†ç¾¤ï¼ˆStandard â†’ Provisionedï¼‰

```bash
# 1. æ£€æŸ¥å½“å‰ Control Plane æ¨¡å¼
aws eks describe-cluster --name my-cluster --query 'cluster.controlPlaneProvisioning'

# 2. å‡çº§åˆ° Provisionedï¼ˆæ— åœæœºï¼‰
aws eks update-cluster-config \
  --name my-cluster \
  --control-plane-provisioning mode=PROVISIONED,size=XL

# 3. ç›‘æ§å‡çº§çŠ¶æ€ï¼ˆçº¦éœ€10-15åˆ†é’Ÿï¼‰
aws eks describe-cluster \
  --name my-cluster \
  --query 'cluster.status'

# 4. éªŒè¯ API æ€§èƒ½
kubectl get pods --all-namespaces --watch
kubectl create deployment nginx --image=nginx --replicas=100
```

:::info å‡çº§ç‰¹æ€§
- **æ— åœæœº**: Control Plane è‡ªåŠ¨æ»šåŠ¨å‡çº§
- **æ‰€éœ€æ—¶é—´**: 10-15åˆ†é’Ÿï¼ˆä¸é›†ç¾¤è§„æ¨¡æ— å…³ï¼‰
- **ä¸å¯å›æ»š**: ä¸æ”¯æŒ Provisioned â†’ Standard é™çº§
- **è®¡è´¹å¼€å§‹**: å‡çº§å®Œæˆåç«‹å³å¼€å§‹è®¡è´¹
:::

### å¤§è§„æ¨¡çªå‘æ—¶çš„æ€§èƒ½å¯¹æ¯”

åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­è¿›è¡Œ1,000 Pod åŒæ—¶æ‰©ç¼©å®¹æµ‹è¯•ï¼š

```mermaid
graph TB
    subgraph "Standard Control Planeï¼ˆå—é™ï¼‰"
        STD1[T+0s: å¼€å§‹æ‰©ç¼©å®¹<br/>è¯·æ±‚åˆ›å»º1,000 Pod]
        STD2[T+10s: API é™æµå¼€å§‹<br/>å®Œæˆåˆ›å»º100 Pod]
        STD3[T+30s: é™æµåŠ å‰§<br/>å®Œæˆåˆ›å»º300 Pod]
        STD4[T+90s: æŒç»­é™æµ<br/>å®Œæˆåˆ›å»º700 Pod]
        STD5[T+180s: æœ€ç»ˆå®Œæˆ<br/>å®Œæˆåˆ›å»º1,000 Pod]

        STD1 --> STD2 --> STD3 --> STD4 --> STD5
    end

    subgraph "Provisioned XL Control Planeï¼ˆåŠ é€Ÿï¼‰"
        PROV1[T+0s: å¼€å§‹æ‰©ç¼©å®¹<br/>è¯·æ±‚åˆ›å»º1,000 Pod]
        PROV2[T+10s: é«˜é€Ÿåˆ›å»º<br/>å®Œæˆåˆ›å»º600 Pod]
        PROV3[T+15s: æ¥è¿‘å®Œæˆ<br/>å®Œæˆåˆ›å»º950 Pod]
        PROV4[T+18s: æœ€ç»ˆå®Œæˆ<br/>å®Œæˆåˆ›å»º1,000 Pod]

        PROV1 --> PROV2 --> PROV3 --> PROV4
    end

    subgraph "æ€§èƒ½æå‡"
        IMPROVE[æ‰©ç¼©å®¹é€Ÿåº¦æå‡90%<br/>180ç§’ â†’ 18ç§’<br/>API é™æµé”™è¯¯: 0æ¬¡]
    end

    STD5 -.-> IMPROVE
    PROV4 -.-> IMPROVE

    style STD5 fill:#ff4444,stroke:#232f3e,stroke-width:2px
    style PROV4 fill:#48C9B0,stroke:#232f3e,stroke-width:2px
    style IMPROVE fill:#3498DB,stroke:#232f3e,stroke-width:3px
```

## P3: Warm Pool / Overprovisioning æ¨¡å¼ï¼ˆæ ¸å¿ƒç­–ç•¥ï¼‰

### Pause Pod Overprovisioning åŸç†

Warm Pool ç­–ç•¥é€šè¿‡**é¢„å…ˆéƒ¨ç½²ä½ä¼˜å…ˆçº§çš„"pause" Pod**æ¥æå‰é…ç½®èŠ‚ç‚¹ã€‚å½“å®é™…å·¥ä½œè´Ÿè½½éœ€è¦æ—¶ï¼Œç«‹å³é©±é€(preempt) pause Podï¼Œå¹¶åœ¨è¯¥èŠ‚ç‚¹ä¸Šè°ƒåº¦å®é™… Podã€‚

```mermaid
sequenceDiagram
    participant HPA as HPA Controller
    participant Scheduler as K8s Scheduler
    participant PausePod as Pause Pod<br/>(Priority: -1)
    participant Node as é¢„é…ç½®èŠ‚ç‚¹
    participant RealPod as å®é™…å·¥ä½œè´Ÿè½½ Pod<br/>(Priority: 0)

    Note over Node,PausePod: åˆå§‹çŠ¶æ€: Pause Pod å ç”¨èŠ‚ç‚¹
    PausePod->>Node: Runningï¼ˆæ­£åœ¨é¢„ç•™èµ„æºï¼‰

    Note over HPA: æ£€æµ‹åˆ°æµé‡æ¿€å¢
    HPA->>RealPod: è¯·æ±‚åˆ›å»ºæ–° Pod

    RealPod->>Scheduler: è°ƒåº¦è¯·æ±‚
    Scheduler->>Scheduler: ä¼˜å…ˆçº§è¯„ä¼°<br/>Real (0) > Pause (-1)

    Scheduler->>PausePod: Preempt ä¿¡å·
    PausePod->>Node: ç«‹å³ç»ˆæ­¢ (0.5ç§’)

    Scheduler->>RealPod: è°ƒåº¦åˆ° Node
    RealPod->>Node: ç«‹å³å¯åŠ¨ (1-2ç§’)

    Note over RealPod,Node: æ€»è€—æ—¶: 1.5-2.5ç§’
```
### Overprovisioning å®Œæ•´å·¥ä½œæµç¨‹

```mermaid
graph TB
    subgraph "ç¬¬1é˜¶æ®µ: Warm Pool é¢„è®¾ç½®ï¼ˆé«˜å³°æœŸä¹‹å‰ï¼‰"
        CRON[CronJob è§¦å‘<br/>ä¾‹: ä¸Šåˆ8ç‚¹30åˆ†]
        PAUSE_DEPLOY[åˆ›å»º Pause Deployment<br/>Replicas: é¢„è®¡å³°å€¼çš„15%]
        PAUSE_POD[éƒ¨ç½² Pause Pod<br/>CPU: 1000m, Memory: 2Gi]
        KARP_PROVISION[Karpenter èŠ‚ç‚¹é…ç½®<br/>é€‰æ‹© Spot å®ä¾‹]
        WARM[Warm Pool å°±ç»ª<br/>å¯ç«‹å³ä½¿ç”¨çš„å®¹é‡]

        CRON --> PAUSE_DEPLOY --> PAUSE_POD --> KARP_PROVISION --> WARM
    end

    subgraph "ç¬¬2é˜¶æ®µ: æµé‡æ¿€å¢å“åº”ï¼ˆå®æ—¶ï¼‰"
        TRAFFIC[æµé‡æ¿€å¢å‘ç”Ÿ]
        HPA_SCALE[HPA æ‰©å®¹å†³ç­–<br/>Replicas: 100 â†’ 150]
        REAL_POD[å®é™… Pod åˆ›å»ºè¯·æ±‚<br/>Priority: 0]
        PREEMPT[Pause Pod Preemption<br/>åŸºäºä¼˜å…ˆçº§é©±é€]
        INSTANT[å³æ—¶è°ƒåº¦<br/>è€—æ—¶1-2ç§’]

        TRAFFIC --> HPA_SCALE --> REAL_POD --> PREEMPT --> INSTANT
    end

    subgraph "ç¬¬3é˜¶æ®µ: é¢å¤–æ‰©å±•ï¼ˆå®¹é‡è¶…å‡ºæ—¶ï¼‰"
        OVERFLOW{Warm Pool<br/>è€—å°½?}
        MORE_NODES[Karpenter è¿½åŠ èŠ‚ç‚¹<br/>Layer 2 ç­–ç•¥å¯åŠ¨]

        INSTANT --> OVERFLOW
        OVERFLOW -->|Yes| MORE_NODES
        OVERFLOW -->|No| INSTANT
    end

    subgraph "ç¬¬4é˜¶æ®µ: ç¼©å®¹ä¸è¡¥å……ï¼ˆé«˜å³°ç»“æŸåï¼‰"
        SCALE_DOWN[HPA ç¼©å®¹<br/>Replicas: 150 â†’ 100]
        REFILL[é‡æ–°éƒ¨ç½² Pause Pod<br/>è¡¥å…… Warm Pool]
        CLEANUP[æ¸…ç†ç©ºé—²èŠ‚ç‚¹<br/>ttlSecondsAfterEmpty: 60s]

        SCALE_DOWN --> REFILL --> CLEANUP
    end

    WARM --> TRAFFIC
    MORE_NODES --> SCALE_DOWN

    style INSTANT fill:#48C9B0,stroke:#232f3e,stroke-width:3px
    style WARM fill:#3498DB,stroke:#232f3e,stroke-width:2px
```

### Pause Pod Overprovisioning YAML é…ç½®

#### 1. PriorityClass å®šä¹‰ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: overprovisioning
value: -1  # è´Ÿæ•°ä¼˜å…ˆçº§: ä½äºæ‰€æœ‰å®é™…å·¥ä½œè´Ÿè½½
globalDefault: false
description: "Pause pods for warm pool - will be preempted by real workloads"
```

#### 2. Pause Deploymentï¼ˆåŸºç¡€ Warm Poolï¼‰

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: overprovisioning-pause
  namespace: kube-system
spec:
  replicas: 10  # å¯¹åº”é¢„è®¡å³°å€¼15%çš„ Pod æ•°é‡
  selector:
    matchLabels:
      app: overprovisioning-pause
  template:
    metadata:
      labels:
        app: overprovisioning-pause
    spec:
      priorityClassName: overprovisioning
      terminationGracePeriodSeconds: 0  # ç«‹å³ç»ˆæ­¢

      # è°ƒåº¦çº¦æŸï¼ˆä¸å®é™…å·¥ä½œè´Ÿè½½ä½¿ç”¨åŒä¸€èŠ‚ç‚¹æ± ï¼‰
      nodeSelector:
        karpenter.sh/nodepool: fast-scaling

      containers:
      - name: pause
        image: registry.k8s.io/pause:3.9
        resources:
          requests:
            cpu: "1000m"      # å®é™…å·¥ä½œè´Ÿè½½å¹³å‡ CPU
            memory: "2Gi"     # å®é™…å·¥ä½œè´Ÿè½½å¹³å‡å†…å­˜
          limits:
            cpu: "1000m"
            memory: "2Gi"
```

#### 3. æŒ‰æ—¶æ®µè‡ªåŠ¨è°ƒæ•´ Warm Poolï¼ˆCronJobï¼‰

```yaml
---
# é«˜å³°æœŸå‰æ‰©å±• Warm Poolï¼ˆä¸Šåˆ8:30ï¼‰
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-up-warm-pool
  namespace: kube-system
spec:
  schedule: "30 8 * * 1-5"  # å·¥ä½œæ—¥ä¸Šåˆ8:30
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: warm-pool-scaler
          restartPolicy: OnFailure
          containers:
          - name: kubectl
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - |
              kubectl scale deployment overprovisioning-pause \
                --namespace kube-system \
                --replicas=30  # é«˜å³°æ—¶æ®µæ‰©å±•
---
# é«˜å³°æœŸåç¼©å° Warm Poolï¼ˆä¸‹åˆ7:00ï¼‰
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-down-warm-pool
  namespace: kube-system
spec:
  schedule: "0 19 * * 1-5"  # å·¥ä½œæ—¥ä¸‹åˆ7:00
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: warm-pool-scaler
          restartPolicy: OnFailure
          containers:
          - name: kubectl
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - |
              kubectl scale deployment overprovisioning-pause \
                --namespace kube-system \
                --replicas=5  # å¤œé—´æœ€ä½å®¹é‡
---
# CronJob ç”¨ ServiceAccount å’Œ RBAC
apiVersion: v1
kind: ServiceAccount
metadata:
  name: warm-pool-scaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: warm-pool-scaler
  namespace: kube-system
rules:
- apiGroups: ["apps"]
  resources: ["deployments", "deployments/scale"]
  verbs: ["get", "patch", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: warm-pool-scaler
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: warm-pool-scaler
subjects:
- kind: ServiceAccount
  name: warm-pool-scaler
  namespace: kube-system
```
### Warm Pool å®¹é‡è®¡ç®—æ–¹æ³•

```mermaid
graph TB
    subgraph "ç¬¬1æ­¥: æµé‡æ¨¡å¼åˆ†æ"
        BASELINE[åŸºå‡†å®¹é‡<br/>å¹³æ—¶ Replicas: 100]
        PEAK[å³°å€¼å®¹é‡<br/>æœ€å¤§ Replicas: 200]
        BURST[çªå‘é€Ÿç‡<br/>æ¯ç§’å¢åŠ 10 Pod]

        ANALYSIS[åˆ†æç»“æœ<br/>å³°å€¼å·®å€¼: 100 Pod<br/>10ç§’å†…éœ€è¦: 100 Pod]
    end

    subgraph "ç¬¬2æ­¥: ç¡®å®š Warm Pool å®¹é‡"
        FORMULA[Warm Pool å®¹é‡ = <br/>å³°å€¼å·®å€¼ Ã— å®‰å…¨ç³»æ•°]
        SAFETY[å®‰å…¨ç³»æ•°é€‰æ‹©<br/>- ä¿å®ˆå‹: 0.20 (20%)<br/>- å‡è¡¡å‹: 0.15 (15%)<br/>- æ¿€è¿›å‹: 0.10 (10%)]

        CALC[è®¡ç®—ç¤ºä¾‹<br/>100 Pod Ã— 0.15 = 15 Pod]
    end

    subgraph "ç¬¬3æ­¥: æˆæœ¬ vs é€Ÿåº¦æƒè¡¡"
        COST[Warm Pool æˆæœ¬<br/>15 Pod Ã— $0.05/hr = $0.75/hr<br/>æœˆåº¦: $540]

        BENEFIT[å»¶è¿Ÿé™ä½<br/>60ç§’ â†’ 2ç§’ï¼ˆ97%æ”¹å–„ï¼‰<br/>SLA è¿è§„é˜²æ­¢: $10,000/æœˆ]

        ROI[ROI åˆ†æ<br/>æŠ•å…¥: $540/æœˆ<br/>èŠ‚çœ: $10,000/æœˆ<br/>å‡€æ”¶ç›Š: $9,460/æœˆ]
    end

    BASELINE --> ANALYSIS
    PEAK --> ANALYSIS
    BURST --> ANALYSIS

    ANALYSIS --> FORMULA --> SAFETY --> CALC
    CALC --> COST --> BENEFIT --> ROI

    style ROI fill:#48C9B0,stroke:#232f3e,stroke-width:3px
```

### æˆæœ¬åˆ†æä¸ä¼˜åŒ–

<WarmPoolCostAnalysis />

:::tip Warm Pool ä¼˜åŒ–ç­–ç•¥
**æˆæœ¬èŠ‚çœæ–¹æ³•ï¼š**

1. **æŒ‰æ—¶æ®µæ‰©ç¼©å®¹**: ä½¿ç”¨ CronJob åœ¨å¤œé—´/å‘¨æœ«ç¼©å° Warm Poolï¼ˆèŠ‚çœ50-70%æˆæœ¬ï¼‰
2. **ä½¿ç”¨ Spot å®ä¾‹**: Pause Pod ä¹Ÿéƒ¨ç½²åœ¨ Spot èŠ‚ç‚¹ä¸Šï¼ˆ70%æŠ˜æ‰£ï¼‰
3. **è‡ªé€‚åº”å®¹é‡è°ƒæ•´**: åŸºäº CloudWatch Metrics è‡ªåŠ¨æ‰©ç¼©å®¹
4. **æ··åˆç­–ç•¥**: ä»…åœ¨é«˜å³°æ—¶æ®µä½¿ç”¨ Warm Poolï¼Œå…¶ä»–æ—¶æ®µä¾èµ– Layer 2

**ROI è®¡ç®—å…¬å¼ï¼š**
```
ROI = (SLA è¿è§„é˜²æ­¢æˆæœ¬ + è¥æ”¶æœºä¼šæŸå¤±é˜²æ­¢) - Warm Pool æˆæœ¬

ç¤ºä¾‹:
- SLA è¿è§„ç½šé‡‘: $5,000/æ¬¡
- æœˆå‡è¿è§„æ¬¡æ•°ï¼ˆæ—  Warm Pool æ—¶ï¼‰: 3æ¬¡
- Warm Pool æˆæœ¬: $1,080/æœˆ
- ROI = ($5,000 Ã— 3) - $1,080 = $13,920/æœˆ (1,290% ROI)
```
:::

## P4: Setu - Kueue + Karpenter ä¸»åŠ¨é…ç½®

### Setu æ¦‚è¿°

**Setu** è¿æ¥ Kueueï¼ˆé˜Ÿåˆ—ç³»ç»Ÿï¼‰å’Œ Karpenterï¼Œ**ä¸ºéœ€è¦ Gang Scheduling çš„ AI/ML å·¥ä½œè´Ÿè½½æä¾›é¢„å…ˆèŠ‚ç‚¹é…ç½®**ã€‚ä¼ ç»Ÿ Karpenter åœ¨ Pod åˆ›å»ºåè¢«åŠ¨é…ç½®èŠ‚ç‚¹ï¼Œè€Œ Setu åœ¨ Job è¿›å…¥é˜Ÿåˆ—çš„ç¬é—´å°±é¢„å…ˆé…ç½®æ‰€éœ€èŠ‚ç‚¹ã€‚

```mermaid
graph TB
    subgraph "ä¼ ç»Ÿ Karpenter æ–¹å¼ï¼ˆè¢«åŠ¨ï¼‰"
        OLD1[æäº¤ Job]
        OLD2[Kueue é˜Ÿåˆ—ç­‰å¾…]
        OLD3[è·å–èµ„æºé…é¢]
        OLD4[åˆ›å»º Pod]
        OLD5[Karpenter å“åº”<br/>å¼€å§‹èŠ‚ç‚¹é…ç½®]
        OLD6[èŠ‚ç‚¹å°±ç»ª (60-90ç§’)]
        OLD7[Pod è°ƒåº¦]
        OLD8[Job å¼€å§‹æ‰§è¡Œ]

        OLD1 --> OLD2 --> OLD3 --> OLD4 --> OLD5 --> OLD6 --> OLD7 --> OLD8

        OLD_TIME[æ€»è€—æ—¶: 90-120ç§’]
        OLD8 --> OLD_TIME
    end

    subgraph "Setu æ–¹å¼ï¼ˆä¸»åŠ¨ï¼‰"
        NEW1[æäº¤ Job]
        NEW2[è¿›å…¥ Kueue é˜Ÿåˆ—]
        NEW3[è§¦å‘ Setu AdmissionCheck]
        NEW4[é¢„å…ˆåˆ›å»º Karpenter NodeClaim]
        NEW5[èŠ‚ç‚¹é…ç½® (60-90ç§’)]
        NEW6[è·å–èµ„æºé…é¢]
        NEW7[åˆ›å»º Pod å¹¶ç«‹å³è°ƒåº¦]
        NEW8[Job å¼€å§‹æ‰§è¡Œ]

        NEW1 --> NEW2 --> NEW3 --> NEW4
        NEW4 --> NEW5
        NEW5 --> NEW6
        NEW3 --> NEW6
        NEW6 --> NEW7 --> NEW8

        NEW_TIME[æ€»è€—æ—¶: 15-30ç§’<br/>èŠ‚ç‚¹é…ç½®ä¸é˜Ÿåˆ—ç­‰å¾…å¹¶è¡ŒåŒ–]
        NEW8 --> NEW_TIME
    end

    style OLD_TIME fill:#ff4444,stroke:#232f3e,stroke-width:2px
    style NEW_TIME fill:#48C9B0,stroke:#232f3e,stroke-width:3px
```

### Setu æ¶æ„ä¸å·¥ä½œåŸç†

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Job as Kubernetes Job
    participant Kueue as Kueue Controller
    participant Setu as Setu Controller
    participant Karp as Karpenter
    participant Node as EC2 Node
    participant Pod as Pod

    User->>Job: æäº¤ Jobï¼ˆè¯·æ±‚8 GPUï¼‰
    Job->>Kueue: è¿›å…¥é˜Ÿåˆ—

    Note over Kueue: ClusterQueue ä¸­å­˜åœ¨ AdmissionCheck
    Kueue->>Setu: è§¦å‘ AdmissionCheck

    Setu->>Setu: åˆ†æ Job éœ€æ±‚<br/>- GPU: 8ä¸ª<br/>- å†…å­˜: 128Gi<br/>- é¢„è®¡èŠ‚ç‚¹: p4d.24xlarge

    Setu->>Karp: åˆ›å»º NodeClaim<br/>ï¼ˆç›´æ¥è°ƒç”¨ Karpenter APIï¼‰

    Note over Karp,Node: å¼€å§‹èŠ‚ç‚¹é…ç½®ï¼ˆå¼‚æ­¥ï¼‰
    Karp->>Node: å¯åŠ¨ p4d.24xlarge å®ä¾‹

    par å¹¶è¡Œå¤„ç†
        Node->>Node: åŠ å…¥é›†ç¾¤ (60-90ç§’)
    and
        Kueue->>Kueue: è·å–èµ„æºé…é¢
        Kueue->>Job: æ‰¹å‡† Job Admission
        Job->>Pod: åˆ›å»º Pod
    end

    Node->>Karp: è½¬ä¸º Ready çŠ¶æ€
    Setu->>Kueue: AdmissionCheck å®Œæˆ

    Pod->>Node: ç«‹å³è°ƒåº¦ï¼ˆèŠ‚ç‚¹å·²å°±ç»ªï¼‰
    Pod->>Pod: Job å¼€å§‹æ‰§è¡Œ

    Note over User,Pod: æ€»è€—æ—¶: ä»…ç­‰åŒäºèŠ‚ç‚¹é…ç½®æ—¶é—´<br/>ï¼ˆé˜Ÿåˆ—ç­‰å¾…ä¸é…ç½®å¹¶è¡ŒåŒ–ï¼‰
```
### Setu å®‰è£…ä¸é…ç½®

#### 1. Setu å®‰è£…ï¼ˆHelmï¼‰

```bash
# æ·»åŠ  Setu Helm ä»“åº“
helm repo add setu https://sanjeevrg89.github.io/Setu
helm repo update

# å®‰è£… Setuï¼ˆéœ€è¦ Kueue å’Œ Karpenterï¼‰
helm install setu setu/setu \
  --namespace kueue-system \
  --create-namespace \
  --set karpenter.enabled=true \
  --set karpenter.namespace=karpenter
```

#### 2. å¸¦æœ‰ AdmissionCheck çš„ ClusterQueue

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: gpu-cluster-queue
spec:
  namespaceSelector: {}

  # èµ„æºé…é¢ï¼ˆæ•´ä¸ªé›†ç¾¤é™åˆ¶ï¼‰
  resourceGroups:
  - coveredResources: ["cpu", "memory", "nvidia.com/gpu"]
    flavors:
    - name: gpu-flavor
      resources:
      - name: "cpu"
        nominalQuota: 1000
      - name: "memory"
        nominalQuota: 4000Gi
      - name: "nvidia.com/gpu"
        nominalQuota: 64

  # å¯ç”¨ Setu AdmissionCheck
  admissionChecks:
  - setu-provisioning  # Setu é¢„å…ˆé…ç½®èŠ‚ç‚¹
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: AdmissionCheck
metadata:
  name: setu-provisioning
spec:
  controllerName: setu.kueue.x-k8s.io/provisioning

  # Setu å‚æ•°
  parameters:
    apiGroup: setu.kueue.x-k8s.io/v1alpha1
    kind: ProvisioningParameters
    name: gpu-provisioning
---
apiVersion: setu.kueue.x-k8s.io/v1alpha1
kind: ProvisioningParameters
metadata:
  name: gpu-provisioning
spec:
  # Karpenter NodePool å¼•ç”¨
  nodePoolName: gpu-nodepool

  # é…ç½®ç­–ç•¥
  strategy:
    type: Proactive  # é¢„å…ˆé…ç½®
    bufferTime: 15s  # Job Admission å‰ç­‰å¾…æ—¶é—´

  # èŠ‚ç‚¹éœ€æ±‚æ˜ å°„
  nodeSelectorRequirements:
  - key: node.kubernetes.io/instance-type
    operator: In
    values:
    - p4d.24xlarge
    - p4de.24xlarge
  - key: karpenter.sh/capacity-type
    operator: In
    values:
    - on-demand  # GPU é¿å… Spot ä¸­æ–­é£é™©
```

#### 3. GPU NodePoolï¼ˆKarpenterï¼‰

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-nodepool
spec:
  template:
    spec:
      requirements:
      - key: node.kubernetes.io/instance-type
        operator: In
        values:
        - p4d.24xlarge   # 8Ã— A100 (40GB)
        - p4de.24xlarge  # 8Ã— A100 (80GB)
        - p5.48xlarge    # 8Ã— H100

      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand  # GPU å·¥ä½œè´Ÿè½½é¿å…ä¸­æ–­é£é™©

      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodeclass

  # GPU èŠ‚ç‚¹é•¿æ—¶é—´ä¿æŒï¼ˆè€ƒè™‘è®­ç»ƒæ—¶é—´ï¼‰
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 300s  # ç©ºé—²5åˆ†é’Ÿåç§»é™¤
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu-nodeclass
spec:
  amiSelectorTerms:
  - alias: al2023@latest  # åŒ…å« GPU é©±åŠ¨

  subnetSelectorTerms:
  - tags:
      karpenter.sh/discovery: "${CLUSTER_NAME}"

  securityGroupSelectorTerms:
  - tags:
      karpenter.sh/discovery: "${CLUSTER_NAME}"

  role: "KarpenterNodeRole-${CLUSTER_NAME}"

  # GPU ä¼˜åŒ– UserData
  userData: |
    #!/bin/bash
    # EKS ä¼˜åŒ– GPU AMI è®¾ç½®
    /etc/eks/bootstrap.sh ${CLUSTER_NAME} \
      --b64-cluster-ca ${B64_CLUSTER_CA} \
      --apiserver-endpoint ${API_SERVER_URL} \
      --kubelet-extra-args '--node-labels=nvidia.com/gpu=true --max-pods=110'

    # NVIDIA é©±åŠ¨éªŒè¯
    nvidia-smi || echo "GPU driver not loaded"
```

#### 4. AI/ML Job æäº¤ç¤ºä¾‹

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: llm-training
  labels:
    kueue.x-k8s.io/queue-name: gpu-queue  # æŒ‡å®š LocalQueue
spec:
  parallelism: 8  # Gang Schedulingï¼ˆ8 Pod åŒæ—¶æ‰§è¡Œï¼‰
  completions: 8

  template:
    spec:
      restartPolicy: OnFailure

      # ç”¨äº Gang Scheduling çš„ PodGroup
      schedulerName: default-scheduler

      containers:
      - name: training
        image: nvcr.io/nvidia/pytorch:24.01-py3

        command:
        - python3
        - /workspace/train.py
        - --distributed
        - --nodes=8

        resources:
          requests:
            nvidia.com/gpu: 1  # æ¯ Pod 1 GPU
            cpu: "48"
            memory: "320Gi"
          limits:
            nvidia.com/gpu: 1
            cpu: "48"
            memory: "320Gi"
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: gpu-queue
  namespace: default
spec:
  clusterQueue: gpu-cluster-queue  # å¼•ç”¨ ClusterQueue
```
### Setu æ€§èƒ½æ”¹å–„æµ‹é‡

```mermaid
graph TB
    subgraph "æ—  Setuï¼ˆä¼ ç»Ÿ Karpenterï¼‰"
        NO1[æäº¤ Job]
        NO2[Kueue ç­‰å¾…: 30ç§’<br/>è·å–èµ„æºé…é¢]
        NO3[åˆ›å»º Pod]
        NO4[Karpenter å“åº”: 5ç§’]
        NO5[èŠ‚ç‚¹é…ç½®: 90ç§’<br/>p4d.24xlarge]
        NO6[Pod è°ƒåº¦: 10ç§’]
        NO7[Job å¼€å§‹æ‰§è¡Œ]

        NO1 --> NO2 --> NO3 --> NO4 --> NO5 --> NO6 --> NO7

        NO_TOTAL[æ€»è€—æ—¶: 135ç§’]
        NO7 --> NO_TOTAL
    end

    subgraph "ä½¿ç”¨ Setuï¼ˆä¸»åŠ¨ï¼‰"
        YES1[æäº¤ Job]
        YES2[Kueue + Setu åŒæ—¶è§¦å‘]

        YES3A[Kueue: èµ„æºéªŒè¯ 30ç§’]
        YES3B[Setu: ç«‹å³åˆ›å»º NodeClaim]

        YES4[èŠ‚ç‚¹é…ç½®: 90ç§’<br/>å¹¶è¡Œè¿›è¡Œ]
        YES5[åˆ›å»º Pod å¹¶ç«‹å³è°ƒåº¦: 5ç§’]
        YES6[Job å¼€å§‹æ‰§è¡Œ]

        YES1 --> YES2
        YES2 --> YES3A
        YES2 --> YES3B

        YES3A --> YES5
        YES3B --> YES4
        YES4 --> YES5
        YES5 --> YES6

        YES_TOTAL[æ€»è€—æ—¶: 95ç§’<br/>æ”¹å–„40ç§’ï¼ˆç¼©çŸ­30%ï¼‰]
        YES6 --> YES_TOTAL
    end

    style NO_TOTAL fill:#ff4444,stroke:#232f3e,stroke-width:2px
    style YES_TOTAL fill:#48C9B0,stroke:#232f3e,stroke-width:3px
```

:::info Setu GitHub åŠæ›´å¤šä¿¡æ¯
**GitHub**: https://github.com/sanjeevrg89/Setu

**ä¸»è¦ç‰¹ç‚¹ï¼š**
- åˆ©ç”¨ Kueue AdmissionCheck API
- ç›´æ¥åˆ›å»º Karpenter NodeClaim
- ä¼˜åŒ– Gang Scheduling å·¥ä½œè´Ÿè½½ï¼ˆæ‰€æœ‰ Pod éœ€è¦åŒæ—¶æ‰§è¡Œçš„åœºæ™¯ï¼‰
- é€šè¿‡é¢„å…ˆé…ç½® GPU èŠ‚ç‚¹æ¶ˆé™¤ç­‰å¾…æ—¶é—´

**é€‚ç”¨åœºæ™¯ï¼š**
- åˆ†å¸ƒå¼ AI/ML è®­ç»ƒï¼ˆPyTorch DDPã€Horovodï¼‰
- åŸºäº MPI çš„ HPC å·¥ä½œè´Ÿè½½
- å¤§è§„æ¨¡æ‰¹é‡ä»¿çœŸ
- å¤šèŠ‚ç‚¹æ•°æ®å¤„ç† Job
:::

## P5: é€šè¿‡ Node Readiness Controller æ¶ˆé™¤å¯åŠ¨å»¶è¿Ÿ

### Node Readiness é—®é¢˜

å³ä½¿ Karpenter å¿«é€Ÿé…ç½®äº†èŠ‚ç‚¹ï¼Œåœ¨å®é™… Pod è°ƒåº¦ä¹‹å‰ï¼Œ**CNI/CSI/GPU é©±åŠ¨åˆå§‹åŒ–å»¶è¿Ÿ**ä»ç„¶ä¼šå‘ç”Ÿã€‚ä¼ ç»Ÿä¸Šï¼Œkubelet ä¼šç­‰å¾…æ‰€æœ‰ DaemonSet è¿è¡Œåæ‰å°†èŠ‚ç‚¹è½¬ä¸º Ready çŠ¶æ€ã€‚

```mermaid
graph TB
    subgraph "ä¼ ç»ŸèŠ‚ç‚¹ Ready æµç¨‹ï¼ˆ60-90ç§’ï¼‰"
        OLD1[EC2 å®ä¾‹å¯åŠ¨: 30ç§’]
        OLD2[kubelet å¯åŠ¨: 5ç§’]
        OLD3[CNI DaemonSet è¿è¡Œ: 15ç§’<br/>VPC CNI åˆå§‹åŒ–]
        OLD4[CSI DaemonSet è¿è¡Œ: 10ç§’<br/>EBS CSI é©±åŠ¨]
        OLD5[GPU DaemonSet è¿è¡Œ: 20ç§’<br/>NVIDIA device plugin]
        OLD6[èŠ‚ç‚¹ Ready çŠ¶æ€: 5ç§’]
        OLD7[å¯è°ƒåº¦ Pod]

        OLD1 --> OLD2 --> OLD3 --> OLD4 --> OLD5 --> OLD6 --> OLD7

        OLD_TOTAL[æ€»å»¶è¿Ÿ: 85ç§’]
        OLD7 --> OLD_TOTAL
    end

    subgraph "Node Readiness Controllerï¼ˆ30-40ç§’ï¼‰"
        NEW1[EC2 å®ä¾‹å¯åŠ¨: 30ç§’]
        NEW2[kubelet å¯åŠ¨: 5ç§’]
        NEW3[ä»…ç­‰å¾…æ ¸å¿ƒ CNI: 5ç§’<br/>ä»… VPC CNI åŸºæœ¬åˆå§‹åŒ–]
        NEW4[èŠ‚ç‚¹ Ready çŠ¶æ€: ç«‹å³]
        NEW5[å¯è°ƒåº¦ Pod]
        NEW6[å…¶ä½™ DaemonSet å¹¶è¡Œè¿è¡Œ<br/>CSIã€GPUï¼ˆåå°ï¼‰]

        NEW1 --> NEW2 --> NEW3 --> NEW4 --> NEW5
        NEW3 --> NEW6

        NEW_TOTAL[æ€»å»¶è¿Ÿ: 40ç§’<br/>ç¼©çŸ­50%]
        NEW5 --> NEW_TOTAL
    end

    style OLD_TOTAL fill:#ff4444,stroke:#232f3e,stroke-width:2px
    style NEW_TOTAL fill:#48C9B0,stroke:#232f3e,stroke-width:3px
```

### Node Readiness Controller åŸç†

**Node Readiness Controller (NRC)** ç²¾ç»†æ§åˆ¶èŠ‚ç‚¹è½¬ä¸º Ready çŠ¶æ€çš„æ¡ä»¶ã€‚é»˜è®¤æƒ…å†µä¸‹ kubelet ä¼šç­‰å¾…æ‰€æœ‰ DaemonSet è¿è¡Œï¼Œä½† NRC å¯ä»¥é…ç½®ä¸º**ä»…é€‰æ‹©æ€§ç­‰å¾…å¿…è¦ç»„ä»¶**ã€‚

```mermaid
sequenceDiagram
    participant EC2 as EC2 å®ä¾‹
    participant Kubelet as kubelet
    participant NRC as Node Readiness Controller
    participant CNI as VPC CNI DaemonSet
    participant CSI as EBS CSI DaemonSet
    participant Scheduler as kube-scheduler
    participant Pod as ç”¨æˆ· Pod

    EC2->>Kubelet: å®ä¾‹å¯åŠ¨å®Œæˆ
    Kubelet->>NRC: æ£€æŸ¥ NodeReadinessRule

    Note over NRC: bootstrap-only æ¨¡å¼<br/>ä»…æ£€æŸ¥å¿…è¦ç»„ä»¶

    NRC->>CNI: ç­‰å¾…åˆå§‹åŒ– (5ç§’)
    CNI->>NRC: åŸºæœ¬ç½‘ç»œå°±ç»ª

    NRC->>Kubelet: Ready æ¡ä»¶æ»¡è¶³
    Kubelet->>Scheduler: èŠ‚ç‚¹è½¬ä¸º Ready çŠ¶æ€

    par å¹¶è¡Œè¿›è¡Œ
        Scheduler->>Pod: ç«‹å³å¼€å§‹ Pod è°ƒåº¦
    and
        CSI->>CSI: åå°åˆå§‹åŒ– (10ç§’)
    end

    Pod->>EC2: å¼€å§‹æ‰§è¡Œï¼ˆä»…éœ€ CNIï¼‰

    Note over EC2,Pod: æ€»å»¶è¿Ÿ: 40ç§’<br/>ï¼ˆæ¶ˆé™¤ CSI ç­‰å¾…ï¼‰
```
### Node Readiness Controller å®‰è£…

#### 1. NRC å®‰è£…ï¼ˆHelmï¼‰

```bash
# éœ€è¦ Node Feature Discovery (NFD)ï¼ˆNRC ä¾èµ–é¡¹ï¼‰
helm repo add nfd https://kubernetes-sigs.github.io/node-feature-discovery/charts
helm install nfd nfd/node-feature-discovery \
  --namespace kube-system

# å®‰è£… Node Readiness Controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/node-readiness-controller/main/deploy/manifests.yaml
```

#### 2. NodeReadinessRule CRD å®šä¹‰

```yaml
apiVersion: nodereadiness.k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: bootstrap-only
spec:
  # bootstrap-only æ¨¡å¼: ä»…ç­‰å¾…å¿…è¦ç»„ä»¶
  mode: bootstrap-only

  # å¿…éœ€ DaemonSetï¼ˆä»…ç­‰å¾…è¿™äº›ï¼‰
  requiredDaemonSets:
  - namespace: kube-system
    name: aws-node  # VPC CNI
    selector:
      matchLabels:
        k8s-app: aws-node

  # å¯é€‰ DaemonSetï¼ˆåå°åˆå§‹åŒ–ï¼‰
  optionalDaemonSets:
  - namespace: kube-system
    name: ebs-csi-node  # EBS CSI ä»…éœ€è¦å—å­˜å‚¨çš„ Pod ä½¿ç”¨
    selector:
      matchLabels:
        app: ebs-csi-node

  - namespace: kube-system
    name: nvidia-device-plugin  # ä»… GPU Pod éœ€è¦
    selector:
      matchLabels:
        name: nvidia-device-plugin-ds

  # Node Selectorï¼ˆåº”ç”¨æ­¤è§„åˆ™çš„èŠ‚ç‚¹ï¼‰
  nodeSelector:
    matchLabels:
      karpenter.sh/nodepool: fast-scaling

  # Readiness è¶…æ—¶ï¼ˆæœ€å¤§ç­‰å¾…æ—¶é—´ï¼‰
  readinessTimeout: 60s
```

### Karpenter + NRC é›†æˆé…ç½®

#### 1. å¸¦æœ‰ NRC Annotation çš„ Karpenter NodePool

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: fast-scaling-nrc
spec:
  template:
    metadata:
      # å¯ç”¨ NRC Annotation
      annotations:
        nodereadiness.k8s.io/rule: bootstrap-only

    spec:
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot", "on-demand"]

      - key: node.kubernetes.io/instance-type
        operator: In
        values:
        - c6i.xlarge
        - c6i.2xlarge
        - c6i.4xlarge

      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: fast-nodepool-nrc

  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: fast-nodepool-nrc
spec:
  amiSelectorTerms:
  - alias: al2023@latest

  subnetSelectorTerms:
  - tags:
      karpenter.sh/discovery: "${CLUSTER_NAME}"

  securityGroupSelectorTerms:
  - tags:
      karpenter.sh/discovery: "${CLUSTER_NAME}"

  role: "KarpenterNodeRole-${CLUSTER_NAME}"

  # NRC ä¼˜åŒ–çš„ UserData
  userData: |
    #!/bin/bash
    # EKS å¼•å¯¼ï¼ˆæœ€å°é€‰é¡¹ï¼‰
    /etc/eks/bootstrap.sh ${CLUSTER_NAME} \
      --b64-cluster-ca ${B64_CLUSTER_CA} \
      --apiserver-endpoint ${API_SERVER_URL} \
      --kubelet-extra-args '--node-labels=karpenter.sh/fast-scaling=true,nodereadiness.k8s.io/enabled=true --max-pods=110'

    # VPC CNI å¿«é€Ÿåˆå§‹åŒ–ï¼ˆå¿…éœ€ï¼‰
    systemctl enable --now aws-node || true
```

#### 2. VPC CNI Readiness Ruleï¼ˆè¯¦ç»†è®¾ç½®ï¼‰

```yaml
apiVersion: nodereadiness.k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: vpc-cni-only
spec:
  mode: bootstrap-only

  # ä»…ç­‰å¾… VPC CNI
  requiredDaemonSets:
  - namespace: kube-system
    name: aws-node
    selector:
      matchLabels:
        k8s-app: aws-node

    # CNI å°±ç»ªçŠ¶æ€æ£€æŸ¥æ¡ä»¶
    readinessProbe:
      exec:
        command:
        - sh
        - -c
        - |
          # ç¡®è®¤ aws-node Pod çš„ aws-vpc-cni-init å®¹å™¨å®Œæˆ
          kubectl wait --for=condition=Initialized \
            pod -l k8s-app=aws-node \
            -n kube-system \
            --timeout=30s

      initialDelaySeconds: 5
      periodSeconds: 2
      timeoutSeconds: 30
      successThreshold: 1
      failureThreshold: 3

  # æ‰€æœ‰å…¶ä»– DaemonSet ä¸ºå¯é€‰
  optionalDaemonSets:
  - namespace: kube-system
    name: "*"  # é€šé…ç¬¦: æ‰€æœ‰å…¶ä»– DaemonSet

  nodeSelector:
    matchLabels:
      karpenter.sh/nodepool: fast-scaling-nrc

  readinessTimeout: 60s
```

### NRC æ€§èƒ½å¯¹æ¯”

åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­è¿›è¡Œ100èŠ‚ç‚¹æ‰©ç¼©å®¹æµ‹è¯•ï¼š

```mermaid
graph TB
    subgraph "æ—  NRCï¼ˆç­‰å¾…æ‰€æœ‰ DaemonSetï¼‰"
        NO1[èŠ‚ç‚¹é…ç½®: 30ç§’]
        NO2[CNI åˆå§‹åŒ–: 15ç§’]
        NO3[CSI åˆå§‹åŒ–: 10ç§’]
        NO4[Monitoring åˆå§‹åŒ–: 10ç§’]
        NO5[GPU Plugin åˆå§‹åŒ–: 20ç§’]
        NO6[èŠ‚ç‚¹ Ready: 5ç§’]
        NO7[å¯è°ƒåº¦ Pod]

        NO1 --> NO2 --> NO3 --> NO4 --> NO5 --> NO6 --> NO7

        NO_TOTAL[æ€»å»¶è¿Ÿ: 90ç§’<br/>P95: 120ç§’]
        NO7 --> NO_TOTAL
    end

    subgraph "ä½¿ç”¨ NRCï¼ˆä»…ç­‰å¾… CNIï¼‰"
        YES1[èŠ‚ç‚¹é…ç½®: 30ç§’]
        YES2[CNI åˆå§‹åŒ–: 15ç§’]
        YES3[èŠ‚ç‚¹ Ready: ç«‹å³]
        YES4[å¯è°ƒåº¦ Pod]
        YES5[å…¶ä½™ DaemonSet åå°è¿è¡Œ<br/>CSIã€Monitoringã€GPU]

        YES1 --> YES2 --> YES3 --> YES4
        YES2 --> YES5

        YES_TOTAL[æ€»å»¶è¿Ÿ: 45ç§’<br/>P95: 55ç§’<br/>æ”¹å–„50%]
        YES4 --> YES_TOTAL
    end

    subgraph "æµ‹é‡æŒ‡æ ‡ï¼ˆ100èŠ‚ç‚¹æ‰©ç¼©å®¹ï¼‰"
        METRIC1[èŠ‚ç‚¹é…ç½®å¼€å§‹ â†’ Ready<br/>æ—  NRC: å¹³å‡90ç§’, P95 120ç§’<br/>ä½¿ç”¨ NRC: å¹³å‡45ç§’, P95 55ç§’]

        METRIC2[åˆ°é¦–ä¸ª Pod è°ƒåº¦<br/>æ—  NRC: å¹³å‡95ç§’<br/>ä½¿ç”¨ NRC: å¹³å‡48ç§’]

        METRIC3[å…¨éƒ¨100èŠ‚ç‚¹ Ready<br/>æ—  NRC: 180ç§’<br/>ä½¿ç”¨ NRC: 90ç§’]
    end

    NO_TOTAL -.-> METRIC1
    YES_TOTAL -.-> METRIC1

    style NO_TOTAL fill:#ff4444,stroke:#232f3e,stroke-width:2px
    style YES_TOTAL fill:#48C9B0,stroke:#232f3e,stroke-width:3px
    style METRIC3 fill:#3498DB,stroke:#232f3e,stroke-width:2px
```
:::warning NRC ä½¿ç”¨æ³¨æ„äº‹é¡¹
**ä¼˜ç‚¹ï¼š**
- èŠ‚ç‚¹ Ready æ—¶é—´ç¼©çŸ­50%
- Pod è°ƒåº¦å»¶è¿Ÿæœ€å°åŒ–
- å¤§è§„æ¨¡æ‰©ç¼©å®¹æ—¶ API è´Ÿè½½é™ä½

**ç¼ºç‚¹ä¸é£é™©ï¼š**
- **éœ€è¦ CSI çš„ Pod å¯èƒ½å¤±è´¥**: æŒ‚è½½ EBS å·çš„ Pod å¦‚æœåœ¨ CSI é©±åŠ¨å°±ç»ªå‰è¢«è°ƒåº¦ï¼Œä¼šå‡ºç° CrashLoopBackOff
- **GPU Pod åˆå§‹åŒ–å»¶è¿Ÿ**: NVIDIA device plugin åå°åˆå§‹åŒ–æœŸé—´ GPU Pod å¤„äº Pending çŠ¶æ€
- **ç›‘æ§ç›²åŒº**: Prometheus node-exporter ç­‰å»¶è¿Ÿå¯åŠ¨ä¼šå¯¼è‡´åˆå§‹æŒ‡æ ‡ç¼ºå¤±

**è§£å†³æ–¹æ¡ˆï¼š**
1. **ä½¿ç”¨ PodSchedulingGate**: ä¸ºéœ€è¦ CSI/GPU çš„ Pod è®¾ç½®æ‰‹åŠ¨é—¨æ§
2. **NodeAffinity æ¡ä»¶**: ç­‰å¾… `nodereadiness.k8s.io/csi-ready=true` æ ‡ç­¾
3. **InitContainer éªŒè¯**: Pod å¯åŠ¨å‰ç¡®è®¤æ‰€éœ€é©±åŠ¨å­˜åœ¨

```yaml
# éœ€è¦ CSI çš„ Pod ç¤ºä¾‹ï¼ˆå®‰å…¨ç­‰å¾…ï¼‰
apiVersion: v1
kind: Pod
metadata:
  name: app-with-ebs
spec:
  initContainers:
  - name: wait-for-csi
    image: busybox
    command:
    - sh
    - -c
    - |
      until [ -f /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock ]; do
        echo "Waiting for EBS CSI driver..."
        sleep 2
      done

  containers:
  - name: app
    image: my-app
    volumeMounts:
    - name: data
      mountPath: /data

  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: ebs-pvc
```
:::

## æ€»ç»“

åœ¨ EKS ä¸­å®ç°é«˜æ•ˆçš„è‡ªåŠ¨æ‰©ç¼©å®¹ä¼˜åŒ–ä¸æ˜¯å¯é€‰é¡¹ï¼Œè€Œæ˜¯å¿…éœ€çš„ã€‚Karpenter çš„æ™ºèƒ½é…ç½®ã€å…³é”®æŒ‡æ ‡çš„é«˜åˆ†è¾¨ç‡ç›‘æ§ã€ä»¥åŠé€‚å½“è°ƒä¼˜çš„ HPA é…ç½®çš„ç»„åˆï¼Œèƒ½å¤Ÿå®ç°é€‚åˆå·¥ä½œè´Ÿè½½ç‰¹å¾çš„æœ€ä¼˜æ‰©ç¼©å®¹ç­–ç•¥ã€‚

**æ ¸å¿ƒè¦ç‚¹ï¼š**

- **Karpenter æ˜¯åŸºç¡€**: é€šè¿‡ç›´æ¥ EC2 é…ç½®ç¼©çŸ­æ•°åˆ†é’Ÿçš„æ‰©ç¼©å®¹æ—¶é—´
- **é€‰æ‹©æ€§é«˜åˆ†è¾¨ç‡æŒ‡æ ‡**: ä»¥1-5ç§’é—´éš”ç›‘æ§å…³é”®æŒ‡æ ‡
- **æ¿€è¿›çš„ HPA é…ç½®**: æ¶ˆé™¤æ‰©ç¼©å®¹å†³ç­–çš„äººä¸ºå»¶è¿Ÿ
- **é€šè¿‡æ™ºèƒ½ä¼˜åŒ–æˆæœ¬**: å¿«é€Ÿæ‰©ç¼©å®¹å‡å°‘è¿‡åº¦é…ç½®
- **æ¶æ„é€‰æ‹©**: æ ¹æ®è§„æ¨¡å’Œéœ€æ±‚é€‰æ‹© CloudWatch æˆ– Prometheus

**P1 è¶…é«˜é€Ÿæ‰©ç¼©å®¹ç­–ç•¥æ€»ç»“ï¼š**

1. **å¤šå±‚å›é€€ç­–ç•¥**: Warm Pool (0-2ç§’) â†’ Fast Provisioning (5-15ç§’) â†’ On-Demand Fallback (15-30ç§’) è¦†ç›–æ‰€æœ‰åœºæ™¯
2. **Provisioned Control Plane**: æ¶ˆé™¤ API é™æµï¼Œå¤§è§„æ¨¡çªå‘æ—¶ Pod åˆ›å»ºé€Ÿåº¦æå‡10å€ï¼ˆæœˆ $350 é˜²æ­¢10åˆ†é’Ÿåœæœºï¼‰
3. **Pause Pod Overprovisioning**: æŒ‰æ—¶æ®µè‡ªåŠ¨è°ƒæ•´å®ç°0-2ç§’æ‰©ç¼©å®¹ï¼ŒROI 1,290%ï¼ˆé˜²æ­¢ SLA è¿è§„ï¼‰
4. **Setu (Kueue-Karpenter)**: AI/ML Gang Scheduling å·¥ä½œè´Ÿè½½ä¸­é€šè¿‡å¹¶è¡ŒåŒ–èŠ‚ç‚¹é…ç½®å’Œé˜Ÿåˆ—ç­‰å¾…ï¼Œå»¶è¿Ÿç¼©çŸ­30%
5. **Node Readiness Controller**: ä»…ç­‰å¾… CNI ä½¿èŠ‚ç‚¹ Ready æ—¶é—´ç¼©çŸ­50%ï¼ˆ85ç§’ â†’ 45ç§’ï¼‰

è¿™é‡Œå±•ç¤ºçš„æ¶æ„å·²åœ¨æ¯æ—¥å¤„ç†æ•°ç™¾ä¸‡è¯·æ±‚çš„ç”Ÿäº§ç¯å¢ƒä¸­å¾—åˆ°éªŒè¯ã€‚é€šè¿‡å®æ–½è¿™äº›æ¨¡å¼ï¼Œå¯ä»¥ç¡®ä¿ EKS é›†ç¾¤ä»¥ä¸šåŠ¡éœ€æ±‚çš„é€Ÿåº¦è¿›è¡Œæ‰©ç¼©å®¹â€”â€”ä»¥ç§’ä¸ºå•ä½è€Œéåˆ†é’Ÿã€‚

<PracticalGuide />

### ç»¼åˆå»ºè®®

ä»¥ä¸Šæ¨¡å¼è™½ç„¶å¼ºå¤§ï¼Œä½†å¤§å¤šæ•°å·¥ä½œè´Ÿè½½å¹¶ä¸éœ€è¦å…¨éƒ¨é‡‡ç”¨ã€‚åœ¨å®é™…åº”ç”¨æ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹é¡ºåºè¯„ä¼°ï¼š

1. **é¦–å…ˆ**: ä¼˜åŒ–åŸºæœ¬ Karpenter è®¾ç½®ï¼ˆNodePool å¤šç§å®ä¾‹ç±»å‹ã€Spot åˆ©ç”¨ï¼‰â€”â€”ä»…æ­¤ä¸€é¡¹å³å¯å®ç°180ç§’ â†’ 45-65ç§’
2. **å…¶æ¬¡**: HPA è°ƒä¼˜ï¼ˆç¼©çŸ­ stabilizationWindowã€å¼•å…¥ KEDAï¼‰â€”â€”æŒ‡æ ‡æ£€æµ‹ä»60ç§’ â†’ 2-5ç§’
3. **ç„¶å**: æ¶æ„å¼¹æ€§è®¾è®¡ï¼ˆåŸºäºé˜Ÿåˆ—ã€Circuit Breakerï¼‰â€”â€”ä½¿æ‰©ç¼©å®¹å»¶è¿Ÿå¯¹ç”¨æˆ·ä¸å¯è§
4. **ä»…åœ¨éœ€è¦æ—¶**: Warm Poolã€Provisioned CPã€Setuã€NRCâ€”â€”å½“æœ‰å…³é”®ä»»åŠ¡ SLA è¦æ±‚æ—¶

:::caution åŠ¡å¿…è®¡ç®—æˆæœ¬æ•ˆç›Š
Warm Poolï¼ˆæœˆ $1,080ï¼‰+ Provisioned CPï¼ˆæœˆ $350ï¼‰= æœˆ $1,430 çš„é¢å¤–æˆæœ¬ã€‚æŒ‰28ä¸ªé›†ç¾¤è®¡ç®—ä¸ºæœˆ $40,000ã€‚ç”¨åŒæ ·çš„æˆæœ¬å¢åŠ 30%åŸºç¡€ replicaï¼Œæ— éœ€å¤æ‚åŸºç¡€è®¾æ–½å³å¯è·å¾—ç±»ä¼¼æ•ˆæœã€‚åŠ¡å¿…è‡ªé—®**"è¿™ç§å¤æ‚åº¦æ˜¯å¦èƒ½ä¸ºä¸šåŠ¡ä»·å€¼æä¾›å……åˆ†ç†ç”±ï¼Ÿ"**
:::

---

## EKS Auto Mode å®Œå…¨æŒ‡å—

:::info EKS Auto Modeï¼ˆ2024å¹´12æœˆ GAï¼‰
EKS Auto Mode ä»¥å®Œå…¨æ‰˜ç®¡æ–¹å¼æä¾› Karpenterï¼ŒåŒ…å«è‡ªåŠ¨åŸºç¡€è®¾æ–½ç®¡ç†ã€OS è¡¥ä¸å’Œå®‰å…¨æ›´æ–°ã€‚åœ¨æœ€å°åŒ–è¿ç»´å¤æ‚åº¦çš„åŒæ—¶æ”¯æŒè¶…é«˜é€Ÿæ‰©ç¼©å®¹ã€‚
:::

### Managed Karpenter: è‡ªåŠ¨åŸºç¡€è®¾æ–½ç®¡ç†

EKS Auto Mode è‡ªåŠ¨åŒ–ä»¥ä¸‹å†…å®¹ï¼š

- **Karpenter æ§åˆ¶å™¨å‡çº§**: AWS ç¡®ä¿å…¼å®¹æ€§å¹¶è‡ªåŠ¨æ›´æ–°
- **å®‰å…¨è¡¥ä¸**: AL2023 AMI è‡ªåŠ¨è¡¥ä¸å’ŒèŠ‚ç‚¹æ»šåŠ¨æ›¿æ¢
- **NodePool é»˜è®¤é…ç½®**: systemã€general-purpose æ± å·²é¢„é…ç½®
- **IAM è§’è‰²**: KarpenterNodeRoleã€KarpenterControllerRole è‡ªåŠ¨åˆ›å»º

### Auto Mode vs Self-managed è¯¦ç»†å¯¹æ¯”

<AutoModeComparison />
### Auto Mode ä¸­çš„è¶…é«˜é€Ÿæ‰©ç¼©å®¹æ–¹æ³•

Auto Mode ä½¿ç”¨ä¸ Self-managed ç›¸åŒçš„ Karpenter å¼•æ“ï¼Œå› æ­¤æ‰©ç¼©å®¹é€Ÿåº¦ç›¸åŒã€‚ä½†å¯ä»¥è¿›è¡Œä»¥ä¸‹ä¼˜åŒ–ï¼š

1. **åˆ©ç”¨å†…ç½® NodePool**: `system`ã€`general-purpose` æ± å·²ç»è¿‡ä¼˜åŒ–
2. **æ‰©å±•å®ä¾‹ç±»å‹**: åœ¨é»˜è®¤æ± ä¸­æ·»åŠ æ›´å¤šå®ä¾‹ç±»å‹
3. **è°ƒä¼˜ Consolidation ç­–ç•¥**: å¯ç”¨ `WhenEmptyOrUnderutilized`
4. **è°ƒæ•´ Disruption Budget**: åœ¨æµé‡é«˜å³°æ—¶æœ€å°åŒ–èŠ‚ç‚¹æ›¿æ¢

### å†…ç½® NodePool é…ç½®

EKS Auto Mode æä¾›ä¸¤ç§é»˜è®¤ NodePoolï¼š

```yaml
# system æ± ï¼ˆkube-systemã€monitoring ç­‰ï¼‰
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: system
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["t3.medium", "t3.large"]
      taints:
        - key: CriticalAddonsOnly
          value: "true"
          effect: NoSchedule
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 300s
---
# general-purpose æ± ï¼ˆåº”ç”¨å·¥ä½œè´Ÿè½½ï¼‰
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general-purpose
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - c6i.xlarge
            - c6i.2xlarge
            - c6i.4xlarge
            - m6i.xlarge
            - m6i.2xlarge
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
    budgets:
    - nodes: "10%"
```

### Self-managed â†’ Auto Mode è¿ç§»æŒ‡å—

:::warning è¿ç§»æ³¨æ„äº‹é¡¹
ä¸ºç¡®ä¿è¿ç§»æœŸé—´å·¥ä½œè´Ÿè½½å¯ç”¨æ€§ï¼Œå»ºè®®é‡‡ç”¨è“/ç»¿åˆ‡æ¢æ–¹å¼ã€‚
:::

**åˆ†æ­¥è¿ç§»ï¼š**

```bash
# ç¬¬1æ­¥: åˆ›å»ºæ–°çš„ Auto Mode é›†ç¾¤
aws eks create-cluster \
  --name my-cluster-auto \
  --version 1.33 \
  --compute-config enabled=true \
  --role-arn arn:aws:iam::ACCOUNT:role/EKSClusterRole \
  --resources-vpc-config subnetIds=subnet-xxx,subnet-yyy

# ç¬¬2æ­¥: å¤‡ä»½ç°æœ‰å·¥ä½œè´Ÿè½½
kubectl get all --all-namespaces -o yaml > workloads-backup.yaml

# ç¬¬3æ­¥: åˆ›å»ºè‡ªå®šä¹‰ NodePoolï¼ˆå¯é€‰ï¼‰
kubectl apply -f custom-nodepool.yaml

# ç¬¬4æ­¥: æ¸è¿›å¼å·¥ä½œè´Ÿè½½è¿ç§»
# - ä½¿ç”¨ DNS åŠ æƒè·¯ç”±é€æ­¥åˆ‡æ¢æµé‡
# - ä»ç°æœ‰é›†ç¾¤ â†’ Auto Mode é›†ç¾¤

# ç¬¬5æ­¥: éªŒè¯åç§»é™¤æ—§é›†ç¾¤
kubectl drain --ignore-daemonsets --delete-emptydir-data <node-name>
```

### Auto Mode é›†ç¾¤åˆ›å»º YAML

```yaml
# ä½¿ç”¨ eksctl
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: auto-mode-cluster
  region: us-east-1
  version: "1.33"

# å¯ç”¨ Auto Mode
computeConfig:
  enabled: true
  nodePoolDefaults:
    instanceTypes:
      - c6i.xlarge
      - c6i.2xlarge
      - c6i.4xlarge
      - c7i.xlarge
      - c7i.2xlarge
      - m6i.xlarge
      - m6i.2xlarge

# VPC è®¾ç½®
vpc:
  id: vpc-xxx
  subnets:
    private:
      us-east-1a: { id: subnet-xxx }
      us-east-1b: { id: subnet-yyy }
      us-east-1c: { id: subnet-zzz }

# IAM è®¾ç½®ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
iam:
  withOIDC: true
```

### Auto Mode NodePool è‡ªå®šä¹‰

```yaml
# é«˜æ€§èƒ½å·¥ä½œè´Ÿè½½è‡ªå®šä¹‰ NodePool
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: high-performance
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - c7i.4xlarge
            - c7i.8xlarge
            - c7i.16xlarge
        - key: topology.kubernetes.io/zone
          operator: In
          values: ["us-east-1a", "us-east-1b"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: high-perf-class

  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 600s  # ç­‰å¾…10åˆ†é’Ÿ
    budgets:
    - nodes: "0"  # é«˜å³°æ—¶åœæ­¢æ›¿æ¢
      schedule: "0 8-18 * * MON-FRI"  # å·¥ä½œæ—¶é—´
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: high-perf-class
spec:
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: auto-mode-cluster
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: auto-mode-cluster
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 100Gi
        volumeType: gp3
        iops: 10000
        throughput: 500
```
---

## Karpenter v1.x æœ€æ–°åŠŸèƒ½

### Consolidation ç­–ç•¥: é€Ÿåº¦ vs æˆæœ¬

ä» Karpenter v1.0 å¼€å§‹ï¼Œ`consolidationPolicy` å­—æ®µå·²ç§»è‡³ `disruption` éƒ¨åˆ†ã€‚

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: optimized-pool
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s

    # æ•´åˆæ’é™¤æ¡ä»¶
    expireAfter: 720h  # 30å¤©åè‡ªåŠ¨æ›¿æ¢èŠ‚ç‚¹
```

**ç­–ç•¥å¯¹æ¯”ï¼š**

| ç­–ç•¥ | è¡Œä¸º | é€Ÿåº¦ | æˆæœ¬ä¼˜åŒ– | é€‚ç”¨ç¯å¢ƒ |
|------|------|------|---------|---------|
| `WhenEmpty` | ä»…ç§»é™¤ç©ºèŠ‚ç‚¹ | â­â­â­â­â­ å¿«é€Ÿ | â­â­ æœ‰é™ | ç¨³å®šæµé‡ |
| `WhenEmptyOrUnderutilized` | ç©ºèŠ‚ç‚¹ + ä½åˆ©ç”¨ç‡èŠ‚ç‚¹æ•´åˆ | â­â­â­ ä¸­ç­‰ | â­â­â­â­â­ ä¼˜ç§€ | æ³¢åŠ¨æµé‡ |

**æ‰©ç¼©å®¹é€Ÿåº¦å½±å“åˆ†æï¼š**

```mermaid
graph LR
    subgraph "WhenEmptyï¼ˆå¿«é€Ÿæ‰©ç¼©å®¹ï¼‰"
        E1[èŠ‚ç‚¹ä¸ºç©º] --> E2[ç­‰å¾…30ç§’]
        E2 --> E3[ç«‹å³ç§»é™¤]
        E3 --> E4[éœ€è¦æ–°èŠ‚ç‚¹æ—¶<br/>45ç§’é…ç½®]
    end

    subgraph "WhenEmptyOrUnderutilizedï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰"
        U1[èŠ‚ç‚¹ä½¿ç”¨ç‡ä½äº30%] --> U2[ç­‰å¾…30ç§’]
        U2 --> U3[é‡æ–°è°ƒåº¦æ¨¡æ‹Ÿ<br/>5-10ç§’]
        U3 --> U4[Pod é‡æ–°è°ƒåº¦<br/>10-20ç§’]
        U4 --> U5[ç§»é™¤èŠ‚ç‚¹]
    end

    style E4 fill:#48C9B0
    style U4 fill:#ff9900
```

### Disruption Budgets: çªå‘æµé‡æ—¶çš„è®¾ç½®

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: burst-ready
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s

    # æŒ‰æ—¶æ®µ Disruption Budget
    budgets:
    - nodes: "0"  # åœæ­¢æ›¿æ¢
      schedule: "0 8-18 * * MON-FRI"  # å·¥ä½œæ—¶é—´
      reasons:
        - Drifted
        - Expired
        - Consolidation

    - nodes: "20%"  # å…è®¸æ›¿æ¢20%
      schedule: "0 19-7 * * *"  # å¤œé—´
      reasons:
        - Drifted
        - Expired

    - nodes: "50%"  # å‘¨æœ«ç§¯æä¼˜åŒ–
      schedule: "0 0-23 * * SAT,SUN"
```

**Budget ç­–ç•¥ï¼š**

- **Black Friday ç­‰æ´»åŠ¨**: `nodes: "0"`ï¼ˆå®Œå…¨åœæ­¢æ›¿æ¢ï¼‰
- **æ­£å¸¸è¿è¥**: `nodes: "10-20%"`ï¼ˆæ¸è¿›å¼ä¼˜åŒ–ï¼‰
- **å¤œé—´/å‘¨æœ«**: `nodes: "50%"`ï¼ˆç§¯æèŠ‚çœæˆæœ¬ï¼‰

### Drift Detection: è‡ªåŠ¨èŠ‚ç‚¹æ›¿æ¢

Drift Detection åœ¨ NodePool è§„æ ¼å‘ç”Ÿå˜æ›´æ—¶è‡ªåŠ¨æ›¿æ¢ç°æœ‰èŠ‚ç‚¹ã€‚

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: drift-enabled
spec:
  template:
    spec:
      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["c6i.xlarge", "c7i.xlarge"]  # è§„æ ¼å˜æ›´æ—¶è§¦å‘ Drift æ£€æµ‹

      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: drift-class

  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
    budgets:
    - nodes: "20%"  # æ§åˆ¶ Drift æ›¿æ¢é€Ÿåº¦
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: drift-class
spec:
  amiSelectorTerms:
    - alias: al2023@latest  # AMI å˜æ›´æ—¶è‡ªåŠ¨è§¦å‘ Drift

  # AMI æ›´æ–°åœºæ™¯
  # 1. AWS å‘å¸ƒæ–°çš„ AL2023 AMI
  # 2. Karpenter æ£€æµ‹åˆ° Drift
  # 3. æ ¹æ® Budget ä¾æ¬¡æ›¿æ¢èŠ‚ç‚¹
```

**Drift è§¦å‘æ¡ä»¶ï¼š**

- NodePool å®ä¾‹ç±»å‹å˜æ›´
- EC2NodeClass AMI å˜æ›´
- userData è„šæœ¬ä¿®æ”¹
- blockDeviceMappings å˜æ›´

### NodePool Weights: Spot â†’ On-Demand å›é€€

```yaml
# Weight 0: æœ€é«˜ä¼˜å…ˆï¼ˆSpotï¼‰
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: spot-primary
spec:
  weight: 0  # æœ€ä½ weight = æœ€é«˜ä¼˜å…ˆ
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
---
# Weight 50: Spot ä¸è¶³æ—¶çš„å¤‡é€‰
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: on-demand-fallback
spec:
  weight: 50
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]
```

**Weight ç­–ç•¥ï¼š**

```mermaid
graph TB
    POD[ç­‰å¾…ä¸­çš„ Pod] --> W0{Weight 0<br/>Spot Pool}
    W0 -->|æœ‰å®¹é‡| SPOT[åˆ›å»º Spot èŠ‚ç‚¹]
    W0 -->|ICE<br/>InsufficientCapacity| W50{Weight 50<br/>On-Demand Pool}
    W50 --> OD[åˆ›å»º On-Demand èŠ‚ç‚¹]

    style SPOT fill:#48C9B0
    style OD fill:#ff9900
```

---

## æŒ‡æ ‡é‡‡é›†ä¼˜åŒ–

### KEDA + Prometheus: äº‹ä»¶é©±åŠ¨æ‰©ç¼©å®¹ï¼ˆ1-3ç§’å“åº”ï¼‰

KEDA ä»¥1-3ç§’é—´éš”è½®è¯¢ Prometheus æŒ‡æ ‡ï¼Œå®ç°è¶…é«˜é€Ÿæ‰©ç¼©å®¹ã€‚

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: ultra-fast-scaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app

  pollingInterval: 2  # æ¯2ç§’è½®è¯¢
  cooldownPeriod: 60
  minReplicaCount: 10
  maxReplicaCount: 1000

  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: http_requests_per_second
      query: |
        sum(rate(http_requests_total[30s])) by (service)
      threshold: "100"

  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: p99_latency_ms
      query: |
        histogram_quantile(0.99,
          sum(rate(http_request_duration_seconds_bucket[30s])) by (le)
        ) * 1000
      threshold: "500"  # è¶…è¿‡500msæ—¶æ‰©å®¹

  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
          - type: Percent
            value: 100
            periodSeconds: 5  # æ¯5ç§’å¯å¢åŠ 100%
```

**KEDA vs HPA æ‰©ç¼©å®¹é€Ÿåº¦ï¼š**

| é…ç½® | æŒ‡æ ‡æ›´æ–° | æ‰©ç¼©å®¹å†³ç­– | æ€»æ—¶é—´ |
|------|---------|-----------|-------|
| HPA + Metrics API | 15ç§’ | 15ç§’ | 30ç§’ |
| KEDA + Prometheus | 2ç§’ | 1ç§’ | 3ç§’ |
### ADOT Collector è°ƒä¼˜: æœ€å°åŒ– Scrape Interval

```yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: adot-collector-ultra-fast
spec:
  mode: daemonset
  config: |
    receivers:
      prometheus:
        config:
          scrape_configs:
          # å…³é”®æŒ‡æ ‡: 1ç§’æŠ“å–
          - job_name: 'critical-metrics'
            scrape_interval: 1s
            scrape_timeout: 800ms
            static_configs:
            - targets: ['web-app:8080']
            metric_relabel_configs:
            - source_labels: [__name__]
              regex: '(http_requests_total|http_request_duration_seconds.*|queue_depth)'
              action: keep

          # ä¸€èˆ¬æŒ‡æ ‡: 15ç§’æŠ“å–
          - job_name: 'standard-metrics'
            scrape_interval: 15s
            static_configs:
            - targets: ['web-app:8080']

    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
        send_batch_max_size: 2048

      memory_limiter:
        check_interval: 1s
        limit_mib: 512

    exporters:
      prometheus:
        endpoint: "0.0.0.0:8889"

      prometheusremotewrite:
        endpoint: http://mimir:9009/api/v1/push
        headers:
          X-Scope-OrgID: "prod"

    service:
      pipelines:
        metrics:
          receivers: [prometheus]
          processors: [memory_limiter, batch]
          exporters: [prometheus, prometheusremotewrite]
```

### CloudWatch Metric Streams

CloudWatch Metric Streams å°†æŒ‡æ ‡å®æ—¶æµå¼ä¼ è¾“åˆ° Kinesis Data Firehoseã€‚

```bash
# åˆ›å»º Metric Stream
aws cloudwatch put-metric-stream \
  --name eks-metrics-stream \
  --firehose-arn arn:aws:firehose:us-east-1:ACCOUNT:deliverystream/metrics \
  --role-arn arn:aws:iam::ACCOUNT:role/CloudWatchMetricStreamRole \
  --output-format json \
  --include-filters Namespace=AWS/EKS \
  --include-filters Namespace=ContainerInsights
```

**æ¶æ„ï¼š**

```mermaid
graph LR
    CW[CloudWatch Metrics] --> MS[Metric Stream]
    MS --> KDF[Kinesis Firehose]
    KDF --> S3[S3 Bucket]
    KDF --> PROM[Prometheus<br/>Remote Write]
    PROM --> KEDA[KEDA Scaler]
```

### Custom Metrics API HPA

```yaml
apiVersion: v1
kind: Service
metadata:
  name: custom-metrics-api
spec:
  ports:
  - port: 443
    targetPort: 6443
  selector:
    app: custom-metrics-apiserver
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: custom-metrics-apiserver
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: custom-metrics-apiserver
        image: your-registry/custom-metrics-api:v1
        args:
        - --secure-port=6443
        - --logtostderr=true
        - --v=4
        - --prometheus-url=http://prometheus:9090
        - --cache-ttl=5s  # 5ç§’ç¼“å­˜
```

---

## å®¹å™¨é•œåƒä¼˜åŒ–

### é•œåƒå¤§å°ä¸æ‰©ç¼©å®¹é€Ÿåº¦çš„å…³ç³»

```mermaid
graph TB
    subgraph "æŒ‰é•œåƒå¤§å°çš„æ‹‰å–æ—¶é—´"
        S1[100MB<br/>2-3ç§’]
        S2[500MB<br/>10-15ç§’]
        S3[1GB<br/>20-30ç§’]
        S4[5GB<br/>2-3åˆ†é’Ÿ]
    end

    subgraph "æ‰©ç¼©å®¹å½±å“"
        I1[æ€»æ‰©ç¼©å®¹æ—¶é—´<br/>40-50ç§’]
        I2[æ€»æ‰©ç¼©å®¹æ—¶é—´<br/>55-70ç§’]
        I3[æ€»æ‰©ç¼©å®¹æ—¶é—´<br/>65-85ç§’]
        I4[æ€»æ‰©ç¼©å®¹æ—¶é—´<br/>3-4åˆ†é’Ÿ]
    end

    S1 --> I1
    S2 --> I2
    S3 --> I3
    S4 --> I4

    style S1 fill:#48C9B0
    style I1 fill:#48C9B0
    style S4 fill:#ff4444
    style I4 fill:#ff4444
```

**ä¼˜åŒ–ç­–ç•¥ï¼š**

- ç›®æ ‡é•œåƒå¤§å°500MBä»¥ä¸‹
- ä½¿ç”¨ Multi-stage æ„å»ºæœ€å°åŒ–è¿è¡Œæ—¶å±‚
- ç§»é™¤ä¸å¿…è¦çš„åŒ…

### ECR Pull-Through Cache

```bash
# åˆ›å»º Pull-Through Cache è§„åˆ™
aws ecr create-pull-through-cache-rule \
  --ecr-repository-prefix docker-hub \
  --upstream-registry-url registry-1.docker.io \
  --region us-east-1

# ä½¿ç”¨ç¤ºä¾‹
# åŸå§‹: docker.io/library/nginx:latest
# ç¼“å­˜: ACCOUNT.dkr.ecr.us-east-1.amazonaws.com/docker-hub/library/nginx:latest
```

**ä¼˜ç‚¹ï¼š**

- é¦–æ¬¡æ‹‰å–åç¼“å­˜åˆ° ECR
- ç¬¬äºŒæ¬¡æ‹‰å–èµ·é€Ÿåº¦æå‡3-5å€
- è§„é¿ DockerHub é€Ÿç‡é™åˆ¶

### Image Pre-pull: DaemonSet vs userData

**æ–¹æ³•1: é€šè¿‡ DaemonSet é¢„æ‹‰å–é•œåƒ**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: image-prepull
spec:
  selector:
    matchLabels:
      app: image-prepull
  template:
    metadata:
      labels:
        app: image-prepull
    spec:
      initContainers:
      - name: prepull-web-app
        image: your-registry/web-app:v1.2.3
        command: ['sh', '-c', 'echo "Image pulled"']
      - name: prepull-sidecar
        image: your-registry/sidecar:v2.0.0
        command: ['sh', '-c', 'echo "Image pulled"']
      containers:
      - name: pause
        image: public.ecr.aws/eks-distro/kubernetes/pause:3.9
        resources:
          requests:
            cpu: 10m
            memory: 20Mi
```

**æ–¹æ³•2: åœ¨ userData ä¸­é¢„æ‹‰å–**

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: prepull-class
spec:
  userData: |
    #!/bin/bash
    /etc/eks/bootstrap.sh ${CLUSTER_NAME}

    # é¢„æ‹‰å–å…³é”®é•œåƒ
    ctr -n k8s.io images pull your-registry.com/web-app:v1.2.3 &
    ctr -n k8s.io images pull your-registry.com/sidecar:v2.0.0 &
    ctr -n k8s.io images pull your-registry.com/init-db:v3.1.0 &
    wait
```

**å¯¹æ¯”ï¼š**

| æ–¹æ³• | æ—¶æœº | æ–°èŠ‚ç‚¹æ•ˆæœ | ç»´æŠ¤éš¾åº¦ |
|------|------|-----------|---------|
| DaemonSet | èŠ‚ç‚¹ Ready å | â­â­â­ ä¸€èˆ¬ | â­â­â­â­ ç®€å• |
| userData | å¼•å¯¼æœŸé—´ | â­â­â­â­â­ æœ€ä½³ | â­â­ å›°éš¾ |
### æœ€å°åŸºç¡€é•œåƒ: distrolessã€scratch

```dockerfile
# ä¼˜åŒ–å‰: Ubuntu åŸºç¡€ (500MB)
FROM ubuntu:22.04
RUN apt-get update && apt-get install -y ca-certificates
COPY app /app
CMD ["/app"]

# ä¼˜åŒ–å: distroless (50MB)
FROM gcr.io/distroless/base-debian12
COPY app /app
CMD ["/app"]

# ä¼˜åŒ–å: scratch (20MB, ä»…é™æ€äºŒè¿›åˆ¶)
FROM scratch
COPY app /app
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
CMD ["/app"]
```

### SOCI (Seekable OCI) ç”¨äºå¤§å‹é•œåƒ

SOCI æ— éœ€æ‹‰å–å®Œæ•´é•œåƒï¼Œä»…æŒ‰éœ€åŠ è½½æ‰€éœ€éƒ¨åˆ†ã€‚

```bash
# åˆ›å»º SOCI ç´¢å¼•
soci create your-registry/large-ml-model:v1.0.0

# å°† SOCI ç´¢å¼•æ¨é€åˆ°æ³¨å†Œè¡¨
soci push your-registry/large-ml-model:v1.0.0

# Containerd é…ç½®
cat <<EOF > /etc/containerd/config.toml
[plugins."io.containerd.snapshotter.v1.soci"]
  enable_image_lazy_loading = true
EOF
```

**æ•ˆæœï¼š**

- 5GB é•œåƒ â†’ 10-15ç§’å¯åŠ¨ï¼ˆåŸéœ€2-3åˆ†é’Ÿï¼‰
- é€‚ç”¨äº ML æ¨¡å‹ã€å¤§å‹æ•°æ®é›†

### Bottlerocket ä¼˜åŒ–

Bottlerocket æ˜¯å®¹å™¨ä¼˜åŒ–æ“ä½œç³»ç»Ÿï¼Œå¯åŠ¨æ—¶é—´æ¯” AL2023 å¿«30%ã€‚

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: bottlerocket-class
spec:
  amiSelectorTerms:
    - alias: bottlerocket@latest

  userData: |
    [settings.kubernetes]
    cluster-name = "${CLUSTER_NAME}"

    [settings.kubernetes.node-labels]
    "karpenter.sh/fast-boot" = "true"
```

---

## In-Place Pod Vertical Scaling (K8s 1.33+)

ä» K8s 1.33 èµ·ï¼Œå¯ä»¥æ— éœ€é‡å¯ Pod å³å¯è°ƒæ•´èµ„æºã€‚

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resizable-pod
spec:
  containers:
  - name: app
    image: your-app:v1
    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "1000m"
        memory: "1Gi"
    resizePolicy:
    - resourceName: cpu
      restartPolicy: NotRequired  # CPU æ— éœ€é‡å¯
    - resourceName: memory
      restartPolicy: RestartContainer  # å†…å­˜éœ€è¦é‡å¯
```

**æ‰©ç¼©å®¹ vs è°ƒæ•´èµ„æºçš„é€‰æ‹©æ ‡å‡†ï¼š**

| åœºæ™¯ | ä½¿ç”¨æ–¹æ³• | åŸå›  |
|------|---------|------|
| æµé‡æ¿€å¢ï¼ˆ2å€ä»¥ä¸Šï¼‰ | HPA æ°´å¹³æ‰©å®¹ | éœ€è¦è´Ÿè½½å‡è¡¡ |
| CPU ä½¿ç”¨ç‡è¶…è¿‡80% | In-Place Resize | å•ä¸ª Pod æ€§èƒ½ä¸è¶³ |
| å†…å­˜ OOM é£é™© | In-Place Resize | èŠ‚çœé‡å¯æ—¶é—´ |
| éœ€è¦10+ Pod | HPA æ°´å¹³æ‰©å®¹ | æé«˜å¯ç”¨æ€§ |

---

## é«˜çº§æ¨¡å¼

### Pod Scheduling Readiness Gates (K8s 1.30+)

é€šè¿‡ `schedulingGates` æ§åˆ¶è°ƒåº¦æ—¶æœºã€‚

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gated-pod
spec:
  schedulingGates:
  - name: "example.com/image-preload"  # ç­‰å¾…é•œåƒé¢„åŠ è½½
  - name: "example.com/config-ready"   # ç­‰å¾… ConfigMap å°±ç»ª
  containers:
  - name: app
    image: your-app:v1
```

**Gate ç§»é™¤æ§åˆ¶å™¨ç¤ºä¾‹ï¼š**

```go
// Gate ç§»é™¤é€»è¾‘
func (c *Controller) removeGateWhenReady(pod *v1.Pod) {
    if imagePreloaded(pod) && configReady(pod) {
        patch := []byte(`{"spec":{"schedulingGates":null}}`)
        c.client.CoreV1().Pods(pod.Namespace).Patch(
            ctx, pod.Name, types.StrategicMergePatchType, patch, metav1.PatchOptions{})
    }
}
```

### ARC + Karpenter AZ æ•…éšœæ¢å¤

ç»“åˆ AWS Route 53 Application Recovery Controller (ARC) å’Œ Karpenterï¼Œå®ç° AZ æ•…éšœæ—¶è‡ªåŠ¨æ¢å¤ã€‚

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: az-resilient
spec:
  template:
    spec:
      requirements:
        - key: topology.kubernetes.io/zone
          operator: In
          values: ["us-east-1a", "us-east-1b", "us-east-1c"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]

      # AZ æ•…éšœæ—¶è‡ªåŠ¨æ›¿æ¢
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: az-resilient-class
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: az-resilient-class
spec:
  subnetSelectorTerms:
    # ARC Zonal Shift è”åŠ¨: è‡ªåŠ¨æ’é™¤æ•…éšœ AZ
    - tags:
        karpenter.sh/discovery: my-cluster
        aws:cloudformation:logical-id: PrivateSubnet*
```

**Zonal Shift åœºæ™¯ï¼š**

1. us-east-1a å‘ç”Ÿæ•…éšœ
2. ARC è§¦å‘ Zonal Shift
3. Karpenter æ’é™¤ 1a å­ç½‘ï¼Œä»…åœ¨ 1bã€1c åˆ›å»ºèŠ‚ç‚¹
4. æ•…éšœæ¢å¤åè‡ªåŠ¨é‡æ–°çº³å…¥ 1a

---

## ç»¼åˆæ‰©ç¼©å®¹åŸºå‡†æµ‹è¯•å¯¹æ¯”è¡¨

<ScalingBenchmark />
