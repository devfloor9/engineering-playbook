---
title: "EKS Pod è°ƒåº¦ä¸å¯ç”¨æ€§ç®¡ç†"
sidebar_label: "6. Pod è°ƒåº¦ä¸å¯ç”¨æ€§"
description: "Kubernetes Pod è°ƒåº¦æœºåˆ¶ä¸å¯ç”¨æ€§ç®¡ç†ç»¼åˆæŒ‡å—ï¼Œæ¶µç›–äº²å’Œæ€§ã€æ‹“æ‰‘åˆ†å¸ƒã€PDB åŠåŸºäºä¼˜å…ˆçº§çš„è°ƒåº¦"
sidebar_position: 6
tags: [EKS, Kubernetes, Pod, Scheduling, Availability, Affinity, TopologySpread, PDB, Priority, Preemption]
category: "observability-monitoring"
last_update:
  date: 2026-02-14
  author: devfloor9
---

# EKS Pod è°ƒåº¦ä¸å¯ç”¨æ€§æ¨¡å¼

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2025-10-15 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 30 åˆ†é’Ÿ

> **ğŸ“Œ å‚è€ƒç¯å¢ƒ**: EKS 1.30+ã€Karpenter v1.xã€Kubernetes 1.30+

## 1. æ¦‚è¿°

Kubernetes Pod è°ƒåº¦æ˜¯ç›´æ¥å½±å“æœåŠ¡å¯ç”¨æ€§ã€æ€§èƒ½å’Œæˆæœ¬æ•ˆç‡çš„æ ¸å¿ƒæœºåˆ¶ã€‚åº”ç”¨æ­£ç¡®çš„è°ƒåº¦ç­–ç•¥å¯è·å¾—ä»¥ä¸‹æ”¶ç›Šï¼š

- **é«˜å¯ç”¨æ€§**ï¼šé€šè¿‡æ•…éšœåŸŸéš”ç¦»æœ€å¤§é™åº¦å‡å°‘æœåŠ¡ä¸­æ–­
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå°†å·¥ä½œè´Ÿè½½æ”¾ç½®åœ¨åˆé€‚çš„èŠ‚ç‚¹ä¸Šä»¥æå‡å“åº”é€Ÿåº¦
- **èµ„æºæ•ˆç‡**ï¼šé€šè¿‡å‡è¡¡åˆ©ç”¨èŠ‚ç‚¹èµ„æºé™ä½æˆæœ¬
- **ç¨³å®šè¿ç»´**ï¼šé€šè¿‡åŸºäºä¼˜å…ˆçº§çš„è°ƒåº¦å’ŒæŠ¢å æ§åˆ¶ç¡®ä¿å…³é”®å·¥ä½œè´Ÿè½½è·å¾—èµ„æº

æœ¬æ–‡æ¶µç›–ä» Pod è°ƒåº¦æ ¸å¿ƒæ¦‚å¿µåˆ°é«˜çº§æ¨¡å¼çš„å…¨éƒ¨å†…å®¹ï¼Œä¸º EKS ç¯å¢ƒæä¾›ç”Ÿäº§å°±ç»ªçš„ YAML ç¤ºä¾‹å’Œå†³ç­–æŒ‡å—ã€‚

:::info é«˜å¯ç”¨æ¶æ„å‚è€ƒ
æœ¬æ–‡æ¡£ä¸“æ³¨äº **Pod çº§åˆ«** çš„è°ƒåº¦æ¨¡å¼ã€‚æœ‰å…³é›†ç¾¤çº§é«˜å¯ç”¨æ¶æ„ï¼ˆMulti-AZ ç­–ç•¥ã€Topology Spreadã€Cell Architectureï¼‰ï¼Œè¯·å‚é˜… [EKS é«˜å¯ç”¨æ¶æ„æŒ‡å—](/docs/operations-observability/eks-resiliency-guide)ã€‚
:::

### ä¸ºä»€ä¹ˆè°ƒåº¦å¦‚æ­¤é‡è¦

| åœºæ™¯ | ä¸å½“è°ƒåº¦ | åˆç†è°ƒåº¦ |
|---------|----------------|----------------|
| **æ•…éšœéš”ç¦»** | æ‰€æœ‰å‰¯æœ¬åœ¨åŒä¸€èŠ‚ç‚¹ â†’ èŠ‚ç‚¹æ•…éšœæ—¶å®Œå…¨ä¸­æ–­ | Anti-Affinity åˆ†å¸ƒåˆ°å¤šä¸ªèŠ‚ç‚¹ â†’ ä»…éƒ¨åˆ†æ•…éšœ |
| **èµ„æºäº‰ç”¨** | CPU å¯†é›†å‹ Pod é›†ä¸­åœ¨ä¸€ä¸ªèŠ‚ç‚¹ â†’ æ€§èƒ½ä¸‹é™ | Node Affinity åˆ†ç¦»å·¥ä½œè´Ÿè½½ â†’ æ€§èƒ½ç¨³å®š |
| **æˆæœ¬ä¼˜åŒ–** | ä¸éœ€è¦ GPU çš„ Pod è¢«æ”¾ç½®åœ¨ GPU èŠ‚ç‚¹ â†’ æˆæœ¬æµªè´¹ | Taints/Tolerations éš”ç¦»ä¸“ç”¨èŠ‚ç‚¹ â†’ èŠ‚çœæˆæœ¬ |
| **å‡çº§å®‰å…¨** | æœªé…ç½® PDB â†’ æ»šåŠ¨æ›´æ–°æ—¶æœåŠ¡ä¸­æ–­ | é…ç½® PDB â†’ ä¿è¯æœ€ä½å¯ç”¨ Pod æ•°é‡ |
| **ç´§æ€¥å“åº”** | æœªè®¾ç½®ä¼˜å…ˆçº§ â†’ å…³é”®å·¥ä½œè´Ÿè½½å¡åœ¨ Pending çŠ¶æ€ | é…ç½® PriorityClass â†’ å…³é”® Pod ä¼˜å…ˆè°ƒåº¦ |

---

## 2. Kubernetes è°ƒåº¦åŸºç¡€

### 2.1 è°ƒåº¦æµç¨‹

Kubernetes è°ƒåº¦å™¨é€šè¿‡ä¸‰é˜¶æ®µæµç¨‹å°† Pod æ”¾ç½®åˆ°èŠ‚ç‚¹ä¸Šï¼š

```mermaid
flowchart TB
    subgraph "é˜¶æ®µ 1: è¿‡æ»¤ï¼ˆFilteringï¼‰"
        P1[æ–° Pod åˆ›å»ºè¯·æ±‚]
        P2[è·å–æ‰€æœ‰èŠ‚ç‚¹åˆ—è¡¨]
        P3{èŠ‚ç‚¹è¿‡æ»¤<br/>Predicates}
        P4[èµ„æºä¸è¶³]
        P5[Taint ä¸åŒ¹é…]
        P6[Node Selector ä¸åŒ¹é…]
        P7[ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹åˆ—è¡¨]
    end

    subgraph "é˜¶æ®µ 2: è¯„åˆ†ï¼ˆScoringï¼‰"
        S1[å¯¹æ¯ä¸ªèŠ‚ç‚¹è®¡ç®—åˆ†æ•°<br/>Priorities]
        S2[èµ„æºå‡è¡¡åº¦]
        S3[Affinity/Anti-Affinity]
        S4[é•œåƒç¼“å­˜å‘½ä¸­]
        S5[é€‰æ‹©æœ€é«˜åˆ†èŠ‚ç‚¹]
    end

    subgraph "é˜¶æ®µ 3: ç»‘å®šï¼ˆBindingï¼‰"
        B1[å°† Pod åˆ†é…åˆ°èŠ‚ç‚¹<br/>Bind]
        B2[é€šçŸ¥ Kubelet]
        B3[å¯åŠ¨å®¹å™¨]
    end

    P1 --> P2
    P2 --> P3
    P3 -->|æ’é™¤ä¸ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹| P4
    P3 --> P5
    P3 --> P6
    P3 -->|ä»…ä¿ç•™ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹| P7
    P7 --> S1
    S1 --> S2
    S1 --> S3
    S1 --> S4
    S2 --> S5
    S3 --> S5
    S4 --> S5
    S5 --> B1
    B1 --> B2
    B2 --> B3

    style P1 fill:#4286f4,stroke:#2a6acf,color:#fff
    style P3 fill:#fbbc04,stroke:#c99603,color:#000
    style P7 fill:#34a853,stroke:#2a8642,color:#fff
    style S5 fill:#34a853,stroke:#2a8642,color:#fff
    style B3 fill:#34a853,stroke:#2a8642,color:#fff
```

**1. è¿‡æ»¤ï¼ˆPredicatesï¼‰**ï¼šæ’é™¤ä¸æ»¡è¶³è¦æ±‚çš„èŠ‚ç‚¹
- èµ„æºä¸è¶³ï¼ˆCPUã€Memoryï¼‰
- Taints/Tolerations ä¸åŒ¹é…
- Node Selector æ¡ä»¶ä¸æ»¡è¶³
- å·æ‹“æ‰‘çº¦æŸï¼ˆEBS AZ-Pinningï¼‰
- ç«¯å£å†²çª

**2. è¯„åˆ†ï¼ˆPrioritiesï¼‰**ï¼šå¯¹å‰©ä½™èŠ‚ç‚¹æ‰“åˆ†ä»¥é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
- èµ„æºå‡è¡¡åº¦ï¼ˆå‡åŒ€åˆ©ç”¨ï¼‰
- Pod Affinity/Anti-Affinity æ»¡è¶³åº¦
- é•œåƒç¼“å­˜å‘½ä¸­
- Topology Spread å‡åŒ€åº¦
- èŠ‚ç‚¹åå¥½ï¼ˆPreferredDuringSchedulingï¼‰

**3. ç»‘å®š**ï¼šå°† Pod åˆ†é…åˆ°æœ€é«˜åˆ†èŠ‚ç‚¹å¹¶é€šçŸ¥ Kubelet

:::tip è°ƒåº¦å¤±è´¥è°ƒè¯•
å¦‚æœ Pod æŒç»­å¤„äº `Pending` çŠ¶æ€ï¼Œè¯·ä½¿ç”¨ `kubectl describe pod <pod-name>` æ£€æŸ¥ Events éƒ¨åˆ†ã€‚`Insufficient cpu`ã€`No nodes available` æˆ– `Taint not tolerated` ç­‰æ¶ˆæ¯æœ‰åŠ©äºç¡®å®šå¤±è´¥åŸå› ã€‚
:::

### 2.2 å½±å“è°ƒåº¦çš„å› ç´ 

| å› ç´  | ç±»å‹ | å½±å“é˜¶æ®µ | çº¦æŸå¼ºåº¦ | ä¸»è¦ä½¿ç”¨åœºæ™¯ |
|------|------|-----------|--------|---------------|
| **Node Selector** | Pod | è¿‡æ»¤ | ç¡¬æ€§ | æŒ‡å®šç‰¹å®šèŠ‚ç‚¹ç±»å‹ï¼ˆGPUã€ARMï¼‰ |
| **Node Affinity** | Pod | è¿‡æ»¤/è¯„åˆ† | ç¡¬æ€§/è½¯æ€§ | ç»†ç²’åº¦èŠ‚ç‚¹é€‰æ‹©æ¡ä»¶ |
| **Pod Affinity** | Pod | è¯„åˆ† | ç¡¬æ€§/è½¯æ€§ | å°†ç›¸å…³ Pod æ”¾åœ¨ä¸€èµ· |
| **Pod Anti-Affinity** | Pod | è¿‡æ»¤/è¯„åˆ† | ç¡¬æ€§/è½¯æ€§ | å°† Pod åˆ†æ•£å¼€æ¥ |
| **Taints/Tolerations** | èŠ‚ç‚¹ + Pod | è¿‡æ»¤ | ç¡¬æ€§ | ä¸“ç”¨èŠ‚ç‚¹éš”ç¦» |
| **Topology Spread** | Pod | è¯„åˆ† | ç¡¬æ€§/è½¯æ€§ | è·¨ AZ/èŠ‚ç‚¹å‡åŒ€åˆ†å¸ƒ |
| **PriorityClass** | Pod | æŠ¢å  | ç¡¬æ€§ | åŸºäºä¼˜å…ˆçº§çš„èµ„æºæŠ¢å  |
| **Resource Requests** | Pod | è¿‡æ»¤ | ç¡¬æ€§ | ä¿è¯æœ€ä½èµ„æº |
| **PDB** | Pod ç»„ | é©±é€ | ç¡¬æ€§ | ä¿è¯æœ€ä½å¯ç”¨ Pod æ•°é‡ |

**ç¡¬æ€§ vs è½¯æ€§çº¦æŸï¼š**
- **ç¡¬æ€§ï¼ˆRequiredï¼‰**ï¼šä¸æ»¡è¶³æ¡ä»¶åˆ™è°ƒåº¦å¤±è´¥ â†’ `Pending` çŠ¶æ€
- **è½¯æ€§ï¼ˆPreferredï¼‰**ï¼šä¼˜å…ˆæ»¡è¶³æ¡ä»¶ï¼Œä½†å³ä½¿ä¸æ»¡è¶³ä¹Ÿä¼šç»§ç»­è°ƒåº¦ â†’ å…è®¸å›é€€

---

## 3. Node Affinity ä¸ Anti-Affinity

### 3.1 Node Selectorï¼ˆåŸºç¡€ï¼‰

Node Selector æ˜¯æœ€ç®€å•çš„èŠ‚ç‚¹é€‰æ‹©æœºåˆ¶ï¼Œä»…æ”¯æŒåŸºäºæ ‡ç­¾çš„ç²¾ç¡®åŒ¹é…ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-workload
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ml-training
  template:
    metadata:
      labels:
        app: ml-training
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: g5.2xlarge
        workload-type: gpu
      containers:
      - name: trainer
        image: ml/trainer:v2.0
        resources:
          requests:
            nvidia.com/gpu: 1
```

**é™åˆ¶**ï¼šNode Selector ä»…æ”¯æŒ `AND` æ¡ä»¶ï¼Œä¸æ”¯æŒ `OR`ã€`NOT` æˆ–æ¯”è¾ƒè¿ç®—ç¬¦ã€‚éœ€è¦å¤æ‚æ¡ä»¶æ—¶è¯·ä½¿ç”¨ Node Affinityã€‚

### 3.2 Node Affinity è¯¦è§£

Node Affinity æ˜¯ Node Selector çš„æ‰©å±•ç‰ˆæœ¬ï¼Œå¯ä»¥è¡¨è¾¾å¤æ‚çš„é€»è¾‘æ¡ä»¶å’Œåå¥½ã€‚

#### Required vs Preferred

| ç±»å‹ | è¡Œä¸º | ä½¿ç”¨æ—¶æœº |
|------|------|----------|
| `requiredDuringSchedulingIgnoredDuringExecution` | å¿…é¡»æ»¡è¶³æ¡ä»¶ï¼ˆç¡¬æ€§ï¼‰ | Pod å¿…é¡»æ”¾ç½®åœ¨ç‰¹å®šèŠ‚ç‚¹ä¸Šæ—¶ |
| `preferredDuringSchedulingIgnoredDuringExecution` | ä¼˜å…ˆæ»¡è¶³æ¡ä»¶ï¼ˆè½¯æ€§ï¼ŒåŸºäºæƒé‡ï¼‰ | å¸Œæœ›ä¼˜å…ˆä½†å¯æ¥å—æ›¿ä»£æ–¹æ¡ˆæ—¶ |

:::info IgnoredDuringExecution çš„å«ä¹‰
`IgnoredDuringExecution` è¡¨ç¤ºå³ä½¿ Pod **å·²ç»è¿è¡Œ**åèŠ‚ç‚¹æ ‡ç­¾å‘ç”Ÿå˜åŒ–ï¼ŒPod ä¹Ÿä¸ä¼šè¢«é©±é€ã€‚æœªæ¥å¼•å…¥ `RequiredDuringExecution` åï¼Œå¦‚æœè¿è¡ŒæœŸé—´æ¡ä»¶ä¸å†æ»¡è¶³ï¼ŒPod å°†è¢«é‡æ–°è°ƒåº¦ã€‚
:::

#### Operator ç±»å‹

| Operator | è¯´æ˜ | ç¤ºä¾‹ |
|--------|------|------|
| `In` | å€¼åœ¨åˆ—è¡¨ä¸­ | `values: ["t3.xlarge", "t3.2xlarge"]` |
| `NotIn` | å€¼ä¸åœ¨åˆ—è¡¨ä¸­ | `values: ["t2.micro", "t2.small"]` |
| `Exists` | é”®å­˜åœ¨ï¼ˆå€¼æ— å…³ï¼‰ | ä»…æ£€æŸ¥æ ‡ç­¾æ˜¯å¦å­˜åœ¨ |
| `DoesNotExist` | é”®ä¸å­˜åœ¨ | é€‰æ‹©æ²¡æœ‰ç‰¹å®šæ ‡ç­¾çš„èŠ‚ç‚¹ |
| `Gt` | å€¼å¤§äºï¼ˆæ•°å€¼å‹ï¼‰ | `values: ["100"]`ï¼ˆå¦‚ CPU æ ¸å¿ƒæ•°ï¼‰ |
| `Lt` | å€¼å°äºï¼ˆæ•°å€¼å‹ï¼‰ | `values: ["10"]` |

#### æŒ‰ä½¿ç”¨åœºæ™¯çš„ YAML ç¤ºä¾‹

**ç¤ºä¾‹ 1ï¼šå°† ML å·¥ä½œè´Ÿè½½æ”¾ç½®åœ¨ GPU èŠ‚ç‚¹ä¸Šï¼ˆç¡¬æ€§ï¼‰**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-training
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-training
  template:
    metadata:
      labels:
        app: ml-training
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - g5.xlarge
                - g5.2xlarge
                - g5.4xlarge
              - key: karpenter.sh/capacity-type
                operator: NotIn
                values:
                - spot  # GPU å·¥ä½œè´Ÿè½½æ’é™¤ Spot
      containers:
      - name: trainer
        image: ml/trainer:v3.0
        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: "4"
            memory: 16Gi
```

**ç¤ºä¾‹ 2ï¼šå®ä¾‹ç³»åˆ—åå¥½ï¼ˆè½¯æ€§ï¼Œå¸¦æƒé‡ï¼‰**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 6
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      affinity:
        nodeAffinity:
          # å¿…é¡»ï¼šä»…ä½¿ç”¨ On-Demand èŠ‚ç‚¹
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: karpenter.sh/capacity-type
                operator: In
                values:
                - on-demand
          # ä¼˜å…ˆï¼šæŒ‰ c7i > c6i > m6i é¡ºåºåå¥½
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - c7i.xlarge
                - c7i.2xlarge
          - weight: 80
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - c6i.xlarge
                - c6i.2xlarge
          - weight: 50
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - m6i.xlarge
                - m6i.2xlarge
      containers:
      - name: api
        image: api-server:v2.5
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
```

**ç¤ºä¾‹ 3ï¼šæŒ‡å®šç‰¹å®š AZï¼ˆæ•°æ®åº“å®¢æˆ·ç«¯ï¼‰**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: db-client
spec:
  replicas: 4
  selector:
    matchLabels:
      app: db-client
  template:
    metadata:
      labels:
        app: db-client
    spec:
      affinity:
        nodeAffinity:
          # æ”¾ç½®åœ¨ä¸ RDS å®ä¾‹ç›¸åŒçš„ AZï¼ˆus-east-1aï¼‰ä»¥å‡å°‘è·¨ AZ æµé‡æˆæœ¬
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-east-1a
      containers:
      - name: client
        image: db-client:v1.2
        env:
        - name: DB_ENDPOINT
          value: "mydb.us-east-1a.rds.amazonaws.com"
```

### 3.3 Node Anti-Affinity

Node Anti-Affinity æ²¡æœ‰æ˜¾å¼è¯­æ³•ï¼Œè€Œæ˜¯é€šè¿‡ Node Affinity çš„ `NotIn` å’Œ `DoesNotExist` è¿ç®—ç¬¦æ¥å®ç°ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: avoid-spot
spec:
  replicas: 3
  selector:
    matchLabels:
      app: critical-service
  template:
    metadata:
      labels:
        app: critical-service
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              # é¿å¼€ Spot èŠ‚ç‚¹
              - key: karpenter.sh/capacity-type
                operator: NotIn
                values:
                - spot
              # é¿å¼€ ARM æ¶æ„
              - key: kubernetes.io/arch
                operator: NotIn
                values:
                - arm64
      containers:
      - name: app
        image: critical-service:v1.0
```

---

## 4. Pod Affinity ä¸ Anti-Affinity

Pod Affinity å’Œ Anti-Affinity åŸºäº **Pod ä¹‹é—´çš„å…³ç³»** åšå‡ºè°ƒåº¦å†³ç­–ã€‚è¿™ä½¿æ‚¨å¯ä»¥å°†ç›¸å…³ Pod æ”¾åœ¨ä¸€èµ·ï¼ˆAffinityï¼‰æˆ–åˆ†æ•£å¼€æ¥ï¼ˆAnti-Affinityï¼‰ã€‚

### 4.1 Pod Affinity

Pod Affinity å°† Pod æ”¾ç½®åœ¨è¿è¡Œç‰¹å®š Pod çš„åŒä¸€æ‹“æ‰‘åŸŸï¼ˆèŠ‚ç‚¹ã€AZã€åŒºåŸŸï¼‰ä¸­ã€‚

**ä¸»è¦ä½¿ç”¨åœºæ™¯ï¼š**
- **ç¼“å­˜å±€éƒ¨æ€§**ï¼šå°†ç¼“å­˜æœåŠ¡å™¨å’Œåº”ç”¨æ”¾åœ¨åŒä¸€èŠ‚ç‚¹ä»¥æœ€å°åŒ–å»¶è¿Ÿ
- **æ•°æ®å±€éƒ¨æ€§**ï¼šå°†æ•°æ®å¤„ç†å·¥ä½œè´Ÿè½½æ”¾åœ¨é è¿‘æ•°æ®æºçš„ä½ç½®
- **é€šä¿¡å¯†é›†å‹**ï¼šå°†é¢‘ç¹é€šä¿¡çš„å¾®æœåŠ¡æ”¾åœ¨åŒä¸€ AZ

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cache-client
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cache-client
  template:
    metadata:
      labels:
        app: cache-client
    spec:
      affinity:
        podAffinity:
          # ç¡¬æ€§ï¼šä¸ Redis Pod æ”¾åœ¨åŒä¸€èŠ‚ç‚¹ï¼ˆè¶…ä½å»¶è¿Ÿéœ€æ±‚ï¼‰
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis
            topologyKey: kubernetes.io/hostname
      containers:
      - name: client
        image: cache-client:v1.0
```

**topologyKey è¯´æ˜ï¼š**

| topologyKey | èŒƒå›´ | è¯´æ˜ |
|-------------|------|------|
| `kubernetes.io/hostname` | èŠ‚ç‚¹ | æ”¾åœ¨åŒä¸€èŠ‚ç‚¹ï¼ˆæœ€å¼ºå…±ç½®ï¼‰ |
| `topology.kubernetes.io/zone` | AZ | æ”¾åœ¨åŒä¸€ AZ |
| `topology.kubernetes.io/region` | åŒºåŸŸ | æ”¾åœ¨åŒä¸€åŒºåŸŸ |
| è‡ªå®šä¹‰æ ‡ç­¾ | ç”¨æˆ·è‡ªå®šä¹‰ | ä¾‹å¦‚ `rack`ã€`datacenter` |

**è½¯æ€§ Affinity ç¤ºä¾‹ï¼ˆä¼˜å…ˆæ»¡è¶³ï¼Œå…è®¸æ›¿ä»£ï¼‰ï¼š**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
spec:
  replicas: 6
  selector:
    matchLabels:
      app: web-frontend
  template:
    metadata:
      labels:
        app: web-frontend
    spec:
      affinity:
        podAffinity:
          # è½¯æ€§ï¼šä¼˜å…ˆä¸ API æœåŠ¡å™¨åœ¨åŒä¸€ AZï¼ˆå‡å°‘è·¨ AZ æµé‡æˆæœ¬ï¼‰
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - api-server
              topologyKey: topology.kubernetes.io/zone
      containers:
      - name: frontend
        image: web-frontend:v2.0
```

### 4.2 Pod Anti-Affinity

Pod Anti-Affinity é˜²æ­¢ Pod ä¸ç‰¹å®šå…¶ä»– Pod è¢«æ”¾ç½®åœ¨åŒä¸€æ‹“æ‰‘åŸŸä¸­ã€‚å®ƒæ˜¯ç¡®ä¿é«˜å¯ç”¨æ€§çš„å…³é”®æ¨¡å¼ã€‚

```mermaid
flowchart TB
    subgraph "èŠ‚ç‚¹ 1 (AZ-1a)"
        N1P1[replica-1<br/>app=api-server]
        N1P2[...]
    end

    subgraph "èŠ‚ç‚¹ 2 (AZ-1b)"
        N2P1[replica-2<br/>app=api-server]
        N2P2[...]
    end

    subgraph "èŠ‚ç‚¹ 3 (AZ-1c)"
        N3P1[replica-3<br/>app=api-server]
        N3P2[...]
    end

    subgraph "Pod Anti-Affinity è§„åˆ™"
        RULE[topologyKey: topology.kubernetes.io/zone<br/>å°† app=api-server çš„ Pod æ”¾åœ¨ä¸åŒ AZ]
    end

    RULE -.->|åº”ç”¨| N1P1
    RULE -.->|åº”ç”¨| N2P1
    RULE -.->|åº”ç”¨| N3P1

    style N1P1 fill:#34a853,stroke:#2a8642,color:#fff
    style N2P1 fill:#4286f4,stroke:#2a6acf,color:#fff
    style N3P1 fill:#fbbc04,stroke:#c99603,color:#000
    style RULE fill:#ff9900,stroke:#cc7a00,color:#fff
```

#### ç¡¬æ€§ Anti-Affinityï¼ˆæ•…éšœåŸŸéš”ç¦»ï¼‰

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 6
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      affinity:
        podAntiAffinity:
          # ç¡¬æ€§ï¼šæ¯ä¸ªèŠ‚ç‚¹æœ€å¤šæ”¾ç½® 1 ä¸ªå‰¯æœ¬ï¼ˆèŠ‚ç‚¹æ•…éšœéš”ç¦»ï¼‰
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - api-server
            topologyKey: kubernetes.io/hostname
      containers:
      - name: api
        image: api-server:v3.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
```

:::warning ç¡¬æ€§ Anti-Affinity æ³¨æ„äº‹é¡¹
å½“ä½¿ç”¨ `kubernetes.io/hostname` åº”ç”¨ç¡¬æ€§ Anti-Affinity æ—¶ï¼Œå¦‚æœå‰¯æœ¬æ•°è¶…è¿‡èŠ‚ç‚¹æ•°ï¼Œéƒ¨åˆ† Pod å°†ä¿æŒ `Pending` çŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œåœ¨ 3 ä¸ªèŠ‚ç‚¹ä¸Šéƒ¨ç½² 5 ä¸ªå‰¯æœ¬å°†å¯¼è‡´ 2 ä¸ªæ— æ³•è°ƒåº¦ã€‚æ­¤æƒ…å†µä¸‹è¯·ä½¿ç”¨è½¯æ€§ Anti-Affinityã€‚
:::

#### è½¯æ€§ Anti-Affinityï¼ˆæ¨èæ¨¡å¼ï¼‰

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker
spec:
  replicas: 10
  selector:
    matchLabels:
      app: worker
  template:
    metadata:
      labels:
        app: worker
    spec:
      affinity:
        podAntiAffinity:
          # è½¯æ€§ï¼šå°½å¯èƒ½åˆ†æ•£åˆ°ä¸åŒèŠ‚ç‚¹ï¼ˆçµæ´»æ€§ï¼‰
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - worker
              topologyKey: kubernetes.io/hostname
      containers:
      - name: worker
        image: worker:v2.1
        resources:
          requests:
            cpu: "500m"
            memory: 1Gi
```

#### ç¡¬æ€§ vs è½¯æ€§é€‰æ‹©æ ‡å‡†

| åœºæ™¯ | å»ºè®® | åŸå›  |
|---------|------|------|
| å‰¯æœ¬æ•° &lt;= èŠ‚ç‚¹æ•° | ç¡¬æ€§ | å¯ä»¥ç²¾ç¡®å®ç°æ¯èŠ‚ç‚¹ 1 ä¸ª |
| å‰¯æœ¬æ•° > èŠ‚ç‚¹æ•° | è½¯æ€§ | å…è®¸éƒ¨åˆ†èŠ‚ç‚¹æ”¾ç½® 2 ä¸ªä»¥ä¸Š |
| å…³é”®ä»»åŠ¡æœåŠ¡ | ç¡¬æ€§ï¼ˆAZ çº§åˆ«ï¼‰ | å®Œå…¨æ•…éšœåŸŸéš”ç¦» |
| ä¸€èˆ¬å·¥ä½œè´Ÿè½½ | è½¯æ€§ | è°ƒåº¦çµæ´»æ€§ |
| éœ€è¦å¿«é€Ÿæ‰©å±• | è½¯æ€§ | é¿å… Pending çŠ¶æ€ |

### 4.3 Affinity/Anti-Affinity vs Topology Spread å¯¹æ¯”

| å¯¹æ¯”é¡¹ | Pod Anti-Affinity | Topology Spread Constraints |
|----------|-------------------|----------------------------|
| **ç›®çš„** | Pod ä¹‹é—´çš„åˆ†ç¦» | Pod çš„å‡åŒ€åˆ†å¸ƒ |
| **ç²’åº¦** | æŒ‰ Pod æ§åˆ¶ | è·¨åŸŸçš„å‡è¡¡æ§åˆ¶ |
| **å¤æ‚åº¦** | ä½ | ä¸­ |
| **çµæ´»æ€§** | ç¡¬æ€§/è½¯æ€§é€‰æ‹© | é€šè¿‡ maxSkew å®ç°å®¹å·®èŒƒå›´ |
| **ä¸»è¦ç”¨é€”** | åˆ†ç¦»åŒä¸€åº”ç”¨çš„å‰¯æœ¬ | å¤šåº”ç”¨é—´çš„æ•´ä½“å‡è¡¡ |
| **AZ åˆ†å¸ƒ** | å¯ä»¥ | æ›´ç²¾ç»†ï¼ˆminDomainsï¼‰ |
| **èŠ‚ç‚¹åˆ†å¸ƒ** | å¯ä»¥ | æ›´ç²¾ç»†ï¼ˆmaxSkewï¼‰ |
| **æ¨èç»„åˆ** | Topology Spread (AZ) + Anti-Affinity (èŠ‚ç‚¹) | |

:::info Topology Spread Constraints å‚è€ƒ
Topology Spread Constraints æä¾›æ¯” Pod Anti-Affinity æ›´ç²¾ç»†çš„åˆ†å¸ƒæ§åˆ¶ã€‚è¯¦ç»†ä¿¡æ¯å’Œ YAML ç¤ºä¾‹è¯·å‚é˜… [EKS é«˜å¯ç”¨æ¶æ„æŒ‡å—](/docs/operations-observability/eks-resiliency-guide#pod-topology-spread-constraints)ã€‚
:::

#### 4.3.1 Topology Spread Constraints å®æˆ˜æ¨¡å¼

Topology Spread Constraints å¯ä»¥ä¼˜é›…åœ°è§£å†³å¤æ‚çš„åˆ†å¸ƒéœ€æ±‚ã€‚ä»¥ä¸‹æ˜¯ç”Ÿäº§ç¯å¢ƒä¸­å¸¸ç”¨çš„æ¨¡å¼åŠ YAML ç¤ºä¾‹ã€‚

##### æ¨¡å¼ 1ï¼šMulti-AZ å‡åŒ€åˆ†å¸ƒï¼ˆåŸºç¡€ï¼‰

æœ€å¸¸è§çš„æ¨¡å¼ï¼Œå°†æ‰€æœ‰å‰¯æœ¬å‡åŒ€åˆ†å¸ƒåˆ°å„ AZã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-az-app
  namespace: production
spec:
  replicas: 9
  selector:
    matchLabels:
      app: multi-az-app
  template:
    metadata:
      labels:
        app: multi-az-app
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: multi-az-app
      containers:
      - name: app
        image: myapp:v1.0
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
```

**å·¥ä½œåŸç†ï¼š**
- `maxSkew: 1`ï¼šå„ AZ ä¹‹é—´çš„ Pod æ•°é‡å·®å¼‚æœ€å¤šå…è®¸ä¸º 1
- 9 ä¸ªå‰¯æœ¬ â†’ us-east-1a(3)ã€us-east-1b(3)ã€us-east-1c(3)
- `whenUnsatisfiable: DoNotSchedule`ï¼šè¿åçº¦æŸæ—¶ä¿æŒ Pod ä¸º Pending çŠ¶æ€

**ä½¿ç”¨åœºæ™¯ï¼š**
- å…³é”®ä»»åŠ¡æœåŠ¡çš„ AZ æ•…éšœåº”å¯¹
- å®¢æˆ·ç«¯æµé‡ä»æ‰€æœ‰ AZ å‡åŒ€åˆ°è¾¾æ—¶
- éœ€è¦æ•°æ®ä¸­å¿ƒçº§æ•…éšœéš”ç¦»æ—¶

##### æ¨¡å¼ 2ï¼šä½¿ç”¨ minDomainsï¼ˆæœ€å° AZ ä¿è¯ï¼‰

`minDomains` ä¿è¯ Pod å¿…é¡»åˆ†å¸ƒåˆ°çš„æœ€å°‘åŸŸï¼ˆAZï¼‰æ•°é‡ã€‚å®ƒå¯ä»¥é˜²æ­¢åœ¨ AZ å‡å°‘åœºæ™¯ä¸­ Pod å †ç§¯åœ¨ä¸€ä¸ªä½ç½®ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ha-critical-service
  namespace: production
spec:
  replicas: 6
  selector:
    matchLabels:
      app: ha-critical-service
      tier: critical
  template:
    metadata:
      labels:
        app: ha-critical-service
        tier: critical
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        minDomains: 3  # å¿…é¡»åˆ†å¸ƒåˆ°è‡³å°‘ 3 ä¸ª AZ
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: ha-critical-service
      containers:
      - name: service
        image: critical-service:v2.5
        resources:
          requests:
            cpu: "1"
            memory: 1Gi
          limits:
            cpu: "2"
            memory: 2Gi
```

**å·¥ä½œåŸç†ï¼š**
- `minDomains: 3`ï¼šä¿è¯ Pod åˆ†å¸ƒåˆ°è‡³å°‘ 3 ä¸ª AZ
- 6 ä¸ªå‰¯æœ¬ â†’ æ¯ä¸ª AZ è‡³å°‘ 2 ä¸ª
- å³ä½¿ç‰¹å®š AZ ç¼ºå°‘èµ„æºï¼ŒPod ä¹Ÿä¸ä¼šå…¨éƒ¨æ±‡èšåˆ°å…¶ä»– AZ

**ä½¿ç”¨åœºæ™¯ï¼š**
- é‡‘èå’Œæ”¯ä»˜ç³»ç»Ÿç­‰è¶…é«˜å¯ç”¨æœåŠ¡
- éœ€è¦ 99.99% ä»¥ä¸Š SLA æ—¶
- AZ ç¼©å‡ï¼ˆZonal Shiftï¼‰æœŸé—´ç»´æŒæœ€ä½å¯ç”¨æ€§

:::warning è®¾ç½® minDomains æ—¶çš„æ³¨æ„äº‹é¡¹
å½“è®¾ç½®äº† `minDomains` ä½†æ‰€éœ€æ•°é‡çš„åŸŸä¸å­˜åœ¨æˆ–ç¼ºå°‘èµ„æºæ—¶ï¼ŒPod å°†ä¿æŒ Pending çŠ¶æ€ã€‚é…ç½®å‰è¯·ç¡®è®¤é›†ç¾¤ä¸­å®é™…å¯ç”¨çš„ AZ æ•°é‡ã€‚
:::

##### æ¨¡å¼ 3ï¼šAnti-Affinity + Topology Spread ç»„åˆ

ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹æœ€å¤š 1 ä¸ªå‰¯æœ¬ï¼ŒåŒæ—¶ä¿è¯è·¨ AZ å‡åŒ€åˆ†å¸ƒçš„æ¨¡å¼ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: combined-constraints-app
  namespace: production
spec:
  replicas: 12
  selector:
    matchLabels:
      app: combined-app
  template:
    metadata:
      labels:
        app: combined-app
        version: v3.0
    spec:
      # 1. Topology Spreadï¼šè·¨ AZ å‡åŒ€åˆ†å¸ƒï¼ˆç¡¬æ€§ï¼‰
      topologySpreadConstraints:
      - maxSkew: 1
        minDomains: 3
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: combined-app

      # 2. Anti-Affinityï¼šè·¨èŠ‚ç‚¹åˆ†å¸ƒï¼ˆç¡¬æ€§ï¼‰
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - combined-app
            topologyKey: kubernetes.io/hostname

      containers:
      - name: app
        image: combined-app:v3.0
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
```

**å·¥ä½œåŸç†ï¼š**
- **ç¬¬ä¸€å±‚ï¼ˆAZï¼‰**ï¼š12 ä¸ªå‰¯æœ¬ â†’ æ¯ä¸ª AZ å‡åŒ€åˆ†å¸ƒ 4 ä¸ª
- **ç¬¬äºŒå±‚ï¼ˆèŠ‚ç‚¹ï¼‰**ï¼šæ¯ä¸ªèŠ‚ç‚¹æœ€å¤š 1 ä¸ª Pod

**æ•ˆæœï¼š**
- èŠ‚ç‚¹æ•…éšœæœ€å¤šå½±å“ 1 ä¸ª Pod
- AZ æ•…éšœæœ€å¤šå½±å“ 4 ä¸ª Pod
- 12 ä¸ªä¸­çš„ 8 ä¸ªï¼ˆ66.7%ï¼‰å§‹ç»ˆå¯ç”¨

**ä½¿ç”¨åœºæ™¯ï¼š**
- å®Œå…¨æ¶ˆé™¤å•ç‚¹æ•…éšœ
- å…¼é¡¾ç¡¬ä»¶å’Œæ•°æ®ä¸­å¿ƒæ•…éšœçš„å¼¹æ€§
- é«˜æµé‡ API æœåŠ¡å™¨ã€æ”¯ä»˜ç½‘å…³

##### æ¨¡å¼ 4ï¼šå¤šçº§ Topology Spreadï¼ˆZone + Nodeï¼‰

åœ¨å•ä¸ª Pod Spec ä¸­åŒæ—¶æ§åˆ¶å¤šä¸ªæ‹“æ‰‘çº§åˆ«çš„åˆ†å¸ƒã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-level-spread
  namespace: production
spec:
  replicas: 18
  selector:
    matchLabels:
      app: multi-level-app
  template:
    metadata:
      labels:
        app: multi-level-app
    spec:
      topologySpreadConstraints:
      # çº¦æŸ 1ï¼šAZ çº§åˆ«åˆ†å¸ƒï¼ˆç¡¬æ€§ï¼‰
      - maxSkew: 1
        minDomains: 3
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: multi-level-app

      # çº¦æŸ 2ï¼šèŠ‚ç‚¹çº§åˆ«åˆ†å¸ƒï¼ˆè½¯æ€§ï¼‰
      - maxSkew: 2
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: multi-level-app

      containers:
      - name: app
        image: multi-level-app:v1.5
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
```

**å·¥ä½œåŸç†ï¼š**
- **æ­¥éª¤ 1ï¼ˆAZï¼‰**ï¼š18 â†’ us-east-1a(6)ã€us-east-1b(6)ã€us-east-1c(6)
- **æ­¥éª¤ 2ï¼ˆèŠ‚ç‚¹ï¼‰**ï¼šæ¯ä¸ª AZ å†…ï¼Œå„èŠ‚ç‚¹çš„ Pod æ•°é‡å·®å¼‚æœ€å¤šä¸º 2
- èŠ‚ç‚¹çº¦æŸè®¾ä¸ºè½¯æ€§ï¼ˆ`ScheduleAnyway`ï¼‰ä»¥é˜²æ­¢è°ƒåº¦å¤±è´¥

**ä½¿ç”¨åœºæ™¯ï¼š**
- å¤§è§„æ¨¡å‰¯æœ¬éƒ¨ç½²ï¼ˆ10+ å‰¯æœ¬ï¼‰
- åŠ¨æ€èŠ‚ç‚¹æ•°é‡çš„ç¯å¢ƒï¼ˆKarpenter è‡ªåŠ¨ä¼¸ç¼©ï¼‰
- AZ åˆ†å¸ƒæ˜¯å¿…é¡»çš„ï¼ŒèŠ‚ç‚¹åˆ†å¸ƒæ˜¯ä¼˜å…ˆçš„

##### æ¨¡å¼å¯¹æ¯”è¡¨

| æ¨¡å¼ | maxSkew | minDomains | whenUnsatisfiable | é™„åŠ çº¦æŸ | å¤æ‚åº¦ | æ¨èå‰¯æœ¬æ•° |
|------|---------|------------|-------------------|----------|--------|----------------|
| **æ¨¡å¼ 1ï¼šåŸºç¡€ Multi-AZ** | 1 | - | DoNotSchedule | æ—  | ä½ | 3~12 |
| **æ¨¡å¼ 2ï¼šminDomains** | 1 | 3 | DoNotSchedule | æ—  | ä¸­ | 6~20 |
| **æ¨¡å¼ 3ï¼šAnti-Affinity ç»„åˆ** | 1 | 3 | DoNotSchedule | ç¡¬æ€§ Anti-Affinity | é«˜ | 12~50 |
| **æ¨¡å¼ 4ï¼šå¤šçº§ Spread** | 1, 2 | 3 | æ··åˆ | 2 çº§ Topology | é«˜ | 15+ |

##### æ•…éšœæ’é™¤ï¼šTopology Spread å¤±è´¥åŸå› 

| ç—‡çŠ¶ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| Pod å¡åœ¨ Pending çŠ¶æ€ | `maxSkew` è¶…é™æˆ– `minDomains` ä¸æ»¡è¶³ | ä½¿ç”¨ `kubectl describe pod` æ£€æŸ¥ Eventsï¼Œè°ƒæ•´å‰¯æœ¬æ•°æˆ–æ·»åŠ èŠ‚ç‚¹ |
| Pod é›†ä¸­åœ¨ç‰¹å®š AZ | ä½¿ç”¨äº† `whenUnsatisfiable: ScheduleAnyway` | æ”¹ä¸º `DoNotSchedule` ç¡¬æ€§çº¦æŸ |
| æ·»åŠ æ–° AZ åæœªé‡æ–°å‡è¡¡ | è°ƒåº¦å™¨ä¸ä¼šé‡æ–°å‡è¡¡å·²æœ‰ Pod | ä½¿ç”¨ Descheduler æˆ– Rolling Restart |
| è®¾ç½® `minDomains` åæ‰€æœ‰ Pod Pending | é›†ç¾¤ç¼ºå°‘æ‰€éœ€æ•°é‡çš„ AZ | å°† `minDomains` è°ƒæ•´ä¸ºå®é™… AZ æ•°é‡ |

:::tip Topology Spread è°ƒè¯•å‘½ä»¤
```bash
# æ£€æŸ¥å·²æ”¾ç½® Pod çš„ AZ åˆ†å¸ƒ
kubectl get pods -n production -l app=multi-az-app \
  -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,ZONE:.spec.nodeSelector.topology\.kubernetes\.io/zone

# æ£€æŸ¥æ¯ä¸ªèŠ‚ç‚¹çš„ Pod æ•°é‡
kubectl get pods -A -o wide --no-headers | \
  awk '{print $8}' | sort | uniq -c | sort -rn
```
:::

**æ¨èç»„åˆæ¨¡å¼ï¼š**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: best-practice-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: best-practice-app
  template:
    metadata:
      labels:
        app: best-practice-app
    spec:
      # Topology Spreadï¼šè·¨ AZ å‡åŒ€åˆ†å¸ƒï¼ˆç¡¬æ€§ï¼‰
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: best-practice-app
        minDomains: 3
      # Anti-Affinityï¼šè·¨èŠ‚ç‚¹åˆ†å¸ƒï¼ˆè½¯æ€§ï¼‰
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - best-practice-app
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: app:v1.0
```

---

## 5. Taints ä¸ Tolerations

Taints å’Œ Tolerations æ˜¯**èŠ‚ç‚¹çº§æ’æ–¥æœºåˆ¶**ã€‚å½“èŠ‚ç‚¹ä¸Šåº”ç”¨äº† Taint åï¼Œåªæœ‰å®¹å¿è¯¥ Taint çš„ Pod æ‰ä¼šè¢«è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ã€‚

**æ¦‚å¿µï¼š**
- **Taint**ï¼šåº”ç”¨äºèŠ‚ç‚¹ï¼ˆä¾‹å¦‚ï¼Œ"è¯¥èŠ‚ç‚¹æ˜¯ GPU ä¸“ç”¨çš„"ï¼‰
- **Toleration**ï¼šåº”ç”¨äº Podï¼ˆä¾‹å¦‚ï¼Œ"æˆ‘å¯ä»¥å®¹å¿ GPU èŠ‚ç‚¹"ï¼‰

### 5.1 Taint æ•ˆæœ

| æ•ˆæœ | è¡Œä¸º | å¯¹å·²æœ‰ Pod çš„å½±å“ | ä½¿ç”¨æ—¶æœº |
|--------|------|--------------|----------|
| `NoSchedule` | é˜»æ­¢æ–° Pod è°ƒåº¦ | å·²æœ‰ Pod ä¿ç•™ | åˆ›å»ºæ–°çš„ä¸“ç”¨èŠ‚ç‚¹æ—¶ |
| `PreferNoSchedule` | å°½å¯èƒ½é˜»æ­¢è°ƒåº¦ï¼ˆè½¯æ€§ï¼‰ | å·²æœ‰ Pod ä¿ç•™ | ä¼˜å…ˆé¿å¼€ï¼ˆå…è®¸æ›¿ä»£ï¼‰ |
| `NoExecute` | é˜»æ­¢è°ƒåº¦ + é©±é€å·²æœ‰ Pod | å·²æœ‰ Pod ç«‹å³è¢«é©±é€ | èŠ‚ç‚¹ç»´æŠ¤ã€ç´§æ€¥ç–æ•£ |

**Taint åº”ç”¨å‘½ä»¤ï¼š**

```bash
# NoScheduleï¼šé˜»æ­¢æ–° Pod è°ƒåº¦
kubectl taint nodes node1 workload-type=gpu:NoSchedule

# NoExecuteï¼šé˜»æ­¢æ–°è°ƒåº¦ + é©±é€å·²æœ‰ Pod
kubectl taint nodes node1 maintenance=true:NoExecute

# ç§»é™¤ Taintï¼ˆæœ«å°¾åŠ  '-'ï¼‰
kubectl taint nodes node1 workload-type=gpu:NoSchedule-
```

### 5.2 å¸¸è§ Taint æ¨¡å¼

#### æ¨¡å¼ 1ï¼šä¸“ç”¨èŠ‚ç‚¹ç»„ï¼ˆGPUã€é«˜å†…å­˜ï¼‰

```yaml
# åœ¨èŠ‚ç‚¹ä¸Šåº”ç”¨ Taintï¼ˆkubectl æˆ– Karpenterï¼‰
# kubectl taint nodes gpu-node-1 nvidia.com/gpu=present:NoSchedule

# GPU Pod å£°æ˜ Toleration
apiVersion: v1
kind: Pod
metadata:
  name: gpu-job
spec:
  tolerations:
  - key: nvidia.com/gpu
    operator: Equal
    value: present
    effect: NoSchedule
  nodeSelector:
    node.kubernetes.io/instance-type: g5.2xlarge
  containers:
  - name: trainer
    image: ml/trainer:v1.0
    resources:
      limits:
        nvidia.com/gpu: 1
```

#### æ¨¡å¼ 2ï¼šç³»ç»Ÿå·¥ä½œè´Ÿè½½éš”ç¦»

```yaml
# ä½¿ç”¨ Karpenter åˆ›å»ºç³»ç»Ÿä¸“ç”¨ NodePool
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: system-pool
spec:
  template:
    spec:
      requirements:
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["c6i.large", "c6i.xlarge"]
      taints:
      - key: workload-type
        value: system
        effect: NoSchedule
  limits:
    cpu: "20"
---
# ç³»ç»Ÿ DaemonSetï¼ˆç›‘æ§ä»£ç†ï¼‰
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-agent
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      tolerations:
      - key: workload-type
        operator: Equal
        value: system
        effect: NoSchedule
      # ç”±äºå¿…é¡»éƒ¨ç½²åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šï¼Œè¿˜éœ€å®¹å¿é»˜è®¤ Taint
      - key: node.kubernetes.io/not-ready
        operator: Exists
        effect: NoExecute
      - key: node.kubernetes.io/unreachable
        operator: Exists
        effect: NoExecute
      containers:
      - name: agent
        image: monitoring-agent:v2.0
```

#### æ¨¡å¼ 3ï¼šèŠ‚ç‚¹ç»´æŠ¤ï¼ˆDrain å‡†å¤‡ï¼‰

```bash
# æ­¥éª¤ 1ï¼šåœ¨èŠ‚ç‚¹ä¸Šåº”ç”¨ NoExecute Taint
kubectl taint nodes node-1 maintenance=true:NoExecute

# ç»“æœï¼šæ‰€æœ‰æ²¡æœ‰åŒ¹é… Toleration çš„ Pod ç«‹å³è¢«é©±é€å¹¶è¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹
# å¦‚æœé…ç½®äº† PDBï¼Œé©±é€ä¼šéµå®ˆ minAvailable å¹¶æŒ‰é¡ºåºè¿›è¡Œ

# æ­¥éª¤ 2ï¼šç»´æŠ¤å®Œæˆåç§»é™¤ Taint
kubectl taint nodes node-1 maintenance=true:NoExecute-
kubectl uncordon node-1
```

### 5.3 Toleration é…ç½®

#### Operatorï¼šEqual vs Exists

```yaml
# Equalï¼šéœ€è¦ç²¾ç¡®çš„ key=value åŒ¹é…
tolerations:
- key: workload-type
  operator: Equal
  value: gpu
  effect: NoSchedule

# Existsï¼šä»…éœ€ key å­˜åœ¨ï¼ˆå¿½ç•¥ valueï¼‰
tolerations:
- key: workload-type
  operator: Exists
  effect: NoSchedule

# å®¹å¿æ‰€æœ‰ Taintï¼ˆç”¨äº DaemonSet ç­‰ï¼‰
tolerations:
- operator: Exists
```

#### tolerationSecondsï¼ˆä»…é™ NoExecuteï¼‰

å½“åº”ç”¨ `NoExecute` Taint æ—¶ï¼ŒPod é»˜è®¤ç«‹å³è¢«é©±é€ï¼Œä½† `tolerationSeconds` å¯ä»¥æä¾›å®½é™æœŸã€‚

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resilient-app
spec:
  tolerations:
  # å³ä½¿èŠ‚ç‚¹å˜ä¸º NotReady ä¹Ÿåœç•™ 300 ç§’ï¼ˆç¬æ€æ•…éšœåº”å¯¹ï¼‰
  - key: node.kubernetes.io/not-ready
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 300
  # å³ä½¿èŠ‚ç‚¹å˜ä¸º Unreachable ä¹Ÿåœç•™ 300 ç§’
  - key: node.kubernetes.io/unreachable
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 300
  containers:
  - name: app
    image: app:v1.0
```

**é»˜è®¤å€¼**ï¼šæœªæŒ‡å®š `tolerationSeconds` æ—¶ï¼ŒKubernetes ä½¿ç”¨ä»¥ä¸‹é»˜è®¤å€¼ï¼š
- `node.kubernetes.io/not-ready`ï¼š300 ç§’
- `node.kubernetes.io/unreachable`ï¼š300 ç§’

### 5.4 EKS é»˜è®¤ Taint

EKS è‡ªåŠ¨å¯¹æŸäº›èŠ‚ç‚¹åº”ç”¨ Taintï¼š

| Taint | åº”ç”¨å¯¹è±¡ | æ•ˆæœ | åº”å¯¹æ–¹å¼ |
|-------|----------|------|----------|
| `node.kubernetes.io/not-ready` | æœªå°±ç»ªçš„èŠ‚ç‚¹ | NoExecute | è‡ªåŠ¨ Tolerationï¼ˆkubeletï¼‰ |
| `node.kubernetes.io/unreachable` | ä¸å¯è¾¾çš„èŠ‚ç‚¹ | NoExecute | è‡ªåŠ¨ Tolerationï¼ˆkubeletï¼‰ |
| `node.kubernetes.io/disk-pressure` | ç£ç›˜ç©ºé—´ä¸è¶³çš„èŠ‚ç‚¹ | NoSchedule | ä»… DaemonSet å®¹å¿ |
| `node.kubernetes.io/memory-pressure` | å†…å­˜ä¸è¶³çš„èŠ‚ç‚¹ | NoSchedule | ä»… DaemonSet å®¹å¿ |
| `node.kubernetes.io/pid-pressure` | PID ä¸è¶³çš„èŠ‚ç‚¹ | NoSchedule | ä»… DaemonSet å®¹å¿ |
| `node.kubernetes.io/network-unavailable` | ç½‘ç»œæœªé…ç½®çš„èŠ‚ç‚¹ | NoSchedule | ç”± CNI æ’ä»¶ç§»é™¤ |

### 5.5 Karpenter ä¸­çš„ Taint ç®¡ç†

Karpenter åœ¨ NodePool ä¸­ä»¥å£°æ˜å¼æ–¹å¼ç®¡ç† Taintï¼š

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-pool
spec:
  template:
    spec:
      requirements:
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["g5.xlarge", "g5.2xlarge"]
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand"]
      # èŠ‚ç‚¹é…ç½®æ—¶è‡ªåŠ¨åº”ç”¨ Taint
      taints:
      - key: nvidia.com/gpu
        value: present
        effect: NoSchedule
      - key: workload-type
        value: ml
        effect: NoSchedule
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: gpu-nodes
  limits:
    cpu: "100"
    memory: 500Gi
```

ç”±äº Taint ä¼šè‡ªåŠ¨åº”ç”¨åˆ° Karpenter é…ç½®çš„æ‰€æœ‰èŠ‚ç‚¹ä¸Šï¼Œå› æ­¤æ— éœ€æ‰‹åŠ¨æ‰§è¡Œ `kubectl taint` å‘½ä»¤ã€‚

### 5.6 ä» Cluster Autoscaler è¿ç§»åˆ° Karpenter

Cluster Autoscaler å’Œ Karpenter éƒ½æä¾›èŠ‚ç‚¹è‡ªåŠ¨ä¼¸ç¼©ï¼Œä½†å®ƒä»¬ä½¿ç”¨æ ¹æœ¬ä¸åŒçš„æ–¹æ³•ã€‚æœ¬èŠ‚ä»‹ç»è°ƒåº¦è¡Œä¸ºçš„å·®å¼‚å¹¶æä¾›è¿ç§»æ£€æŸ¥æ¸…å•ã€‚

#### 5.6.1 è°ƒåº¦è¡Œä¸ºå·®å¼‚

Cluster Autoscaler å’Œ Karpenter çš„å…³é”®åŒºåˆ«åœ¨äº**èŠ‚ç‚¹é…ç½®æ–¹å¼**å’Œ**ä¸ Pod è°ƒåº¦çš„é›†æˆç¨‹åº¦**ã€‚

##### è¡Œä¸ºå¯¹æ¯”

| å¯¹æ¯”é¡¹ | Cluster Autoscaler | Karpenter |
|----------|-------------------|-----------|
| **è§¦å‘æ–¹å¼** | æ£€æµ‹ Pending Pod â†’ è¯·æ±‚ ASG æ‰©å±• | æ£€æµ‹ Pending Pod â†’ ç«‹å³é…ç½® EC2 |
| **æ‰©å±•é€Ÿåº¦** | æ•°åç§’åˆ°æ•°åˆ†é’Ÿï¼ˆASG ç­‰å¾…æ—¶é—´ï¼‰ | ç§’çº§ï¼ˆç›´æ¥ EC2 API è°ƒç”¨ï¼‰ |
| **èŠ‚ç‚¹é€‰æ‹©** | ä»é¢„å®šä¹‰çš„ ASG ç»„ä¸­é€‰æ‹© | æ ¹æ® Pod éœ€æ±‚å®æ—¶é€‰æ‹©å®ä¾‹ç±»å‹ |
| **å®ä¾‹ç±»å‹å¤šæ ·æ€§** | æ¯ä¸ª ASG å›ºå®šç±»å‹ï¼ˆLaunchTemplateï¼‰ | ä» 100+ ç±»å‹ä¸­æœ€ä¼˜é€‰æ‹©ï¼ˆNodePool requirementsï¼‰ |
| **æˆæœ¬ä¼˜åŒ–** | éœ€è¦æ‰‹åŠ¨é…ç½® ASG | è‡ªåŠ¨ Spot/On-Demand æ··åˆï¼Œé€‰æ‹©æœ€ä½ä»·æ ¼ |
| **è£…ç®±æ•ˆç‡** | æœ‰é™ï¼ˆASG çº§åˆ«ï¼‰ | é«˜çº§ï¼ˆæ„ŸçŸ¥ Pod éœ€æ±‚ï¼‰ |
| **Taints/Tolerations æ„ŸçŸ¥** | æœ‰é™ | åŸç”Ÿé›†æˆ |
| **Topology Spread æ„ŸçŸ¥** | æœ‰é™ | åŸç”Ÿé›†æˆ |
| **é›†æˆç¨‹åº¦** | Kubernetes çš„å¤–éƒ¨å·¥å…· | Kubernetes åŸç”Ÿï¼ˆåŸºäº CRDï¼‰ |

##### æ‰©å±•åœºæ™¯ç¤ºä¾‹

**åœºæ™¯ï¼šåˆ›å»ºäº† 3 ä¸ªè¯·æ±‚ GPU çš„ Pod**

**Cluster Autoscaler è¡Œä¸ºï¼š**
```
1. 3 ä¸ª Pod è¿›å…¥ Pending çŠ¶æ€ï¼ˆGPU è¯·æ±‚ï¼‰
2. Cluster Autoscaler æ¯ 10 ç§’æ‰«æ Pending Pod
3. æ‰¾åˆ° GPU ASG å¹¶è¯·æ±‚æ‰©å±•ï¼ˆä¾‹å¦‚ g5.2xlarge ASGï¼‰
4. AWS ASG å¼€å§‹èŠ‚ç‚¹é…ç½®ï¼ˆ30~90 ç§’ï¼‰
5. èŠ‚ç‚¹ Ready åï¼Œkubelet è°ƒåº¦ Pod
6. æ€»è€—æ—¶ï¼š1~2 åˆ†é’Ÿ
```

**Karpenter è¡Œä¸ºï¼š**
```
1. 3 ä¸ª Pod è¿›å…¥ Pending çŠ¶æ€ï¼ˆGPU è¯·æ±‚ï¼‰
2. Karpenter ç«‹å³æ£€æµ‹ï¼ˆ1~2 ç§’ï¼‰
3. æ ¹æ® NodePool requirements é€‰æ‹©æœ€ä¼˜å®ä¾‹ï¼ˆg5.xlargeã€g5.2xlarge ä¹‹ä¸­ï¼‰
4. ç›´æ¥è°ƒç”¨ EC2 RunInstances API
5. èŠ‚ç‚¹ Ready åè°ƒåº¦ Pod
6. æ€»è€—æ—¶ï¼š30~45 ç§’
```

##### æˆæœ¬ä¼˜åŒ–å·®å¼‚

**Cluster Autoscalerï¼š**
- æ¯ä¸ª ASG éœ€è¦åˆ†åˆ«é…ç½® Spot/On-Demand
- æ›´æ”¹å®ä¾‹ç±»å‹éœ€è¦æ‰‹åŠ¨æ›´æ–° LaunchTemplate
- å¯èƒ½å‡ºç°è¿‡åº¦é…ç½®

**Karpenterï¼š**
- åœ¨ NodePool ä¸­å£°æ˜å¼é…ç½® Spot/On-Demand ä¼˜å…ˆçº§
- å®æ—¶é€‰æ‹©æœ€ä¾¿å®œçš„å®ä¾‹ç±»å‹
- é…ç½®ç²¾ç¡®åŒ¹é… Pod éœ€æ±‚çš„èŠ‚ç‚¹

**æˆæœ¬èŠ‚çº¦ç¤ºä¾‹ï¼ˆå®æµ‹æ•°æ®ï¼‰ï¼š**
```yaml
# Cluster Autoscalerï¼šå›ºå®š ASG
# m5.2xlarge (8 vCPU, 32GB) â†’ $0.384/å°æ—¶
# â†’ å³ä½¿ Pod ä»…è¯·æ±‚ 2 vCPU ä¹Ÿéœ€æ”¯ä»˜æ•´ä¸ªèŠ‚ç‚¹è´¹ç”¨

# Karpenterï¼šçµæ´»é€‰æ‹©
# m5.large (2 vCPU, 8GB) â†’ $0.096/å°æ—¶
# â†’ é€‰æ‹©åŒ¹é… Pod éœ€æ±‚çš„è¾ƒå°èŠ‚ç‚¹
# â†’ æˆæœ¬èŠ‚çº¦ 75%
```

#### 5.6.2 è¿ç§»æ£€æŸ¥æ¸…å•

ä» Cluster Autoscaler å®‰å…¨è¿‡æ¸¡åˆ° Karpenter çš„åˆ†æ­¥æŒ‡å—ã€‚

##### æ­¥éª¤ 1ï¼šå®šä¹‰ NodePoolï¼ˆASG â†’ NodePool æ˜ å°„ï¼‰

å°†ç°æœ‰ ASG è®¾ç½®è½¬æ¢ä¸º Karpenter NodePool CRDã€‚

**ç°æœ‰ Cluster Autoscaler é…ç½®ï¼š**
```yaml
# ASG: eks-general-purpose-asg
# - å®ä¾‹ç±»å‹: m5.xlarge, m5.2xlarge
# - å®¹é‡ç±»å‹: On-Demand
# - AZ: us-east-1a, us-east-1b, us-east-1c
```

**Karpenter NodePool è½¬æ¢ï¼š**
```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general-purpose
spec:
  template:
    spec:
      requirements:
      # å®ä¾‹ç±»å‹ï¼šå–è‡ª ASG LaunchTemplate
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["m5.xlarge", "m5.2xlarge", "m5a.xlarge", "m5a.2xlarge"]

      # å®¹é‡ç±»å‹ï¼šä¼˜å…ˆ On-Demandï¼Œå…è®¸ Spot
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]

      # AZï¼šç»´æŒç°æœ‰ ASG çš„ AZ
      - key: topology.kubernetes.io/zone
        operator: In
        values: ["us-east-1a", "us-east-1b", "us-east-1c"]

      # æ¶æ„ï¼šä»… x86_64ï¼ˆæ’é™¤ ARMï¼‰
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]

      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default

  # èµ„æºé™åˆ¶ï¼šåŸºäº ASG Max Size
  limits:
    cpu: "1000"
    memory: 1000Gi

  # ä¸­æ–­ç­–ç•¥ï¼šå¯ç”¨ Consolidation
  disruption:
    consolidationPolicy: WhenUnderutilized
    expireAfter: 720h  # 30 å¤©
```

**è½¬æ¢æŒ‡å—ï¼š**

| ASG è®¾ç½® | NodePool å­—æ®µ | å¤‡æ³¨ |
|---------|--------------|------|
| LaunchTemplate å®ä¾‹ç±»å‹ | `requirements[instance-type]` | å»ºè®®èŒƒå›´æ›´å¹¿ï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰ |
| Spot/On-Demand | `requirements[capacity-type]` | æ”¹ä¸ºä¼˜å…ˆçº§æ•°ç»„ |
| å­ç½‘ï¼ˆAZï¼‰ | `requirements[zone]` | ä¹Ÿå¯ä½¿ç”¨ SubnetSelector |
| Max Size | `limits.cpu`ã€`limits.memory` | è½¬æ¢ä¸ºæ€» vCPU/å†…å­˜ |
| æ ‡ç­¾ | `EC2NodeClass.tags` | ç”¨äºå®‰å…¨ã€æˆæœ¬è¿½è¸ªæ ‡ç­¾ |

##### æ­¥éª¤ 2ï¼šéªŒè¯ Taints/Tolerations å…¼å®¹æ€§

ç°æœ‰ ASG ä¸Šåº”ç”¨çš„ Taint å¿…é¡»åœ¨ NodePool ä¸­åŒæ ·åº”ç”¨ã€‚

**ç°æœ‰ ASG Taintï¼ˆUserData è„šæœ¬ï¼‰ï¼š**
```bash
# /etc/eks/bootstrap.sh é€‰é¡¹
--kubelet-extra-args '--register-with-taints=workload-type=batch:NoSchedule'
```

**Karpenter NodePool Taintï¼š**
```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: batch-workload
spec:
  template:
    spec:
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot"]  # æ‰¹å¤„ç†ä½¿ç”¨ Spot

      # åº”ç”¨ Taintï¼šä¸ç°æœ‰ ASG ç›¸åŒ
      taints:
      - key: workload-type
        value: batch
        effect: NoSchedule
```

**éªŒè¯å‘½ä»¤ï¼š**
```bash
# æ£€æŸ¥ç°æœ‰ ASG èŠ‚ç‚¹ä¸Šçš„ Taint
kubectl get nodes -l eks.amazonaws.com/nodegroup=batch-asg \
  -o jsonpath='{.items[*].spec.taints}' | jq

# æ£€æŸ¥ Karpenter èŠ‚ç‚¹ä¸Šçš„ Taint
kubectl get nodes -l karpenter.sh/nodepool=batch-workload \
  -o jsonpath='{.items[*].spec.taints}' | jq

# éªŒè¯æ˜¯å¦ä¸€è‡´
```

##### æ­¥éª¤ 3ï¼šéªŒè¯ PDBï¼ˆè¿ç§»æœŸé—´æœ€å°åŒ–ä¸­æ–­ï¼‰

PodDisruptionBudget å¿…é¡»æ­£ç¡®é…ç½®ï¼Œä»¥åœ¨è¿ç§»æœŸé—´æœ€å¤§é™åº¦å‡å°‘ Pod ä¸­æ–­ã€‚

**æ£€æŸ¥ PDB é…ç½®ï¼š**
```bash
# åˆ—å‡ºæ‰€æœ‰ PDB
kubectl get pdb -A

# æ£€æŸ¥ç‰¹å®š PDB è¯¦æƒ…
kubectl describe pdb api-server-pdb -n production
```

**æ¨èçš„ PDB é…ç½®ï¼ˆç”¨äºè¿ç§»ï¼‰ï¼š**
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
  namespace: production
spec:
  minAvailable: 2  # è¿ç§»æœŸé—´è‡³å°‘ç»´æŒ 2 ä¸ª
  selector:
    matchLabels:
      app: critical-app
```

**éªŒè¯æ£€æŸ¥æ¸…å•ï¼š**
- [ ] éªŒè¯æ‰€æœ‰ç”Ÿäº§å·¥ä½œè´Ÿè½½çš„ PDB é…ç½®
- [ ] ç¡®ä¿ `minAvailable` æˆ– `maxUnavailable` è®¾ç½®åˆç†
- [ ] ç‰¹åˆ«æ³¨æ„ StatefulSetï¼ˆéªŒè¯é¡ºåºå…³é—­ï¼‰

##### æ­¥éª¤ 4ï¼šé‡æ–°éªŒè¯ Topology Spread

Karpenter åŸç”Ÿæ”¯æŒ Topology Spread Constraintsï¼Œä½†ç°æœ‰é…ç½®åº”é‡æ–°éªŒè¯ã€‚

**éªŒè¯è¦ç‚¹ï¼š**

| é¡¹ç›® | æ£€æŸ¥å†…å®¹ |
|------|----------|
| **maxSkew** | å½±å“ Karpenter åœ¨å“ªä¸ª AZ åˆ›å»ºæ–°èŠ‚ç‚¹ |
| **minDomains** | éªŒè¯æ˜¯å¦ä¸é›†ç¾¤ä¸­å®é™… AZ æ•°é‡åŒ¹é… |
| **whenUnsatisfiable** | ä½¿ç”¨ `DoNotSchedule` æ—¶ï¼Œå³ä½¿ Karpenter åˆ›å»ºäº†èŠ‚ç‚¹ Pod ä¹Ÿå¯èƒ½ä¿æŒ Pending |

**ç¤ºä¾‹ï¼šè°ƒè¯• Topology Spread é—®é¢˜**
```bash
# æ£€æŸ¥ Pod ä¸ºä½•å¤„äº Pending çŠ¶æ€
kubectl describe pod my-app-xyz -n production

# Events éƒ¨åˆ†å¯è§çš„æ¶ˆæ¯ï¼š
# "0/10 nodes are available: 3 node(s) didn't match pod topology spread constraints."

# è§£å†³æ–¹æ¡ˆï¼šæ”¾å®½ maxSkew æˆ–è°ƒæ•´å‰¯æœ¬æ•°
```

##### æ­¥éª¤ 5ï¼šè¿‡æ¸¡ç›‘æ§ï¼ˆæŒ‡æ ‡å˜æ›´ï¼‰

Cluster Autoscaler å’Œ Karpenter æä¾›ä¸åŒçš„æŒ‡æ ‡ã€‚

**Cluster Autoscaler æŒ‡æ ‡ï¼š**
```promql
# ä¹‹å‰çš„æŒ‡æ ‡ç¤ºä¾‹
cluster_autoscaler_scaled_up_nodes_total
cluster_autoscaler_scaled_down_nodes_total
cluster_autoscaler_unschedulable_pods_count
```

**Karpenter æŒ‡æ ‡ï¼š**
```promql
# æ–°çš„æŒ‡æ ‡ç¤ºä¾‹
karpenter_nodes_created
karpenter_nodes_terminated
karpenter_pods_startup_duration_seconds
karpenter_disruption_queue_depth
karpenter_nodepool_usage
```

**CloudWatch Dashboard æ›´æ–°ï¼š**
```yaml
# CloudWatch Container Insights widget ç¤ºä¾‹
{
  "type": "metric",
  "properties": {
    "metrics": [
      [ "AWS/Karpenter", "NodesCreated", { "stat": "Sum" } ],
      [ ".", "NodesTerminated", { "stat": "Sum" } ],
      [ ".", "PendingPods", { "stat": "Average" } ]
    ],
    "period": 300,
    "stat": "Average",
    "region": "us-east-1",
    "title": "Karpenter èŠ‚ç‚¹è‡ªåŠ¨ä¼¸ç¼©"
  }
}
```

**å‘Šè­¦è¿‡æ¸¡æ£€æŸ¥æ¸…å•ï¼š**
- [ ] ç¦ç”¨ Cluster Autoscaler å‘Šè­¦
- [ ] åŸºäº Karpenter æŒ‡æ ‡åˆ›å»ºæ–°å‘Šè­¦
- [ ] èŠ‚ç‚¹åˆ›å»ºå¤±è´¥å‘Šè­¦ï¼ˆ`karpenter_nodeclaims_created{reason="failed"}`ï¼‰
- [ ] æŒç»­ Pending Pod å‘Šè­¦ï¼ˆ`karpenter_pods_state{state="pending"} > 5`ï¼‰

##### æ­¥éª¤ 6ï¼šåˆ†é˜¶æ®µè¿ç§»ç­–ç•¥

é€šè¿‡æŒ‰é¡ºåºè¿ç§»å·¥ä½œè´Ÿè½½æ¥æœ€å°åŒ–é£é™©ã€‚

**é˜¶æ®µ 1ï¼šéç”Ÿäº§å·¥ä½œè´Ÿè½½ï¼ˆç¬¬ 1-2 å‘¨ï¼‰**
```yaml
# ä» dev/staging å‘½åç©ºé—´å¼€å§‹
# 1. åˆ›å»º Karpenter NodePoolï¼ˆdev-workloadï¼‰
# 2. åœ¨ç°æœ‰ ASG èŠ‚ç‚¹ä¸Šæ·»åŠ  Taintï¼ˆé˜»æ­¢æ–° Podï¼‰
kubectl taint nodes -l eks.amazonaws.com/nodegroup=dev-asg \
  migration=in-progress:NoSchedule

# 3. æ»šåŠ¨é‡å¯ dev å·¥ä½œè´Ÿè½½
kubectl rollout restart deployment -n dev --all

# 4. éªŒè¯æ–° Pod è¢«è°ƒåº¦åˆ° Karpenter èŠ‚ç‚¹
kubectl get pods -n dev -o wide

# 5. ç¼©å‡ç°æœ‰ ASG
```

**é˜¶æ®µ 2ï¼šç”Ÿäº§å·¥ä½œè´Ÿè½½ï¼ˆç¬¬ 3-4 å‘¨ï¼‰**
```yaml
# é‡‘ä¸é›€éƒ¨ç½²æ–¹å¼ï¼šä»…å°†éƒ¨åˆ†å‰¯æœ¬è¿ç§»åˆ° Karpenter
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server-karpenter
  namespace: production
spec:
  replicas: 2  # ç°æœ‰ 10 ä¸ªä¸­ä»… 2 ä¸ª
  selector:
    matchLabels:
      app: api-server
      migration: karpenter
  template:
    metadata:
      labels:
        app: api-server
        migration: karpenter
    spec:
      # ç§»é™¤ NodeSelectorï¼ˆKarpenter è‡ªåŠ¨é€‰æ‹©ï¼‰
      # nodeSelector:
      #   eks.amazonaws.com/nodegroup: prod-asg  # ç§»é™¤
      containers:
      - name: api
        image: api-server:v3.0
```

**é˜¶æ®µ 3ï¼šå¹¶è¡Œè¿è¡ŒéªŒè¯ï¼ˆç¬¬ 5-6 å‘¨ï¼‰**
- åŒæ—¶è¿è¡Œ Cluster Autoscaler å’Œ Karpenter
- ç›‘æ§æµé‡æ¨¡å¼
- å¯¹æ¯”æˆæœ¬åˆ†æ
- å¯¹æ¯”æ‰©å±•é€Ÿåº¦

**é˜¶æ®µ 4ï¼šå®Œå…¨è¿‡æ¸¡ï¼ˆç¬¬ 7-8 å‘¨ï¼‰**
```bash
# 1. éªŒè¯æ‰€æœ‰å·¥ä½œè´Ÿè½½è¿è¡Œåœ¨ Karpenter èŠ‚ç‚¹ä¸Š
kubectl get pods -A -o wide | grep -v karpenter

# 2. ç¦ç”¨ Cluster Autoscaler
kubectl scale deployment cluster-autoscaler \
  -n kube-system --replicas=0

# 3. åˆ é™¤ç°æœ‰ ASG
aws autoscaling delete-auto-scaling-group \
  --auto-scaling-group-name eks-prod-asg \
  --force-delete

# 4. åˆ é™¤ Cluster Autoscaler Deployment
kubectl delete deployment cluster-autoscaler -n kube-system
```

#### 5.6.3 å¹¶è¡Œè¿è¡Œæ¨¡å¼ï¼ˆCluster Autoscaler + Karpenterï¼‰

åœ¨è¿ç§»æœŸé—´å®‰å…¨åœ°åŒæ—¶è¿è¡Œä¸¤ä¸ªè‡ªåŠ¨ä¼¸ç¼©å™¨çš„æ–¹æ³•ã€‚

##### å†²çªé¢„é˜²é…ç½®

**1. åœ¨ NodePool ä¸­æ’é™¤èŠ‚ç‚¹ç»„**

é…ç½® Karpenter ä¸å¹²é¢„ Cluster Autoscaler ç®¡ç†çš„èŠ‚ç‚¹ã€‚

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: karpenter-only
spec:
  template:
    spec:
      requirements:
      # æ’é™¤ Cluster Autoscaler ç®¡ç†çš„èŠ‚ç‚¹
      - key: eks.amazonaws.com/nodegroup
        operator: DoesNotExist  # ä»…ç®¡ç†æ²¡æœ‰ NodeGroup æ ‡ç­¾çš„èŠ‚ç‚¹

      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]
```

**2. åœ¨ Cluster Autoscaler ä¸­æ’é™¤èŠ‚ç‚¹**

é…ç½® Cluster Autoscaler ä¸ç¼©å‡ Karpenter ç®¡ç†çš„èŠ‚ç‚¹ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  template:
    spec:
      containers:
      - name: cluster-autoscaler
        image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.30.0
        command:
        - ./cluster-autoscaler
        - --v=4
        - --cloud-provider=aws
        - --skip-nodes-with-system-pods=false
        # æ’é™¤ Karpenter èŠ‚ç‚¹
        - --skip-nodes-with-local-storage=false
        - --balance-similar-node-groups
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/my-cluster
```

**3. é€šè¿‡ Pod NodeSelector æ˜¾å¼åˆ†ç¦»**

æ˜¾å¼æŒ‡å®šç‰¹å®šå·¥ä½œè´Ÿè½½åº”æ”¾ç½®åœ¨å“ªä¸ªè‡ªåŠ¨ä¼¸ç¼©å™¨çš„èŠ‚ç‚¹ä¸Šã€‚

```yaml
# æ”¾ç½®åœ¨ Cluster Autoscaler èŠ‚ç‚¹ä¸Š
apiVersion: apps/v1
kind: Deployment
metadata:
  name: legacy-app
spec:
  template:
    spec:
      nodeSelector:
        eks.amazonaws.com/nodegroup: prod-asg  # ä»… ASG èŠ‚ç‚¹
---
# æ”¾ç½®åœ¨ Karpenter èŠ‚ç‚¹ä¸Š
apiVersion: apps/v1
kind: Deployment
metadata:
  name: new-app
spec:
  template:
    spec:
      nodeSelector:
        karpenter.sh/nodepool: general-purpose  # ä»… Karpenter èŠ‚ç‚¹
```

##### å¹¶è¡Œè¿è¡Œæ£€æŸ¥æ¸…å•

- [ ] åœ¨ NodePool ä¸­è®¾ç½® `eks.amazonaws.com/nodegroup: DoesNotExist`
- [ ] åœ¨ Cluster Autoscaler ä¸­æ·»åŠ  Karpenter èŠ‚ç‚¹æ’é™¤æ ‡å¿—
- [ ] ä¸ºæ¯ä¸ªå·¥ä½œè´Ÿè½½é…ç½® NodeSelector æˆ– NodeAffinity
- [ ] åŒæ—¶ç›‘æ§ä¸¤ä¸ªè‡ªåŠ¨ä¼¸ç¼©å™¨çš„æŒ‡æ ‡
- [ ] åˆ›å»ºæˆæœ¬å¯¹æ¯”ä»ªè¡¨æ¿
- [ ] å»ºç«‹å›æ»šè®¡åˆ’ï¼ˆKarpenter å‡ºç°é—®é¢˜æ—¶å›é€€åˆ° ASGï¼‰

:::warning å¹¶è¡Œè¿è¡Œæ³¨æ„äº‹é¡¹
åŒæ—¶è¿è¡Œ Cluster Autoscaler å’Œ Karpenter å¯èƒ½å¯¼è‡´ä»¥ä¸‹é—®é¢˜ï¼š
- èŠ‚ç‚¹é…ç½®ç«äº‰æ¡ä»¶ï¼ˆä¸¤ä¸ªè‡ªåŠ¨ä¼¸ç¼©å™¨åŒæ—¶å¤„ç†ç›¸åŒå·¥ä½œè´Ÿè½½ï¼‰
- æˆæœ¬é¢„æµ‹å›°éš¾ï¼ˆéœ€è¦è¿½è¸ªå“ªä¸ªè‡ªåŠ¨ä¼¸ç¼©å™¨åˆ›å»ºäº†å“ªä¸ªèŠ‚ç‚¹ï¼‰
- è°ƒè¯•å¤æ‚åº¦å¢åŠ 

**æ¨èåšæ³•ï¼š**
- å¹¶è¡Œè¿è¡ŒæœŸé™æœ€é•¿ 2 å‘¨
- ä¿æŒæ¸…æ™°çš„å·¥ä½œè´Ÿè½½åˆ†ç¦»ï¼ˆå¿…é¡»ä½¿ç”¨ NodeSelectorï¼‰
- å»ºç«‹åˆ†é˜¶æ®µè¿‡æ¸¡è®¡åˆ’
:::

##### å›æ»šæµç¨‹

å¦‚æœè¿‡æ¸¡åˆ° Karpenter åå‡ºç°é—®é¢˜ï¼Œä»¥ä¸‹æ˜¯å›é€€åˆ° Cluster Autoscaler çš„æ–¹æ³•ã€‚

```bash
# 1. åˆ é™¤ Karpenter NodePoolï¼ˆèŠ‚ç‚¹ä¿ç•™ï¼‰
kubectl delete nodepool --all

# 2. é‡æ–°å¯ç”¨ Cluster Autoscaler
kubectl scale deployment cluster-autoscaler \
  -n kube-system --replicas=1

# 3. æ‰©å±•ç°æœ‰ ASG
aws autoscaling set-desired-capacity \
  --auto-scaling-group-name eks-prod-asg \
  --desired-capacity 10

# 4. åœ¨ Karpenter èŠ‚ç‚¹ä¸Šæ·»åŠ  Taintï¼ˆé˜»æ­¢æ–° Podï¼‰
kubectl taint nodes -l karpenter.sh/nodepool \
  rollback=true:NoSchedule

# 5. æ»šåŠ¨é‡å¯å·¥ä½œè´Ÿè½½
kubectl rollout restart deployment -n production --all

# 6. åˆ é™¤ Karpenter èŠ‚ç‚¹
kubectl delete nodes -l karpenter.sh/nodepool
```

---

## 6. PodDisruptionBudget (PDB) é«˜çº§æ¨¡å¼

PodDisruptionBudget åœ¨**è‡ªæ„¿ä¸­æ–­**æœŸé—´ä¿è¯æœ€ä½ Pod å¯ç”¨æ€§ã€‚

### 6.1 PDB åŸºç¡€å›é¡¾

:::info PDB åŸºæœ¬æ¦‚å¿µ
PDB çš„åŸºæœ¬æ¦‚å¿µåŠå…¶ä¸ Karpenter çš„äº¤äº’åœ¨ [EKS é«˜å¯ç”¨æ¶æ„æŒ‡å—](/docs/operations-observability/eks-resiliency-guide#poddisruptionbudgets-pdb) ä¸­ä»‹ç»ã€‚æœ¬èŠ‚é‡ç‚¹å…³æ³¨é«˜çº§æ¨¡å¼å’Œæ•…éšœæ’é™¤ã€‚
:::

**è‡ªæ„¿ vs éè‡ªæ„¿ä¸­æ–­ï¼š**

| ä¸­æ–­ç±»å‹ | ç¤ºä¾‹ | PDB æ˜¯å¦é€‚ç”¨ | ç¼“è§£æªæ–½ |
|----------|------|---------|----------|
| **è‡ªæ„¿** | Node Drainã€é›†ç¾¤å‡çº§ã€Karpenter Consolidation | æ˜¯ | PDB é…ç½® |
| **éè‡ªæ„¿** | èŠ‚ç‚¹å´©æºƒã€OOM Killã€ç¡¬ä»¶æ•…éšœã€AZ æ•…éšœ | å¦ | å¢åŠ å‰¯æœ¬æ•°ã€Anti-Affinity |

### 6.2 é«˜çº§ PDB ç­–ç•¥

#### ç­–ç•¥ 1ï¼šRolling Update + PDB ç»„åˆ

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2         # å…è®¸æœ€å¤šæ‰©å±•åˆ° 12 ä¸ª
      maxUnavailable: 0   # ä»»ä½•æ—¶å€™éƒ½ä¸å…è®¸æœ‰ä¸å¯ç”¨ Podï¼ˆé›¶åœæœºéƒ¨ç½²ï¼‰
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      containers:
      - name: api
        image: api-server:v3.0
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-server-pdb
spec:
  minAvailable: 8  # å§‹ç»ˆä¿æŒè‡³å°‘ 8 ä¸ª Podï¼ˆ80% å¯ç”¨æ€§ï¼‰
  selector:
    matchLabels:
      app: api-server
```

**æ•ˆæœï¼š**
- Rolling Update æœŸé—´ï¼š`maxUnavailable: 0` ç¡®ä¿æ–° Pod Ready å‰ä¿ç•™ç°æœ‰ Pod
- Node Drain æœŸé—´ï¼šPDB ä¿è¯æœ€å°‘ 8 ä¸ª Pod â†’ æœ€å¤šåŒæ—¶é©±é€ 2 ä¸ª Pod

#### ç­–ç•¥ 2ï¼šStatefulSet + PDBï¼ˆæ•°æ®åº“é›†ç¾¤ï¼‰

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
spec:
  serviceName: cassandra
  replicas: 5
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      containers:
      - name: cassandra
        image: cassandra:4.1
        ports:
        - containerPort: 9042
          name: cql
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: cassandra-pdb
spec:
  maxUnavailable: 1  # æ¯æ¬¡ä»…å…è®¸ 1 ä¸ªèŠ‚ç‚¹ä¸­æ–­ï¼ˆç»´æŒæ³•å®šäººæ•°ï¼‰
  selector:
    matchLabels:
      app: cassandra
```

**æ•ˆæœï¼š**
- åœ¨ç»´æŒ Cassandra æ³•å®šäººæ•°ï¼ˆ5 ä¸ªä¸­ 3 ä¸ªä»¥ä¸Šï¼‰çš„åŒæ—¶å®‰å…¨åœ° drain èŠ‚ç‚¹
- Karpenter consolidation æœŸé—´é€ä¸ªç§»é™¤èŠ‚ç‚¹

#### ç­–ç•¥ 3ï¼šåŸºäºç™¾åˆ†æ¯”çš„ PDBï¼ˆå¤§è§„æ¨¡éƒ¨ç½²ï¼‰

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: worker-pdb
spec:
  maxUnavailable: "25%"  # æ¯æ¬¡å…è®¸æœ€å¤š 25% ä¸­æ–­
  selector:
    matchLabels:
      app: worker
```

| å‰¯æœ¬æ•° | maxUnavailable: "25%" | æœ€å¤§åŒæ—¶é©±é€æ•° |
|-----------|---------------------|------------------|
| 4 | 1 | 1 |
| 10 | 2.5 â†’ 2 | 2 |
| 100 | 25 | 25 |

**åŸºäºç™¾åˆ†æ¯” PDB çš„ä¼˜åŠ¿ï¼š**
- æ‰©ç¼©å®¹æ—¶è‡ªåŠ¨æŒ‰æ¯”ä¾‹è°ƒæ•´
- ä¸ Cluster Autoscaler / Karpenter è‡ªç„¶é…åˆ

### 6.3 PDB æ•…éšœæ’é™¤

#### é—®é¢˜ 1ï¼šDrain æ°¸ä¹…é˜»å¡

**ç—‡çŠ¶ï¼š**
```bash
$ kubectl drain node-1 --ignore-daemonsets
error: cannot delete Pods with local storage (use --delete-emptydir-data to override)
Cannot evict pod as it would violate the pod's disruption budget.
```

**åŸå› ï¼š** PDB çš„ `minAvailable` ç­‰äºå½“å‰ `replicas`ï¼Œæˆ– PDB ç›®æ ‡ Pod è¿‡åº¦é›†ä¸­åœ¨è¯¥èŠ‚ç‚¹

```yaml
# é”™è¯¯é…ç½®ç¤ºä¾‹
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
spec:
  replicas: 3  # é—®é¢˜ï¼šä¸ minAvailable ç›¸åŒ
  # ...
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
spec:
  minAvailable: 3  # é—®é¢˜ï¼šä¸å‰¯æœ¬æ•°ç›¸åŒ
  selector:
    matchLabels:
      app: critical-app
```

**è§£å†³æ–¹æ¡ˆï¼š**

```yaml
# æ­£ç¡®é…ç½®ç¤ºä¾‹
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
spec:
  minAvailable: 2  # è®¾ç½®ä½äºå‰¯æœ¬æ•°ï¼ˆ3ï¼‰
  selector:
    matchLabels:
      app: critical-app
```

æˆ–ä½¿ç”¨ç™¾åˆ†æ¯”ï¼š

```yaml
spec:
  minAvailable: "67%"  # 3 ä¸ªä¸­çš„ 2 ä¸ªï¼ˆ67%ï¼‰
```

:::warning PDB é…ç½®æ³¨æ„äº‹é¡¹
è®¾ç½® `minAvailable: replicas` æ„å‘³ç€**æ— æ³• drain ä»»ä½•èŠ‚ç‚¹**ã€‚å§‹ç»ˆè®¾ç½® `minAvailable < replicas` æˆ– `maxUnavailable >= 1` ä»¥å…è®¸è‡³å°‘ 1 ä¸ª Pod è¢«é©±é€ã€‚
:::

#### é—®é¢˜ 2ï¼šPDB æœªè¢«åº”ç”¨

**ç—‡çŠ¶ï¼š** Node drain æœŸé—´ PDB è¢«å¿½ç•¥ï¼Œæ‰€æœ‰ Pod è¢«åŒæ—¶é©±é€

**åŸå› ï¼š**
1. PDB çš„ `selector` ä¸ Pod `labels` ä¸åŒ¹é…
2. PDB åˆ›å»ºåœ¨ä¸åŒçš„å‘½åç©ºé—´
3. PDB è®¾ç½®äº† `minAvailable: 0` æˆ– `maxUnavailable: "100%"`

**éªŒè¯ï¼š**

```bash
# æ£€æŸ¥ PDB çŠ¶æ€
kubectl get pdb -A
kubectl describe pdb <pdb-name>

# æ£€æŸ¥ PDB é€‰ä¸­çš„ Pod æ•°é‡
# å¦‚æœ ALLOWED DISRUPTIONS åˆ—ä¸º 0ï¼Œdrain è¢«é˜»å¡ï¼›1 æˆ–æ›´å¤šè¡¨ç¤ºå…è®¸
```

#### é—®é¢˜ 3ï¼šKarpenter Consolidation ä¸ PDB å†²çª

**ç—‡çŠ¶ï¼š** Karpenter å°è¯•ç§»é™¤èŠ‚ç‚¹ä½†å›  PDB å¤±è´¥ï¼ŒèŠ‚ç‚¹ä¿æŒ `cordoned` çŠ¶æ€

**åŸå› ï¼š** PDB è¿‡äºä¸¥æ ¼ï¼Œä¸ Karpenter çš„ Disruption budget å†²çª

**è§£å†³æ–¹æ¡ˆï¼š**

```yaml
# åœ¨ Karpenter NodePool ä¸Šè®¾ç½® Disruption budget
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general-pool
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m
    # å…è®¸åŒæ—¶ä¸­æ–­æœ€å¤š 20% çš„èŠ‚ç‚¹
    budgets:
    - nodes: "20%"
  # ...
```

**å‡è¡¡çš„ PDB ç¤ºä¾‹ï¼š**

```yaml
# åº”ç”¨ PDBï¼šä¿è¯æœ€ä½å¯ç”¨æ€§
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: app-pdb
spec:
  maxUnavailable: "33%"  # æ¯æ¬¡å…è®¸æœ€å¤š 33% ä¸­æ–­
  selector:
    matchLabels:
      app: my-app
```

é€šè¿‡æ­¤é…ç½®ï¼ŒKarpenter å¯ä»¥çµæ´»åœ°è¿›è¡ŒèŠ‚ç‚¹ consolidationï¼ŒåŒæ—¶ä»ç„¶éµå®ˆ PDBã€‚

---

## 7. Priority ä¸ Preemption

PriorityClass å®šä¹‰ Pod ä¼˜å…ˆçº§ï¼Œå½“èµ„æºä¸è¶³æ—¶ï¼Œé©±é€ï¼ˆæŠ¢å ï¼‰ä½ä¼˜å…ˆçº§ Pod ä»¥è°ƒåº¦é«˜ä¼˜å…ˆçº§ Podã€‚

### 7.1 PriorityClass å®šä¹‰

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000  # å€¼è¶Šé«˜ä¼˜å…ˆçº§è¶Šé«˜ï¼ˆæœ€å¤§ 10 äº¿ï¼‰
globalDefault: false
description: "High priority for mission-critical services"
```

**å…³é”®å±æ€§ï¼š**

| å±æ€§ | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `value` | ä¼˜å…ˆçº§å€¼ï¼ˆæ•´æ•°ï¼‰ | 0 ~ 1,000,000,000 |
| `globalDefault` | æ˜¯å¦ä¸ºé»˜è®¤ PriorityClass | `false`ï¼ˆæ¨èæ˜¾å¼åˆ†é…ï¼‰ |
| `preemptionPolicy` | æŠ¢å ç­–ç•¥ | `PreemptLowerPriority`ï¼ˆé»˜è®¤ï¼‰æˆ– `Never` |
| `description` | æè¿° | æŒ‡å®šç”¨é€” |

:::warning ç³»ç»Ÿ PriorityClass ä¿ç•™èŒƒå›´
10 äº¿åŠä»¥ä¸Šçš„å€¼ä¿ç•™ç»™ Kubernetes ç³»ç»Ÿç»„ä»¶ï¼ˆkube-systemï¼‰ã€‚ç”¨æˆ·å®šä¹‰çš„ PriorityClass åº”ä½¿ç”¨ 10 äº¿ä»¥ä¸‹çš„å€¼ã€‚
:::

### 7.2 ç”Ÿäº§ç¯å¢ƒ 4 å±‚ä¼˜å…ˆçº§ä½“ç³»

**æ¨èä¼˜å…ˆçº§å±‚çº§ï¼š**

```yaml
# ç¬¬ 1 å±‚ï¼šå…³é”®ç³»ç»Ÿï¼ˆä½äº 10 äº¿çš„æœ€é«˜å€¼ï¼‰
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: system-critical
value: 999999000
globalDefault: false
description: "Critical system components (DNS, CNI, monitoring)"
---
# ç¬¬ 2 å±‚ï¼šä¸šåŠ¡å…³é”®ï¼ˆ100 ä¸‡ï¼‰
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: business-critical
value: 1000000
globalDefault: false
description: "Revenue-impacting services (payment, checkout, auth)"
---
# ç¬¬ 3 å±‚ï¼šé«˜ä¼˜å…ˆçº§ï¼ˆ10 ä¸‡ï¼‰
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 100000
globalDefault: false
description: "Important services (API, web frontend)"
---
# ç¬¬ 4 å±‚ï¼šæ ‡å‡†ï¼ˆ1 ä¸‡ï¼Œé»˜è®¤ï¼‰
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: standard-priority
value: 10000
globalDefault: true  # æœªæŒ‡å®š PriorityClass æ—¶çš„é»˜è®¤å€¼
description: "Standard workloads"
---
# ç¬¬ 5 å±‚ï¼šä½ä¼˜å…ˆçº§ï¼ˆ1 åƒï¼‰
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 1000
globalDefault: false
preemptionPolicy: Never  # ä¸æŠ¢å å…¶ä»– Pod
description: "Batch jobs, non-critical background tasks"
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-service
spec:
  replicas: 5
  selector:
    matchLabels:
      app: payment-service
  template:
    metadata:
      labels:
        app: payment-service
    spec:
      priorityClassName: business-critical  # ä¿è¯æœ€é«˜ä¼˜å…ˆçº§
      containers:
      - name: payment
        image: payment-service:v2.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: data-cleanup
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          priorityClassName: low-priority  # æ‰¹å¤„ç†ä»»åŠ¡è®¾ä¸ºä½ä¼˜å…ˆçº§
          containers:
          - name: cleanup
            image: data-cleanup:v1.0
```

### 7.3 ç†è§£æŠ¢å è¡Œä¸º

æŠ¢å æ˜¯å½“é«˜ä¼˜å…ˆçº§ Pod æ— æ³•è°ƒåº¦æ—¶ï¼Œé©±é€ä½ä¼˜å…ˆçº§ Pod ä»¥é‡Šæ”¾èµ„æºçš„æœºåˆ¶ã€‚

```mermaid
flowchart TB
    START[é«˜ä¼˜å…ˆçº§ Pod<br/>è°ƒåº¦è¯·æ±‚]
    CHECK{èµ„æº<br/>æ˜¯å¦å……è¶³ï¼Ÿ}
    SCHEDULE[ç«‹å³è°ƒåº¦]
    FIND[æœç´¢<br/>å¯æŠ¢å å€™é€‰èŠ‚ç‚¹]
    CANDIDATE{æ˜¯å¦å­˜åœ¨<br/>ä½ä¼˜å…ˆçº§ Podï¼Ÿ}
    EVICT[é©±é€ä½ä¼˜å…ˆçº§<br/>Pod]
    WAIT[ç­‰å¾…é©±é€<br/>gracePeriod]
    BIND[è°ƒåº¦<br/>é«˜ä¼˜å…ˆçº§ Pod]
    PENDING[ä¿æŒ Pending<br/>ç­‰å¾… Cluster Autoscaler]

    START --> CHECK
    CHECK -->|æ˜¯| SCHEDULE
    CHECK -->|å¦| FIND
    FIND --> CANDIDATE
    CANDIDATE -->|æ˜¯| EVICT
    CANDIDATE -->|å¦| PENDING
    EVICT --> WAIT
    WAIT --> BIND

    style START fill:#4286f4,stroke:#2a6acf,color:#fff
    style EVICT fill:#ff4444,stroke:#cc3636,color:#fff
    style BIND fill:#34a853,stroke:#2a8642,color:#fff
    style PENDING fill:#fbbc04,stroke:#c99603,color:#000
```

**æŠ¢å å†³ç­–è¿‡ç¨‹ï¼š**

1. **é«˜ä¼˜å…ˆçº§ Pod è°ƒåº¦å¤±è´¥**
2. **æœç´¢å¯æŠ¢å å€™é€‰èŠ‚ç‚¹**ï¼šæ‰¾åˆ°é€šè¿‡ç§»é™¤ä½ä¼˜å…ˆçº§ Pod å¯ä»¥å®ç°è°ƒåº¦çš„èŠ‚ç‚¹
3. **é€‰æ‹©å—å®³ Pod**ï¼šä»æœ€ä½ä¼˜å…ˆçº§å¼€å§‹é€‰æ‹©ç§»é™¤ç›®æ ‡
4. **æ£€æŸ¥ PDB**ï¼šéªŒè¯å—å®³ Pod æ˜¯å¦å— PDB ä¿æŠ¤ â†’ å¦‚æœè¿å PDBï¼Œæœç´¢å…¶ä»–èŠ‚ç‚¹
5. **ä¼˜é›…é©±é€**ï¼šéµå®ˆ `terminationGracePeriodSeconds` è¿›è¡Œé©±é€
6. **èµ„æºé‡Šæ”¾åè°ƒåº¦**ï¼šæ”¾ç½®é«˜ä¼˜å…ˆçº§ Pod

:::tip æŠ¢å ä¸ PDB çš„å…³ç³»
æŠ¢å **éµå®ˆ** PDBã€‚è¿å PDB çš„ `minAvailable` çš„é©±é€ä¸ä¼šå‘ç”Ÿã€‚è¿™æ„å‘³ç€å³ä½¿æ˜¯ä½ä¼˜å…ˆçº§çš„ Podï¼Œå¦‚æœå— PDB ä¿æŠ¤ä¹Ÿå¯ä»¥è¢«ä¿ç•™ã€‚
:::

**æŠ¢å ç¤ºä¾‹åœºæ™¯ï¼š**

```yaml
# å½“å‰é›†ç¾¤çŠ¶æ€ï¼šèŠ‚ç‚¹èµ„æºæ¥è¿‘æ»¡è½½
# Node-1: low-priority-pod (CPU: 2, Memory: 4Gi)
# Node-2: standard-priority-pod (CPU: 2, Memory: 4Gi)

# é«˜ä¼˜å…ˆçº§ Pod åˆ›å»ºè¯·æ±‚
apiVersion: v1
kind: Pod
metadata:
  name: critical-payment
spec:
  priorityClassName: business-critical  # Priority: 1000000
  containers:
  - name: payment
    image: payment:v1.0
    resources:
      requests:
        cpu: "2"
        memory: 4Gi

# ç»“æœï¼š
# 1. è°ƒåº¦å™¨æ£€æµ‹åˆ°èµ„æºä¸è¶³
# 2. é€‰æ‹© low-priority-podï¼ˆpriority: 1000ï¼‰ä½œä¸ºå—å®³è€…
# 3. é©±é€ low-priority-podï¼ˆä¼˜é›…å…³é—­ï¼‰
# 4. è°ƒåº¦ critical-payment Pod
```

### 7.4 PreemptionPolicy: Never

å¯ä»¥é…ç½®ç‰¹å®šå·¥ä½œè´Ÿè½½ä¸æŠ¢å å…¶ä»– Podï¼š

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: batch-job
value: 5000
globalDefault: false
preemptionPolicy: Never  # ä¸æŠ¢å å…¶ä»– Pod
description: "Batch jobs that wait for available resources"
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- **æ‰¹å¤„ç†ä»»åŠ¡**ï¼šç­‰å¾…èµ„æºå¯ç”¨æ¯”æŠ¢å æ›´åˆé€‚æ—¶
- **æµ‹è¯•/å¼€å‘å·¥ä½œè´Ÿè½½**ï¼šä¸èƒ½å½±å“ç”Ÿäº§å·¥ä½œè´Ÿè½½æ—¶
- **ä½ç´§æ€¥æ€§**ï¼šä¸éœ€è¦ç«‹å³è¿è¡Œçš„ä»»åŠ¡

### 7.5 é«˜çº§æ¨¡å¼ï¼šPriority + QoS Class ç»„åˆ

PriorityClass å’Œ QoS Class æ˜¯ç›®çš„ä¸åŒçš„æœºåˆ¶ï¼Œä½†é…åˆä½¿ç”¨å¯ä»¥åœ¨èµ„æºäº‰ç”¨æ—¶ç¡®ä¿æ›´å¯é¢„æµ‹çš„è¡Œä¸ºã€‚æœ¬èŠ‚ä»‹ç»ä¸¤ä¸ªæ¦‚å¿µçš„äº¤äº’ä»¥åŠç»è¿‡ç”Ÿäº§éªŒè¯çš„ç»„åˆæ¨¡å¼ã€‚

#### QoS Class å›é¡¾

Kubernetes æ ¹æ® Pod çš„èµ„æº requests å’Œ limits é…ç½®è‡ªåŠ¨åˆ†é… QoS Classã€‚

| QoS Class | æ¡ä»¶ | CPU é™æµ | OOM é©±é€é¡ºåº | å…¸å‹ç”¨é€” |
|-----------|------|-------------|-------------------|------------|
| **Guaranteed** | æ‰€æœ‰å®¹å™¨ requests = limits | ä»…è¾¾åˆ° limit æ—¶ | æœ€åï¼ˆæœ€å®‰å…¨ï¼‰ | å…³é”®ä»»åŠ¡ã€DB |
| **Burstable** | è‡³å°‘ä¸€ä¸ªå®¹å™¨è®¾ç½®äº† requestsï¼Œrequests &lt; limits | ä»…è¾¾åˆ° limit æ—¶ | ä¸­é—´ | ä¸€èˆ¬ Web åº”ç”¨ã€API |
| **BestEffort** | æœªè®¾ç½® requests/limits | æ— é™åˆ¶ | æœ€å…ˆï¼ˆé£é™©ï¼‰ | æ‰¹å¤„ç†ä»»åŠ¡ã€æµ‹è¯• |

**QoS Class åˆ¤å®šè§„åˆ™ï¼š**

```yaml
# Guaranteedï¼šrequests = limitsï¼ˆæ‰€æœ‰å®¹å™¨ï¼‰
resources:
  requests:
    cpu: "1"
    memory: 2Gi
  limits:
    cpu: "1"      # ä¸ requests ç›¸åŒ
    memory: 2Gi   # ä¸ requests ç›¸åŒ

# Burstableï¼šrequests &lt; limits
resources:
  requests:
    cpu: "500m"
    memory: 1Gi
  limits:
    cpu: "2"      # å¤§äº requests
    memory: 4Gi   # å¤§äº requests

# BestEffortï¼šä»€ä¹ˆéƒ½ä¸è®¾ç½®
resources: {}
```

**æ£€æŸ¥ QoS Classï¼š**
```bash
# æ£€æŸ¥ Pod çš„ QoS Class
kubectl get pod my-pod -o jsonpath='{.status.qosClass}'

# æ£€æŸ¥å‘½åç©ºé—´ä¸­æ‰€æœ‰ Pod çš„ QoS åˆ†å¸ƒ
kubectl get pods -n production \
  -o custom-columns=NAME:.metadata.name,QOS:.status.qosClass
```

#### æ¨èç»„åˆçŸ©é˜µ

Priority å’Œ QoS çš„ç»„åˆå†³å®šäº†èµ„æºä¿éšœçº§åˆ«å’Œæˆæœ¬ã€‚

| ç»„åˆ | ä¼˜å…ˆçº§ | QoS | è°ƒåº¦ä¼˜å…ˆçº§ | OOM å­˜æ´»ç‡ | æˆæœ¬ | æ¨èå·¥ä½œè´Ÿè½½ | ç¤ºä¾‹ |
|------|----------|-----|-----------------|-------------|------|-------------|------|
| **ç¬¬ 1 å±‚** | critical (10000) | Guaranteed | æœ€é«˜ | æœ€é«˜ | é«˜ | å…³é”®ä»»åŠ¡ | æ”¯ä»˜ç³»ç»Ÿã€DB |
| **ç¬¬ 2 å±‚** | high (5000) | Guaranteed | é«˜ | é«˜ | ä¸­é«˜ | æ ¸å¿ƒæœåŠ¡ | API Gateway |
| **ç¬¬ 3 å±‚** | standard (1000) | Burstable | æ­£å¸¸ | ä¸­ | ä¸­ | ä¸€èˆ¬ Web åº”ç”¨ | å‰ç«¯ã€åå° |
| **ç¬¬ 4 å±‚** | low (500) | Burstable | ä½ | ä½ | ä½ | å†…éƒ¨å·¥å…· | ç›‘æ§ã€æ—¥å¿— |
| **ç¬¬ 5 å±‚** | batch (100) | BestEffort | æœ€ä½ | æä½ | æä½ | æ‰¹å¤„ç†ã€CI/CD | æ•°æ®ç®¡é“ |

**å„ç»„åˆè¯¦ç»†è¯´æ˜ï¼š**

##### ç¬¬ 1 å±‚ï¼šGuaranteed + critical-priorityï¼ˆæœ€å¤§ä¿éšœï¼‰

**ç‰¹å¾ï¼š**
- è°ƒåº¦æ—¶æŠ¢å å…¶ä»– Pod ä»¥ç«‹å³æ”¾ç½®
- CPU/Memory ä¿éšœï¼ˆrequests = limitsï¼‰
- OOM äº‹ä»¶ä¸­æœ€åè¢«ç»ˆæ­¢
- å³ä½¿åœ¨èŠ‚ç‚¹èµ„æºå‹åŠ›ä¸‹ä¹Ÿä¸ä¼šè¢«é©±é€

**ç”Ÿäº§ YAMLï¼š**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-gateway
  namespace: production
spec:
  replicas: 6
  selector:
    matchLabels:
      app: payment-gateway
      tier: critical
  template:
    metadata:
      labels:
        app: payment-gateway
        tier: critical
    spec:
      priorityClassName: critical-priority  # Priority: 10000
      containers:
      - name: gateway
        image: payment-gateway:v3.5
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
          limits:
            cpu: "2"       # ä¸ requests ç›¸åŒ â†’ Guaranteed
            memory: 4Gi    # ä¸ requests ç›¸åŒ â†’ Guaranteed
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: payment-gateway-pdb
  namespace: production
spec:
  minAvailable: 4  # 6 ä¸ªä¸­å§‹ç»ˆä¿æŒè‡³å°‘ 4 ä¸ª
  selector:
    matchLabels:
      app: payment-gateway
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- é‡‘èäº¤æ˜“ç³»ç»Ÿï¼ˆæ”¯ä»˜ã€è½¬è´¦ï¼‰
- å®æ—¶è®¢å•å¤„ç†
- æ•°æ®åº“ï¼ˆMySQLã€PostgreSQLï¼‰
- æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆKafkaã€RabbitMQï¼‰

##### ç¬¬ 2 å±‚ï¼šGuaranteed + high-priorityï¼ˆæ ¸å¿ƒæœåŠ¡ï¼‰

**ç‰¹å¾ï¼š**
- ä»…æ¬¡äº critical çš„ä¼˜å…ˆçº§
- CPU/Memory ä¿éšœ
- OOM æ—¶åœ¨ BestEffort å’Œ Burstable ä¹‹åè¢«ç»ˆæ­¢
- å…¸å‹ç”Ÿäº§æœåŠ¡çš„æ¨èè®¾ç½®

**ç”Ÿäº§ YAMLï¼š**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
  namespace: production
spec:
  replicas: 10
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      priorityClassName: high-priority  # Priority: 5000
      containers:
      - name: api
        image: api-server:v2.8
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
          limits:
            cpu: "1"       # Guaranteed
            memory: 2Gi    # Guaranteed
        env:
        - name: MAX_CONNECTIONS
          value: "1000"
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- REST API æœåŠ¡å™¨
- GraphQL æœåŠ¡å™¨
- è®¤è¯/æˆæƒæœåŠ¡
- ä¼šè¯ç®¡ç†æœåŠ¡

##### ç¬¬ 3 å±‚ï¼šBurstable + standard-priorityï¼ˆä¸€èˆ¬ Web åº”ç”¨ï¼‰

**ç‰¹å¾ï¼š**
- åŸºç¡€èµ„æºä¿éšœï¼ˆrequestsï¼‰
- ç©ºé—²æ—¶å¯ä½¿ç”¨é¢å¤–èµ„æºï¼ˆlimits > requestsï¼‰
- æ€§ä»·æ¯”é«˜ä¸”ç¨³å®š
- é€‚åˆå¤§å¤šæ•° Web åº”ç”¨

**ç”Ÿäº§ YAMLï¼š**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
  namespace: production
spec:
  replicas: 8
  selector:
    matchLabels:
      app: web-frontend
  template:
    metadata:
      labels:
        app: web-frontend
    spec:
      priorityClassName: standard-priority  # Priority: 1000
      containers:
      - name: frontend
        image: web-frontend:v1.12
        resources:
          requests:
            cpu: "500m"    # æœ€ä½ä¿éšœ
            memory: 1Gi    # æœ€ä½ä¿éšœ
          limits:
            cpu: "2"       # å…è®¸æœ€å¤š 4 å€çªå‘
            memory: 4Gi    # å…è®¸æœ€å¤š 4 å€çªå‘
        env:
        - name: NODE_ENV
          value: "production"
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- Web å‰ç«¯ï¼ˆReactã€Vueã€Angularï¼‰
- åå°ç®¡ç†åº”ç”¨
- å†…éƒ¨ä»ªè¡¨æ¿
- CMSï¼ˆå†…å®¹ç®¡ç†ç³»ç»Ÿï¼‰

##### ç¬¬ 4 å±‚ï¼šBurstable + low-priorityï¼ˆå†…éƒ¨å·¥å…·ï¼‰

**ç‰¹å¾ï¼š**
- æœ€å°èµ„æºä¿éšœ
- èµ„æºä¸è¶³æ—¶æˆä¸ºæŠ¢å ç›®æ ‡
- æˆæœ¬æœ€å°åŒ–
- æœåŠ¡ä¸­æ–­æ—¶å½±å“æœ‰é™

**ç”Ÿäº§ YAMLï¼š**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-agent
  namespace: monitoring
spec:
  replicas: 3
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      priorityClassName: low-priority  # Priority: 500
      containers:
      - name: agent
        image: monitoring-agent:v2.1
        resources:
          requests:
            cpu: "100m"    # æœ€å°ä¿éšœ
            memory: 256Mi
          limits:
            cpu: "500m"
            memory: 1Gi
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- ç›‘æ§ä»£ç†
- æ—¥å¿—æ”¶é›†å™¨ï¼ˆFluent Bitã€Fluentdï¼‰
- Metrics Exporter
- å¼€å‘å·¥å…·

##### ç¬¬ 5 å±‚ï¼šBestEffort + batch-priorityï¼ˆæ‰¹å¤„ç†ä»»åŠ¡ï¼‰

**ç‰¹å¾ï¼š**
- æ— èµ„æºä¿éšœï¼ˆä»…ä½¿ç”¨ç©ºé—²èµ„æºï¼‰
- OOM äº‹ä»¶ä¸­æœ€å…ˆè¢«ç»ˆæ­¢
- æˆæœ¬æœ€å°åŒ–ï¼ˆå¯åˆ©ç”¨ Spot å®ä¾‹ï¼‰
- é€‚åˆå¯é‡è¯•çš„ä»»åŠ¡

**ç”Ÿäº§ YAMLï¼š**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: data-pipeline
  namespace: batch
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨ 2:00
  jobTemplate:
    spec:
      template:
        spec:
          priorityClassName: batch-priority  # Priority: 100
          restartPolicy: OnFailure
          containers:
          - name: etl
            image: data-pipeline:v1.8
            resources: {}  # BestEffortï¼šä¸è®¾ç½® requests/limits
            env:
            - name: BATCH_SIZE
              value: "10000"
          # æ”¾ç½®åœ¨ Spot å®ä¾‹ä¸Š
          nodeSelector:
            karpenter.sh/capacity-type: spot
          tolerations:
          - key: karpenter.sh/capacity-type
            operator: Equal
            value: spot
            effect: NoSchedule
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- ETL ç®¡é“
- æ•°æ®åˆ†æä»»åŠ¡
- CI/CD æ„å»º
- å›¾ç‰‡/è§†é¢‘å¤„ç†

#### é©±é€é¡ºåºï¼ˆOOM äº‹ä»¶æœŸé—´ï¼‰

å½“èŠ‚ç‚¹å†…å­˜è€—å°½æ—¶ï¼ŒKubelet æŒ‰ä»¥ä¸‹é¡ºåºç»ˆæ­¢ Podï¼š

```mermaid
flowchart TD
    A[èŠ‚ç‚¹å†…å­˜ä¸è¶³]
    B[é˜¶æ®µ 1ï¼šç»ˆæ­¢ BestEffort Pod<br/>æœ€ä½ä¼˜å…ˆçº§ä¼˜å…ˆ]
    C[é˜¶æ®µ 2ï¼šç»ˆæ­¢ Burstable Pod<br/>å†…å­˜è¶…ç”¨æœ€å¤šçš„ä¼˜å…ˆ]
    D[é˜¶æ®µ 3ï¼šç»ˆæ­¢ Guaranteed Pod<br/>æœ€ä½ä¼˜å…ˆçº§ä¼˜å…ˆ]
    E[å†…å­˜å·²å›æ”¶]

    A --> B
    B -->|ä»ç„¶ä¸è¶³| C
    C -->|ä»ç„¶ä¸è¶³| D
    D --> E

    style A fill:#ea4335,stroke:#c5221f,color:#fff
    style B fill:#fbbc04,stroke:#f9ab00,color:#000
    style C fill:#ff9800,stroke:#f57c00,color:#fff
    style D fill:#f44336,stroke:#d32f2f,color:#fff
    style E fill:#34a853,stroke:#2a8642,color:#fff
```

**é©±é€å†³ç­–å› ç´ ï¼š**

1. **QoS Class**ï¼ˆä¸»è¦æ ‡å‡†ï¼‰
   - BestEffort â†’ Burstable â†’ Guaranteed é¡ºåº

2. **Priority**ï¼ˆæ¬¡è¦æ ‡å‡†ï¼ŒQoS ç›¸åŒæ—¶ï¼‰
   - ä½ Priority ä¼˜å…ˆè¢«ç»ˆæ­¢

3. **å†…å­˜ä½¿ç”¨é‡**ï¼ˆç¬¬ä¸‰æ ‡å‡†ï¼ŒQoS + Priority ç›¸åŒæ—¶ï¼‰
   - ç›¸å¯¹äº requests å†…å­˜è¶…ç”¨æœ€å¤šçš„ Pod ä¼˜å…ˆè¢«ç»ˆæ­¢

**ç¤ºä¾‹åœºæ™¯ï¼š**

```yaml
# èŠ‚ç‚¹çŠ¶æ€ï¼š32GB ä¸­å·²ä½¿ç”¨ 31GBï¼Œå³å°† OOM

# Pod 1ï¼šBestEffort + low-priority (500)
# - ä½¿ç”¨ä¸­ï¼š4GB
# â†’ é©±é€é¡ºåºï¼šç¬¬ 1

# Pod 2ï¼šBurstable + standard-priority (1000)
# - requests: 2GB, limits: 8GB
# - ä½¿ç”¨ä¸­ï¼š6GBï¼ˆè¶…å‡º requests 4GBï¼‰
# â†’ é©±é€é¡ºåºï¼šç¬¬ 2

# Pod 3ï¼šBurstable + high-priority (5000)
# - requests: 4GB, limits: 8GB
# - ä½¿ç”¨ä¸­ï¼š5GBï¼ˆè¶…å‡º requests 1GBï¼‰
# â†’ é©±é€é¡ºåºï¼šç¬¬ 3

# Pod 4ï¼šGuaranteed + critical-priority (10000)
# - requests = limits: 8GB
# - ä½¿ç”¨ä¸­ï¼š8GBï¼ˆæ— è¶…ç”¨ï¼‰
# â†’ é©±é€é¡ºåºï¼šç¬¬ 4ï¼ˆæœ€åï¼‰
```

#### Kubelet é©±é€é…ç½®

Kubelet é©±é€é˜ˆå€¼åœ¨èŠ‚ç‚¹çº§åˆ«é…ç½®ã€‚åœ¨ EKS ä¸Šï¼Œå¯ä»¥é€šè¿‡ User Data è„šæœ¬è‡ªå®šä¹‰ã€‚

**é»˜è®¤è®¾ç½®ï¼ˆEKSï¼‰ï¼š**
```yaml
# /etc/kubernetes/kubelet/kubelet-config.json
{
  "evictionHard": {
    "memory.available": "100Mi",
    "nodefs.available": "10%",
    "imagefs.available": "15%"
  },
  "evictionSoft": {
    "memory.available": "500Mi",
    "nodefs.available": "15%"
  },
  "evictionSoftGracePeriod": {
    "memory.available": "1m30s",
    "nodefs.available": "2m"
  }
}
```

**è‡ªå®šä¹‰ç¤ºä¾‹ï¼ˆKarpenter EC2NodeClassï¼‰ï¼š**
```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: custom-eviction
spec:
  amiFamily: AL2023
  userData: |
    #!/bin/bash
    # ä¿®æ”¹ Kubelet é…ç½®
    cat <<EOF > /etc/kubernetes/kubelet/kubelet-config.json
    {
      "evictionHard": {
        "memory.available": "200Mi",  # æ›´ä¿å®ˆçš„è®¾ç½®
        "nodefs.available": "10%"
      },
      "evictionSoft": {
        "memory.available": "1Gi",    # æé«˜è½¯é˜ˆå€¼
        "nodefs.available": "15%"
      },
      "evictionSoftGracePeriod": {
        "memory.available": "2m",     # å¢åŠ å®½é™æœŸ
        "nodefs.available": "3m"
      }
    }
    EOF

    systemctl restart kubelet
```

**é©±é€é˜ˆå€¼è¯´æ˜ï¼š**

| è®¾ç½® | å«ä¹‰ | é»˜è®¤å€¼ | æ¨èå€¼ï¼ˆç”Ÿäº§ï¼‰ |
|------|------|--------|-----------------|
| `evictionHard.memory.available` | ä½äºæ­¤å€¼æ—¶ç«‹å³é©±é€ | 100Mi | 200~500Mi |
| `evictionSoft.memory.available` | æŒç»­ä½äºæ­¤å€¼æ—¶é©±é€ | 500Mi | 1Gi |
| `evictionSoftGracePeriod.memory.available` | è½¯é˜ˆå€¼å®½é™æœŸ | 1m30s | 2~5m |

:::warning é©±é€é…ç½®æ³¨æ„äº‹é¡¹
`evictionHard` é˜ˆå€¼è®¾ç½®è¿‡ä½ä¼šå¯¼è‡´ OOM Killer å…ˆè¡ŒåŠ¨ï¼Œä½¿ Kubelet çš„ä¼˜é›…é©±é€å¤±æ•ˆã€‚åä¹‹ï¼Œè®¾ç½®è¿‡é«˜ä¼šé™ä½èŠ‚ç‚¹èµ„æºåˆ©ç”¨ç‡å¹¶å¢åŠ æˆæœ¬ã€‚

**æ¨èåšæ³•ï¼š**
- ä¸€èˆ¬å·¥ä½œè´Ÿè½½ï¼š`evictionHard: 200Mi`ã€`evictionSoft: 1Gi`
- å†…å­˜å¯†é›†å‹å·¥ä½œè´Ÿè½½ï¼š`evictionHard: 500Mi`ã€`evictionSoft: 2Gi`
- ç›‘æ§ï¼šè·Ÿè¸ª `kube_node_status_condition{condition="MemoryPressure"}` æŒ‡æ ‡
:::

#### åœ¨å®è·µä¸­éªŒè¯ç»„åˆæ¨¡å¼

**æ¨¡å¼ 1ï¼šå¤šå±‚æ¶æ„**

```yaml
# ç¬¬ 1 å±‚ï¼šæ•°æ®åº“ï¼ˆGuaranteed + criticalï¼‰
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 3
  template:
    spec:
      priorityClassName: critical-priority
      containers:
      - name: postgres
        image: postgres:16
        resources:
          requests:
            cpu: "4"
            memory: 16Gi
          limits:
            cpu: "4"
            memory: 16Gi
---
# ç¬¬ 2 å±‚ï¼šAPI æœåŠ¡å™¨ï¼ˆGuaranteed + highï¼‰
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 10
  template:
    spec:
      priorityClassName: high-priority
      containers:
      - name: api
        resources:
          requests: { cpu: "1", memory: 2Gi }
          limits: { cpu: "1", memory: 2Gi }
---
# ç¬¬ 3 å±‚ï¼šå‰ç«¯ï¼ˆBurstable + standardï¼‰
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 8
  template:
    spec:
      priorityClassName: standard-priority
      containers:
      - name: frontend
        resources:
          requests: { cpu: "500m", memory: 1Gi }
          limits: { cpu: "2", memory: 4Gi }
---
# ç¬¬ 4 å±‚ï¼šç›‘æ§ï¼ˆBurstable + lowï¼‰
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
spec:
  template:
    spec:
      priorityClassName: low-priority
      containers:
      - name: exporter
        resources:
          requests: { cpu: "100m", memory: 128Mi }
          limits: { cpu: "200m", memory: 256Mi }
```

**éªŒè¯å‘½ä»¤ï¼š**
```bash
# æ£€æŸ¥ QoS + Priority åˆ†å¸ƒ
kubectl get pods -A -o custom-columns=\
NAME:.metadata.name,\
NAMESPACE:.metadata.namespace,\
QOS:.status.qosClass,\
PRIORITY:.spec.priorityClassName,\
CPU_REQ:.spec.containers[0].resources.requests.cpu,\
MEM_REQ:.spec.containers[0].resources.requests.memory

# æ£€æŸ¥æ¯ä¸ªèŠ‚ç‚¹çš„ QoS åˆ†å¸ƒ
kubectl describe node <node-name> | grep -A 10 "Non-terminated Pods"
```

#### æ•…éšœæ’é™¤ï¼šQoS + Priority ç»„åˆé—®é¢˜

| ç—‡çŠ¶ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| Guaranteed Pod è¢« OOM Kill | Limits è®¾ç½®è¿‡ä½ | åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µå¹¶æé«˜ limits |
| Burstable Pod å‡ºç° CPU é™æµ | è¾¾åˆ° limitï¼ŒèŠ‚ç‚¹èµ„æºä¸è¶³ | æé«˜ requests æˆ–æ·»åŠ èŠ‚ç‚¹ |
| ä½ä¼˜å…ˆçº§ Pod æ— é™æœŸ Pending | é«˜ä¼˜å…ˆçº§ Pod å„æ–­èµ„æº | æ·»åŠ èŠ‚ç‚¹æˆ–é‡æ–°å‡è¡¡ä¼˜å…ˆçº§ |
| BestEffort Pod ç«‹å³è¢«ç»ˆæ­¢ | è¾¾åˆ°é©±é€é˜ˆå€¼ | æ”¹ä¸º Burstable å¹¶è®¾ç½® requests |

:::tip QoS + Priority ä¼˜åŒ–å»ºè®®
1. **ç›‘æ§**ï¼šä½¿ç”¨ Prometheus æŒ‡æ ‡ `container_memory_working_set_bytes` å’Œ `container_cpu_usage_seconds_total` è·Ÿè¸ªå®é™…ä½¿ç”¨é‡
2. **åˆç†è°ƒæ•´**ï¼šå‚è€ƒ VPAï¼ˆVertical Pod Autoscalerï¼‰å»ºè®®
3. **æ¸è¿›è¿‡æ¸¡**ï¼šä» BestEffort â†’ Burstable â†’ Guaranteed é€æ­¥åº”ç”¨
4. **æˆæœ¬å‡è¡¡**ï¼šå°†æ‰€æœ‰ Pod è®¾ä¸º Guaranteed ä¼šå¢åŠ æˆæœ¬ï¼›æ ¹æ®å·¥ä½œè´Ÿè½½é‡è¦æ€§å·®å¼‚åŒ–è®¾ç½®
:::

---

## 8. Descheduler

Descheduler æ˜¯ä¸€ä¸ª**é‡æ–°åˆ†é…**å·²è°ƒåº¦ Pod ä»¥å‡è¡¡é›†ç¾¤çš„å·¥å…·ã€‚ç”±äº Kubernetes è°ƒåº¦å™¨ä»…å¤„ç†åˆå§‹æ”¾ç½®ï¼ŒèŠ‚ç‚¹ä¸å‡è¡¡ä¼šéšæ—¶é—´é€æ¸åŠ å‰§ã€‚

### 8.1 ä¸ºä»€ä¹ˆéœ€è¦ Descheduler

**åœºæ™¯ 1ï¼šæ·»åŠ èŠ‚ç‚¹åçš„ä¸å‡è¡¡**
- Pod é›†ä¸­åœ¨ç°æœ‰èŠ‚ç‚¹ä¸Šï¼Œæ–°æ·»åŠ çš„èŠ‚ç‚¹ä¿æŒç©ºé—²
- Descheduler é©±é€æ—§ Pod â†’ è°ƒåº¦å™¨å°†å…¶é‡æ–°åˆ†é…åˆ°æ–°èŠ‚ç‚¹

**åœºæ™¯ 2ï¼šAffinity/Anti-Affinity è¿è§„**
- Pod æ”¾ç½®åèŠ‚ç‚¹æ ‡ç­¾å‘ç”Ÿå˜åŒ–ï¼Œè¿å Affinity æ¡ä»¶
- Descheduler é©±é€è¿è§„ Pod â†’ é‡æ–°åˆ†é…åˆ°æ»¡è¶³æ¡ä»¶çš„èŠ‚ç‚¹

**åœºæ™¯ 3ï¼šèµ„æºç¢ç‰‡åŒ–**
- æŸäº›èŠ‚ç‚¹ CPU ä½¿ç”¨è¿‡é«˜è€Œå…¶ä»–èŠ‚ç‚¹ç©ºé—²
- Descheduler è§£å†³ä¸å‡è¡¡é—®é¢˜

### 8.2 Descheduler å®‰è£…ï¼ˆHelmï¼‰

```bash
# æ·»åŠ  Descheduler Helm Chart
helm repo add descheduler https://kubernetes-sigs.github.io/descheduler/
helm repo update

# åŸºç¡€å®‰è£…
helm install descheduler descheduler/descheduler \
  --namespace kube-system \
  --set cronJobApiVersion="batch/v1" \
  --set schedule="*/15 * * * *"  # æ¯ 15 åˆ†é’Ÿè¿è¡Œä¸€æ¬¡
```

**CronJob vs Deployment æ¨¡å¼ï¼š**

| æ¨¡å¼ | æ‰§è¡Œé¢‘ç‡ | èµ„æºä½¿ç”¨ | æ¨èç¯å¢ƒ |
|------|----------|------------|----------|
| **CronJob** | å®šæœŸï¼ˆå¦‚æ¯ 15 åˆ†é’Ÿï¼‰ | ä»…æ‰§è¡ŒæœŸé—´ä½¿ç”¨èµ„æº | ä¸­å°å‹é›†ç¾¤ï¼ˆæ¨èï¼‰ |
| **Deployment** | æŒç»­è¿è¡Œ | å§‹ç»ˆä½¿ç”¨èµ„æº | å¤§å‹é›†ç¾¤ï¼ˆ1000+ èŠ‚ç‚¹ï¼‰ |

### 8.3 Descheduler å…³é”®ç­–ç•¥

#### ç­–ç•¥ 1ï¼šRemoveDuplicates

**ç›®çš„**ï¼šå½“åŒä¸€ Controllerï¼ˆReplicaSetã€Deploymentï¼‰çš„å¤šä¸ª Pod è¢«æ”¾ç½®åœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šæ—¶è¿›è¡Œåˆ†æ•£

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: RemoveDuplicates
          args:
            # æ¯ä¸ª Controller æ¯ä¸ªèŠ‚ç‚¹ä»…ä¿ç•™ 1 ä¸ª Pod
            excludeOwnerKinds:
            - "ReplicaSet"
            - "StatefulSet"
        plugins:
          balance:
            enabled:
            - RemoveDuplicates
```

**æ•ˆæœ**ï¼šå¦‚æœåŒä¸€ Deployment çš„å¤šä¸ªå‰¯æœ¬åœ¨åŒä¸€èŠ‚ç‚¹ä¸Šï¼Œéƒ¨åˆ†ä¼šè¢«é©±é€å¹¶åˆ†æ•£åˆ°å…¶ä»–èŠ‚ç‚¹

#### ç­–ç•¥ 2ï¼šLowNodeUtilization

**ç›®çš„**ï¼šåœ¨ä½åˆ©ç”¨ç‡å’Œé«˜åˆ©ç”¨ç‡èŠ‚ç‚¹ä¹‹é—´å®ç°å‡è¡¡

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: LowNodeUtilization
          args:
            # ä½åˆ©ç”¨ç‡é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼ = åˆ©ç”¨ä¸è¶³ï¼‰
            thresholds:
              cpu: 20
              memory: 20
              pods: 20
            # é«˜åˆ©ç”¨ç‡é˜ˆå€¼ï¼ˆé«˜äºæ­¤å€¼ = è¿‡åº¦åˆ©ç”¨ï¼‰
            targetThresholds:
              cpu: 50
              memory: 50
              pods: 50
        plugins:
          balance:
            enabled:
            - LowNodeUtilization
```

**è¡Œä¸ºï¼š**
1. è¯†åˆ« CPU/Memory/Pod æ•°é‡ä½äº 20% çš„èŠ‚ç‚¹ï¼ˆåˆ©ç”¨ä¸è¶³ï¼‰
2. è¯†åˆ«é«˜äº 50% çš„èŠ‚ç‚¹ï¼ˆè¿‡åº¦åˆ©ç”¨ï¼‰
3. ä»è¿‡åº¦åˆ©ç”¨çš„èŠ‚ç‚¹é©±é€ Pod
4. Kubernetes è°ƒåº¦å™¨å°†å…¶é‡æ–°åˆ†é…åˆ°åˆ©ç”¨ä¸è¶³çš„èŠ‚ç‚¹

#### ç­–ç•¥ 3ï¼šRemovePodsViolatingNodeAffinity

**ç›®çš„**ï¼šç§»é™¤è¿å Node Affinity æ¡ä»¶çš„ Podï¼ˆèŠ‚ç‚¹æ ‡ç­¾å˜æ›´åï¼‰

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: RemovePodsViolatingNodeAffinity
          args:
            nodeAffinityType:
            - requiredDuringSchedulingIgnoredDuringExecution
        plugins:
          deschedule:
            enabled:
            - RemovePodsViolatingNodeAffinity
```

**åœºæ™¯**ï¼šGPU èŠ‚ç‚¹ä¸Šçš„ `gpu=true` æ ‡ç­¾è¢«ç§»é™¤ â†’ éœ€è¦ GPU çš„ Pod ä»åœ¨æ²¡æœ‰è¯¥æ ‡ç­¾çš„èŠ‚ç‚¹ä¸Š â†’ Descheduler é©±é€å®ƒä»¬ â†’ é‡æ–°åˆ†é…åˆ° GPU èŠ‚ç‚¹

#### ç­–ç•¥ 4ï¼šRemovePodsViolatingInterPodAntiAffinity

**ç›®çš„**ï¼šç§»é™¤è¿å Pod Anti-Affinity æ¡ä»¶çš„ Pod

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        plugins:
          deschedule:
            enabled:
            - RemovePodsViolatingInterPodAntiAffinity
```

**åœºæ™¯**ï¼šæœ€åˆæœ‰è¶³å¤ŸèŠ‚ç‚¹æ»¡è¶³ Anti-Affinity â†’ èŠ‚ç‚¹ç¼©å‡å¯¼è‡´è¿è§„ Pod åœ¨åŒä¸€èŠ‚ç‚¹ä¸Š â†’ æ·»åŠ èŠ‚ç‚¹å Descheduler é‡æ–°åˆ†æ•£

#### ç­–ç•¥ 5ï¼šRemovePodsHavingTooManyRestarts

**ç›®çš„**ï¼šç§»é™¤é‡å¯æ¬¡æ•°è¿‡å¤šçš„é—®é¢˜ Podï¼ˆåœ¨ä¸åŒèŠ‚ç‚¹ä¸Šé‡è¯•ï¼‰

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: RemovePodsHavingTooManyRestarts
          args:
            podRestartThreshold: 10  # è¶…è¿‡ 10 æ¬¡é‡å¯åé©±é€
            includingInitContainers: true
        plugins:
          deschedule:
            enabled:
            - RemovePodsHavingTooManyRestarts
```

#### ç­–ç•¥ 6ï¼šPodLifeTime

**ç›®çš„**ï¼šç§»é™¤æ—§ Pod ä»¥æ›¿æ¢ä¸ºæœ€æ–°çš„é•œåƒ/é…ç½®

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: PodLifeTime
          args:
            maxPodLifeTimeSeconds: 604800  # 7 å¤© (7 * 24 * 3600)
            # ä»…é’ˆå¯¹ç‰¹å®šçŠ¶æ€çš„ Pod
            states:
            - Running
            # æ’é™¤å¸¦æœ‰ç‰¹å®šæ ‡ç­¾çš„ Pod
            labelSelector:
              matchExpressions:
              - key: app
                operator: NotIn
                values:
                - stateful-db
        plugins:
          deschedule:
            enabled:
            - PodLifeTime
```

### 8.4 Descheduler vs Karpenter Consolidation å¯¹æ¯”

| ç‰¹æ€§ | Descheduler | Karpenter Consolidation |
|------|------------|------------------------|
| **ç›®çš„** | Pod é‡æ–°åˆ†é…ï¼ˆå‡è¡¡ï¼‰ | èŠ‚ç‚¹ç§»é™¤ï¼ˆæˆæœ¬é™ä½ï¼‰ |
| **èŒƒå›´** | Pod çº§åˆ« | èŠ‚ç‚¹çº§åˆ« |
| **æ‰§è¡Œé¢‘ç‡** | CronJobï¼ˆå¦‚æ¯ 15 åˆ†é’Ÿï¼‰ | æŒç»­ç›‘æ§ï¼ˆå®æ—¶ï¼‰ |
| **ç­–ç•¥** | å¤šç§ç­–ç•¥ï¼ˆ6+ï¼‰ | ç©ºèŠ‚ç‚¹ / åˆ©ç”¨ä¸è¶³èŠ‚ç‚¹ |
| **éµå®ˆ PDB** | æ˜¯ | æ˜¯ |
| **èŠ‚ç‚¹å¢å‡** | å¦ | æ˜¯ |
| **Cluster Autoscaler å…¼å®¹** | æ˜¯ | N/Aï¼ˆæ›¿ä»£æ–¹æ¡ˆï¼‰ |
| **ä¸»è¦ç”¨é€”** | è§£å†³ä¸å‡è¡¡ã€ä¿®å¤ Affinity è¿è§„ | æˆæœ¬ä¼˜åŒ–ã€èŠ‚ç‚¹æ•´åˆ |
| **å¯åŒæ—¶ä½¿ç”¨** | æ˜¯ï¼Œå¯ä¸ Karpenter é…åˆ | æ˜¯ï¼Œå¯ä¸ Descheduler é…åˆ |

:::tip æ¨èï¼šDescheduler + Karpenter ç»„åˆ
Descheduler ä¸“é•¿äº Pod é‡æ–°åˆ†é…ï¼Œè€Œ Karpenter ä¸“é•¿äºèŠ‚ç‚¹ç®¡ç†ã€‚ä¸¤ä¸ªå·¥å…·é…åˆä½¿ç”¨å¯äº§ç”ŸååŒæ•ˆåº”ï¼š
- Descheduler é©±é€ä¸å‡è¡¡çš„ Pod
- Kubernetes è°ƒåº¦å™¨å°† Pod é‡æ–°åˆ†é…åˆ°å…¶ä»–èŠ‚ç‚¹
- Karpenter ç§»é™¤ç©ºèŠ‚ç‚¹ä»¥èŠ‚çœæˆæœ¬
:::

**ç»„åˆä½¿ç”¨é…ç½®ç¤ºä¾‹ï¼š**

```yaml
# Deschedulerï¼šæ¯ 15 åˆ†é’Ÿè¿›è¡Œå‡è¡¡è°ƒæ•´
apiVersion: batch/v1
kind: CronJob
metadata:
  name: descheduler
  namespace: kube-system
spec:
  schedule: "*/15 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: descheduler
            image: registry.k8s.io/descheduler/descheduler:v0.29.0
            command:
            - /bin/descheduler
            - --policy-config-file=/policy/policy.yaml
---
# Karpenterï¼šæŒç»­èŠ‚ç‚¹æ•´åˆ
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m  # 5 åˆ†é’Ÿåå¼€å§‹æ•´åˆ
    budgets:
    - nodes: "20%"
```

#### 8.4.1 Descheduler + Karpenter å®æˆ˜ç»„åˆæ¨¡å¼

å½“ Descheduler å’Œ Karpenter é…åˆä½¿ç”¨æ—¶ï¼ŒPod é‡æ–°åˆ†é…å’ŒèŠ‚ç‚¹æ•´åˆè‡ªåŠ¨åè°ƒï¼ŒåŒæ—¶å®ç°é›†ç¾¤æ•ˆç‡å’Œæˆæœ¬èŠ‚çº¦ã€‚

**ç»„åˆçš„ååŒåŸç†ï¼š**

1. **é˜¶æ®µ 1ï¼ˆDeschedulerï¼‰**ï¼šæ£€æµ‹èµ„æºä¸å‡è¡¡å¹¶é‡æ–°åˆ†é… Pod
   - ä½¿ç”¨ `LowNodeUtilization` ç­–ç•¥ä»è¿‡åº¦åˆ©ç”¨çš„èŠ‚ç‚¹é©±é€ Pod
   - ä½¿ç”¨ `RemoveDuplicates`ã€`RemovePodsViolatingNodeAffinity` ç­‰é‡æ–°åˆ†é…ä¸å¿…è¦çš„ Pod

2. **é˜¶æ®µ 2ï¼ˆKubernetes è°ƒåº¦å™¨ï¼‰**ï¼šå°†é©±é€çš„ Pod é‡æ–°è°ƒåº¦åˆ°æœ€ä¼˜èŠ‚ç‚¹
   - é€‰æ‹©æœ‰å¯ç”¨èµ„æºçš„èŠ‚ç‚¹
   - æ»¡è¶³ Affinity/Anti-Affinity å’Œ Topology Spread æ¡ä»¶

3. **é˜¶æ®µ 3ï¼ˆKarpenterï¼‰**ï¼šç§»é™¤ç©ºæˆ–åˆ©ç”¨ä¸è¶³çš„èŠ‚ç‚¹
   - `consolidateAfter` æ—¶é—´åæ•´åˆç©ºèŠ‚ç‚¹
   - å°†å¤šä¸ªåˆ©ç”¨ä¸è¶³çš„èŠ‚ç‚¹ä¸Šçš„ Pod æ•´åˆåˆ°æ›´å°‘çš„èŠ‚ç‚¹ä¸Š
   - ç»ˆæ­¢ä¸å¿…è¦çš„èŠ‚ç‚¹ä»¥èŠ‚çœæˆæœ¬

**æ—¶åºåè°ƒç¤ºä¾‹ï¼š**

```yaml
# Deschedulerï¼šæ¯ 15 åˆ†é’Ÿè¿è¡Œ LowNodeUtilization
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler-policy
  namespace: kube-system
data:
  policy.yaml: |
    apiVersion: "descheduler/v1alpha2"
    kind: "DeschedulerPolicy"
    profiles:
      - name: default
        pluginConfig:
        - name: LowNodeUtilization
          args:
            thresholds:
              cpu: 20
              memory: 20
              pods: 20
            targetThresholds:
              cpu: 50
              memory: 50
              pods: 50
        plugins:
          balance:
            enabled:
            - LowNodeUtilization
---
# Karpenterï¼š5 åˆ†é’Ÿåæ•´åˆç©ºèŠ‚ç‚¹ï¼ˆä¸º Descheduler è¿è¡Œåç•™å‡ºè¶³å¤Ÿæ—¶é—´ï¼‰
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general-pool
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m  # Descheduler è¿ç§» Pod åç­‰å¾… 5 åˆ†é’Ÿ
    budgets:
    - nodes: "20%"  # åŒæ—¶æ•´åˆæœ€å¤š 20% çš„èŠ‚ç‚¹
  template:
    spec:
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]
```

**å®é™…è¿è¡Œåœºæ™¯ï¼š**

```
æ—¶é—´: 00:00 - Descheduler è¿è¡Œï¼ˆ15 åˆ†é’Ÿå‘¨æœŸï¼‰
  â””â”€ Node-A (CPU 80%, Memory 85%) â†’ æ£€æµ‹ä¸ºè¿‡åº¦åˆ©ç”¨
  â””â”€ Node-B (CPU 15%, Memory 10%) â†’ æ£€æµ‹ä¸ºåˆ©ç”¨ä¸è¶³
  â””â”€ ä» Node-A é©±é€ Pod-1ã€Pod-2

æ—¶é—´: 00:01 - Kubernetes è°ƒåº¦å™¨é‡æ–°åˆ†é…
  â””â”€ Pod-1 â†’ è°ƒåº¦åˆ° Node-B
  â””â”€ Pod-2 â†’ è°ƒåº¦åˆ° Node-C
  â””â”€ Node-A ç°åœ¨ CPU 50%, Memory 55%ï¼ˆæ­£å¸¸èŒƒå›´ï¼‰

æ—¶é—´: 00:06 - Karpenter æ•´åˆï¼ˆ5 åˆ†é’Ÿå·²è¿‡ï¼‰
  â””â”€ Node-Bï¼šä»ç„¶åˆ©ç”¨ä¸è¶³ä½†è¿è¡Œç€ Pod â†’ ä¿ç•™
  â””â”€ Node-Dï¼šæ£€æµ‹åˆ°ç©ºèŠ‚ç‚¹ï¼ˆPod å·²è¢«è¿ç§»èµ°ï¼‰ â†’ ç»ˆæ­¢
  â””â”€ å®ç°æˆæœ¬èŠ‚çº¦
```

## 10. 2025-2026 AWS åˆ›æ–°ä¸è°ƒåº¦ç­–ç•¥

AWS re:Invent 2025 å‘å¸ƒçš„å…³é”®åˆ›æ–°æ­£åœ¨å¯¹ EKS è°ƒåº¦ç­–ç•¥äº§ç”Ÿé‡å¤§å½±å“ã€‚æœ¬èŠ‚ä»‹ç»æœ€æ–°åŠŸèƒ½â€”â€”Provisioned Control Planeã€EKS Auto Modeã€Karpenter + ARC é›†æˆä»¥åŠå®¹å™¨ç½‘ç»œå¯è§‚æµ‹æ€§â€”â€”å¦‚ä½•åº”ç”¨äº Pod è°ƒåº¦å’Œå¯ç”¨æ€§ã€‚

### 10.1 Provisioned Control Plane è°ƒåº¦æ€§èƒ½

**æ¦‚è¿°ï¼š**

Provisioned Control Plane é€šè¿‡ä»¥é¢„å®šä¹‰å±‚çº§ï¼ˆå¦‚ XLã€2XL å’Œ 4XLï¼‰é¢„é…æ§åˆ¶å¹³é¢å®¹é‡ï¼Œæä¾›å¯é¢„æµ‹çš„é«˜æ€§èƒ½ Kubernetes è¿ç»´èƒ½åŠ›ã€‚

**å„å±‚çº§æ€§èƒ½ç‰¹å¾ï¼š**

| å±‚çº§ | API å¹¶å‘èƒ½åŠ› | Pod è°ƒåº¦é€Ÿåº¦ | é›†ç¾¤è§„æ¨¡ | é€‚ç”¨åœºæ™¯ |
|------|-----------|-----------------|------------|----------|
| **Standard** | åŠ¨æ€æ‰©ç¼© | æ­£å¸¸ | ~1,000 èŠ‚ç‚¹ | é€šç”¨å·¥ä½œè´Ÿè½½ |
| **XL** | é«˜ | å¿«é€Ÿ | ~2,000 èŠ‚ç‚¹ | å¤§è§„æ¨¡éƒ¨ç½² |
| **2XL** | éå¸¸é«˜ | éå¸¸å¿« | ~4,000 èŠ‚ç‚¹ | AI/ML è®­ç»ƒã€HPC |
| **4XL** | æœ€å¤§ | æœ€å¿« | ~8,000 èŠ‚ç‚¹ | è¶…å¤§è§„æ¨¡é›†ç¾¤ |

**è°ƒåº¦æ€§èƒ½æå‡ï¼š**

Provisioned Control Plane é€šè¿‡ä»¥ä¸‹æ–¹å¼æå‡è°ƒåº¦æ€§èƒ½ï¼š

1. **API Server å¹¶å‘å¤„ç†**ï¼šåŒæ—¶å¤„ç†æ›´å¤šè°ƒåº¦è¯·æ±‚
2. **etcd å®¹é‡æ‰©å±•**ï¼šå­˜å‚¨æ›´å¤šèŠ‚ç‚¹å’Œ Pod å…ƒæ•°æ®
3. **è°ƒåº¦å™¨ååé‡æå‡**ï¼šæ¯ç§’å¤„ç†æ›´å¤š Pod ç»‘å®š
4. **å¯é¢„æµ‹çš„å»¶è¿Ÿ**ï¼šå³ä½¿åœ¨æµé‡çªå¢æœŸé—´ä¹Ÿèƒ½ä¿è¯ä¸€è‡´çš„è°ƒåº¦å»¶è¿Ÿ

**å¤§è§„æ¨¡é›†ç¾¤è°ƒåº¦ç­–ç•¥ï¼š**

```yaml
# ç¤ºä¾‹ï¼šåœ¨ Provisioned Control Plane XL å±‚çº§ä¸Šè¿›è¡Œå¤§è§„æ¨¡ Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: large-scale-app
spec:
  replicas: 1000  # åŒæ—¶éƒ¨ç½² 1000 ä¸ªå‰¯æœ¬
  selector:
    matchLabels:
      app: large-scale-app
  template:
    metadata:
      labels:
        app: large-scale-app
    spec:
      # Topology Spreadï¼šå°† 1000 ä¸ª Pod å‡åŒ€åˆ†å¸ƒ
      topologySpreadConstraints:
      - maxSkew: 10  # å¤§è§„æ¨¡éƒ¨ç½²æ—¶å¢å¤§ maxSkew ä»¥æé«˜çµæ´»æ€§
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: large-scale-app
      - maxSkew: 50
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: large-scale-app
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: large-scale-app
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: app:v1.0
        resources:
          requests:
            cpu: "500m"
            memory: 1Gi
```

**AI/ML è®­ç»ƒå·¥ä½œè´Ÿè½½ä¼˜åŒ–ï¼ˆæ•°åƒä¸ª GPU Podï¼‰ï¼š**

Provisioned Control Plane é’ˆå¯¹ AI/ML è®­ç»ƒå·¥ä½œè´Ÿè½½ä¸­æ•°åƒä¸ª GPU Pod åŒæ—¶è°ƒåº¦çš„åœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚

```mermaid
sequenceDiagram
    participant User
    participant APIServer as API Server<br/>(Provisioned XL)
    participant Scheduler as Kube-Scheduler<br/>(Enhanced)
    participant Karpenter
    participant EC2 as EC2 Auto Scaling

    User->>APIServer: åˆ›å»º Jobï¼ˆ1000 ä¸ª GPU Podï¼‰
    APIServer->>Scheduler: è°ƒåº¦ 1000 ä¸ª Pod

    Note over Scheduler: å¹¶è¡Œè°ƒåº¦<br/>(100+ Pod/ç§’)

    Scheduler->>Karpenter: è¯·æ±‚é¢å¤– GPU èŠ‚ç‚¹
    Karpenter->>EC2: é¢„é… 250 ä¸ª GPU èŠ‚ç‚¹

    Note over EC2: å¹¶è¡Œåˆ›å»ºèŠ‚ç‚¹<br/>(5-10 åˆ†é’Ÿ)

    EC2-->>Karpenter: èŠ‚ç‚¹å°±ç»ª
    Karpenter-->>Scheduler: æ³¨å†ŒèŠ‚ç‚¹

    Scheduler->>APIServer: Pod ç»‘å®šï¼ˆ250 æ‰¹æ¬¡ï¼‰
    APIServer->>Scheduler: è°ƒåº¦ä¸‹ä¸€æ‰¹æ¬¡

    Note over Scheduler,APIServer: é‡å¤ 4 æ¬¡<br/>(1000 Pod å®Œæˆ)

    APIServer-->>User: Job å¼€å§‹æ‰§è¡Œ
```

**æŒ‰ä½¿ç”¨åœºæ™¯æ¨èçš„å±‚çº§ï¼š**

| ä½¿ç”¨åœºæ™¯ | æ¨èå±‚çº§ | åŸå›  |
|----------|----------|------|
| **é€šç”¨ Web åº”ç”¨** | Standard | åŠ¨æ€æ‰©ç¼©å·²è¶³å¤Ÿ |
| **å¤§æ‰¹é‡ Jobï¼ˆ500+ Podï¼‰** | XL | éœ€è¦å¿«é€Ÿå¹¶å‘è°ƒåº¦ |
| **åˆ†å¸ƒå¼ ML è®­ç»ƒï¼ˆ1000+ GPU Podï¼‰** | 2XL | è¶…å¿«è°ƒåº¦ + é«˜ API å¹¶å‘ |
| **HPC é›†ç¾¤ï¼ˆæ•°åƒèŠ‚ç‚¹ï¼‰** | 4XL | æœ€å¤§è§„æ¨¡ + å¯é¢„æµ‹æ€§èƒ½ |
| **å…³é”®ä¸šåŠ¡æœåŠ¡** | XL åŠä»¥ä¸Š | å³ä½¿æµé‡çªå¢ä¹Ÿèƒ½ä¿æŒä¸€è‡´å»¶è¿Ÿ |

:::tip Provisioned Control Plane é€‰æ‹©æ ‡å‡†
- **èŠ‚ç‚¹æ•° > 1,000**ï¼šè€ƒè™‘ XL åŠä»¥ä¸Š
- **é¢‘ç¹å¤§è§„æ¨¡éƒ¨ç½²ï¼ˆ500+ Podï¼‰**ï¼šXL åŠä»¥ä¸Š
- **GPU å·¥ä½œè´Ÿè½½ï¼ˆ100+ GPUï¼‰**ï¼š2XL åŠä»¥ä¸Š
- **éœ€è¦å¯é¢„æµ‹æ€§èƒ½**ï¼šä»»ä½•è§„æ¨¡éƒ½å¯è€ƒè™‘ Provisioned
:::

### 10.2 EKS Auto Mode è‡ªåŠ¨èŠ‚ç‚¹é…ç½®

**æ¦‚è¿°ï¼š**

EKS Auto Mode é€šè¿‡å…¨é¢è‡ªåŠ¨åŒ–ä»è®¡ç®—ã€å­˜å‚¨å’Œç½‘ç»œé¢„é…åˆ°æŒç»­ç»´æŠ¤çš„æ‰€æœ‰ç¯èŠ‚ï¼Œç®€åŒ– Kubernetes è¿ç»´ã€‚

**Auto Mode å¯¹è°ƒåº¦çš„å½±å“ï¼š**

| åŠŸèƒ½ | ä¼ ç»Ÿæ–¹å¼ï¼ˆæ‰‹åŠ¨ï¼‰ | Auto Mode |
|------|---------------|----------|
| **èŠ‚ç‚¹é€‰æ‹©** | æ˜¾å¼ NodeSelectorã€Node Affinity | è‡ªåŠ¨å®ä¾‹ç±»å‹é€‰æ‹© |
| **åŠ¨æ€æ‰©ç¼©** | Cluster Autoscaler æˆ– Karpenter é…ç½® | è‡ªåŠ¨æ‰©ç¼©ï¼ˆæ— éœ€é…ç½®ï¼‰ |
| **æˆæœ¬ä¼˜åŒ–** | æ‰‹åŠ¨ Spotã€Graviton é…ç½® | è‡ªåŠ¨ Spot + Graviton åˆ©ç”¨ |
| **AZ æ”¾ç½®** | æ‰‹åŠ¨ Topology Spread é…ç½® | è‡ªåŠ¨ Multi-AZ åˆ†å¸ƒ |
| **èŠ‚ç‚¹å‡çº§** | æ‰‹åŠ¨ AMI æ›´æ–° | è‡ªåŠ¨ OS è¡¥ä¸ |

**æ‰‹åŠ¨ NodeSelector/Affinity ä¸ Auto Mode å¯¹æ¯”ï¼š**

```yaml
# ä¼ ç»Ÿæ–¹å¼ï¼šæ‰‹åŠ¨ NodeSelector + Karpenter NodePool
---
# åˆ›å»º Karpenter NodePool
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: general-pool
spec:
  template:
    spec:
      requirements:
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["c6i.xlarge", "c6i.2xlarge", "c6a.xlarge"]
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]
---
# Deploymentï¼šé€šè¿‡ NodeSelector æŒ‡å®šèŠ‚ç‚¹
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 10
  template:
    spec:
      nodeSelector:
        karpenter.sh/nodepool: general-pool
      containers:
      - name: api
        image: api:v1.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
```

```yaml
# Auto Mode æ–¹å¼ï¼šæœ€å°åŒ–é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 10
  template:
    spec:
      # æ— éœ€ NodeSelector æˆ– Affinity - Auto Mode è‡ªåŠ¨é€‰æ‹©
      containers:
      - name: api
        image: api:v1.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
      # Auto Mode è‡ªåŠ¨ï¼š
      # - é€‰æ‹©åˆé€‚çš„å®ä¾‹ç±»å‹ï¼ˆc6iã€c6aã€c7i ç­‰ï¼‰
      # - ä¼˜åŒ– Spot ä¸ On-Demand ç»„åˆ
      # - è·¨ Multi-AZ åˆ†å¸ƒ
      # - åœ¨å¯ç”¨æ—¶ä½¿ç”¨ Gravitonï¼ˆARMï¼‰
```

**åœ¨ Auto Mode ä¸­ä»éœ€é…ç½®çš„è°ƒåº¦è®¾ç½®ï¼š**

Auto Mode è‡ªåŠ¨åŒ–äº†èŠ‚ç‚¹é¢„é…ï¼Œä½†ä»¥ä¸‹è°ƒåº¦è®¾ç½®**ä»å¿…é¡»æ˜¾å¼é…ç½®**ï¼š

| è®¾ç½®é¡¹ | Auto Mode æ˜¯å¦è‡ªåŠ¨åŒ– | è¯´æ˜ |
|------|---------------------|------|
| **Resource Requests/Limits** | âŒ å¿…é¡»é…ç½® | å¿…é¡»æŒ‡å®šå·¥ä½œè´Ÿè½½èµ„æºéœ€æ±‚ |
| **Topology Spread** | âš ï¸ æä¾›åŸºç¡€åˆ†å¸ƒ + ç²¾ç»†æ§åˆ¶éœ€é…ç½® | Auto Mode æä¾›åŸºç¡€åˆ†å¸ƒï¼›ç²¾ç»†æ§åˆ¶éœ€è‡ªè¡ŒæŒ‡å®š |
| **Pod Anti-Affinity** | âŒ å¿…é¡»é…ç½® | åˆ†æ•£åŒä¸€åº”ç”¨çš„å‰¯æœ¬å¿…é¡»è‡ªè¡ŒæŒ‡å®š |
| **PDB** | âŒ å¿…é¡»é…ç½® | æœ€ä½å¯ç”¨æ€§ä¿éšœç”±åº”ç”¨è‡ªè¡Œè´Ÿè´£ |
| **PriorityClass** | âŒ å¿…é¡»é…ç½® | ä¼˜å…ˆçº§ç”±åº”ç”¨è‡ªè¡Œè´Ÿè´£ |
| **Taints/Tolerations** | âš ï¸ ä»…ç‰¹æ®ŠèŠ‚ç‚¹éœ€è¦ | GPU ç­‰ç‰¹æ®Šå·¥ä½œè´Ÿè½½å¿…é¡»æŒ‡å®š |

**Auto Mode æ¨èè°ƒåº¦æ¨¡å¼ï¼š**

```yaml
# Auto Mode æ¨èæœ€å°è°ƒåº¦é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: production-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: production-app
  template:
    metadata:
      labels:
        app: production-app
    spec:
      # 1. Resource Requestsï¼ˆå¿…é¡»ï¼‰
      containers:
      - name: app
        image: app:v1.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
          limits:
            cpu: "2"
            memory: 4Gi

      # 2. Topology Spreadï¼ˆç²¾ç»† AZ åˆ†å¸ƒæ§åˆ¶ï¼‰
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: production-app
        minDomains: 3

      # 3. Pod Anti-Affinityï¼ˆèŠ‚ç‚¹åˆ†å¸ƒï¼‰
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: production-app
              topologyKey: kubernetes.io/hostname

      # 4. PriorityClassï¼ˆä¼˜å…ˆçº§ï¼‰
      priorityClassName: high-priority
---
# 5. PDBï¼ˆå¯ç”¨æ€§ä¿éšœï¼‰
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: production-app-pdb
spec:
  minAvailable: 4
  selector:
    matchLabels:
      app: production-app
```

**Auto Mode + PDB + Karpenter äº¤äº’ï¼š**

Auto Mode å†…éƒ¨æä¾›ç±»ä¼¼ Karpenter çš„è‡ªåŠ¨æ‰©ç¼©åŠŸèƒ½ï¼Œå¹¶éµå®ˆ PDBã€‚

```mermaid
flowchart TB
    subgraph "Auto Mode ç¯å¢ƒ"
        POD[æ–° Pod åˆ›å»ºè¯·æ±‚]
        AUTOMODE[EKS Auto Mode]
        SCHEDULE[Kubernetes Scheduler]
        NODE[èŠ‚ç‚¹é¢„é…]
        PDB[PDB æ£€æŸ¥]
    end

    POD --> SCHEDULE
    SCHEDULE -->|æ— åˆé€‚èŠ‚ç‚¹| AUTOMODE
    AUTOMODE -->|è‡ªåŠ¨å®ä¾‹ç±»å‹é€‰æ‹©| NODE
    NODE -->|èŠ‚ç‚¹å°±ç»ª| SCHEDULE
    SCHEDULE -->|Pod æ”¾ç½®| DONE[Running]

    subgraph "èŠ‚ç‚¹æ•´åˆ"
        UNDERUTIL[æ£€æµ‹åˆ°åˆ©ç”¨ä¸è¶³çš„èŠ‚ç‚¹]
        EVICT[Pod é©±é€å°è¯•]
        UNDERUTIL --> PDB
        PDB -->|æ£€æŸ¥ minAvailable| EVICT
        EVICT -->|éµå®ˆ PDB| REBALANCE[é‡æ–°å¹³è¡¡]
    end

    style AUTOMODE fill:#4286f4,stroke:#2a6acf,color:#fff
    style PDB fill:#ff9900,stroke:#cc7a00,color:#fff
    style DONE fill:#34a853,stroke:#2a8642,color:#fff
```

### 10.3 ARC + Karpenter é›†æˆ AZ ç–æ•£

**æ¦‚è¿°ï¼š**

AWS Application Recovery Controller (ARC) ä¸ Karpenter çš„é›†æˆï¼Œåœ¨ AZ æ•…éšœæœŸé—´é€šè¿‡è‡ªåŠ¨ Zonal Shift å°†å·¥ä½œè´Ÿè½½è‡ªåŠ¨ç–æ•£åˆ°å¥åº·çš„ AZã€‚

**è‡ªåŠ¨ AZ æ•…éšœæ¢å¤æ¨¡å¼ï¼š**

```mermaid
sequenceDiagram
    participant AZ1 as AZ us-east-1a<br/>(æ•…éšœ)
    participant ARC as AWS ARC<br/>(Zonal Shift)
    participant Karpenter
    participant AZ2 as AZ us-east-1b<br/>(å¥åº·)
    participant AZ3 as AZ us-east-1c<br/>(å¥åº·)
    participant PDB as PodDisruptionBudget
    participant LB as Load Balancer

    Note over AZ1: ç°è‰²æ•…éšœå‘ç”Ÿ<br/>(é«˜å»¶è¿Ÿã€ä¸¢åŒ…)

    AZ1->>ARC: CloudWatch æŒ‡æ ‡å¼‚å¸¸æ£€æµ‹
    ARC->>ARC: å¯åŠ¨ Zonal Shift<br/>(é˜»æ­¢ us-east-1a æµé‡)
    ARC->>LB: ç§»é™¤ us-east-1a æµé‡

    ARC->>Karpenter: è¯·æ±‚ AZ-1a Pod ç–æ•£
    Karpenter->>PDB: æ£€æŸ¥ minAvailable
    PDB-->>Karpenter: å…è®¸å®‰å…¨é©±é€

    Karpenter->>AZ2: é¢„é…æ–°èŠ‚ç‚¹
    Karpenter->>AZ3: é¢„é…æ–°èŠ‚ç‚¹

    AZ2-->>Karpenter: èŠ‚ç‚¹å°±ç»ª
    AZ3-->>Karpenter: èŠ‚ç‚¹å°±ç»ª

    Karpenter->>AZ1: AZ-1a Pod é©±é€
    Note over AZ1: ç°æœ‰ Pod è¢«ç»ˆæ­¢

    Karpenter->>AZ2: é‡æ–°è°ƒåº¦ Pod
    Karpenter->>AZ3: é‡æ–°è°ƒåº¦ Pod

    Note over AZ2,AZ3: æœåŠ¡æ¢å¤å®Œæˆ<br/>(2-3 åˆ†é’Ÿ)

    AZ2->>LB: æ–° Pod å°±ç»ª
    AZ3->>LB: æ–° Pod å°±ç»ª
    LB-->>ARC: å¥åº·çŠ¶æ€ç¡®è®¤
```

**ARC + Karpenter é›†æˆé…ç½®ç¤ºä¾‹ï¼š**

```yaml
# Karpenter NodePoolï¼šAZ ç–æ•£æ”¯æŒ
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: arc-enabled-pool
spec:
  template:
    spec:
      requirements:
      - key: topology.kubernetes.io/zone
        operator: In
        values:
        - us-east-1a
        - us-east-1b
        - us-east-1c
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand"]  # AZ ç–æ•£æœŸé—´æ¨èä½¿ç”¨ On-Demand
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m
    budgets:
    - nodes: "30%"  # ä¸º AZ ç–æ•£æœŸé—´çš„å¿«é€Ÿé‡æ–°å¹³è¡¡ç•™å‡ºä½™é‡
---
# åº”ç”¨ï¼šTopology Spread + PDB
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resilient-app
spec:
  replicas: 9  # 3 AZ x 3 å‰¯æœ¬
  selector:
    matchLabels:
      app: resilient-app
  template:
    metadata:
      labels:
        app: resilient-app
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: resilient-app
        minDomains: 3  # å¿…é¡»è·¨ 3 ä¸ª AZ åˆ†å¸ƒ
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: resilient-app
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: app:v1.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
---
# PDBï¼šAZ ç–æ•£æœŸé—´ä¿æŒ 6 ä¸ª Podï¼ˆå…è®¸é©±é€ 9 ä¸ªä¸­çš„ 3 ä¸ªï¼‰
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: resilient-app-pdb
spec:
  minAvailable: 6
  selector:
    matchLabels:
      app: resilient-app
```

**Istio Service Mesh é›†æˆçš„ç«¯åˆ°ç«¯æ¢å¤ï¼š**

å°† Istio ä¸ ARC ç»“åˆä½¿ç”¨æ—¶ï¼Œé€šè¿‡åœ¨ AZ æ•…éšœæœŸé—´åè°ƒæµé‡è·¯ç”±å’Œ Pod é‡æ–°å¹³è¡¡å®ç°ç«¯åˆ°ç«¯æ¢å¤ã€‚

```yaml
# Istio DestinationRuleï¼šæŒ‰ AZ åˆ’åˆ†å­é›†
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: resilient-app-dr
spec:
  host: resilient-app.default.svc.cluster.local
  trafficPolicy:
    loadBalancer:
      localityLbSetting:
        enabled: true
        failover:
        - from: us-east-1a
          to: us-east-1b
        - from: us-east-1b
          to: us-east-1c
        - from: us-east-1c
          to: us-east-1a
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  subsets:
  - name: az-1a
    labels:
      topology.kubernetes.io/zone: us-east-1a
  - name: az-1b
    labels:
      topology.kubernetes.io/zone: us-east-1b
  - name: az-1c
    labels:
      topology.kubernetes.io/zone: us-east-1c
---
# Istio VirtualServiceï¼šä»…å°†æµé‡è·¯ç”±åˆ°å¥åº·çš„ AZ
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: resilient-app-vs
spec:
  hosts:
  - resilient-app.default.svc.cluster.local
  http:
  - route:
    - destination:
        host: resilient-app.default.svc.cluster.local
        subset: az-1b
      weight: 50
    - destination:
        host: resilient-app.default.svc.cluster.local
        subset: az-1c
      weight: 50
    # az-1a åœ¨ ARC Zonal Shift æœŸé—´è‡ªåŠ¨ç§»é™¤
```

**ç°è‰²æ•…éšœæ£€æµ‹æ¨¡å¼ï¼š**

ç°è‰²æ•…éšœï¼ˆGray Failureï¼‰æ˜¯æŒ‡è™½ç„¶ä¸æ˜¯å®Œå…¨ä¸­æ–­ï¼Œä½†ç”±äºæ€§èƒ½ä¸‹é™å¯¼è‡´æœåŠ¡è´¨é‡é™ä½çš„æƒ…å†µã€‚ARC åŸºäº CloudWatch æŒ‡æ ‡æ£€æµ‹ç°è‰²æ•…éšœã€‚

```yaml
# CloudWatch Alarmï¼šç°è‰²æ•…éšœæ£€æµ‹
apiVersion: v1
kind: ConfigMap
metadata:
  name: gray-failure-detection
data:
  alarm.json: |
    {
      "AlarmName": "EKS-AZ-1a-HighLatency",
      "MetricName": "TargetResponseTime",
      "Namespace": "AWS/ApplicationELB",
      "Statistic": "Average",
      "Period": 60,
      "EvaluationPeriods": 3,
      "Threshold": 1.0,
      "ComparisonOperator": "GreaterThanThreshold",
      "Dimensions": [
        {
          "Name": "AvailabilityZone",
          "Value": "us-east-1a"
        }
      ],
      "TreatMissingData": "notBreaching"
    }
```

**AZ ç–æ•£ç­–ç•¥æ±‡æ€»ï¼š**

| åœºæ™¯ | PDB è®¾ç½® | Topology Spread | Karpenter è®¾ç½® | æ¢å¤æ—¶é—´ |
|---------|---------|----------------|---------------|----------|
| **AZ å®Œå…¨æ•…éšœ** | `minAvailable: 6`ï¼ˆå…± 9 ä¸ªï¼‰ | `minDomains: 3` | On-Demand ä¼˜å…ˆ | 2-3 åˆ†é’Ÿ |
| **ç°è‰²æ•…éšœ** | `minAvailable: 6`ï¼ˆå…± 9 ä¸ªï¼‰ | å…è®¸ `minDomains: 2` | å¯ç”¨ Spot | 3-5 åˆ†é’Ÿ |
| **è®¡åˆ’ç»´æŠ¤** | `maxUnavailable: 3` | å…è®¸ `minDomains: 2` | Spot + On-Demand | 5-10 åˆ†é’Ÿ |

### 10.4 å®¹å™¨ç½‘ç»œå¯è§‚æµ‹æ€§ä¸è°ƒåº¦

**æ¦‚è¿°ï¼š**

å®¹å™¨ç½‘ç»œå¯è§‚æµ‹æ€§æä¾›ç»†ç²’åº¦çš„ç½‘ç»œæŒ‡æ ‡ï¼Œæ”¯æŒåˆ†æ Pod æ”¾ç½®ä¸ç½‘ç»œæ€§èƒ½ä¹‹é—´çš„å…³è”ï¼Œä»è€Œä¼˜åŒ–è°ƒåº¦ç­–ç•¥ã€‚

**Pod æ”¾ç½®ä¸ç½‘ç»œæ€§èƒ½çš„å…³è”ï¼š**

| Pod æ”¾ç½®æ¨¡å¼ | ç½‘ç»œå»¶è¿Ÿ | è·¨ AZ æµé‡æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|-------------|-------------|-------------------|----------|
| **åŒä¸€èŠ‚ç‚¹** | ~0.1ms | $0 | ç¼“å­˜æœåŠ¡å™¨ + åº”ç”¨ |
| **åŒä¸€ AZ** | ~0.5ms | $0 | é¢‘ç¹é€šä¿¡çš„å¾®æœåŠ¡ |
| **è·¨ AZ** | ~2-5ms | $0.01/GB | éœ€è¦é«˜å¯ç”¨çš„æœåŠ¡ |
| **è·¨ Region** | ~50-100ms | $0.02/GB | åŒºåŸŸåˆ†å¸ƒå¼æœåŠ¡ |

**è€ƒè™‘è·¨ AZ æµé‡æˆæœ¬çš„è°ƒåº¦ï¼š**

```yaml
# ç¤ºä¾‹ï¼šå°† API Gateway + Backend Service æ”¾ç½®åœ¨åŒä¸€ AZ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
spec:
  replicas: 6
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
        network-locality: same-az  # ç½‘ç»œå¯è§‚æµ‹æ€§æ ‡ç­¾
    spec:
      # Topology Spreadï¼šè·¨ AZ å‡åŒ€åˆ†å¸ƒ
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: api-gateway
      containers:
      - name: gateway
        image: api-gateway:v1.0
        resources:
          requests:
            cpu: "1"
            memory: 2Gi
---
# Backend Serviceï¼šä¼˜å…ˆä¸ API Gateway åœ¨åŒä¸€ AZ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-service
spec:
  replicas: 6
  selector:
    matchLabels:
      app: backend-service
  template:
    metadata:
      labels:
        app: backend-service
        network-locality: same-az
    spec:
      affinity:
        # Pod Affinityï¼šä¼˜å…ˆä¸ API Gateway åœ¨åŒä¸€ AZï¼ˆå‡å°‘è·¨ AZ æˆæœ¬ï¼‰
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - api-gateway
              topologyKey: topology.kubernetes.io/zone
      containers:
      - name: backend
        image: backend-service:v1.0
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
```

**åŸºäºç½‘ç»œå¯è§‚æµ‹æ€§çš„ Topology Spread ä¼˜åŒ–ï¼š**

åˆ†æå®¹å™¨ç½‘ç»œå¯è§‚æµ‹æ€§æŒ‡æ ‡ä»¥è°ƒæ•´è°ƒåº¦ç­–ç•¥ã€‚

```yaml
# CloudWatch Container Insights æŒ‡æ ‡æŸ¥è¯¢ç¤ºä¾‹
apiVersion: v1
kind: ConfigMap
metadata:
  name: network-metrics-query
data:
  query.json: |
    {
      "MetricName": "pod_network_rx_bytes",
      "Namespace": "ContainerInsights",
      "Dimensions": [
        {"Name": "PodName", "Value": "api-gateway-*"},
        {"Name": "Namespace", "Value": "default"}
      ],
      "Period": 300,
      "Stat": "Sum"
    }
```

**åŸºäºç½‘ç»œå¯è§‚æµ‹æ€§çš„ä¼˜åŒ–æ¨¡å¼ï¼š**

1. **æ£€æµ‹åˆ°é«˜è·¨ AZ æµé‡** â†’ ä½¿ç”¨ Pod Affinity æ”¾ç½®åœ¨åŒä¸€ AZ
2. **æ£€æµ‹åˆ°ç‰¹å®š AZ ç½‘ç»œæ‹¥å¡** â†’ ä½¿ç”¨ Topology Spread åˆ†æ•£åˆ°å…¶ä»– AZ
3. **Pod é—´é€šä¿¡æ¨¡å¼åˆ†æ** â†’ ä½¿ç”¨ Service Meshï¼ˆIstioï¼‰ä¼˜åŒ–æµé‡
4. **æ£€æµ‹åˆ°ç½‘ç»œå»¶è¿Ÿæ¿€å¢** â†’ ä½¿ç”¨ ARC Zonal Shift ç–æ•£æ•…éšœ AZ

```mermaid
flowchart TB
    subgraph "å®¹å™¨ç½‘ç»œå¯è§‚æµ‹æ€§"
        METRICS[ç½‘ç»œæŒ‡æ ‡æ”¶é›†]
        ANALYZE[æµé‡æ¨¡å¼åˆ†æ]
        ALERT[å¼‚å¸¸æ£€æµ‹å‘Šè­¦]
    end

    subgraph "è°ƒåº¦ä¼˜åŒ–"
        DECISION{ä¼˜åŒ–ç±»å‹}
        AFFINITY[è°ƒæ•´ Pod Affinity]
        SPREAD[è°ƒæ•´ Topology Spread]
        SHIFT[AZ Shift]
    end

    METRICS --> ANALYZE
    ANALYZE --> ALERT
    ALERT --> DECISION

    DECISION -->|é«˜è·¨ AZ æµé‡| AFFINITY
    DECISION -->|ç‰¹å®š AZ æ‹¥å¡| SPREAD
    DECISION -->|AZ æ•…éšœ| SHIFT

    style METRICS fill:#4286f4,stroke:#2a6acf,color:#fff
    style ALERT fill:#ff9900,stroke:#cc7a00,color:#fff
    style DECISION fill:#fbbc04,stroke:#c99603,color:#000
```

**å®æˆ˜ç¤ºä¾‹ï¼šML æ¨ç†æœåŠ¡ç½‘ç»œä¼˜åŒ–ï¼š**

```yaml
# ML æ¨ç†æœåŠ¡ï¼šä½å»¶è¿Ÿ + æˆæœ¬ä¼˜åŒ–
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference-optimized
spec:
  replicas: 9
  selector:
    matchLabels:
      app: ml-inference
  template:
    metadata:
      labels:
        app: ml-inference
    spec:
      # 1. Topology Spreadï¼šå‡åŒ€ AZ åˆ†å¸ƒï¼ˆé«˜å¯ç”¨ï¼‰
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: ml-inference
        minDomains: 3

      # 2. Pod Affinityï¼šä¸ API Gateway åœ¨åŒä¸€ AZï¼ˆä½å»¶è¿Ÿï¼‰
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - api-gateway
              topologyKey: topology.kubernetes.io/zone

      containers:
      - name: inference
        image: ml-inference:v1.0
        resources:
          requests:
            cpu: "2"
            memory: 8Gi
```

**åŸºäºç½‘ç»œå¯è§‚æµ‹æ€§ä¼˜åŒ–çš„æˆæœ¬èŠ‚çº¦ï¼š**

| ä¼˜åŒ–å‰ | ä¼˜åŒ–å | èŠ‚çº¦ |
|----------|----------|----------|
| è·¨ AZ æµé‡ï¼š1TB/æœˆ | è·¨ AZ æµé‡ï¼š0.2TB/æœˆ | æ¯æœˆèŠ‚çœ $8 |
| å¹³å‡å»¶è¿Ÿï¼š3ms | å¹³å‡å»¶è¿Ÿï¼š0.5ms | æ€§èƒ½æå‡ 6 å€ |
| æ—  Pod Affinity | Pod Affinity ä¼˜åŒ–å | è¿ç»´æ•ˆç‡æå‡ |

---

### 10.5 Node Readiness Controller â€” å¢å¼ºè°ƒåº¦å®‰å…¨

**æ¦‚è¿°ï¼š**

Node Readiness Controller (NRC) æ˜¯ Kubernetes 1.32 ä¸­å¼•å…¥çš„ Alpha åŠŸèƒ½ï¼Œç”¨äºé˜²æ­¢èŠ‚ç‚¹å¤„äº `Ready` çŠ¶æ€ä½†å®é™…æ— æ³•å®‰å…¨è¿è¡Œ Pod çš„æƒ…å†µã€‚å®ƒé€šè¿‡åœ¨ CNI æ’ä»¶ã€CSI é©±åŠ¨ã€GPU é©±åŠ¨ç­‰åŸºç¡€è®¾æ–½ç»„ä»¶å®Œå…¨å°±ç»ªä¹‹å‰é˜»æ­¢ Pod è°ƒåº¦ï¼Œæ˜¾è‘—æå‡äº†è°ƒåº¦å®‰å…¨æ€§ã€‚

**ä»è°ƒåº¦è§’åº¦çœ‹å­˜åœ¨çš„é—®é¢˜ï¼š**

ç°æœ‰çš„ Kubernetes è°ƒåº¦å™¨ä»…æ£€æŸ¥èŠ‚ç‚¹çš„ `Ready` çŠ¶æ€æ¥æ”¾ç½® Podã€‚ç„¶è€Œï¼Œåœ¨ä»¥ä¸‹æƒ…å†µä¸‹ Pod æ”¾ç½®å¯èƒ½å¤±è´¥ï¼š

| åœºæ™¯ | èŠ‚ç‚¹çŠ¶æ€ | å®é™…æƒ…å†µ | ç»“æœ |
|---------|---------|----------|------|
| **CNI æ’ä»¶æœªå°±ç»ª** | `Ready` | Calico/Cilium Pod æ­£åœ¨å¯åŠ¨ | Pod ç½‘ç»œè¿æ¥å¤±è´¥ |
| **CSI é©±åŠ¨æœªå°±ç»ª** | `Ready` | EBS CSI Driver æ­£åœ¨åˆå§‹åŒ– | PVC æŒ‚è½½å¤±è´¥ |
| **GPU é©±åŠ¨æœªå°±ç»ª** | `Ready` | NVIDIA Device Plugin æ­£åœ¨åŠ è½½ | GPU å·¥ä½œè´Ÿè½½å¯åŠ¨å¤±è´¥ |
| **é•œåƒé¢„æ‹‰å–è¿›è¡Œä¸­** | `Ready` | æ­£åœ¨ä¸‹è½½å¤§å‹é•œåƒï¼ˆ10GBï¼‰ | Pod å¯åŠ¨å»¶è¿Ÿï¼ˆ5+ åˆ†é’Ÿï¼‰ |

**Node Readiness Controller å·¥ä½œåŸç†ï¼š**

NRC ä½¿ç”¨ `NodeReadinessRule` CRD (`readiness.node.x-k8s.io/v1alpha1`)ï¼Œå…¶å·¥ä½œæ–¹å¼å¦‚ä¸‹ï¼š

1. **åŸºäºæ¡ä»¶çš„ Taint ç®¡ç†**ï¼šåœ¨æ»¡è¶³ç‰¹å®š Node Condition ä¹‹å‰åº”ç”¨ taint
2. **è°ƒåº¦å™¨é˜»æ­¢**ï¼šPod æ— æ³•è¢«è°ƒåº¦åˆ°åº”ç”¨äº† taint çš„èŠ‚ç‚¹ä¸Š
3. **è‡ªåŠ¨ Taint ç§»é™¤**ï¼šå½“æ¡ä»¶æ»¡è¶³æ—¶è‡ªåŠ¨ç§»é™¤ taintï¼Œå…è®¸ Pod è°ƒåº¦

```mermaid
sequenceDiagram
    participant Karpenter
    participant Node
    participant InfraAgent as åŸºç¡€è®¾æ–½ Agent<br/>(CNI/CSI/GPU)
    participant NRC as Node Readiness<br/>Controller
    participant Scheduler as Kube Scheduler
    participant Pod

    Karpenter->>Node: é¢„é…æ–°èŠ‚ç‚¹
    NRC->>Node: åº”ç”¨ taint<br/>(NoSchedule)

    Note over Node: èŠ‚ç‚¹å·² Ready<br/>ä½†è°ƒåº¦è¢«é˜»æ­¢

    Node->>InfraAgent: å¯åŠ¨åŸºç¡€è®¾æ–½åˆå§‹åŒ–
    InfraAgent->>InfraAgent: CNI/CSI/GPU å‡†å¤‡

    InfraAgent->>Node: Condition æ›´æ–°<br/>(NetworkReady=True)

    Node->>NRC: Condition å˜æ›´äº‹ä»¶
    NRC->>NRC: æ£€æŸ¥è§„åˆ™<br/>(æ¡ä»¶æ˜¯å¦æ»¡è¶³ï¼Ÿ)

    alt æ¡ä»¶æ»¡è¶³
        NRC->>Node: ç§»é™¤ taint
        Note over Node: å¯è°ƒåº¦çŠ¶æ€
        Scheduler->>Node: å¼€å§‹ Pod æ”¾ç½®
        Node->>Pod: å¯åŠ¨å®¹å™¨
    else æ¡ä»¶æœªæ»¡è¶³
        NRC->>Node: ç»´æŒ taint
        Note over Scheduler: Pod ä¿æŒ Pending
    end
```

**ä¸¤ç§æ‰§è¡Œæ¨¡å¼ï¼š**

NRC ä»¥ä¸¤ç§æ¨¡å¼è¿è¡Œï¼Œå„è‡ªå¯¹è°ƒåº¦å®‰å…¨æœ‰ä¸åŒçš„å½±å“ï¼š

| æ¨¡å¼ | è¡Œä¸º | è°ƒåº¦å½±å“ | é€‚ç”¨åœºæ™¯ |
|------|---------|-------------|----------|
| **bootstrap-only** | ä»…åœ¨èŠ‚ç‚¹åˆå§‹åŒ–æœŸé—´åº”ç”¨ taint<br/>â†’ å°±ç»ªåé‡Šæ”¾ï¼Œåœæ­¢ç›‘æ§ | ç¡®ä¿åˆå§‹è°ƒåº¦å®‰å…¨<br/>è¿è¡Œæ—¶æ•…éšœä¸æ£€æµ‹ | CNI æ’ä»¶ã€é•œåƒé¢„æ‹‰å–<br/>ï¼ˆä»…éœ€æ£€æŸ¥ä¸€æ¬¡ï¼‰ |
| **continuous** | æŒç»­ç›‘æ§<br/>â†’ é©±åŠ¨å´©æºƒæ—¶ç«‹å³é‡æ–°åº”ç”¨ taint | å³ä½¿è¿è¡Œæ—¶æ•…éšœä¹Ÿé˜»æ­¢æ–° Pod è°ƒåº¦ | GPU é©±åŠ¨ã€CSI é©±åŠ¨<br/>ï¼ˆå¯èƒ½å‘ç”Ÿè¿è¡Œæ—¶æ•…éšœï¼‰ |

**å®æˆ˜ç¤ºä¾‹ 1ï¼šCNI æ’ä»¶å°±ç»ªæ£€æŸ¥ï¼ˆBootstrap-onlyï¼‰**

```yaml
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: network-readiness-rule
spec:
  # ç­‰å¾… CNI æ’ä»¶æŠ¥å‘Š NetworkReady Condition ä¸º True
  conditions:
    - type: "cniplugin.example.net/NetworkReady"
      requiredStatus: "True"

  # åœ¨å°±ç»ªä¹‹å‰åº”ç”¨æ­¤ taint
  taint:
    key: "readiness.k8s.io/network-unavailable"
    effect: "NoSchedule"
    value: "pending"

  # Bootstrap-onlyï¼šå°±ç»ªååœæ­¢ç›‘æ§
  enforcementMode: "bootstrap-only"

  # ä»…åº”ç”¨äº worker èŠ‚ç‚¹
  nodeSelector:
    matchLabels:
      node.kubernetes.io/role: worker
```

**å®æˆ˜ç¤ºä¾‹ 2ï¼šGPU é©±åŠ¨æŒç»­ç›‘æ§ï¼ˆContinuousï¼‰**

```yaml
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-driver-readiness-rule
spec:
  # ç­‰å¾… NVIDIA Device Plugin æŠ¥å‘Š GPUReady Condition ä¸º True
  conditions:
    - type: "nvidia.com/gpu.present"
      requiredStatus: "True"
    - type: "nvidia.com/gpu.driver.ready"
      requiredStatus: "True"

  # åœ¨ GPU å°±ç»ªä¹‹å‰åº”ç”¨æ­¤ taint
  taint:
    key: "readiness.k8s.io/gpu-unavailable"
    effect: "NoSchedule"
    value: "pending"

  # Continuousï¼šGPU é©±åŠ¨å´©æºƒæ—¶é‡æ–°åº”ç”¨ taint é˜»æ­¢æ–° Pod è°ƒåº¦
  enforcementMode: "continuous"

  # ä»…åº”ç”¨äº GPU èŠ‚ç‚¹ç»„
  nodeSelector:
    matchLabels:
      node.kubernetes.io/instance-type: "p4d.24xlarge"
```

**ä¸ Pod Scheduling Readiness (schedulingGates) çš„æ¯”è¾ƒï¼š**

Kubernetes å¯ä»¥åœ¨ Pod çº§åˆ«å’ŒèŠ‚ç‚¹çº§åˆ«æ§åˆ¶è°ƒåº¦å®‰å…¨ï¼š

| æ¯”è¾ƒé¡¹ç›® | `schedulingGates`ï¼ˆPod çº§åˆ«ï¼‰ | `NodeReadinessRule`ï¼ˆèŠ‚ç‚¹çº§åˆ«ï¼‰ |
|----------|------------------------------|--------------------------------|
| **æ§åˆ¶ç›®æ ‡** | ç‰¹å®š Pod çš„è°ƒåº¦ | ç‰¹å®šèŠ‚ç‚¹ä¸Šæ‰€æœ‰ Pod çš„è°ƒåº¦ |
| **é€‚ç”¨åœºæ™¯** | åœ¨å¤–éƒ¨æ¡ä»¶æ»¡è¶³å‰æŒæœ‰ Pod<br/>ï¼ˆä¾‹å¦‚ç­‰å¾…æ•°æ®åº“å°±ç»ªï¼‰ | åœ¨åŸºç¡€è®¾æ–½å°±ç»ªå‰é˜»æ­¢èŠ‚ç‚¹<br/>ï¼ˆä¾‹å¦‚ CNI/GPU é©±åŠ¨åŠ è½½ï¼‰ |
| **æ¡ä»¶ä½ç½®** | åœ¨ Pod Spec ä¸­æŒ‡å®š | ä½œä¸º Node Condition æŠ¥å‘Š |
| **ç§»é™¤æ–¹å¼** | å¤–éƒ¨æ§åˆ¶å™¨ç§»é™¤ gate | NRC è‡ªåŠ¨ç§»é™¤ taint |
| **å½±å“èŒƒå›´** | å•ä¸ª Pod | èŠ‚ç‚¹ä¸Šæ‰€æœ‰æ–° Pod |

**ç»„åˆæ¨¡å¼ï¼š**

```yaml
# ç»„åˆ Pod çº§åˆ« + èŠ‚ç‚¹çº§åˆ«è°ƒåº¦å®‰å…¨
apiVersion: v1
kind: Pod
metadata:
  name: ml-training-job
spec:
  # Pod çº§åˆ«ï¼šåœ¨æ•°æ®é›†å°±ç»ªå‰æŒæœ‰è°ƒåº¦
  schedulingGates:
    - name: "example.com/dataset-ready"

  # èŠ‚ç‚¹çº§åˆ«ï¼šä»…æ”¾ç½®åœ¨ GPU é©±åŠ¨å°±ç»ªçš„èŠ‚ç‚¹ä¸Šï¼ˆNodeReadinessRule ç®¡ç† taintï¼‰
  tolerations:
    - key: "readiness.k8s.io/gpu-unavailable"
      operator: "DoesNotExist"  # ä»…å…è®¸æ²¡æœ‰ taint çš„èŠ‚ç‚¹ï¼ˆ= GPU å°±ç»ªèŠ‚ç‚¹ï¼‰

  containers:
    - name: trainer
      image: ml-trainer:v1.0
      resources:
        limits:
          nvidia.com/gpu: 8
```

**Karpenter + NRC é›†æˆæ¨¡å¼ï¼š**

åœ¨ä½¿ç”¨ Karpenter è¿›è¡ŒåŠ¨æ€èŠ‚ç‚¹é¢„é…çš„ç¯å¢ƒä¸­ï¼ŒNRC æä¾›ä»¥ä¸‹å·¥ä½œæµç¨‹ï¼š

```mermaid
flowchart TB
    subgraph "1. èŠ‚ç‚¹é¢„é…"
        PENDING[æ£€æµ‹åˆ° Pending Pod]
        KARP[Karpenter:<br/>åˆ›å»ºæ–°èŠ‚ç‚¹]
        NODE_UP[èŠ‚ç‚¹ Ready çŠ¶æ€]
    end

    subgraph "2. NRC Taint åº”ç”¨"
        NRC_DETECT[NRCï¼šæ£€æµ‹åˆ°æ–°èŠ‚ç‚¹]
        TAINT_APPLY[åº”ç”¨ taint<br/>NoSchedule]
        SCHED_BLOCK[Schedulerï¼š<br/>é˜»æ­¢æ”¾ç½®]
    end

    subgraph "3. åŸºç¡€è®¾æ–½å‡†å¤‡"
        CNI_INIT[CNI æ’ä»¶åˆå§‹åŒ–]
        CSI_INIT[CSI é©±åŠ¨åˆå§‹åŒ–]
        GPU_INIT[GPU é©±åŠ¨åŠ è½½]
        COND_UPDATE[Node Condition æ›´æ–°]
    end

    subgraph "4. Taint ç§»é™¤ä¸è°ƒåº¦"
        NRC_CHECK[NRCï¼šæ£€æŸ¥æ¡ä»¶]
        TAINT_REMOVE[ç§»é™¤ taint]
        POD_SCHED[å¼€å§‹ Pod è°ƒåº¦]
    end

    PENDING --> KARP
    KARP --> NODE_UP
    NODE_UP --> NRC_DETECT
    NRC_DETECT --> TAINT_APPLY
    TAINT_APPLY --> SCHED_BLOCK

    SCHED_BLOCK -.ç­‰å¾….-> CNI_INIT
    CNI_INIT --> CSI_INIT
    CSI_INIT --> GPU_INIT
    GPU_INIT --> COND_UPDATE

    COND_UPDATE --> NRC_CHECK
    NRC_CHECK --> TAINT_REMOVE
    TAINT_REMOVE --> POD_SCHED

    style PENDING fill:#ff9900,stroke:#cc7a00,color:#fff
    style TAINT_APPLY fill:#ea4335,stroke:#c53929,color:#fff
    style SCHED_BLOCK fill:#fbbc04,stroke:#c99603,color:#000
    style TAINT_REMOVE fill:#34a853,stroke:#2a8642,color:#fff
    style POD_SCHED fill:#4286f4,stroke:#2a6acf,color:#fff
```

**GPU èŠ‚ç‚¹ç»„å®æˆ˜ç¤ºä¾‹ï¼š**

å¯¹äºæœåŠ¡ AI/ML å·¥ä½œè´Ÿè½½çš„ GPU èŠ‚ç‚¹ç»„ï¼Œä½¿ç”¨ NRC å°† AI å·¥ä½œè´Ÿè½½è°ƒåº¦å»¶è¿Ÿåˆ° NVIDIA é©±åŠ¨åŠ è½½å®Œæˆåï¼Œé˜²æ­¢æ”¾ç½®å¤±è´¥ï¼š

```yaml
# Karpenter NodePoolï¼šGPU èŠ‚ç‚¹ç»„
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-pool
spec:
  template:
    spec:
      requirements:
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["p4d.24xlarge", "p5.48xlarge"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand"]
      nodeClassRef:
        name: gpu-nodeclass
---
# NodeReadinessRuleï¼šGPU é©±åŠ¨å°±ç»ªæ£€æŸ¥
apiVersion: readiness.node.x-k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-readiness-rule
spec:
  conditions:
    - type: "nvidia.com/gpu.driver.ready"
      requiredStatus: "True"
  taint:
    key: "readiness.k8s.io/gpu-unavailable"
    effect: "NoSchedule"
    value: "pending"
  enforcementMode: "continuous"
  nodeSelector:
    matchLabels:
      karpenter.sh/nodepool: gpu-pool
---
# AI å·¥ä½œè´Ÿè½½ï¼šé€šè¿‡ Toleration ä»…æ”¾ç½®åœ¨ GPU å°±ç»ªçš„èŠ‚ç‚¹ä¸Š
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-training
spec:
  template:
    spec:
      # ä»…æ”¾ç½®åœ¨ GPU å°±ç»ªçš„èŠ‚ç‚¹ä¸Š
      tolerations:
        - key: "readiness.k8s.io/gpu-unavailable"
          operator: "DoesNotExist"

      containers:
        - name: trainer
          image: ml-trainer:v1.0
          resources:
            limits:
              nvidia.com/gpu: 8

      restartPolicy: OnFailure
```

:::tip è°ƒåº¦å®‰å…¨ä¼˜åŒ–å»ºè®®
- **CNI æ’ä»¶**ï¼šä½¿ç”¨ `bootstrap-only` æ¨¡å¼éªŒè¯åˆå§‹ç½‘ç»œå°±ç»ª
- **GPU é©±åŠ¨**ï¼šä½¿ç”¨ `continuous` æ¨¡å¼ï¼Œå³ä½¿è¿è¡Œæ—¶æ•…éšœä¹Ÿé˜»æ­¢æ–° Pod æ”¾ç½®
- **CSI é©±åŠ¨**ï¼šä½¿ç”¨ `continuous` æ¨¡å¼å¤„ç†å­˜å‚¨é©±åŠ¨å´©æºƒ
- **é•œåƒé¢„æ‹‰å–**ï¼šä½¿ç”¨ `bootstrap-only` æ¨¡å¼ç­‰å¾…å¤§å‹é•œåƒä¸‹è½½å®Œæˆ
- **Karpenter é›†æˆ**ï¼šä¸ºæ¯ä¸ª NodePool é…ç½® NodeReadinessRule ä»¥å®ç°å·¥ä½œè´Ÿè½½ç‰¹å®šçš„å°±ç»ªæ¡ä»¶
:::

:::warning ä½¿ç”¨ Alpha åŠŸèƒ½çš„æ³¨æ„äº‹é¡¹
Node Readiness Controller æ˜¯ Kubernetes 1.32 çš„ Alpha åŠŸèƒ½ï¼š

1. **éœ€è¦æ¿€æ´» Feature Gate**ï¼š`--feature-gates=NodeReadiness=true`ï¼ˆkube-apiserverã€kube-controller-managerï¼‰
2. **API å¯èƒ½å˜æ›´**ï¼š`NodeReadinessRule` CRD schema åœ¨ Beta/GA è¿‡æ¸¡æœŸé—´å¯èƒ½å˜æ›´
3. **ç”Ÿäº§ç¯å¢ƒ**ï¼šå»ºè®®åœ¨é‡‡ç”¨å‰è¿›è¡Œå……åˆ†æµ‹è¯•
4. **æ›¿ä»£æ–¹æ¡ˆ**ï¼šå¦‚æœæ‹…å¿ƒä½¿ç”¨ Alpha åŠŸèƒ½ï¼Œå¯è€ƒè™‘æ‰‹åŠ¨ Node Taint ç®¡ç†æˆ– Init Container æ¨¡å¼
:::

**å‚è€ƒæ–‡çŒ®ï¼š**

- [Kubernetes Blog: Introducing Node Readiness Controller](https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/)
- [Node Readiness Controller GitHub](https://github.com/kubernetes-sigs/node-readiness-controller)

---

## 11. ç»¼åˆæ£€æŸ¥æ¸…å•ä¸å‚è€ƒæ–‡çŒ®

### 11.1 ç»¼åˆæ£€æŸ¥æ¸…å•

åœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‰ï¼Œä½¿ç”¨ä»¥ä¸‹æ£€æŸ¥æ¸…å•éªŒè¯æ‚¨çš„è°ƒåº¦é…ç½®ã€‚

#### åŸºç¡€è°ƒåº¦ï¼ˆæ‰€æœ‰å·¥ä½œè´Ÿè½½ï¼‰

| é¡¹ç›® | è¯´æ˜ | æ£€æŸ¥ |
|------|------|------|
| **é…ç½® Resource Requests** | æ‰€æœ‰å®¹å™¨æŒ‡å®š CPU å’Œ Memory requests | [ ] |
| **åˆ†é… PriorityClass** | ä¸å·¥ä½œè´Ÿè½½é‡è¦æ€§åŒ¹é…çš„ PriorityClass | [ ] |
| **Liveness/Readiness Probe** | é…ç½®å¥åº·æ£€æŸ¥ç¡®ä¿ Pod ç¨³å®šæ€§ | [ ] |
| **ä¼˜é›…å…³é—­** | preStop Hook + terminationGracePeriodSeconds | [ ] |
| **é•œåƒæ‹‰å–ç­–ç•¥** | ç”Ÿäº§ç¯å¢ƒï¼š`IfNotPresent` æˆ– `Always` | [ ] |

#### é«˜å¯ç”¨ï¼ˆå…³é”®å·¥ä½œè´Ÿè½½ï¼‰

| é¡¹ç›® | è¯´æ˜ | æ£€æŸ¥ |
|------|------|------|
| **å‰¯æœ¬æ•° >= 3** | æ•…éšœåŸŸéš”ç¦»æ‰€éœ€çš„æœ€å°å‰¯æœ¬æ•° | [ ] |
| **Topology Spread Constraints** | è·¨ AZ å‡åŒ€åˆ†å¸ƒï¼ˆmaxSkew: 1ï¼‰ | [ ] |
| **Pod Anti-Affinity** | èŠ‚ç‚¹åˆ†å¸ƒï¼ˆSoft æˆ– Hardï¼‰ | [ ] |
| **é…ç½® PDB** | æŒ‡å®š minAvailable æˆ– maxUnavailable | [ ] |
| **PDB éªŒè¯** | éªŒè¯ `minAvailable &lt; replicas` | [ ] |
| **Multi-AZ éƒ¨ç½²éªŒè¯** | ä½¿ç”¨ `kubectl get pods -o wide` éªŒè¯ AZ åˆ†å¸ƒ | [ ] |

#### èµ„æºä¼˜åŒ–

| é¡¹ç›® | è¯´æ˜ | æ£€æŸ¥ |
|------|------|------|
| **Spot èŠ‚ç‚¹åˆ©ç”¨** | å¯é‡å¯å·¥ä½œè´Ÿè½½å…è®¸ä½¿ç”¨ Spot èŠ‚ç‚¹ | [ ] |
| **Node Affinity ä¼˜åŒ–** | ä¸ºå·¥ä½œè´Ÿè½½é€‰æ‹©åˆé€‚çš„å®ä¾‹ç±»å‹ | [ ] |
| **Taints/Tolerations** | ä¸º GPUã€é«˜æ€§èƒ½ç­‰éš”ç¦»ä¸“ç”¨èŠ‚ç‚¹ | [ ] |
| **é…ç½® Descheduler** | è§£å†³èŠ‚ç‚¹ä¸å‡è¡¡ï¼ˆå¯é€‰ï¼‰ | [ ] |
| **Karpenter é›†æˆ** | é…ç½® disruption budget | [ ] |

#### ç‰¹æ®Šå·¥ä½œè´Ÿè½½

| é¡¹ç›® | è¯´æ˜ | æ£€æŸ¥ |
|------|------|------|
| **GPU å·¥ä½œè´Ÿè½½** | GPU Taint Tolerate + GPU èµ„æºè¯·æ±‚ | [ ] |
| **StatefulSet** | ä½¿ç”¨ WaitForFirstConsumer StorageClass | [ ] |
| **DaemonSet** | é…ç½®æ‰€æœ‰ Taints Tolerate | [ ] |
| **æ‰¹å¤„ç† Job** | PriorityClass: low-priority, preemptionPolicy: Never | [ ] |

### Pod è°ƒåº¦éªŒè¯å‘½ä»¤

```bash
# 1. æ£€æŸ¥ Pod æ”¾ç½®ï¼ˆAZã€èŠ‚ç‚¹åˆ†å¸ƒï¼‰
kubectl get pods -n <namespace> -o wide

# 2. æ£€æŸ¥ Pod è°ƒåº¦äº‹ä»¶ï¼ˆå®šä½ Pending åŸå› ï¼‰
kubectl describe pod <pod-name> -n <namespace>

# 3. æ£€æŸ¥ PDB çŠ¶æ€
kubectl get pdb -A
kubectl describe pdb <pdb-name> -n <namespace>

# 4. åˆ—å‡º PriorityClasses
kubectl get priorityclass

# 5. æ£€æŸ¥èŠ‚ç‚¹ Taints
kubectl describe node <node-name> | grep Taints

# 6. æ£€æŸ¥æ¯ä¸ªèŠ‚ç‚¹çš„ Pod åˆ†å¸ƒ
kubectl get pods -A -o wide | awk '{print $8}' | sort | uniq -c

# 7. æ£€æŸ¥æ¯ä¸ª AZ çš„ Pod åˆ†å¸ƒ
kubectl get pods -A -o json | \
  jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name) \(.spec.nodeName)"' | \
  while read ns pod node; do
    az=$(kubectl get node $node -o jsonpath='{.metadata.labels.topology\.kubernetes\.io/zone}')
    echo "$ns $pod $node $az"
  done | column -t

# 8. åˆ†æ Pending Pod åŸå› 
kubectl get events --sort-by='.lastTimestamp' -A | grep -i warning

# 9. æ£€æŸ¥ Descheduler æ—¥å¿—ï¼ˆå¦‚å·²å®‰è£…ï¼‰
kubectl logs -n kube-system -l app=descheduler --tail=100
```

### 11.2 ç›¸å…³æ–‡æ¡£

**å†…éƒ¨æ–‡æ¡£ï¼š**
- [EKS é«˜å¯ç”¨æ¶æ„æŒ‡å—](/docs/operations-observability/eks-resiliency-guide) â€” Multi-AZ ç­–ç•¥ã€Topology Spreadã€Cell Architecture
- [Karpenter è¶…å¿«é€Ÿè‡ªåŠ¨æ‰©ç¼©](/docs/infrastructure-optimization/karpenter-autoscaling) â€” æ·±å…¥ Karpenter NodePool é…ç½®
- [EKS èµ„æºä¼˜åŒ–æŒ‡å—](/docs/infrastructure-optimization/eks-resource-optimization) â€” Resource Requests/Limits ä¼˜åŒ–
- [EKS Pod å¥åº·æ£€æŸ¥ä¸ç”Ÿå‘½å‘¨æœŸ](/docs/operations-observability/eks-pod-health-lifecycle) â€” Probesã€Lifecycle Hooks

### 11.3 å¤–éƒ¨å‚è€ƒæ–‡çŒ®

**Kubernetes å®˜æ–¹æ–‡æ¡£ï¼š**
- [Kubernetes Scheduling Framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
- [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)
- [Pod Priority and Preemption](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/)
- [Taints and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)
- [Pod Topology Spread Constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)
- [PodDisruptionBudget](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)

**Deschedulerï¼š**
- [Descheduler GitHub](https://github.com/kubernetes-sigs/descheduler)
- [Descheduler Strategies](https://github.com/kubernetes-sigs/descheduler#policy-and-strategies)

**AWS EKS å®˜æ–¹æ–‡æ¡£ï¼š**
- [EKS Best Practices â€” Reliability](https://docs.aws.amazon.com/eks/latest/best-practices/reliability.html)
- [Karpenter Scheduling](https://karpenter.sh/docs/concepts/scheduling/)
- [EKS Node Taints](https://docs.aws.amazon.com/eks/latest/userguide/node-taints-managed-node-groups.html)

**AWS re:Invent 2025 èµ„æºï¼š**
- [Amazon EKS introduces Provisioned Control Plane](https://aws.amazon.com/blogs/containers/amazon-eks-introduces-provisioned-control-plane/) â€” XL/2XL/4XL å±‚çº§çš„è°ƒåº¦æ€§èƒ½
- [Getting started with Amazon EKS Auto Mode](https://aws.amazon.com/blogs/containers/getting-started-with-amazon-eks-auto-mode) â€” è‡ªåŠ¨èŠ‚ç‚¹é…ç½®
- [Enhance Kubernetes high availability with ARC and Karpenter](https://aws.amazon.com/blogs/containers/enhance-kubernetes-high-availability-with-amazon-application-recovery-controller-and-karpenter-integration/) â€” è‡ªåŠ¨ AZ ç–æ•£æ¨¡å¼
- [Monitor network performance across EKS clusters](https://aws.amazon.com/blogs/aws/monitor-network-performance-and-traffic-across-your-eks-clusters-with-container-network-observability/) â€” å®¹å™¨ç½‘ç»œå¯è§‚æµ‹æ€§
- [Proactive EKS monitoring with CloudWatch Operator](https://aws.amazon.com/blogs/containers/proactive-amazon-eks-monitoring-with-amazon-cloudwatch-operator-and-aws-control-plane-metrics/) â€” Control Plane æŒ‡æ ‡

**Red Hat OpenShift æ–‡æ¡£ï¼š**
- [Controlling Pod Placement with Taints and Tolerations](https://docs.openshift.com/container-platform/4.18/nodes/scheduling/nodes-scheduler-taints-tolerations.html) â€” Taints/Tolerations æ“ä½œ
- [Placing Pods on Specific Nodes with Pod Affinity](https://docs.openshift.com/container-platform/4.18/nodes/scheduling/nodes-scheduler-pod-affinity.html) â€” Pod Affinity/Anti-Affinity é…ç½®
- [Evicting Pods Using the Descheduler](https://docs.openshift.com/container-platform/4.18/nodes/scheduling/nodes-descheduler.html) â€” Descheduler ç­–ç•¥å’Œé…ç½®
- [Managing Pods](https://docs.openshift.com/container-platform/4.18/nodes/pods/nodes-pods-configuring.html) â€” Pod ç®¡ç†å’Œè°ƒåº¦åŸºç¡€

**ç¤¾åŒºï¼š**
- [CNCF Scheduler SIG](https://github.com/kubernetes/community/tree/master/sig-scheduling)
- [Kubernetes Scheduling Deep Dive (KubeCon)](https://www.youtube.com/results?search_query=kubecon+scheduling)
- [AWS re:Invent 2025 â€” Amazon EKS Sessions](https://aws.amazon.com/blogs/containers/guide-to-amazon-eks-and-kubernetes-sessions-at-aws-reinvent-2025/)