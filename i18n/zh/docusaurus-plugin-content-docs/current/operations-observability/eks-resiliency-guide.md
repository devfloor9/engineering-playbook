---
title: "EKS é«˜å¯ç”¨æ¶æ„æŒ‡å—"
sidebar_label: "4. EKS HA Architecture"
description: "åœ¨ Amazon EKS ç¯å¢ƒä¸­å®ç°é«˜å¯ç”¨æ€§å’Œå®¹é”™èƒ½åŠ›çš„æ¶æ„æ¨¡å¼ä¸è¿ç»´ç­–ç•¥"
tags: [eks, kubernetes, resiliency, high-availability, cell-architecture, chaos-engineering, multi-az]
category: "observability-monitoring"
sidebar_position: 4
last_update:
  date: 2026-02-13
  author: devfloor9
---

# EKS é«˜å¯ç”¨æ¶æ„æŒ‡å—

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2026-02-10 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-13 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 17 åˆ†é’Ÿ

> **ğŸ“Œ å‚è€ƒç¯å¢ƒ**: EKS 1.30+, Karpenter v1.x, Istio 1.22+

## 1. æ¦‚è¿°

å¼¹æ€§ï¼ˆResiliencyï¼‰æ˜¯æŒ‡ç³»ç»Ÿåœ¨é¢å¯¹æ•…éšœæ—¶æ¢å¤åˆ°æ­£å¸¸çŠ¶æ€çš„èƒ½åŠ›ï¼Œæˆ–åœ¨æœ€å°åŒ–æ•…éšœå½±å“çš„åŒæ—¶ç»´æŒæœåŠ¡çš„èƒ½åŠ›ã€‚äº‘åŸç”Ÿç¯å¢ƒä¸­å¼¹æ€§çš„æ ¸å¿ƒåŸåˆ™å¾ˆç®€å•ï¼š**æ•…éšœå¿…ç„¶ä¼šå‘ç”Ÿâ€”â€”ä¸ºæ­¤åšå¥½è®¾è®¡ã€‚**

ä»å•ä¸ª Pod æ•…éšœåˆ°åŒºåŸŸçº§ä¸­æ–­ï¼Œç†è§£æ¯ä¸€å±‚çš„æ•…éšœåŸŸï¼ˆFailure Domainï¼‰å¹¶å»ºç«‹ç›¸åº”çš„é˜²å¾¡ç­–ç•¥ï¼Œæ˜¯ EKS è¿ç»´çš„å…³é”®ã€‚

### æ•…éšœåŸŸå±‚çº§

```mermaid
graph TB
    subgraph "æ•…éšœåŸŸå±‚çº§"
        POD[Pod æ•…éšœ<br/>å®¹å™¨å´©æºƒ, OOM]
        NODE[èŠ‚ç‚¹æ•…éšœ<br/>å®ä¾‹ç»ˆæ­¢, ç¡¬ä»¶æ•…éšœ]
        AZ[AZ æ•…éšœ<br/>æ•°æ®ä¸­å¿ƒç”µåŠ›, ç½‘ç»œåˆ†åŒº]
        REGION[åŒºåŸŸæ•…éšœ<br/>åŒºåŸŸçº§æœåŠ¡ä¸­æ–­]
        GLOBAL[å…¨å±€æ•…éšœ<br/>å…¨çƒæœåŠ¡ä¸­æ–­]
    end

    subgraph "ç¼“è§£ç­–ç•¥"
        S1[Liveness/Readiness Probe<br/>PDB, è‡ªåŠ¨é‡å¯]
        S2[Topology Spread<br/>Pod Anti-Affinity]
        S3[Multi-AZ éƒ¨ç½²<br/>ARC Zonal Shift]
        S4[Multi-Region æ¶æ„<br/>Global Accelerator]
        S5[Multi-Cloud / CDN<br/>DNS Failover]
    end

    subgraph "å½±å“èŒƒå›´"
        I1[å•ä¸ªæœåŠ¡éƒ¨åˆ†é™çº§]
        I2[å—å½±å“èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰ Pod]
        I3[è¯¥ AZ ä¸­çš„æ‰€æœ‰å·¥ä½œè´Ÿè½½]
        I4[è¯¥åŒºåŸŸä¸­çš„æ‰€æœ‰æœåŠ¡]
        I5[æœåŠ¡å®Œå…¨ä¸å¯ç”¨]
    end

    POD --> S1
    NODE --> S2
    AZ --> S3
    REGION --> S4
    GLOBAL --> S5

    S1 --> I1
    S2 --> I2
    S3 --> I3
    S4 --> I4
    S5 --> I5

    style POD fill:#34a853,stroke:#2a8642,color:#fff
    style NODE fill:#fbbc04,stroke:#c99603,color:#000
    style AZ fill:#ff9900,stroke:#cc7a00,color:#fff
    style REGION fill:#ff4444,stroke:#cc3636,color:#fff
    style GLOBAL fill:#ff4444,stroke:#cc3636,color:#fff
    style S1 fill:#4286f4,stroke:#2a6acf,color:#fff
    style S2 fill:#4286f4,stroke:#2a6acf,color:#fff
    style S3 fill:#4286f4,stroke:#2a6acf,color:#fff
    style S4 fill:#4286f4,stroke:#2a6acf,color:#fff
    style S5 fill:#4286f4,stroke:#2a6acf,color:#fff
```

### å¼¹æ€§æˆç†Ÿåº¦æ¨¡å‹

ç»„ç»‡å¯ä»¥å°†å…¶å¼¹æ€§çº§åˆ«åˆ†ä¸º 4 ä¸ªé˜¶æ®µï¼Œå¹¶ä»å½“å‰ä½ç½®é€æ­¥æå‡ã€‚

| çº§åˆ« | é˜¶æ®µ | æ ¸å¿ƒèƒ½åŠ› | å®æ–½é¡¹ç›® | å¤æ‚åº¦ | æˆæœ¬å½±å“ |
|-------|-------|----------|----------|--------|----------|
| **1** | åŸºç¡€ | Pod çº§å¼¹æ€§ | Probe é…ç½®, PDB, Graceful Shutdown, Resource Limits | ä½ | æœ€å° |
| **2** | Multi-AZ | AZ å®¹é”™ | Topology Spread, Multi-AZ NodePool, ARC Zonal Shift | ä¸­ | è·¨ AZ æµé‡æˆæœ¬ |
| **3** | Cell-Based | çˆ†ç‚¸åŠå¾„éš”ç¦» | Cell Architecture, Shuffle Sharding, ç‹¬ç«‹éƒ¨ç½² | é«˜ | æ¯ä¸ª Cell çš„é¢å¤–å¼€é”€ |
| **4** | Multi-Region | åŒºåŸŸå®¹é”™ | Active-Active æ¶æ„, Global Accelerator, æ•°æ®å¤åˆ¶ | æé«˜ | æ¯åŒºåŸŸåŸºç¡€è®¾æ–½æˆæœ¬ |

:::info æ•…éšœè¯Šæ–­ä¸å“åº”æŒ‡å—å‚è€ƒ
æœ‰å…³è¿ç»´æ•…éšœæ’é™¤å’Œäº‹ä»¶è§£å†³ï¼Œè¯·å‚é˜… [EKS æ•…éšœè¯Šæ–­ä¸å“åº”æŒ‡å—](./eks-debugging-guide.md)ã€‚æœ¬æ–‡æ¡£ä¾§é‡äºæ•…éšœçš„**é¢„é˜²**å’Œ**è®¾è®¡**ï¼Œè€Œå®æ—¶æ•…éšœæ’é™¤åˆ™åœ¨æ•…éšœè¯Šæ–­æŒ‡å—ä¸­ä»‹ç»ã€‚
:::

---

## 2. Multi-AZ ç­–ç•¥

Multi-AZ éƒ¨ç½²æ˜¯ EKS å¼¹æ€§ä¸­æœ€åŸºç¡€ä½†ä¹Ÿæœ€å¼ºå¤§çš„ç­–ç•¥ã€‚å®ƒå°†å·¥ä½œè´Ÿè½½åˆ†å¸ƒåˆ°å¤šä¸ªå¯ç”¨åŒºï¼Œä½¿å•ä¸ª AZ æ•…éšœä¸ä¼šå¯¼è‡´æ•´ä¸ªæœåŠ¡ä¸­æ–­ã€‚

### Pod Topology Spread Constraints

Topology Spread Constraints å°† Pod å‡åŒ€åˆ†å¸ƒåˆ° AZã€èŠ‚ç‚¹å’Œè‡ªå®šä¹‰æ‹“æ‰‘åŸŸä¸­ã€‚åœ¨ K8s 1.30+ ä¸­ï¼Œ`minDomains` å‚æ•°å…è®¸æŒ‡å®šæœ€å°åˆ†å¸ƒåŸŸæ•°é‡ã€‚

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `maxSkew` | åŸŸé—´æœ€å¤§ Pod æ•°é‡å·®å¼‚ | AZ: 1, Node: 2 |
| `topologyKey` | åˆ†å¸ƒä¾æ®çš„æ ‡ç­¾ | `topology.kubernetes.io/zone` |
| `whenUnsatisfiable` | çº¦æŸæ— æ³•æ»¡è¶³æ—¶çš„è¡Œä¸º | `DoNotSchedule`ï¼ˆç¡¬çº¦æŸï¼‰æˆ– `ScheduleAnyway`ï¼ˆè½¯çº¦æŸï¼‰ |
| `minDomains` | æœ€å°åˆ†å¸ƒåŸŸæ•°é‡ | ä¸ AZ æ•°é‡ç›¸åŒï¼ˆä¾‹å¦‚ 3ï¼‰ |
| `labelSelector` | ç›®æ ‡ Pod é€‰æ‹©å™¨ | ä¸ Deployment çš„ matchLabels ç›¸åŒ |

**ç¡¬çº¦æŸ + è½¯çº¦æŸç»„åˆç­–ç•¥**ï¼ˆæ¨èï¼‰ï¼š

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: critical-app
  template:
    metadata:
      labels:
        app: critical-app
    spec:
      topologySpreadConstraints:
      # ç¡¬çº¦æŸï¼šè·¨ AZ å‡åŒ€åˆ†å¸ƒï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: critical-app
        minDomains: 3
      # è½¯çº¦æŸï¼šè·¨èŠ‚ç‚¹åˆ†å¸ƒï¼ˆå°½åŠ›è€Œä¸ºï¼‰
      - maxSkew: 2
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: critical-app
```

:::tip maxSkew é…ç½®æç¤º
`maxSkew: 1` ç¡®ä¿æœ€ä¸¥æ ¼çš„å‡åŒ€åˆ†å¸ƒã€‚åœ¨ 3 ä¸ª AZ ä¸­éƒ¨ç½² 6 ä¸ªå‰¯æœ¬æ—¶ï¼Œæ¯ä¸ª AZ æ°å¥½æ”¾ç½® 2 ä¸ªã€‚å¦‚æœæ‰©å®¹é€Ÿåº¦å¾ˆé‡è¦ï¼Œè®¾ç½® `maxSkew: 2` å¯ä»¥åœ¨è¾ƒå®½æ¾çš„åˆ†å¸ƒä¸‹æä¾›æ›´å¤šè°ƒåº¦çµæ´»æ€§ã€‚
:::

### AZ æ„ŸçŸ¥çš„ Karpenter é…ç½®

åœ¨ Karpenter v1 GA ä¸­ï¼ŒMulti-AZ åˆ†å¸ƒã€Disruption é¢„ç®—å’Œ Spot + On-Demand æ··åˆç­–ç•¥å‡é€šè¿‡æ¯ä¸ª NodePool å£°æ˜å¼é…ç½®ã€‚

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: multi-az-pool
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 5m
    # Disruption é¢„ç®—ï¼šé™åˆ¶åŒæ—¶ä¸­æ–­çš„èŠ‚ç‚¹ä¸è¶…è¿‡ 20%
    budgets:
    - nodes: "20%"
  template:
    spec:
      requirements:
      # è·¨ 3 ä¸ª AZ é…ç½®èŠ‚ç‚¹
      - key: topology.kubernetes.io/zone
        operator: In
        values: ["us-east-1a", "us-east-1b", "us-east-1c"]
      # æ··åˆ Spot + On-Demand ä»¥å®ç°æˆæœ¬ä¼˜åŒ– + ç¨³å®šæ€§
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]
      - key: node.kubernetes.io/instance-type
        operator: In
        values:
          - c6i.xlarge
          - c6i.2xlarge
          - c6i.4xlarge
          - c7i.xlarge
          - c7i.2xlarge
          - c7i.4xlarge
          - m6i.xlarge
          - m6i.2xlarge
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: multi-az
  limits:
    cpu: "1000"
    memory: 2000Gi
```

:::warning Spot å®ä¾‹ä¸ Multi-AZ
Spot å®ä¾‹åœ¨æ¯ä¸ª AZ ä¸­æœ‰ä¸åŒçš„å®¹é‡æ± ã€‚æŒ‡å®š 15 ç§æˆ–æ›´å¤šä¸åŒçš„å®ä¾‹ç±»å‹å¯ä»¥æœ€å¤§é™åº¦åœ°å‡å°‘å›  Spot å®¹é‡ä¸è¶³å¯¼è‡´çš„é…ç½®å¤±è´¥ã€‚å§‹ç»ˆå°†å…³é”®ä»»åŠ¡å·¥ä½œè´Ÿè½½çš„åŸºç¡€å®¹é‡è¿è¡Œåœ¨ On-Demand ä¸Šã€‚
:::

### é€šè¿‡èŠ‚ç‚¹å°±ç»ªæœºåˆ¶å®‰å…¨æ”¾ç½®å·¥ä½œè´Ÿè½½

åœ¨ Multi-AZ ç¯å¢ƒä¸­ï¼Œå½“æ–°èŠ‚ç‚¹è¢«é…ç½®æ—¶ï¼ŒèŠ‚ç‚¹å¯èƒ½åœ¨å®é™…å‡†å¤‡å¥½æ¥å—å·¥ä½œè´Ÿè½½ä¹‹å‰å°±è¾¾åˆ° `Ready` çŠ¶æ€ã€‚åˆ©ç”¨ä»¥ä¸‹ Kubernetes å°±ç»ªæœºåˆ¶å¯ä»¥é˜²æ­¢è¿‡æ—©è°ƒåº¦ã€‚

#### Node Readiness Controllerï¼ˆ2026 å¹´ 2 æœˆå‘å¸ƒï¼‰

[Node Readiness Controller](https://github.com/kubernetes-sigs/node-readiness-controller) åœ¨èŠ‚ç‚¹å¼•å¯¼è¿‡ç¨‹ä¸­å£°æ˜å¼ç®¡ç†è‡ªå®šä¹‰ taintï¼Œå»¶è¿Ÿå·¥ä½œè´Ÿè½½è°ƒåº¦ç›´åˆ°æ‰€æœ‰åŸºç¡€è®¾æ–½è¦æ±‚å¾—åˆ°æ»¡è¶³â€”â€”åŒ…æ‹¬ GPU é©±åŠ¨ã€CNI æ’ä»¶ã€CSI é©±åŠ¨å’Œå®‰å…¨ä»£ç†ã€‚

```mermaid
flowchart TD
    subgraph "èŠ‚ç‚¹å¼•å¯¼é˜¶æ®µ"
        NP[èŠ‚ç‚¹å·²é…ç½®<br/>kubelet å¯åŠ¨] --> NR[èŠ‚ç‚¹ Ready çŠ¶æ€]
        NR --> T1[Taint: node.readiness/gpu=NotReady]
        NR --> T2[Taint: node.readiness/cni=NotReady]
        NR --> T3[Taint: node.readiness/security=NotReady]
    end

    subgraph "å¥åº·ä¿¡å·æ”¶é›†"
        T1 --> G[GPU é©±åŠ¨åŠ è½½å®Œæˆ]
        T2 --> C[CNI åˆå§‹åŒ–å®Œæˆ]
        T3 --> S[å®‰å…¨ä»£ç†å®‰è£…å®Œæˆ]
    end

    subgraph "Taint ç§»é™¤"
        G --> R1[GPU Taint å·²ç§»é™¤ âœ…]
        C --> R2[CNI Taint å·²ç§»é™¤ âœ…]
        S --> R3[Security Taint å·²ç§»é™¤ âœ…]
    end

    R1 --> WS[å¼€å§‹å·¥ä½œè´Ÿè½½è°ƒåº¦]
    R2 --> WS
    R3 --> WS
```

**å¼¹æ€§ä¼˜åŠ¿ï¼š**

- **AZ æ•…éšœæ¢å¤æœŸé—´**ï¼šå½“ Karpenter åœ¨æ–° AZ ä¸­é…ç½®èŠ‚ç‚¹æ—¶ï¼Œå·¥ä½œè´Ÿè½½ä»…åœ¨èŠ‚ç‚¹å®Œå…¨å°±ç»ªåæ‰ä¼šè¢«è°ƒåº¦
- **æ‰©å®¹äº‹ä»¶**ï¼šå³ä½¿åœ¨å¿«é€Ÿæ‰©å®¹æœŸé—´ï¼Œå·¥ä½œè´Ÿè½½ä¹Ÿä¸ä¼šè¢«æ”¾ç½®åœ¨æœªå®Œæˆå‡†å¤‡çš„èŠ‚ç‚¹ä¸Š
- **GPU/ML å·¥ä½œè´Ÿè½½**ï¼šé˜²æ­¢åœ¨é©±åŠ¨åŠ è½½å®Œæˆå‰è¿›è¡Œè°ƒåº¦ï¼Œé¿å… `CrashLoopBackOff`

#### Pod Scheduling Readinessï¼ˆK8s 1.30 GAï¼‰

`schedulingGates` å…è®¸ä» Pod ä¾§æ§åˆ¶è°ƒåº¦æ—¶æœºã€‚å¤–éƒ¨ç³»ç»ŸéªŒè¯å°±ç»ªçŠ¶æ€åç§»é™¤é—¨æ§ä»¥å…è®¸è°ƒåº¦ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: validated-pod
spec:
  schedulingGates:
    - name: "example.com/capacity-validation"
    - name: "example.com/security-clearance"
  containers:
    - name: app
      image: app:latest
      resources:
        requests:
          cpu: "4"
          memory: "8Gi"
```

**ä½¿ç”¨åœºæ™¯ï¼š**

- ä»…åœ¨èµ„æºé…é¢é¢„éªŒè¯é€šè¿‡åæ‰å…è®¸è°ƒåº¦
- ä»…åœ¨å®‰å…¨å®¡æ‰¹é€šè¿‡åæ‰å…è®¸è°ƒåº¦
- ä»…åœ¨è‡ªå®šä¹‰å‡†å…¥æ£€æŸ¥é€šè¿‡åæ‰å…è®¸è°ƒåº¦

#### Pod Readiness Gatesï¼ˆAWS LB Controllerï¼‰

AWS Load Balancer Controller çš„ Pod Readiness Gates ç¡®ä¿æ»šåŠ¨æ›´æ–°æœŸé—´çš„**é›¶åœæœºéƒ¨ç½²**ï¼š

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    elbv2.k8s.aws/pod-readiness-gate-inject: enabled  # å¯ç”¨è‡ªåŠ¨æ³¨å…¥
```

åœ¨æ–° Pod æ³¨å†Œä¸º ALB/NLB ç›®æ ‡å¹¶é€šè¿‡å¥åº·æ£€æŸ¥ä¹‹å‰ï¼Œæ—§ Pod ä¸ä¼šè¢«ç»ˆæ­¢ï¼Œç¡®ä¿éƒ¨ç½²æœŸé—´é›¶æµé‡ä¸¢å¤±ã€‚

:::tip å°±ç»ªç‰¹æ€§é€‰æ‹©æŒ‡å—

| éœ€æ±‚ | æ¨èç‰¹æ€§ | å±‚çº§ |
|------|----------|------|
| ä¿è¯èŠ‚ç‚¹å¼•å¯¼å®Œæˆ | Node Readiness Controller | èŠ‚ç‚¹ |
| Pod è°ƒåº¦å‰çš„å¤–éƒ¨éªŒè¯ | Pod Scheduling Readiness | Pod |
| ä»…åœ¨ LB æ³¨å†Œåæ‰æ¥æ”¶æµé‡ | Pod Readiness Gates | Pod |
| GPU/ä¸“ç”¨ç¡¬ä»¶å°±ç»ª | Node Readiness Controller | èŠ‚ç‚¹ |
| é›¶åœæœºæ»šåŠ¨éƒ¨ç½² | Pod Readiness Gates | Pod |
:::

### AZ è§„é¿éƒ¨ç½²ç­–ç•¥ï¼ˆARC Zonal Shiftï¼‰

AWS Application Recovery Controller (ARC) Zonal Shift åœ¨æ£€æµ‹åˆ°é—®é¢˜æ—¶ï¼Œè‡ªåŠ¨æˆ–æ‰‹åŠ¨å°†æµé‡ä»æŸä¸ª AZ é‡å®šå‘ã€‚EKS è‡ª 2024 å¹´ 11 æœˆèµ·æ”¯æŒ ARC Zonal Shiftã€‚

```mermaid
flowchart LR
    subgraph "AZ æ•…éšœæ£€æµ‹ä¸å“åº”"
        HD[AWS Health Dashboard<br/>æ•…éšœäº‹ä»¶æ£€æµ‹]
        EB[EventBridge Rule<br/>äº‹ä»¶è¿‡æ»¤]
        LM[Lambda Function<br/>è‡ªåŠ¨å“åº”]
    end

    subgraph "ARC Zonal Shift"
        ZA[Zonal Autoshift<br/>AWS è‡ªåŠ¨æµé‡è½¬ç§»]
        ZS[æ‰‹åŠ¨ Zonal Shift<br/>è¿ç»´äººå‘˜æ‰‹åŠ¨è½¬ç§»]
    end

    subgraph "EKS é›†ç¾¤"
        AZ1[AZ-1a<br/>å¥åº·]
        AZ2[AZ-1b<br/>å—æŸ]
        AZ3[AZ-1c<br/>å¥åº·]
    end

    HD --> EB
    EB --> LM
    LM --> ZS
    ZA --> AZ2

    AZ2 -.->|æµé‡é˜»æ–­| AZ1
    AZ2 -.->|æµé‡é˜»æ–­| AZ3

    style AZ2 fill:#ff4444,stroke:#cc3636,color:#fff
    style AZ1 fill:#34a853,stroke:#2a8642,color:#fff
    style AZ3 fill:#34a853,stroke:#2a8642,color:#fff
    style ZA fill:#ff9900,stroke:#cc7a00,color:#fff
    style LM fill:#ff9900,stroke:#cc7a00,color:#fff
```

**å¯ç”¨å’Œä½¿ç”¨ ARC Zonal Shiftï¼š**

```bash
# åœ¨ EKS é›†ç¾¤ä¸Šå¯ç”¨ Zonal Shift
aws eks update-cluster-config \
  --name my-cluster \
  --zonal-shift-config enabled=true

# å¯åŠ¨æ‰‹åŠ¨ Zonal Shiftï¼ˆå°†æµé‡ä»æŒ‡å®š AZ é‡å®šå‘ï¼‰
aws arc-zonal-shift start-zonal-shift \
  --resource-identifier arn:aws:eks:us-east-1:123456789012:cluster/my-cluster \
  --away-from us-east-1b \
  --expires-in 3h \
  --comment "AZ-b impairment detected via Health Dashboard"

# æ£€æŸ¥ Zonal Shift çŠ¶æ€
aws arc-zonal-shift list-zonal-shifts \
  --resource-identifier arn:aws:eks:us-east-1:123456789012:cluster/my-cluster
```

:::info Zonal Shift é™åˆ¶
Zonal Shift çš„æœ€å¤§æŒç»­æ—¶é—´ä¸º **3 å¤©**ï¼Œå¦‚æœ‰éœ€è¦å¯ä»¥å»¶é•¿ã€‚å¯ç”¨ Zonal Autoshift åï¼ŒAWS å¯ä»¥è‡ªåŠ¨æ£€æµ‹ AZ çº§åˆ«çš„æŸä¼¤å¹¶è½¬ç§»æµé‡ã€‚
:::

**ç´§æ€¥ AZ ç–æ•£è„šæœ¬ï¼š**

```bash
#!/bin/bash
# az-evacuation.sh - å®‰å…¨ç–æ•£å—æŸ AZ ä¸­çš„æ‰€æœ‰å·¥ä½œè´Ÿè½½
IMPAIRED_AZ=$1

if [ -z "$IMPAIRED_AZ" ]; then
  echo "Usage: $0 <az-name>"
  echo "Example: $0 us-east-1b"
  exit 1
fi

echo "=== AZ ç–æ•£: ${IMPAIRED_AZ} ==="

# 1. å°é”å—å½±å“ AZ ä¸­çš„èŠ‚ç‚¹ï¼ˆé˜»æ­¢æ–° Pod è°ƒåº¦ï¼‰
echo "[æ­¥éª¤ 1] æ­£åœ¨å°é” ${IMPAIRED_AZ} ä¸­çš„èŠ‚ç‚¹..."
kubectl get nodes -l topology.kubernetes.io/zone=${IMPAIRED_AZ} -o name | \
  xargs -I {} kubectl cordon {}

# 2. æ’ç©ºå—å½±å“ AZ ä¸­çš„èŠ‚ç‚¹ï¼ˆå®‰å…¨è¿ç§»ç°æœ‰ Podï¼‰
echo "[æ­¥éª¤ 2] æ­£åœ¨æ’ç©º ${IMPAIRED_AZ} ä¸­çš„èŠ‚ç‚¹..."
kubectl get nodes -l topology.kubernetes.io/zone=${IMPAIRED_AZ} -o name | \
  xargs -I {} kubectl drain {} \
    --ignore-daemonsets \
    --delete-emptydir-data \
    --grace-period=30 \
    --timeout=120s

# 3. éªŒè¯ç–æ•£ç»“æœ
echo "[æ­¥éª¤ 3] æ­£åœ¨éªŒè¯ç–æ•£æƒ…å†µ..."
echo "${IMPAIRED_AZ} ä¸­å‰©ä½™çš„ Podï¼š"
kubectl get pods --all-namespaces -o wide | grep ${IMPAIRED_AZ} | grep -v DaemonSet

echo "=== ç–æ•£å®Œæˆ ==="
```

### EBS AZ ç»‘å®šç¼“è§£

EBS å·ç»‘å®šåˆ°ç‰¹å®šçš„ AZã€‚å¦‚æœè¯¥ AZ å‘ç”Ÿæ•…éšœï¼Œä½¿ç”¨è¯¥å·çš„ Pod æ— æ³•è¿ç§»åˆ°å…¶ä»– AZã€‚

**WaitForFirstConsumer StorageClass**ï¼ˆæ¨èï¼‰ï¼š

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: topology-aware-ebs
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  encrypted: "true"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
```

`WaitForFirstConsumer` å°†å·çš„åˆ›å»ºæ¨è¿Ÿåˆ° Pod è¢«è°ƒåº¦ä¹‹åï¼Œç¡®ä¿å·åœ¨ä¸ Pod ç›¸åŒçš„ AZ ä¸­åˆ›å»ºã€‚

**EFS è·¨ AZ æ›¿ä»£æ–¹æ¡ˆ**ï¼šå¯¹äºå³ä½¿åœ¨ AZ æ•…éšœæœŸé—´ä¹Ÿéœ€è¦å­˜å‚¨è®¿é—®çš„å·¥ä½œè´Ÿè½½ï¼Œè¯·ä½¿ç”¨ Amazon EFSã€‚EFS å…è®¸ä»æ‰€æœ‰ AZ åŒæ—¶è®¿é—®ï¼Œæ¶ˆé™¤äº† AZ ç»‘å®šé—®é¢˜ã€‚

| å­˜å‚¨ | AZ ä¾èµ–æ€§ | æ•…éšœæœŸé—´è¡Œä¸º | é€‚ç”¨å·¥ä½œè´Ÿè½½ |
|------|-----------|-------------|-------------|
| EBS (gp3) | å• AZ ç»‘å®š | AZ æ•…éšœæ—¶æ— æ³•è®¿é—® | æ•°æ®åº“ã€æœ‰çŠ¶æ€åº”ç”¨ |
| EFS | è·¨ AZ | AZ æ•…éšœæ—¶å¯è®¿é—® | å…±äº«æ–‡ä»¶ã€CMSã€æ—¥å¿— |
| Instance Store | èŠ‚ç‚¹ç»‘å®š | èŠ‚ç‚¹ç»ˆæ­¢æ—¶æ•°æ®ä¸¢å¤± | ä¸´æ—¶ç¼“å­˜ã€æš‚å­˜ç©ºé—´ |

### è·¨ AZ æˆæœ¬ä¼˜åŒ–

Multi-AZ éƒ¨ç½²çš„ä¸»è¦æˆæœ¬å› ç´ æ˜¯è·¨ AZ ç½‘ç»œæµé‡ã€‚åœ¨åŒä¸€åŒºåŸŸå†…ï¼ŒAWS çš„ AZ é—´æ•°æ®ä¼ è¾“æˆæœ¬ä¸ºæ¯æ–¹å‘ $0.01/GBã€‚

**Istio Locality-Aware Routing** å¯ä»¥æœ€å¤§é™åº¦åœ°å‡å°‘è·¨ AZ æµé‡ï¼š

```yaml
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: locality-aware-routing
spec:
  host: backend-service
  trafficPolicy:
    connectionPool:
      http:
        http2MaxRequests: 1000
    outlierDetection:
      consecutive5xxErrors: 5
      interval: 10s
      baseEjectionTime: 30s
    loadBalancer:
      localityLbSetting:
        enabled: true
        # ä¼˜å…ˆåŒ AZï¼Œæ•…éšœæ—¶æ•…éšœè½¬ç§»åˆ°å…¶ä»– AZ
        distribute:
        - from: "us-east-1/us-east-1a/*"
          to:
            "us-east-1/us-east-1a/*": 80
            "us-east-1/us-east-1b/*": 10
            "us-east-1/us-east-1c/*": 10
        - from: "us-east-1/us-east-1b/*"
          to:
            "us-east-1/us-east-1b/*": 80
            "us-east-1/us-east-1a/*": 10
            "us-east-1/us-east-1c/*": 10
```

:::tip è·¨ AZ æˆæœ¬èŠ‚çœ
åº”ç”¨ Locality-Aware Routing å¯å°† 80% ä»¥ä¸Šçš„æµé‡ä¿æŒåœ¨åŒä¸€ AZ å†…ï¼Œæ˜¾è‘—é™ä½è·¨ AZ æ•°æ®ä¼ è¾“æˆæœ¬ã€‚å¯¹äºé«˜æµé‡æœåŠ¡ï¼Œè¿™æ¯æœˆå¯èŠ‚çœæ•°åƒç¾å…ƒã€‚
:::

---

## 3. Cell-Based Architecture

Cell-Based Architecture æ˜¯ AWS Well-Architected Framework æ¨èçš„é«˜çº§å¼¹æ€§æ¨¡å¼ã€‚å®ƒå°†ç³»ç»Ÿåˆ’åˆ†ä¸ºç‹¬ç«‹çš„ Cellï¼Œä»¥éš”ç¦»æ•…éšœçš„çˆ†ç‚¸åŠå¾„ï¼ˆBlast Radiusï¼‰ã€‚

### Cell æ¦‚å¿µä¸è®¾è®¡åŸåˆ™

Cell æ˜¯ä¸€ä¸ªå¯ä»¥ç‹¬ç«‹è¿è¡Œçš„è‡ªåŒ…å«æœåŠ¡å•å…ƒã€‚å¦‚æœä¸€ä¸ª Cell å‘ç”Ÿæ•…éšœï¼Œå…¶ä»– Cell ä¸å—å½±å“ã€‚

```mermaid
flowchart TB
    subgraph "æ§åˆ¶å¹³é¢"
        CR[Cell Router<br/>æµé‡è·¯ç”±]
        REG[Cell Registry<br/>Cell çŠ¶æ€ç®¡ç†]
        HC[Health Checker<br/>Cell ç›‘æ§]
    end

    subgraph "æ•°æ®å¹³é¢"
        subgraph "Cell 1ï¼ˆå®¢æˆ· A-Hï¼‰"
            C1_LB[Load Balancer]
            C1_APP[Application Pods]
            C1_DB[(Database)]
            C1_CACHE[(Cache)]
        end

        subgraph "Cell 2ï¼ˆå®¢æˆ· I-Pï¼‰"
            C2_LB[Load Balancer]
            C2_APP[Application Pods]
            C2_DB[(Database)]
            C2_CACHE[(Cache)]
        end

        subgraph "Cell 3ï¼ˆå®¢æˆ· Q-Zï¼‰"
            C3_LB[Load Balancer]
            C3_APP[Application Pods]
            C3_DB[(Database)]
            C3_CACHE[(Cache)]
        end
    end

    CR --> C1_LB
    CR --> C2_LB
    CR --> C3_LB
    REG --> HC
    HC --> C1_APP
    HC --> C2_APP
    HC --> C3_APP

    style CR fill:#4286f4,stroke:#2a6acf,color:#fff
    style REG fill:#4286f4,stroke:#2a6acf,color:#fff
    style HC fill:#4286f4,stroke:#2a6acf,color:#fff
    style C1_LB fill:#34a853,stroke:#2a8642,color:#fff
    style C2_LB fill:#34a853,stroke:#2a8642,color:#fff
    style C3_LB fill:#34a853,stroke:#2a8642,color:#fff
```

**Cell æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š**

1. **ç‹¬ç«‹æ€§**ï¼šæ¯ä¸ª Cell æ‹¥æœ‰è‡ªå·±çš„æ•°æ®å­˜å‚¨ã€ç¼“å­˜å’Œé˜Ÿåˆ—
2. **éš”ç¦»æ€§**ï¼šCell ä¹‹é—´æ²¡æœ‰ç›´æ¥é€šä¿¡â€”â€”ä»…é€šè¿‡æ§åˆ¶å¹³é¢åè°ƒ
3. **åŒè´¨æ€§**ï¼šæ‰€æœ‰ Cell è¿è¡Œç›¸åŒçš„ä»£ç å’Œé…ç½®
4. **å¯æ‰©å±•æ€§**ï¼šé€šè¿‡æ·»åŠ æ–° Cell è€Œéæ‰©å±•ç°æœ‰ Cell æ¥å®ç°å¢é•¿

### åœ¨ EKS ä¸­å®ç° Cell

| å®ç°æ–¹å¼ | åŸºäº Namespace çš„ Cell | åŸºäº Cluster çš„ Cell |
|----------|----------------------|---------------------|
| **éš”ç¦»çº§åˆ«** | é€»è¾‘éš”ç¦»ï¼ˆè½¯éš”ç¦»ï¼‰ | ç‰©ç†éš”ç¦»ï¼ˆç¡¬éš”ç¦»ï¼‰ |
| **èµ„æºéš”ç¦»** | ResourceQuota, LimitRange | å®Œå…¨é›†ç¾¤éš”ç¦» |
| **ç½‘ç»œéš”ç¦»** | NetworkPolicy | VPC/Subnet çº§åˆ« |
| **çˆ†ç‚¸åŠå¾„** | åŒä¸€é›†ç¾¤å†…å¯èƒ½å—å½±å“ | Cell ä¹‹é—´å®Œå…¨éš”ç¦» |
| **è¿ç»´å¤æ‚åº¦** | ä½ï¼ˆå•é›†ç¾¤ï¼‰ | é«˜ï¼ˆå¤šé›†ç¾¤ï¼‰ |
| **æˆæœ¬** | ä½ | é«˜ï¼ˆæ§åˆ¶å¹³é¢æˆæœ¬ x Cell æ•°é‡ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | ä¸­å°è§„æ¨¡ã€å†…éƒ¨æœåŠ¡ | å¤§è§„æ¨¡ã€åˆè§„è¦æ±‚ |

**åŸºäº Namespace çš„ Cell å®ç°ç¤ºä¾‹ï¼š**

```yaml
# Cell-1 Namespace å’Œ ResourceQuota
apiVersion: v1
kind: Namespace
metadata:
  name: cell-1
  labels:
    cell-id: "cell-1"
    partition: "customers-a-h"
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cell-1-quota
  namespace: cell-1
spec:
  hard:
    requests.cpu: "20"
    requests.memory: 40Gi
    limits.cpu: "40"
    limits.memory: 80Gi
    pods: "100"
---
# Cell æ„ŸçŸ¥çš„ Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
  namespace: cell-1
  labels:
    cell-id: "cell-1"
spec:
  replicas: 4
  selector:
    matchLabels:
      app: api-server
      cell-id: "cell-1"
  template:
    metadata:
      labels:
        app: api-server
        cell-id: "cell-1"
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: api-server
            cell-id: "cell-1"
      containers:
      - name: api-server
        image: myapp/api-server:v2.1
        env:
        - name: CELL_ID
          value: "cell-1"
        - name: PARTITION_RANGE
          value: "A-H"
        resources:
          requests:
            cpu: "500m"
            memory: 1Gi
          limits:
            cpu: "1"
            memory: 2Gi
```

### Cell Router å®ç°

Cell Router æ˜¯å°†ä¼ å…¥è¯·æ±‚è·¯ç”±åˆ°é€‚å½“ Cell çš„æ ¸å¿ƒç»„ä»¶ã€‚æœ‰ä¸‰ç§å®ç°æ–¹å¼ã€‚

**1. åŸºäº Route 53 ARC Routing Controlï¼š**

åœ¨ DNS çº§åˆ«æ§åˆ¶ Cell è·¯ç”±ã€‚ä¸ºæ¯ä¸ª Cell é…ç½® Health Check å’Œ Routing Controlï¼Œå½“ Cell æ•…éšœæ—¶åœ¨ DNS çº§åˆ«é˜»æ–­æµé‡ã€‚

**2. åŸºäº ALB Target Groupï¼š**

ä½¿ç”¨ ALB Weighted Target Groups æŒ‰ Cell åˆ†é…æµé‡ã€‚åŸºäº Header çš„è·¯ç”±è§„åˆ™å®ç°æ¯å®¢æˆ·çš„ Cell æ˜ å°„ã€‚

**3. åŸºäº Service Meshï¼ˆIstioï¼‰ï¼š**

ä½¿ç”¨ Istio VirtualService åŸºäº Header çš„è·¯ç”±å®ç° Cell è·¯ç”±ã€‚è¿™æ˜¯æœ€çµæ´»çš„æ–¹å¼ï¼Œä½†å¢åŠ äº† Istio çš„è¿ç»´å¤æ‚æ€§ã€‚

### çˆ†ç‚¸åŠå¾„éš”ç¦»ç­–ç•¥

| ç­–ç•¥ | è¯´æ˜ | éš”ç¦»æ ‡å‡† | ä½¿ç”¨åœºæ™¯ |
|------|------|----------|----------|
| **å®¢æˆ·åˆ†åŒº** | åŸºäºå®¢æˆ· ID å“ˆå¸Œçš„ Cell åˆ†é… | å®¢æˆ·åˆ†ç»„ | SaaS å¹³å° |
| **åœ°ç†åˆ†åŒº** | åŸºäºåœ°ç†ä½ç½®çš„ Cell åˆ†é… | åŒºåŸŸ/å›½å®¶ | å…¨çƒæœåŠ¡ |
| **å®¹é‡åˆ†åŒº** | åŸºäº Cell å®¹é‡çš„åŠ¨æ€åˆ†é… | å¯ç”¨èµ„æº | æµé‡æ³¢åŠ¨å¤§çš„æœåŠ¡ |
| **å±‚çº§åˆ†åŒº** | åŸºäºå®¢æˆ·ç­‰çº§çš„ Cell åˆ†é… | æœåŠ¡çº§åˆ« | é«˜çº§/æ ‡å‡†åˆ†ç¦» |

### Shuffle Sharding æ¨¡å¼

Shuffle Sharding å°†æ¯ä¸ªå®¢æˆ·ï¼ˆæˆ–ç§Ÿæˆ·ï¼‰åˆ†é…åˆ°ä» Cell æ€»æ± ä¸­éšæœºé€‰æ‹©çš„å°‘é‡ Cellã€‚è¿™ç¡®ä¿å•ä¸ª Cell æ•…éšœä»…å½±å“å°‘æ•°å®¢æˆ·ã€‚

**åŸç†**ï¼šæœ‰ 8 ä¸ª Cellï¼Œæ¯ä¸ªå®¢æˆ·åˆ†é… 2 ä¸ª Cell æ—¶ï¼Œå…±æœ‰ C(8,2) = 28 ç§å¯èƒ½çš„ç»„åˆã€‚å¦‚æœæŸä¸ªç‰¹å®š Cell å‘ç”Ÿæ•…éšœï¼Œåªæœ‰ä½¿ç”¨è¯¥ Cell çš„å®¢æˆ·å—åˆ°å½±å“ï¼Œå¹¶ä¸”ä»–ä»¬ä¼šè‡ªåŠ¨æ•…éšœè½¬ç§»åˆ°å¦ä¸€ä¸ªåˆ†é…çš„ Cellã€‚

```yaml
# Shuffle Sharding ConfigMap ç¤ºä¾‹
apiVersion: v1
kind: ConfigMap
metadata:
  name: shuffle-sharding-config
data:
  sharding-config.yaml: |
    totalCells: 8
    shardsPerTenant: 2
    tenantAssignments:
      tenant-acme:
        cells: ["cell-1", "cell-5"]
        primary: "cell-1"
      tenant-globex:
        cells: ["cell-3", "cell-7"]
        primary: "cell-3"
      tenant-initech:
        cells: ["cell-2", "cell-6"]
        primary: "cell-2"
```

:::warning Cell Architecture çš„æƒè¡¡
Cell Architecture æä¾›äº†å¼ºå¤§çš„éš”ç¦»èƒ½åŠ›ï¼Œä½†å¢åŠ äº†è¿ç»´å¤æ‚æ€§å’Œæˆæœ¬ã€‚ç”±äºæ¯ä¸ª Cell éƒ½æœ‰è‡ªå·±çš„æ•°æ®å­˜å‚¨ï¼Œæ•°æ®è¿ç§»ã€è·¨ Cell æŸ¥è¯¢å’Œ Cell é—´ä¸€è‡´æ€§éœ€è¦é¢å¤–çš„è®¾è®¡ã€‚å»ºè®®ä»éœ€è¦ 99.99% ä»¥ä¸Š SLA çš„æœåŠ¡å¼€å§‹è€ƒè™‘é‡‡ç”¨ã€‚
:::

---

## 4. Multi-Cluster / Multi-Region

Multi-Cluster å’Œ Multi-Region ç­–ç•¥ä¸ºåŒºåŸŸçº§æ•…éšœåšå¥½å‡†å¤‡ã€‚

### æ¶æ„æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | è¯´æ˜ | RTO | RPO | æˆæœ¬ | å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|------|------|-----|-----|------|--------|----------|
| **Active-Active** | æ‰€æœ‰åŒºåŸŸåŒæ—¶æœåŠ¡æµé‡ | ~0 | ~0 | æé«˜ | æé«˜ | å…¨çƒæœåŠ¡ã€æç«¯ SLA |
| **Active-Passive** | ä»…ä¸€ä¸ªåŒºåŸŸæ´»è·ƒï¼Œå…¶ä»–å¾…å‘½ | åˆ†é’Ÿåˆ°å°æ—¶ | åˆ†é’Ÿ | é«˜ | é«˜ | å¤§å¤šæ•°ä¸šåŠ¡åº”ç”¨ |
| **åŒºåŸŸéš”ç¦»** | æ¯ä¸ªåŒºåŸŸç‹¬ç«‹è¿è¡Œï¼Œæ•°æ®éš”ç¦» | å„åŒºåŸŸç‹¬ç«‹ | N/A | ä¸­ | ä¸­ | åˆè§„è¦æ±‚ã€æ•°æ®ä¸»æƒ |
| **Hub-Spoke** | ä¸­å¤® Hub ç®¡ç†ï¼ŒSpoke æœåŠ¡ | åˆ†é’Ÿ | ç§’åˆ°åˆ†é’Ÿ | ä¸­åˆ°é«˜ | ä¸­ | æ³¨é‡ç®¡ç†æ•ˆç‡ |

### Global Accelerator + EKS

AWS Global Accelerator åˆ©ç”¨ AWS å…¨çƒç½‘ç»œå°†æµé‡è·¯ç”±åˆ°ç¦»ç”¨æˆ·æœ€è¿‘çš„åŒºåŸŸçš„ EKS é›†ç¾¤ã€‚

```mermaid
flowchart TB
    subgraph "ç”¨æˆ·"
        U1[äºšæ´²ç”¨æˆ·]
        U2[æ¬§æ´²ç”¨æˆ·]
        U3[ç¾æ´²ç”¨æˆ·]
    end

    GA[AWS Global Accelerator<br/>Anycast IP]

    subgraph "ap-northeast-2"
        EKS1[EKS Cluster<br/>é¦–å°”]
        ALB1[ALB]
    end

    subgraph "eu-west-1"
        EKS2[EKS Cluster<br/>çˆ±å°”å…°]
        ALB2[ALB]
    end

    subgraph "us-east-1"
        EKS3[EKS Cluster<br/>å¼—å‰å°¼äºš]
        ALB3[ALB]
    end

    U1 --> GA
    U2 --> GA
    U3 --> GA

    GA -->|åŠ æƒè·¯ç”±| ALB1
    GA -->|åŠ æƒè·¯ç”±| ALB2
    GA -->|åŠ æƒè·¯ç”±| ALB3

    ALB1 --> EKS1
    ALB2 --> EKS2
    ALB3 --> EKS3

    style GA fill:#ff9900,stroke:#cc7a00,color:#fff
    style EKS1 fill:#4286f4,stroke:#2a6acf,color:#fff
    style EKS2 fill:#4286f4,stroke:#2a6acf,color:#fff
    style EKS3 fill:#4286f4,stroke:#2a6acf,color:#fff
```

### ArgoCD Multi-Cluster GitOps

ArgoCD ApplicationSet Generator è‡ªåŠ¨åŒ–è·¨å¤šä¸ªé›†ç¾¤çš„ä¸€è‡´éƒ¨ç½²ã€‚

```yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: multi-cluster-app
  namespace: argocd
spec:
  generators:
  # åŸºäºé›†ç¾¤æ ‡ç­¾çš„åŠ¨æ€éƒ¨ç½²
  - clusters:
      selector:
        matchLabels:
          environment: production
          resiliency-tier: "high"
  template:
    metadata:
      name: 'myapp-{{name}}'
    spec:
      project: default
      source:
        repoURL: https://github.com/myorg/k8s-manifests.git
        targetRevision: main
        path: 'overlays/{{metadata.labels.region}}'
      destination:
        server: '{{server}}'
        namespace: production
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
        - CreateNamespace=true
        retry:
          limit: 5
          backoff:
            duration: 5s
            factor: 2
            maxDuration: 3m
```

### Istio Multi-Cluster Federation

Istio Multi-Primary è®¾ç½®åœ¨æ¯ä¸ªé›†ç¾¤ä¸­è¿è¡Œç‹¬ç«‹çš„ Istio æ§åˆ¶å¹³é¢ï¼ŒåŒæ—¶æä¾›è·¨é›†ç¾¤çš„æœåŠ¡å‘ç°å’Œè´Ÿè½½å‡è¡¡ã€‚

```yaml
# Istio Locality-Aware Routingï¼ˆMulti-Regionï¼‰
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: multi-region-routing
spec:
  host: backend-service
  trafficPolicy:
    loadBalancer:
      localityLbSetting:
        enabled: true
        # ä¼˜å…ˆåŒåŒºåŸŸï¼Œæ•…éšœæ—¶æ•…éšœè½¬ç§»åˆ°å…¶ä»–åŒºåŸŸ
        failover:
        - from: us-east-1
          to: eu-west-1
        - from: eu-west-1
          to: us-east-1
        - from: ap-northeast-2
          to: ap-southeast-1
    outlierDetection:
      consecutive5xxErrors: 3
      interval: 10s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
```

:::info Istio API ç‰ˆæœ¬è¯´æ˜
åœ¨ Istio 1.22+ ä¸­ï¼Œ`networking.istio.io/v1` å’Œ `networking.istio.io/v1beta1` å‡å—æ”¯æŒã€‚æ–°éƒ¨ç½²å»ºè®®ä½¿ç”¨ `v1`ï¼Œç°æœ‰çš„ `v1beta1` é…ç½®ä»ç„¶æœ‰æ•ˆã€‚
:::

---

## 5. åº”ç”¨å¼¹æ€§æ¨¡å¼

é™¤äº†åŸºç¡€è®¾æ–½çº§åˆ«çš„å¼¹æ€§å¤–ï¼Œè¿˜å¿…é¡»å®ç°åº”ç”¨çº§åˆ«çš„å®¹é”™æ¨¡å¼ã€‚

### PodDisruptionBudgets (PDB)

PDB ç¡®ä¿åœ¨è‡ªæ„¿ä¸­æ–­ï¼ˆVoluntary Disruptionï¼‰æœŸé—´â€”â€”å¦‚èŠ‚ç‚¹æ’ç©ºã€é›†ç¾¤å‡çº§å’Œ Karpenter æ•´åˆâ€”â€”ç»´æŒæœ€ä½ Pod å¯ç”¨æ€§ã€‚

| è®¾ç½® | è¡Œä¸º | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| `minAvailable: 2` | å§‹ç»ˆä¿æŒè‡³å°‘ 2 ä¸ª Pod | å‰¯æœ¬æ•°è¾ƒå°‘çš„æœåŠ¡ï¼ˆ3-5 ä¸ªï¼‰ |
| `minAvailable: "50%"` | ä¿æŒè‡³å°‘ 50% çš„æ€»é‡ | å‰¯æœ¬æ•°è¾ƒå¤šçš„æœåŠ¡ |
| `maxUnavailable: 1` | åŒæ—¶æœ€å¤šå…è®¸ 1 ä¸ªä¸­æ–­ | æ»šåŠ¨æ›´æ–°æœŸé—´çš„ç¨³å®šæ€§ |
| `maxUnavailable: "25%"` | å…è®¸æœ€å¤š 25% åŒæ—¶ä¸­æ–­ | éœ€è¦å¿«é€Ÿéƒ¨ç½²æ—¶ |

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: api-server
---
# åŸºäºç™¾åˆ†æ¯”çš„ PDBï¼Œé€‚ç”¨äºå¤§å‹ Deployment
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: worker-pdb
spec:
  maxUnavailable: "25%"
  selector:
    matchLabels:
      app: worker
```

:::warning PDB ä¸ Karpenter çš„äº¤äº’
Karpenter çš„ Disruption é¢„ç®—ï¼ˆ`budgets: - nodes: "20%"`ï¼‰ä¸ PDB ååŒå·¥ä½œã€‚Karpenter åœ¨èŠ‚ç‚¹æ•´åˆæœŸé—´ä¼šéµå®ˆ PDBã€‚å¦‚æœ PDB è¿‡äºä¸¥æ ¼ï¼ˆä¾‹å¦‚ minAvailable ç­‰äºå‰¯æœ¬æ•°ï¼‰ï¼ŒèŠ‚ç‚¹æ’ç©ºå¯èƒ½ä¼šè¢«æ°¸ä¹…é˜»å¡ï¼Œè¯·è°¨æ…ä½¿ç”¨ã€‚
:::

### Graceful Shutdown

Graceful Shutdown æ¨¡å¼åœ¨ Pod è¢«ç»ˆæ­¢æ—¶å®‰å…¨å®Œæˆæ­£åœ¨å¤„ç†çš„è¯·æ±‚å¹¶åœæ­¢æ¥å—æ–°è¯·æ±‚ã€‚

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  template:
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: web
        image: myapp/web:v2.0
        ports:
        - containerPort: 8080
        lifecycle:
          preStop:
            exec:
              # 1. Sleep ç­‰å¾… Endpoint ç§»é™¤ï¼ˆé˜²æ­¢ Kubelet å’Œ Endpoint Controller ä¹‹é—´çš„ç«æ€ï¼‰
              # 2. å‘é€ SIGTERM å¯åŠ¨åº”ç”¨çš„ Graceful Shutdown
              command: ["/bin/sh", "-c", "sleep 5 && kill -TERM 1"]
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          periodSeconds: 5
          failureThreshold: 1
```

**Graceful Shutdown æ—¶åºè®¾è®¡ï¼š**

```mermaid
sequenceDiagram
    participant K8s as Kubernetes
    participant EP as Endpoint Controller
    participant Pod as Pod
    participant App as Application

    K8s->>Pod: Pod åˆ é™¤è¯·æ±‚
    K8s->>EP: å¼€å§‹ Endpoint ç§»é™¤

    par preStop Hook æ‰§è¡Œ
        Pod->>Pod: sleep 5ï¼ˆç­‰å¾… EP ç§»é™¤ï¼‰
    and Endpoint æ›´æ–°
        EP->>EP: ä» Endpoints ä¸­ç§»é™¤ Pod IP
    end

    Pod->>App: å‘é€ SIGTERM
    App->>App: åœæ­¢æ¥å—æ–°è¯·æ±‚
    App->>App: å®Œæˆæ­£åœ¨å¤„ç†çš„è¯·æ±‚ï¼ˆæœ€å¤š 55 ç§’ï¼‰
    App->>K8s: ä¼˜é›…ç»ˆæ­¢

    Note over K8s,App: terminationGracePeriodSeconds: 60
    Note over Pod,App: preStop(5s) + Shutdown(æœ€å¤š 55s) = 60s ä»¥å†…
```

:::tip ä¸ºä»€ä¹ˆéœ€è¦ preStop sleep
å½“ Kubernetes åˆ é™¤ Pod æ—¶ï¼ŒpreStop Hook æ‰§è¡Œå’Œ Endpoint ç§»é™¤æ˜¯**å¼‚æ­¥**è¿›è¡Œçš„ã€‚åœ¨ preStop ä¸­æ·»åŠ  5 ç§’çš„ sleep ç¡®ä¿ Endpoint Controller æœ‰æ—¶é—´ä» Service ä¸­ç§»é™¤ Pod IPï¼Œé˜²æ­¢æµé‡è¢«è·¯ç”±åˆ°æ­£åœ¨å…³é—­çš„ Podã€‚
:::

### Circuit Breakerï¼ˆIstio DestinationRuleï¼‰

Circuit Breaker é˜»æ–­å¯¹æ•…éšœæœåŠ¡çš„è¯·æ±‚ä»¥é˜²æ­¢çº§è”æ•…éšœï¼ˆCascading Failureï¼‰ã€‚å®ƒé€šè¿‡ Istio çš„ DestinationRule å®ç°ã€‚

```yaml
# Istio 1.22+ï¼šv1 å’Œ v1beta1 å‡å—æ”¯æŒ
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: backend-circuit-breaker
spec:
  host: backend-service
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
        connectTimeout: 5s
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
        maxRequestsPerConnection: 10
        maxRetries: 3
    outlierDetection:
      # è¿ç»­ 5 æ¬¡ 5xx é”™è¯¯åå°†å®ä¾‹ä»æ± ä¸­ç§»é™¤
      consecutive5xxErrors: 5
      # æ¯ 30 ç§’æ£€æŸ¥ä¸€æ¬¡å®ä¾‹å¥åº·çŠ¶å†µ
      interval: 30s
      # è¢«é©±é€å®ä¾‹çš„æœ€å°éš”ç¦»æ—¶é—´
      baseEjectionTime: 30s
      # æœ€å¤šå…è®¸é©±é€æ€»å®ä¾‹çš„ 50%
      maxEjectionPercent: 50
```

### Retry / Timeoutï¼ˆIstio VirtualServiceï¼‰

```yaml
apiVersion: networking.istio.io/v1
kind: VirtualService
metadata:
  name: backend-retry
spec:
  hosts:
  - backend-service
  http:
  - route:
    - destination:
        host: backend-service
    timeout: 10s
    retries:
      attempts: 3
      perTryTimeout: 3s
      retryOn: "5xx,reset,connect-failure,retriable-4xx"
      retryRemoteLocalities: true
```

**Retry æœ€ä½³å®è·µï¼š**

| è®¾ç½® | æ¨èå€¼ | åŸå›  |
|------|--------|------|
| `attempts` | 2-3 | è¿‡å¤šé‡è¯•ä¼šæ”¾å¤§è´Ÿè½½ |
| `perTryTimeout` | æ€»è¶…æ—¶çš„ 1/3 | å…è®¸ 3 æ¬¡é‡è¯•åœ¨æ€»è¶…æ—¶å†…å®Œæˆ |
| `retryOn` | `5xx,connect-failure` | ä»…å¯¹ç¬æ€æ•…éšœé‡è¯• |
| `retryRemoteLocalities` | `true` | ä¹Ÿå¯¹å…¶ä»– AZ çš„å®ä¾‹è¿›è¡Œé‡è¯• |

:::warning Rate Limiting é‡‡ç”¨æ³¨æ„äº‹é¡¹
Rate Limiting ä¸ Circuit Breaker å’Œ Retry ä¸€æ ·æ˜¯å¼¹æ€§çš„æ ¸å¿ƒè¦ç´ ï¼Œä½†é…ç½®ä¸å½“å¯èƒ½ä¼šé˜»æ–­åˆæ³•æµé‡ã€‚ä½¿ç”¨ Istio çš„ EnvoyFilter æˆ–å¤–éƒ¨ Rate Limiterï¼ˆå¦‚åŸºäº Redis çš„æ–¹æ¡ˆï¼‰å®ç°æ—¶ï¼Œ**å§‹ç»ˆåˆ†é˜¶æ®µé‡‡ç”¨**ï¼šç›‘æ§æ¨¡å¼ã€è­¦å‘Šæ¨¡å¼ï¼Œç„¶åæ‰æ˜¯é˜»æ–­æ¨¡å¼ã€‚
:::

---

## 6. æ··æ²Œå·¥ç¨‹

æ··æ²Œå·¥ç¨‹ï¼ˆChaos Engineeringï¼‰æ˜¯åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éªŒè¯ç³»ç»Ÿå¼¹æ€§çš„å®ç”¨æ–¹æ³•è®ºã€‚åœ¨"ä¸€åˆ‡æ­£å¸¸æ—¶"è¿›è¡Œæµ‹è¯•ï¼Œä¸º"æ•…éšœå‘ç”Ÿæ—¶"åšå¥½å‡†å¤‡ã€‚

### AWS Fault Injection Service (FIS)

AWS FIS æ˜¯ä¸€é¡¹æ‰˜ç®¡çš„æ··æ²Œå·¥ç¨‹æœåŠ¡ï¼Œå¯ä»¥å‘ EC2ã€EKS å’Œ RDS ç­‰ AWS æœåŠ¡æ³¨å…¥æ•…éšœã€‚

**åœºæ™¯ 1ï¼šPod ç»ˆæ­¢ï¼ˆåº”ç”¨å¼¹æ€§æµ‹è¯•ï¼‰**

```json
{
  "description": "EKS Pod termination test",
  "targets": {
    "eks-pods": {
      "resourceType": "aws:eks:pod",
      "resourceTags": {
        "app": "critical-api"
      },
      "selectionMode": "COUNT(3)",
      "parameters": {
        "clusterIdentifier": "arn:aws:eks:us-east-1:123456789012:cluster/prod-cluster",
        "namespace": "production"
      }
    }
  },
  "actions": {
    "terminate-pods": {
      "actionId": "aws:eks:pod-delete",
      "targets": {
        "Pods": "eks-pods"
      }
    }
  },
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:us-east-1:123456789012:alarm:HighErrorRate"
    }
  ]
}
```

**åœºæ™¯ 2ï¼šAZ æ•…éšœæ¨¡æ‹Ÿ**

```json
{
  "description": "Simulate AZ failure for EKS",
  "targets": {
    "eks-nodes-az1a": {
      "resourceType": "aws:ec2:instance",
      "resourceTags": {
        "kubernetes.io/cluster/my-cluster": "owned"
      },
      "filters": [
        {
          "path": "Placement.AvailabilityZone",
          "values": ["us-east-1a"]
        }
      ],
      "selectionMode": "ALL"
    }
  },
  "actions": {
    "stop-instances": {
      "actionId": "aws:ec2:stop-instances",
      "parameters": {
        "startInstancesAfterDuration": "PT10M"
      },
      "targets": {
        "Instances": "eks-nodes-az1a"
      }
    }
  },
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:us-east-1:123456789012:alarm:CriticalServiceDown"
    }
  ]
}
```

**åœºæ™¯ 3ï¼šç½‘ç»œå»¶è¿Ÿæ³¨å…¥**

```json
{
  "description": "Inject network latency to EKS nodes",
  "targets": {
    "eks-nodes": {
      "resourceType": "aws:ec2:instance",
      "resourceTags": {
        "kubernetes.io/cluster/my-cluster": "owned",
        "app-tier": "backend"
      },
      "selectionMode": "PERCENT(50)"
    }
  },
  "actions": {
    "inject-latency": {
      "actionId": "aws:ssm:send-command",
      "parameters": {
        "documentArn": "arn:aws:ssm:us-east-1::document/AWSFIS-Run-Network-Latency",
        "documentParameters": "{\"DurationSeconds\":\"300\",\"DelayMilliseconds\":\"200\",\"Interface\":\"eth0\"}",
        "duration": "PT5M"
      },
      "targets": {
        "Instances": "eks-nodes"
      }
    }
  }
}
```

### Litmus Chaos on EKS

Litmus æ˜¯ CNCF å­µåŒ–é¡¹ç›®ï¼Œä¹Ÿæ˜¯ä¸€ä¸ª Kubernetes åŸç”Ÿçš„æ··æ²Œå·¥ç¨‹æ¡†æ¶ã€‚

**å®‰è£…ï¼š**

```bash
# å®‰è£… Litmus ChaosCenter
helm repo add litmuschaos https://litmuschaos.github.io/litmus-helm/
helm repo update

helm install litmus litmuschaos/litmus \
  --namespace litmus --create-namespace \
  --set portal.frontend.service.type=LoadBalancer
```

**ChaosEngine ç¤ºä¾‹ï¼ˆPod Deleteï¼‰ï¼š**

```yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: pod-delete-chaos
  namespace: production
spec:
  appinfo:
    appns: production
    applabel: "app=api-server"
    appkind: deployment
  engineState: active
  chaosServiceAccount: litmus-admin
  experiments:
  - name: pod-delete
    spec:
      components:
        env:
        - name: TOTAL_CHAOS_DURATION
          value: "60"
        - name: CHAOS_INTERVAL
          value: "10"
        - name: FORCE
          value: "false"
        - name: PODS_AFFECTED_PERC
          value: "50"
```

### Chaos Mesh

Chaos Mesh æ˜¯ CNCF å­µåŒ–é¡¹ç›®ï¼Œä¹Ÿæ˜¯ä¸€ä¸ª Kubernetes ä¸“ç”¨çš„æ··æ²Œå·¥ç¨‹å¹³å°ï¼Œæ”¯æŒå¤šç§æ•…éšœç±»å‹ã€‚

**å®‰è£…ï¼š**

```bash
# å®‰è£… Chaos Mesh
helm repo add chaos-mesh https://charts.chaos-mesh.org
helm repo update

helm install chaos-mesh chaos-mesh/chaos-mesh \
  --namespace chaos-mesh --create-namespace \
  --set chaosDaemon.runtime=containerd \
  --set chaosDaemon.socketPath=/run/containerd/containerd.sock
```

**NetworkChaos ç¤ºä¾‹ï¼ˆç½‘ç»œåˆ†åŒºï¼‰ï¼š**

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-partition
  namespace: chaos-mesh
spec:
  action: partition
  mode: all
  selector:
    namespaces:
    - production
    labelSelectors:
      "app": "frontend"
  direction: both
  target:
    selector:
      namespaces:
      - production
      labelSelectors:
        "app": "backend"
    mode: all
  duration: "5m"
  scheduler:
    cron: "@every 24h"
```

**PodChaos ç¤ºä¾‹ï¼ˆPod Killï¼‰ï¼š**

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: pod-kill-test
  namespace: chaos-mesh
spec:
  action: pod-kill
  mode: fixed-percent
  value: "30"
  selector:
    namespaces:
    - production
    labelSelectors:
      "app": "api-server"
  duration: "1m"
  gracePeriod: 0
```

### æ··æ²Œå·¥ç¨‹å·¥å…·å¯¹æ¯”

| ç‰¹æ€§ | AWS FIS | Litmus Chaos | Chaos Mesh |
|------|---------|-------------|------------|
| **ç±»å‹** | æ‰˜ç®¡æœåŠ¡ | å¼€æºï¼ˆCNCFï¼‰ | å¼€æºï¼ˆCNCFï¼‰ |
| **èŒƒå›´** | AWS åŸºç¡€è®¾æ–½ + K8s | ä»… Kubernetes | ä»… Kubernetes |
| **æ•…éšœç±»å‹** | EC2, EKS, RDS, ç½‘ç»œ | Pod, Node, ç½‘ç»œ, DNS | Pod, ç½‘ç»œ, I/O, æ—¶é—´, JVM |
| **AZ æ•…éšœæ¨¡æ‹Ÿ** | åŸç”Ÿæ”¯æŒ | æœ‰é™ï¼ˆPod/Node çº§åˆ«ï¼‰ | æœ‰é™ï¼ˆPod/Node çº§åˆ«ï¼‰ |
| **ä»ªè¡¨æ¿** | AWS Console | Litmus Portalï¼ˆWeb UIï¼‰ | Chaos Dashboardï¼ˆWeb UIï¼‰ |
| **æˆæœ¬** | æŒ‰æ‰§è¡Œæ¬¡æ•°ä»˜è´¹ | å…è´¹ï¼ˆä»…åŸºç¡€è®¾æ–½æˆæœ¬ï¼‰ | å…è´¹ï¼ˆä»…åŸºç¡€è®¾æ–½æˆæœ¬ï¼‰ |
| **åœæ­¢æ¡ä»¶** | CloudWatch Alarm é›†æˆ | æ‰‹åŠ¨ / API | æ‰‹åŠ¨ / API |
| **è¿ç»´å¤æ‚åº¦** | ä½ | ä¸­ | ä¸­ |
| **GitOps é›†æˆ** | CloudFormation / CDK | åŸºäº CRDï¼ˆå…¼å®¹ ArgoCDï¼‰ | åŸºäº CRDï¼ˆå…¼å®¹ ArgoCDï¼‰ |
| **æ¨èåœºæ™¯** | åŸºç¡€è®¾æ–½çº§åˆ«æ•…éšœæµ‹è¯• | K8s åŸç”Ÿæµ‹è¯• | éœ€è¦ç»†ç²’åº¦æ•…éšœæ³¨å…¥æ—¶ |

:::tip å·¥å…·é€‰æ‹©æŒ‡å—
ä» AWS FIS å¼€å§‹è¿›è¡ŒåŸºç¡€è®¾æ–½çº§åˆ«çš„æ•…éšœæµ‹è¯•ï¼ˆAZã€ç½‘ç»œï¼‰ï¼Œç„¶åä½¿ç”¨ Litmus æˆ– Chaos Mesh è¿›è¡Œç»†ç²’åº¦çš„åº”ç”¨çº§åˆ«æ•…éšœæµ‹è¯•ã€‚æ¨èé‡‡ç”¨**æ··åˆæ–¹å¼**ã€‚AWS FIS çš„åœæ­¢æ¡ä»¶åŠŸèƒ½ï¼ˆåŸºäº CloudWatch Alarmï¼‰å¯¹äºåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®‰å…¨æµ‹è¯•è‡³å…³é‡è¦ã€‚
:::

### Game Day è¿è¡Œæ‰‹å†Œæ¨¡æ¿

Game Day æ˜¯å›¢é˜Ÿé›†åˆåœ¨ä¸€èµ·æ‰§è¡Œè®¡åˆ’çš„æ•…éšœåœºæ™¯ï¼Œå‘ç°ç³»ç»Ÿå’Œæµç¨‹ä¸­å¼±ç‚¹çš„æ¼”ç»ƒã€‚

**5 é˜¶æ®µ Game Day æ‰§è¡Œæ¡†æ¶ï¼š**

```mermaid
flowchart LR
    subgraph "é˜¶æ®µ 1ï¼šå‡†å¤‡"
        P1[åˆ¶å®šå‡è®¾<br/>ä¾‹å¦‚ï¼šAZ æ•…éšœæ—¶è‡ªåŠ¨æ¢å¤]
        P2[å®šä¹‰æˆåŠŸæ ‡å‡†<br/>ä¾‹å¦‚ï¼š5 åˆ†é’Ÿå†…æ¢å¤]
        P3[è®¾ç½®ä¸­æ­¢æ ‡å‡†<br/>CloudWatch Alarm]
    end

    subgraph "é˜¶æ®µ 2ï¼šæ‰§è¡Œ"
        E1[è®°å½•ç¨³æ€<br/>é‡‡é›†å½“å‰æŒ‡æ ‡]
        E2[æ³¨å…¥æ•…éšœ<br/>å¯åŠ¨ FIS å®éªŒ]
        E3[è§‚å¯Ÿå’Œè®°å½•<br/>å®æ—¶ç›‘æ§]
    end

    subgraph "é˜¶æ®µ 3ï¼šåˆ†æ"
        A1[æµ‹é‡æ¢å¤æ—¶é—´<br/>å®é™… RTO]
        A2[è¯„ä¼°æ•°æ®ä¸¢å¤±<br/>å®é™… RPO]
        A3[åˆ†æç”¨æˆ·å½±å“]
    end

    subgraph "é˜¶æ®µ 4ï¼šæ”¹è¿›"
        I1[ç¼–å½•å‘ç°çš„å¼±ç‚¹]
        I2[åˆ›å»ºæ”¹è¿›å·¥å•]
        I3[æ›´æ–°è¿è¡Œæ‰‹å†Œ]
    end

    subgraph "é˜¶æ®µ 5ï¼šè¿­ä»£"
        R1[å®‰æ’ä¸‹æ¬¡ Game Day]
        R2[æ‰©å±•åœºæ™¯]
        R3[æ‰©å±•è‡ªåŠ¨åŒ–]
    end

    P1 --> P2 --> P3
    P3 --> E1 --> E2 --> E3
    E3 --> A1 --> A2 --> A3
    A3 --> I1 --> I2 --> I3
    I3 --> R1 --> R2 --> R3

    style P1 fill:#4286f4,stroke:#2a6acf,color:#fff
    style E2 fill:#ff4444,stroke:#cc3636,color:#fff
    style A1 fill:#fbbc04,stroke:#c99603,color:#000
    style I1 fill:#34a853,stroke:#2a8642,color:#fff
    style R1 fill:#4286f4,stroke:#2a6acf,color:#fff
```

**Game Day è‡ªåŠ¨åŒ–è„šæœ¬ï¼š**

```bash
#!/bin/bash
# game-day.sh - Game Day æ‰§è¡Œè‡ªåŠ¨åŒ–
set -euo pipefail

CLUSTER_NAME=$1
SCENARIO=$2
NAMESPACE=${3:-production}

echo "============================================"
echo " Game Day: ${SCENARIO}"
echo " é›†ç¾¤: ${CLUSTER_NAME}"
echo " å‘½åç©ºé—´: ${NAMESPACE}"
echo " æ—¶é—´: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
echo "============================================"

# é˜¶æ®µ 1ï¼šè®°å½•ç¨³æ€
echo ""
echo "[é˜¶æ®µ 1] æ­£åœ¨è®°å½•ç¨³æ€..."
echo "--- Pod çŠ¶æ€ ---"
kubectl get pods -n ${NAMESPACE} -o wide | head -20

echo "--- èŠ‚ç‚¹çŠ¶æ€ ---"
kubectl get nodes -o custom-columns=\
NAME:.metadata.name,\
STATUS:.status.conditions[-1].type,\
AZ:.metadata.labels.topology\\.kubernetes\\.io/zone

echo "--- Service Endpoints ---"
kubectl get endpoints -n ${NAMESPACE}

# é˜¶æ®µ 2ï¼šæ³¨å…¥æ•…éšœï¼ˆæŒ‰åœºæ™¯ï¼‰
echo ""
echo "[é˜¶æ®µ 2] æ­£åœ¨æ³¨å…¥æ•…éšœ: ${SCENARIO}..."

case ${SCENARIO} in
  "az-failure")
    echo "ä½¿ç”¨ ARC Zonal Shift æ¨¡æ‹Ÿ AZ æ•…éšœ..."
    # æ‰§è¡Œ ARC Zonal Shiftï¼ˆ1 å°æ—¶ï¼‰
    aws arc-zonal-shift start-zonal-shift \
      --resource-identifier arn:aws:eks:us-east-1:$(aws sts get-caller-identity --query Account --output text):cluster/${CLUSTER_NAME} \
      --away-from us-east-1a \
      --expires-in 1h \
      --comment "Game Day: AZ failure simulation"
    ;;

  "pod-delete")
    echo "æ­£åœ¨åˆ é™¤ ${NAMESPACE} ä¸­ 30% çš„ Pod..."
    TOTAL=$(kubectl get pods -n ${NAMESPACE} -l app=api-server --no-headers | wc -l)
    DELETE_COUNT=$(( TOTAL * 30 / 100 ))
    DELETE_COUNT=$(( DELETE_COUNT < 1 ? 1 : DELETE_COUNT ))
    kubectl get pods -n ${NAMESPACE} -l app=api-server -o name | \
      shuf | head -n ${DELETE_COUNT} | \
      xargs kubectl delete -n ${NAMESPACE}
    ;;

  "node-drain")
    echo "æ­£åœ¨æ’ç©ºä¸€ä¸ªéšæœºèŠ‚ç‚¹..."
    NODE=$(kubectl get nodes --no-headers | shuf -n 1 | awk '{print $1}')
    kubectl cordon ${NODE}
    kubectl drain ${NODE} --ignore-daemonsets --delete-emptydir-data --timeout=120s
    ;;

  *)
    echo "æœªçŸ¥åœºæ™¯: ${SCENARIO}"
    echo "å¯ç”¨åœºæ™¯: az-failure, pod-delete, node-drain"
    exit 1
    ;;
esac

# é˜¶æ®µ 3ï¼šè§‚å¯Ÿæ¢å¤
echo ""
echo "[é˜¶æ®µ 3] æ­£åœ¨è§‚å¯Ÿæ¢å¤..."
echo "ç­‰å¾… 60 ç§’ä»¥è§‚å¯Ÿæ¢å¤..."
sleep 60

echo "--- æ•…éšœå Pod çŠ¶æ€ ---"
kubectl get pods -n ${NAMESPACE} -o wide | head -20

echo "--- Pod é‡å¯æ¬¡æ•° ---"
kubectl get pods -n ${NAMESPACE} -o custom-columns=\
NAME:.metadata.name,\
RESTARTS:.status.containerStatuses[0].restartCount,\
STATUS:.status.phase

echo ""
echo "============================================"
echo " Game Day é˜¶æ®µ 3 å®Œæˆ"
echo " è¯·å®¡æŸ¥ç»“æœå¹¶ç»§ç»­åˆ†æ"
echo "============================================"
```

---

## 7. å¼¹æ€§æ£€æŸ¥æ¸…å•ä¸å‚è€ƒèµ„æ–™

### å¼¹æ€§å®æ–½æ£€æŸ¥æ¸…å•

ä½¿ç”¨ä»¥ä¸‹æ£€æŸ¥æ¸…å•è¯„ä¼°å½“å‰å¼¹æ€§çº§åˆ«å¹¶ç¡®å®šä¸‹ä¸€æ­¥å®æ–½è®¡åˆ’ã€‚

**Level 1 -- åŸºç¡€**

| é¡¹ç›® | è¯´æ˜ | å®Œæˆ |
|------|------|------|
| Liveness/Readiness Probe é…ç½® | æ‰€æœ‰ Deployment é…ç½®äº†é€‚å½“çš„ Probe | [ ] |
| Resource Requests/Limits é…ç½® | æŒ‡å®šäº† CPU å’Œ Memory èµ„æºçº¦æŸ | [ ] |
| PodDisruptionBudget é…ç½® | ä¿è¯æœ€ä½å¯ç”¨ Pod æ•°é‡ | [ ] |
| Graceful Shutdown å®ç° | preStop Hook + terminationGracePeriodSeconds | [ ] |
| Startup Probe é…ç½® | ä¸ºå¯åŠ¨ç¼“æ…¢çš„åº”ç”¨æä¾›åˆå§‹åŒ–ä¿æŠ¤ | [ ] |
| è‡ªåŠ¨é‡å¯ç­–ç•¥ | ç¡®è®¤ restartPolicy: Always | [ ] |

**Level 2 -- Multi-AZ**

| é¡¹ç›® | è¯´æ˜ | å®Œæˆ |
|------|------|------|
| Topology Spread Constraints | è·¨ AZ å‡åŒ€åˆ†å¸ƒ Pod | [ ] |
| Multi-AZ Karpenter NodePool | è·¨ 3 ä¸ªä»¥ä¸Š AZ é…ç½®èŠ‚ç‚¹ | [ ] |
| WaitForFirstConsumer StorageClass | é˜²æ­¢ EBS AZ ç»‘å®š | [ ] |
| ARC Zonal Shift å·²å¯ç”¨ | AZ æ•…éšœæ—¶è‡ªåŠ¨æµé‡è½¬ç§» | [ ] |
| è·¨ AZ æµé‡ä¼˜åŒ– | é…ç½®äº† Locality-Aware è·¯ç”± | [ ] |
| AZ ç–æ•£è¿è¡Œæ‰‹å†Œå·²å‡†å¤‡ | ç´§æ€¥ AZ ç–æ•£ç¨‹åºå·²æ–‡æ¡£åŒ– | [ ] |

**Level 3 -- Cell-Based**

| é¡¹ç›® | è¯´æ˜ | å®Œæˆ |
|------|------|------|
| Cell è¾¹ç•Œå®šä¹‰ | åŸºäº Namespace æˆ– Cluster çš„ Cell é…ç½® | [ ] |
| Cell Router å®ç° | è¯·æ±‚è·¯ç”±åˆ°é€‚å½“çš„ Cell | [ ] |
| Cell é—´éš”ç¦»éªŒè¯ | é€šè¿‡ NetworkPolicy æˆ– VPC çº§åˆ«çš„éš”ç¦» | [ ] |
| Shuffle Sharding å·²åº”ç”¨ | æ¯ä¸ªç§Ÿæˆ·å¤šæ ·åŒ–çš„ Cell åˆ†é… | [ ] |
| Cell å¥åº·ç›‘æ§ | ç›‘æ§å„ä¸ª Cell å¥åº·çŠ¶å†µçš„ä»ªè¡¨æ¿ | [ ] |
| Cell Failover æµ‹è¯• | é€šè¿‡æ··æ²Œå·¥ç¨‹éªŒè¯ Cell æ•…éšœ | [ ] |

**Level 4 -- Multi-Region**

| é¡¹ç›® | è¯´æ˜ | å®Œæˆ |
|------|------|------|
| Multi-Region æ¶æ„è®¾è®¡ | Active-Active æˆ– Active-Passive å†³ç­– | [ ] |
| Global Accelerator é…ç½® | è·¨åŒºåŸŸæµé‡è·¯ç”± | [ ] |
| æ•°æ®å¤åˆ¶ç­–ç•¥ | è·¨åŒºåŸŸæ•°æ®åŒæ­¥ | [ ] |
| ArgoCD Multi-Cluster GitOps | åŸºäº ApplicationSet çš„å¤šé›†ç¾¤éƒ¨ç½² | [ ] |
| Multi-Region æ··æ²Œæµ‹è¯• | åŒºåŸŸæ•…éšœæ¨¡æ‹Ÿ Game Day | [ ] |
| RTO/RPO æµ‹é‡ä¸éªŒè¯ | å®é™…æ¢å¤æ—¶é—´/æ•°æ®ä¸¢å¤±ä¸ç›®æ ‡çš„å¯¹æ¯”éªŒè¯ | [ ] |

### æˆæœ¬ä¼˜åŒ–æŠ€å·§

| ä¼˜åŒ–é¢†åŸŸ | ç­–ç•¥ | é¢„æœŸèŠ‚çœ |
|----------|------|----------|
| **è·¨ AZ æµé‡** | Istio Locality-Aware è·¯ç”±å°† 80% ä»¥ä¸Šæµé‡ä¿æŒåœ¨åŒä¸€ AZ å†… | AZ é—´ä¼ è¾“æˆæœ¬é™ä½ 60-80% |
| **Spot å®ä¾‹** | å¯¹éå…³é”®å·¥ä½œè´Ÿè½½ä½¿ç”¨ Spotï¼ˆKarpenter capacity-type æ··åˆï¼‰ | è®¡ç®—æˆæœ¬é™ä½ 60-90% |
| **Cell åˆ©ç”¨ç‡** | åˆç†è®¾è®¡ Cell å¤§å°ä»¥å‡å°‘èµ„æºæµªè´¹ | è¿‡é‡é…ç½®å‡å°‘ 20-40% |
| **Multi-Region** | Active-Passive æ¨¡å¼ä¸‹ä»¥æœ€ä½å®¹é‡è¿è¡Œ Passive åŒºåŸŸ | Passive åŒºåŸŸæˆæœ¬é™ä½ 50-70% |
| **Karpenter æ•´åˆ** | WhenEmptyOrUnderutilized ç­–ç•¥è‡ªåŠ¨ç§»é™¤æœªä½¿ç”¨çš„èŠ‚ç‚¹ | æ¶ˆé™¤ç©ºé—²èµ„æºæˆæœ¬ |
| **é€‰æ‹©æ€§ä½¿ç”¨ EFS** | ä»…åœ¨éœ€è¦è·¨ AZ æ—¶ä½¿ç”¨ EFSï¼›å…¶ä»–æƒ…å†µä½¿ç”¨ EBS gp3 | å­˜å‚¨æˆæœ¬èŠ‚çœ |

:::danger æˆæœ¬ä¸å¼¹æ€§çš„æƒè¡¡
æ›´é«˜çš„å¼¹æ€§çº§åˆ«ä¼´éšç€æ›´é«˜çš„æˆæœ¬ã€‚Multi-Region Active-Active éœ€è¦æ¯”å•åŒºåŸŸå¤šä¸€å€ä»¥ä¸Šçš„åŸºç¡€è®¾æ–½æˆæœ¬ã€‚æ ¹æ®ä¸šåŠ¡éœ€æ±‚ï¼ˆSLAã€åˆè§„è¦æ±‚ï¼‰ä¸æˆæœ¬è¿›è¡Œå¹³è¡¡ï¼Œé€‰æ‹©é€‚å½“çš„å¼¹æ€§çº§åˆ«ã€‚å¹¶éæ¯ä¸ªæœåŠ¡éƒ½éœ€è¦è¾¾åˆ° Level 4ã€‚
:::

### ç›¸å…³æ–‡æ¡£

- [EKS æ•…éšœè¯Šæ–­ä¸å“åº”æŒ‡å—](./eks-debugging-guide.md) -- è¿ç»´æ•…éšœæ’é™¤ä¸äº‹ä»¶è§£å†³
- [åŸºäº GitOps çš„ EKS é›†ç¾¤è¿ç»´](./gitops-cluster-operation.md) -- ArgoCDã€KRO åŸºç¡€çš„é›†ç¾¤ç®¡ç†
- [Karpenter è¶…å¿«é€Ÿè‡ªåŠ¨æ‰©ç¼©å®¹](/docs/infrastructure-optimization/karpenter-autoscaling) -- æ·±å…¥ Karpenter é…ç½®ä¸ HPA ä¼˜åŒ–

### å¤–éƒ¨å‚è€ƒèµ„æ–™

- [AWS Well-Architected -- Cell-Based Architecture](https://docs.aws.amazon.com/wellarchitected/latest/reducing-scope-of-impact-with-cell-based-architecture/reducing-scope-of-impact-with-cell-based-architecture.html)
- [AWS Cell-Based Architecture Guidance](https://aws.amazon.com/solutions/guidance/cell-based-architecture-on-aws/)
- [AWS Shuffle Sharding](https://aws.amazon.com/blogs/architecture/shuffle-sharding-massive-and-magical-fault-isolation/)
- [EKS Reliability Best Practices](https://docs.aws.amazon.com/eks/latest/best-practices/reliability.html)
- [EKS + ARC Zonal Shift](https://docs.aws.amazon.com/eks/latest/userguide/zone-shift.html)
- [Kubernetes PDB](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)
- [Kubernetes Topology Spread Constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)
- [Istio Circuit Breaking](https://istio.io/latest/docs/tasks/traffic-management/circuit-breaking/)
- [Karpenter Documentation](https://karpenter.sh/docs/)
- [AWS FIS](https://aws.amazon.com/fis/)
- [Litmus Chaos](https://litmuschaos.io/)
- [Chaos Mesh](https://chaos-mesh.org/)
- [Route 53 ARC](https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.html)
