---
title: "EKS æ•…éšœè¯Šæ–­ä¸å“åº”æŒ‡å—"
sidebar_label: "3. EKS Incident Diagnosis & Response"
description: "åœ¨ Amazon EKS ç¯å¢ƒä¸­ç³»ç»Ÿæ€§è¯Šæ–­å’Œè§£å†³åº”ç”¨ç¨‹åºåŠåŸºç¡€è®¾æ–½é—®é¢˜çš„ç»¼åˆæ•…éšœæ’é™¤æŒ‡å—"
tags: [eks, kubernetes, debugging, troubleshooting, observability, incident-response]
category: "observability-monitoring"
sidebar_position: 3
last_update:
  date: 2026-02-13
  author: devfloor9
---

import { IncidentEscalationTable, ZonalShiftImpactTable, ControlPlaneLogTable, ClusterHealthTable, NodeGroupErrorTable, ErrorQuickRefTable } from '@site/src/components/EksDebugTables';

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2026-02-10 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-13 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦ 20 åˆ†é’Ÿ

# EKS æ•…éšœè¯Šæ–­ä¸å“åº”æŒ‡å—

> **ğŸ“Œ å‚è€ƒç¯å¢ƒ**: EKS 1.30+, kubectl 1.30+, AWS CLI v2

## 1. æ¦‚è¿°

EKS è¿ç»´ä¸­å‡ºç°çš„é—®é¢˜æ¶µç›–å¤šä¸ªå±‚é¢ï¼ŒåŒ…æ‹¬ Control Planeã€Nodeã€ç½‘ç»œã€å·¥ä½œè´Ÿè½½ã€å­˜å‚¨å’Œå¯è§‚æµ‹æ€§ã€‚æœ¬æ–‡æ¡£æ˜¯ä¸€ä»½ç»¼åˆè°ƒè¯•æŒ‡å—ï¼Œæ—¨åœ¨å¸®åŠ© SREã€DevOps å·¥ç¨‹å¸ˆå’Œå¹³å°å›¢é˜Ÿ**ç³»ç»Ÿæ€§åœ°è¯Šæ–­å¹¶å¿«é€Ÿè§£å†³**è¿™äº›é—®é¢˜ã€‚

æ‰€æœ‰å‘½ä»¤å’Œç¤ºä¾‹å‡å¯ç›´æ¥æ‰§è¡Œï¼ŒåŒæ—¶æä¾›äº† Decision Tree å’Œæµç¨‹å›¾ä»¥ä¾¿å¿«é€Ÿå†³ç­–ã€‚

### EKS è°ƒè¯•å±‚çº§

```mermaid
flowchart TB
    subgraph "EKS Debugging Layers"
        CP["`**Control Plane**
        API Server, etcd
        AuthN/AuthZ, Add-ons`"]
        NODE["`**Node**
        kubelet, containerd
        Resource Pressure, Karpenter`"]
        NET["`**Networking**
        VPC CNI, DNS
        Service, NetworkPolicy`"]
        WL["`**Workload**
        Pod Status, Probes
        Deployment, HPA`"]
        STOR["`**Storage**
        EBS CSI, EFS CSI
        PV/PVC`"]
        OBS["`**Observability**
        Metrics, Logs
        Alerts, Dashboards`"]
    end

    CP --> NODE
    NODE --> NET
    NET --> WL
    WL --> STOR
    STOR --> OBS

    style CP fill:#4286f4,stroke:#2a6acf,color:#fff
    style NODE fill:#ff9900,stroke:#cc7a00,color:#fff
    style NET fill:#fbbc04,stroke:#c99603,color:#000
    style WL fill:#ff4444,stroke:#cc3636,color:#fff
    style STOR fill:#4286f4,stroke:#2a6acf,color:#fff
    style OBS fill:#34a853,stroke:#2a8642,color:#fff
```

### è°ƒè¯•æ–¹æ³•è®º

è¯Šæ–­ EKS é—®é¢˜æœ‰ä¸¤ç§æ–¹æ³•ã€‚

| æ–¹æ³• | æè¿° | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **è‡ªé¡¶å‘ä¸‹ï¼ˆç—‡çŠ¶ â†’ åŸå› ï¼‰** | ä»ç”¨æˆ·æŠ¥å‘Šçš„ç—‡çŠ¶å‡ºå‘ï¼Œé€æ­¥è¿½æº¯åˆ°æ ¹æœ¬åŸå›  | æœåŠ¡ä¸­æ–­æˆ–æ€§èƒ½ä¸‹é™ç­‰å³æ—¶äº‹ä»¶å“åº” |
| **è‡ªåº•å‘ä¸Šï¼ˆåŸºç¡€è®¾æ–½ â†’ åº”ç”¨ï¼‰** | ä»åŸºç¡€è®¾æ–½å±‚å¼€å§‹é€å±‚å‘ä¸Šæ£€æŸ¥ | é¢„é˜²æ€§æ£€æŸ¥ã€è¿ç§»åéªŒè¯ |

:::tip æ¨èçš„ä¸€èˆ¬æ–¹æ³•
å¯¹äºç”Ÿäº§ç¯å¢ƒäº‹ä»¶ï¼Œæ¨èä½¿ç”¨**è‡ªé¡¶å‘ä¸‹**æ–¹æ³•ã€‚é¦–å…ˆè¯†åˆ«ç—‡çŠ¶ï¼ˆç¬¬ 2 èŠ‚ - äº‹ä»¶åˆ†ç±»ï¼‰ï¼Œç„¶åå¯¼èˆªåˆ°è¯¥å±‚çº§å¯¹åº”çš„è°ƒè¯•ç« èŠ‚ã€‚
:::

---

## 2. äº‹ä»¶åˆ†ç±»ï¼ˆå¿«é€Ÿæ•…éšœè¯„ä¼°ï¼‰

### å‰ 5 åˆ†é’Ÿæ£€æŸ¥æ¸…å•

å½“äº‹ä»¶å‘ç”Ÿæ—¶ï¼Œæœ€é‡è¦çš„æ“ä½œæ˜¯**èŒƒå›´è¯†åˆ«**å’Œ**åˆå§‹å“åº”**ã€‚

#### 30 ç§’ï¼šåˆå§‹è¯Šæ–­

```bash
# Check cluster status
aws eks describe-cluster --name <cluster-name> --query 'cluster.status' --output text

# Check node status
kubectl get nodes

# Check unhealthy Pods
kubectl get pods --all-namespaces | grep -v Running | grep -v Completed
```

#### 2 åˆ†é’Ÿï¼šèŒƒå›´è¯†åˆ«

```bash
# Check recent events (all namespaces)
kubectl get events --all-namespaces --sort-by='.lastTimestamp' | tail -20

# Aggregate Pod status in a specific namespace
kubectl get pods -n <namespace> --no-headers | awk '{print $2}' | sort | uniq -c | sort -rn

# Check distribution of unhealthy Pods by node
kubectl get pods --all-namespaces -o wide --field-selector=status.phase!=Running | \
  awk 'NR>1 {print $8}' | sort | uniq -c | sort -rn
```

#### 5 åˆ†é’Ÿï¼šåˆå§‹å“åº”

```bash
# Detailed information for the problematic Pod
kubectl describe pod <pod-name> -n <namespace>

# Previous container logs (for CrashLoopBackOff)
kubectl logs <pod-name> -n <namespace> --previous

# Check resource usage
kubectl top nodes
kubectl top pods -n <namespace> --sort-by=cpu
```

### èŒƒå›´è¯†åˆ«å†³ç­–æ ‘

```mermaid
flowchart TD
    ALERT["`**Alert / Incident Detected**`"] --> SINGLE{"`Single Pod issue?`"}

    SINGLE -->|Yes| POD_DEBUG["`**Workload Debugging**
    â†’ Section 5`"]

    SINGLE -->|No| SAME_NODE{"`Pods on the
    same Node?`"}

    SAME_NODE -->|Yes| NODE_DEBUG["`**Node-Level Debugging**
    â†’ Section 4`"]

    SAME_NODE -->|No| SAME_AZ{"`Nodes in the
    same AZ?`"}

    SAME_AZ -->|Yes| AZ_DEBUG["`**AZ Failure Detected**
    Consider ARC Zonal Shift`"]

    SAME_AZ -->|No| ALL_NS{"`All Namespaces
    affected?`"}

    ALL_NS -->|Yes| CP_DEBUG["`**Control Plane Debugging**
    â†’ Section 3`"]

    ALL_NS -->|No| NET_DEBUG["`**Networking Debugging**
    â†’ Section 6`"]

    style ALERT fill:#ff4444,stroke:#cc3636,color:#fff
    style POD_DEBUG fill:#4286f4,stroke:#2a6acf,color:#fff
    style NODE_DEBUG fill:#ff9900,stroke:#cc7a00,color:#fff
    style AZ_DEBUG fill:#ff4444,stroke:#cc3636,color:#fff
    style CP_DEBUG fill:#4286f4,stroke:#2a6acf,color:#fff
    style NET_DEBUG fill:#fbbc04,stroke:#c99603,color:#000
```

### AZ æ•…éšœæ£€æµ‹

:::warning AWS Health API è¦æ±‚
`aws health describe-events` API éœ€è¦ **AWS Business æˆ– Enterprise Support** è®¡åˆ’ã€‚å¦‚æœæ‚¨æ²¡æœ‰ Support è®¡åˆ’ï¼Œè¯·ç›´æ¥æŸ¥çœ‹ [AWS Health Dashboard æ§åˆ¶å°](https://health.aws.amazon.com/health/home)ï¼Œæˆ–åˆ›å»º EventBridge è§„åˆ™æ¥æ•è· Health äº‹ä»¶ã€‚
:::

```bash
# Check AWS Health API for EKS/EC2 events (requires Business/Enterprise Support plan)
aws health describe-events \
  --filter '{"services":["EKS","EC2"],"eventStatusCodes":["open"]}' \
  --region us-east-1

# Alternative: Detect AZ failures without a Support plan â€” create EventBridge rule
aws events put-rule \
  --name "aws-health-eks-events" \
  --event-pattern '{
    "source": ["aws.health"],
    "detail-type": ["AWS Health Event"],
    "detail": {
      "service": ["EKS", "EC2"],
      "eventTypeCategory": ["issue"]
    }
  }'

# Aggregate unhealthy Pods by AZ (only pods scheduled to a node)
kubectl get pods --all-namespaces -o json | jq -r '
  .items[] |
  select(.status.phase != "Running" and .status.phase != "Succeeded") |
  select(.spec.nodeName != null) |
  .spec.nodeName
' | sort -u | while read node; do
  zone=$(kubectl get node "$node" -o jsonpath='{.metadata.labels.topology\.kubernetes\.io/zone}' 2>/dev/null)
  [ -n "$zone" ] && echo "$zone"
done | sort | uniq -c | sort -rn

# Check ARC Zonal Shift status
aws arc-zonal-shift list-zonal-shifts \
  --resource-identifier arn:aws:eks:region:account:cluster/name
```

#### ä½¿ç”¨ ARC Zonal Shift è¿›è¡Œ AZ æ•…éšœå“åº”

```bash
# Enable Zonal Shift on EKS
aws eks update-cluster-config \
  --name <cluster-name> \
  --zonal-shift-config enabled=true

# Start manual Zonal Shift (move traffic away from impaired AZ)
aws arc-zonal-shift start-zonal-shift \
  --resource-identifier arn:aws:eks:region:account:cluster/name \
  --away-from us-east-1a \
  --expires-in 3h \
  --comment "AZ impairment detected"
```

:::warning Zonal Shift æ³¨æ„äº‹é¡¹
Zonal Shift çš„æœ€å¤§æŒç»­æ—¶é—´ä¸º **3 å¤©**ï¼Œå¯ä»¥å»¶é•¿ã€‚ä¸€æ—¦å¯åŠ¨ Shiftï¼Œæµå‘å—å½±å“ AZ ä¸­ Node ä¸Šè¿è¡Œçš„ Pod çš„æ–°æµé‡å°†è¢«é˜»æ–­ï¼Œå› æ­¤åœ¨æ‰§è¡Œå‰è¯·ç¡®è®¤å…¶ä»– AZ ä¸­æœ‰è¶³å¤Ÿçš„å®¹é‡ã€‚
:::

:::danger Zonal Shift ä»…é˜»æ–­æµé‡
ARC Zonal Shift **ä»…åœ¨ Load Balancer / Service å±‚é¢æ›´æ”¹æµé‡è·¯ç”±**ã€‚

<ZonalShiftImpactTable />

Karpenter NodePool å’Œ ASGï¼ˆManaged Node Groupï¼‰çš„ AZ é…ç½®ä¸ä¼šè‡ªåŠ¨æ›´æ–°ã€‚å®Œæ•´çš„ AZ ç–æ•£éœ€è¦é¢å¤–æ­¥éª¤ï¼š

1. **å¯åŠ¨ Zonal Shift** â†’ é˜»æ–­æ–°æµé‡ï¼ˆè‡ªåŠ¨ï¼‰
2. **æ’ç©ºå—å½±å“ AZ ä¸­çš„ Node** â†’ è¿ç§»ç°æœ‰ Pod
3. **ä» Karpenter NodePool æˆ– ASG å­ç½‘ä¸­ç§»é™¤å—å½±å“çš„ AZ** â†’ é˜»æ­¢æ–° Node åˆ›å»º

```bash
# 1. Identify and drain nodes in the affected AZ
for node in $(kubectl get nodes -l topology.kubernetes.io/zone=us-east-1a -o name); do
  kubectl cordon $node
  kubectl drain $node --ignore-daemonsets --delete-emptydir-data --grace-period=60
done

# 2. Temporarily exclude the affected AZ from Karpenter NodePool
kubectl patch nodepool default --type=merge -p '{
  "spec": {"template": {"spec": {"requirements": [
    {"key": "topology.kubernetes.io/zone", "operator": "In", "values": ["us-east-1b", "us-east-1c"]}
  ]}}}
}'

# 3. For Managed Node Groups, update ASG subnets (via console or IaC)
```

è¯·è®°ä½åœ¨ Zonal Shift å–æ¶ˆåæ¢å¤è¿™äº›æ›´æ”¹ã€‚
:::

### CloudWatch å¼‚å¸¸æ£€æµ‹

```bash
# Set up Anomaly Detection alarm for Pod restart counts
aws cloudwatch put-anomaly-detector \
  --single-metric-anomaly-detector '{
    "Namespace": "ContainerInsights",
    "MetricName": "pod_number_of_container_restarts",
    "Dimensions": [
      {"Name": "ClusterName", "Value": "<cluster-name>"},
      {"Name": "Namespace", "Value": "production"}
    ],
    "Stat": "Average"
  }'
```

### äº‹ä»¶å“åº”å‡çº§çŸ©é˜µ

<IncidentEscalationTable />

:::info é«˜å¯ç”¨æ¶æ„æŒ‡å—å‚è€ƒ
æœ‰å…³æ¶æ„çº§åˆ«çš„æ•…éšœæ¢å¤ç­–ç•¥ï¼ˆTopologySpreadConstraintsã€PodDisruptionBudgetã€å¤š AZ éƒ¨ç½²ç­‰ï¼‰ï¼Œè¯·å‚é˜… [EKS é«˜å¯ç”¨æ¶æ„æŒ‡å—](./eks-resiliency-guide.md)ã€‚
:::

---

## 3. EKS Control Plane è°ƒè¯•

### Control Plane æ—¥å¿—ç±»å‹

EKS Control Plane å¯ä»¥å‘ CloudWatch Logs å‘é€ 5 ç§æ—¥å¿—ç±»å‹ã€‚

<ControlPlaneLogTable />

### å¯ç”¨æ—¥å¿—

```bash
# Enable all control plane logs
aws eks update-cluster-config \
  --region <region> \
  --name <cluster-name> \
  --logging '{"clusterLogging":[{"types":["api","audit","authenticator","controllerManager","scheduler"],"enabled":true}]}'
```

:::tip æˆæœ¬ä¼˜åŒ–
å¯ç”¨æ‰€æœ‰æ—¥å¿—ç±»å‹ä¼šå¢åŠ  CloudWatch Logs æˆæœ¬ã€‚å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®å°† `audit` å’Œ `authenticator` ä½œä¸ºå¿…é¡»å¯ç”¨é¡¹ï¼Œå…¶ä½™ç±»å‹ä»…åœ¨éœ€è¦è°ƒè¯•æ—¶å¯ç”¨ã€‚
:::

### CloudWatch Logs Insights æŸ¥è¯¢

#### API Server é”™è¯¯ï¼ˆ400+ï¼‰åˆ†æ

```sql
fields @timestamp, @message
| filter @logStream like /kube-apiserver-audit/
| filter responseStatus.code >= 400
| stats count() by responseStatus.code
| sort count desc
```

#### è®¤è¯å¤±è´¥è¿½è¸ª

```sql
fields @timestamp, @message
| filter @logStream like /authenticator/
| filter @message like /error/ or @message like /denied/
| sort @timestamp desc
```

#### aws-auth ConfigMap å˜æ›´æ£€æµ‹

```sql
fields @timestamp, @message
| filter @logStream like /kube-apiserver-audit/
| filter objectRef.resource = "configmaps" and objectRef.name = "aws-auth"
| filter verb in ["update", "patch", "delete"]
| sort @timestamp desc
```

#### API é™æµæ£€æµ‹

```sql
fields @timestamp, @message
| filter @logStream like /kube-apiserver/
| filter @message like /throttle/ or @message like /rate limit/
| stats count() by bin(5m)
```

#### æœªæˆæƒè®¿é—®å°è¯•ï¼ˆå®‰å…¨äº‹ä»¶ï¼‰

```sql
fields @timestamp, @message
| filter @logStream like /kube-apiserver-audit/
| filter responseStatus.code = 403
| stats count() by user.username
| sort count desc
```

### è®¤è¯/æˆæƒè°ƒè¯•

#### IAM è®¤è¯éªŒè¯

```bash
# Check current IAM credentials
aws sts get-caller-identity

# Check cluster authentication mode
aws eks describe-cluster --name <cluster-name> \
  --query 'cluster.accessConfig.authenticationMode' --output text
```

#### aws-auth ConfigMapï¼ˆCONFIG_MAP æ¨¡å¼ï¼‰

```bash
# Check aws-auth ConfigMap
kubectl describe configmap aws-auth -n kube-system
```

#### EKS Access Entriesï¼ˆAPI / API_AND_CONFIG_MAP æ¨¡å¼ï¼‰

```bash
# Create Access Entry
aws eks create-access-entry \
  --cluster-name <cluster-name> \
  --principal-arn arn:aws:iam::ACCOUNT:role/ROLE-NAME \
  --type STANDARD

# List Access Entries
aws eks list-access-entries --cluster-name <cluster-name>
```

#### IRSAï¼ˆIAM Roles for Service Accountsï¼‰è°ƒè¯•æ£€æŸ¥æ¸…å•

```bash
# 1. Verify annotation on ServiceAccount
kubectl get sa <sa-name> -n <namespace> -o yaml

# 2. Check AWS environment variables inside the Pod
kubectl exec -it <pod-name> -- env | grep AWS

# 3. Verify OIDC Provider
aws eks describe-cluster --name <cluster-name> \
  --query 'cluster.identity.oidc.issuer' --output text

# 4. Verify OIDC Provider ARN and conditions in IAM Role Trust Policy
aws iam get-role --role-name <role-name> \
  --query 'Role.AssumeRolePolicyDocument'
```

:::warning å¸¸è§çš„ IRSA é”™è¯¯

- ServiceAccount æ³¨è§£ä¸­çš„ Role ARN æ‹¼å†™é”™è¯¯
- IAM Role Trust Policy ä¸­çš„ namespace/sa åç§°ä¸åŒ¹é…
- OIDC Provider æœªä¸é›†ç¾¤å…³è”
- Pod æœªæŒ‡å®š `spec.serviceAccountName` æ¥ä½¿ç”¨ ServiceAccount
:::

### Service Account Token è¿‡æœŸï¼ˆHTTP 401 Unauthorizedï¼‰

åœ¨ Kubernetes 1.21+ ä¸­ï¼ŒService Account Token **é»˜è®¤æœ‰æ•ˆæœŸä¸º 1 å°æ—¶**ï¼Œç”± kubelet è‡ªåŠ¨åˆ·æ–°ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ—§ç‰ˆ SDK ç¼ºå°‘ Token åˆ·æ–°é€»è¾‘ï¼Œé•¿æ—¶é—´è¿è¡Œçš„å·¥ä½œè´Ÿè½½å¯èƒ½ä¼šé‡åˆ° `401 Unauthorized` é”™è¯¯ã€‚

**ç—‡çŠ¶ï¼š**

- Pod åœ¨ä¸€æ®µæ—¶é—´åï¼ˆé€šå¸¸ä¸º 1 å°æ—¶ï¼‰çªç„¶è¿”å› `HTTP 401 Unauthorized` é”™è¯¯
- é‡å¯åæš‚æ—¶æ¢å¤æ­£å¸¸ï¼Œä¹‹å 401 é”™è¯¯å†æ¬¡å‡ºç°

**åŸå› ï¼š**

- Projected Service Account Token é»˜è®¤åœ¨ 1 å°æ—¶åè¿‡æœŸ
- kubelet ä¼šè‡ªåŠ¨åˆ·æ–° Tokenï¼Œä½†å¦‚æœåº”ç”¨ç¨‹åºåªè¯»å–ä¸€æ¬¡ Token æ–‡ä»¶å¹¶ç¼“å­˜ï¼Œåˆ™ä¼šç»§ç»­ä½¿ç”¨è¿‡æœŸçš„ Token

**æœ€ä½ SDK ç‰ˆæœ¬è¦æ±‚ï¼š**

| è¯­è¨€ | SDK | æœ€ä½ç‰ˆæœ¬ |
|------|-----|----------|
| Go | client-go | v0.15.7+ |
| Python | kubernetes | 12.0.0+ |
| Java | fabric8 | 5.0.0+ |

:::tip Token åˆ·æ–°éªŒè¯
è¯·éªŒè¯æ‚¨çš„ SDK æ˜¯å¦æ”¯æŒè‡ªåŠ¨ Token åˆ·æ–°ã€‚å¦‚æœä¸æ”¯æŒï¼Œæ‚¨çš„åº”ç”¨ç¨‹åºå¿…é¡»å®šæœŸé‡æ–°è¯»å– `/var/run/secrets/kubernetes.io/serviceaccount/token` æ–‡ä»¶ã€‚
:::

### EKS Pod Identity è°ƒè¯•

EKS Pod Identity æ˜¯ IRSA çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸º Pod æˆäºˆ AWS IAM æƒé™æä¾›äº†æ›´ç®€å•çš„è®¾ç½®æ–¹å¼ã€‚

```bash
# Check Pod Identity Associations
aws eks list-pod-identity-associations --cluster-name $CLUSTER
aws eks describe-pod-identity-association --cluster-name $CLUSTER \
  --association-id $ASSOC_ID

# Check Pod Identity Agent status
kubectl get pods -n kube-system -l app.kubernetes.io/name=eks-pod-identity-agent
kubectl logs -n kube-system -l app.kubernetes.io/name=eks-pod-identity-agent --tail=50
```

**Pod Identity è°ƒè¯•æ£€æŸ¥æ¸…å•ï¼š**

- ç¡®è®¤ eks-pod-identity-agent Add-on å·²å®‰è£…
- ç¡®è®¤æ­£ç¡®çš„å…³è”å·²é“¾æ¥åˆ° Pod çš„ ServiceAccount
- ç¡®è®¤ IAM Role Trust Policy åŒ…å« `pods.eks.amazonaws.com` æœåŠ¡ä¸»ä½“

:::info Pod Identity ä¸ IRSA å¯¹æ¯”
Pod Identity æ¯” IRSA è®¾ç½®æ›´ç®€å•ï¼Œå¹¶ä¸”æ›´æ˜“äºè¿›è¡Œè·¨è´¦æˆ·è®¿é—®ã€‚å»ºè®®æ–°å·¥ä½œè´Ÿè½½ä½¿ç”¨ Pod Identityã€‚
:::

### EKS Add-on æ•…éšœæ’é™¤

```bash
# List Add-ons
aws eks list-addons --cluster-name <cluster-name>

# Check Add-on status in detail
aws eks describe-addon --cluster-name <cluster-name> --addon-name <addon-name>

# Update Add-on (resolve conflicts with PRESERVE to keep existing settings)
aws eks update-addon --cluster-name <cluster-name> --addon-name <addon-name> \
  --addon-version <version> --resolve-conflicts PRESERVE
```

| Add-on | å¸¸è§é”™è¯¯æ¨¡å¼ | è¯Šæ–­æ–¹æ³• | è§£å†³æ–¹æ¡ˆ |
|--------|-------------|----------|----------|
| **CoreDNS** | Pod CrashLoopBackOffã€DNS è¶…æ—¶ | `kubectl logs -n kube-system -l k8s-app=kube-dns` | æ£€æŸ¥ ConfigMapï¼Œ`kubectl rollout restart deployment coredns -n kube-system` |
| **kube-proxy** | Service é€šä¿¡å¤±è´¥ã€iptables é”™è¯¯ | `kubectl logs -n kube-system -l k8s-app=kube-proxy` | éªŒè¯ DaemonSet é•œåƒç‰ˆæœ¬ï¼Œ`kubectl rollout restart daemonset kube-proxy -n kube-system` |
| **VPC CNI** | Pod IP åˆ†é…å¤±è´¥ã€ENI é”™è¯¯ | `kubectl logs -n kube-system -l k8s-app=aws-node` | æ£€æŸ¥ IPAMD æ—¥å¿—ï¼ŒéªŒè¯ ENI/IP é™åˆ¶ï¼ˆå‚è§ç¬¬ 6 èŠ‚ï¼‰ |
| **EBS CSI** | PVC Pendingã€å·æŒ‚è½½å¤±è´¥ | `kubectl logs -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver` | æ£€æŸ¥ IRSA æƒé™ï¼ŒéªŒè¯ AZ åŒ¹é…ï¼ˆå‚è§ç¬¬ 7 èŠ‚ï¼‰ |

### é›†ç¾¤å¥åº·çŠ¶æ€é—®é¢˜ä»£ç 

åœ¨è¯Šæ–­ EKS é›†ç¾¤æœ¬èº«çš„åŸºç¡€è®¾æ–½çº§åˆ«é—®é¢˜æ—¶ï¼Œè¯·æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€ã€‚

```bash
# Check cluster health issues
aws eks describe-cluster --name $CLUSTER \
  --query 'cluster.health' --output json
```

<ClusterHealthTable />

:::danger ä¸å¯æ¢å¤çš„é—®é¢˜
`VPC_NOT_FOUND` å’Œ `KMS_KEY_NOT_FOUND` æ˜¯ä¸å¯æ¢å¤çš„ã€‚å¿…é¡»é‡æ–°åˆ›å»ºé›†ç¾¤ã€‚
:::

---
## 4. èŠ‚ç‚¹çº§è°ƒè¯•

### èŠ‚ç‚¹åŠ å…¥å¤±è´¥è°ƒè¯•

å½“èŠ‚ç‚¹æ— æ³•åŠ å…¥é›†ç¾¤æ—¶ï¼Œå¯èƒ½å­˜åœ¨å¤šç§åŸå› ã€‚ä»¥ä¸‹æ˜¯ 8 ç§æœ€å¸¸è§çš„åŸå› åŠå…¶è¯Šæ–­æ–¹æ³•ã€‚

**èŠ‚ç‚¹åŠ å…¥å¤±è´¥çš„å¸¸è§åŸå› ï¼š**

1. **èŠ‚ç‚¹ IAM Role æœªåœ¨ aws-auth ConfigMap ä¸­æ³¨å†Œ**ï¼ˆæˆ–æœªåˆ›å»º Access Entryï¼‰â€” èŠ‚ç‚¹æ— æ³•é€šè¿‡ API server è®¤è¯
2. **bootstrap è„šæœ¬ä¸­çš„ ClusterName ä¸å®é™…é›†ç¾¤åç§°ä¸åŒ¹é…** â€” kubelet å°è¯•è¿æ¥åˆ°é”™è¯¯çš„é›†ç¾¤
3. **èŠ‚ç‚¹å®‰å…¨ç»„ä¸å…è®¸ä¸æ§åˆ¶å¹³é¢é€šä¿¡** â€” éœ€è¦ TCP 443ï¼ˆAPI serverï¼‰å’Œ TCP 10250ï¼ˆkubeletï¼‰ç«¯å£
4. **å…¬æœ‰å­ç½‘ä¸­æœªå¯ç”¨è‡ªåŠ¨åˆ†é…å…¬ç½‘ IP** â€” åœ¨ä»…å¯ç”¨å…¬å…±ç«¯ç‚¹çš„é›†ç¾¤ä¸Šæ— æ³•è®¿é—®äº’è”ç½‘
5. **VPC DNS é…ç½®é—®é¢˜** â€” `enableDnsHostnames` æˆ– `enableDnsSupport` è¢«ç¦ç”¨
6. **STS åŒºåŸŸç«¯ç‚¹è¢«ç¦ç”¨** â€” IAM è®¤è¯è¿‡ç¨‹ä¸­ STS è°ƒç”¨å¤±è´¥
7. **åœ¨ aws-auth ä¸­æ³¨å†Œäº†å®ä¾‹é…ç½®æ–‡ä»¶ ARN è€ŒéèŠ‚ç‚¹ IAM Role ARN** â€” aws-auth ä¸­åªåº”æ³¨å†Œ Role ARN
8. **ç¼ºå°‘ `eks:kubernetes.io/cluster-name` æ ‡ç­¾**ï¼ˆè‡ªç®¡ç†èŠ‚ç‚¹ï¼‰â€” EKS æ— æ³•è¯†åˆ«è¯¥èŠ‚ç‚¹å±äºé›†ç¾¤

**è¯Šæ–­å‘½ä»¤ï¼š**

```bash
# Check node bootstrap logs (after SSM access)
sudo journalctl -u kubelet --no-pager | tail -50
sudo cat /var/log/cloud-init-output.log | tail -50

# Check security group rules
aws ec2 describe-security-groups --group-ids $CLUSTER_SG \
  --query 'SecurityGroups[].IpPermissions' --output table

# Check VPC DNS settings
aws ec2 describe-vpc-attribute --vpc-id $VPC_ID --attribute enableDnsHostnames
aws ec2 describe-vpc-attribute --vpc-id $VPC_ID --attribute enableDnsSupport
```

:::warning åœ¨ aws-auth ä¸­åº”æ³¨å†Œçš„ ARN
aws-auth ConfigMap éœ€è¦çš„æ˜¯ **IAM Role ARN**ï¼ˆ`arn:aws:iam::ACCOUNT:role/...`ï¼‰ï¼Œè€Œéå®ä¾‹é…ç½®æ–‡ä»¶ ARNï¼ˆ`arn:aws:iam::ACCOUNT:instance-profile/...`ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªæå…¶å¸¸è§çš„é”™è¯¯ï¼Œä¹Ÿæ˜¯èŠ‚ç‚¹åŠ å…¥å¤±è´¥çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚
:::

### Node NotReady å†³ç­–æ ‘

```mermaid
flowchart TD
    NR["`**Node NotReady**`"] --> CHECK_INST{"`Check EC2 instance
    status`"}

    CHECK_INST -->|Stopped/Terminated| INST_ISSUE["`Restart instance
    or provision new node`"]

    CHECK_INST -->|Running| CHECK_KUBELET{"`Check kubelet
    status`"}

    CHECK_KUBELET -->|Not Running| KUBELET_FIX["`Restart kubelet
    systemctl restart kubelet`"]

    CHECK_KUBELET -->|Running| CHECK_CONTAINERD{"`Check containerd
    status`"}

    CHECK_CONTAINERD -->|Not Running| CONTAINERD_FIX["`Restart containerd
    systemctl restart containerd`"]

    CHECK_CONTAINERD -->|Running| CHECK_RESOURCE{"`Check resource
    pressure`"}

    CHECK_RESOURCE -->|DiskPressure| DISK_FIX["`Clean up disk
    crictl rmi --prune`"]

    CHECK_RESOURCE -->|MemoryPressure| MEM_FIX["`Evict low-priority Pods
    or replace node`"]

    CHECK_RESOURCE -->|Normal| CHECK_NET{"`Check node
    networking`"}

    CHECK_NET --> NET_FIX["`Inspect Security Group / NACL
    / VPC routing`"]

    style NR fill:#ff4444,stroke:#cc3636,color:#fff
    style INST_ISSUE fill:#34a853,stroke:#2a8642,color:#fff
    style KUBELET_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style CONTAINERD_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style DISK_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style MEM_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style NET_FIX fill:#34a853,stroke:#2a8642,color:#fff
```

### kubelet / containerd è°ƒè¯•

```bash
# Connect to node via SSM
aws ssm start-session --target <instance-id>

# Check kubelet status
systemctl status kubelet
journalctl -u kubelet -n 100 -f

# Check containerd status
systemctl status containerd

# Check container runtime status
crictl pods
crictl ps -a

# Check logs for a specific container
crictl logs <container-id>
```

:::info SSM è®¿é—®å‰ææ¡ä»¶
SSM è®¿é—®éœ€è¦å°† `AmazonSSMManagedInstanceCore` ç­–ç•¥é™„åŠ åˆ°èŠ‚ç‚¹çš„ IAM Roleã€‚æ­¤ç­–ç•¥åœ¨ EKS æ‰˜ç®¡èŠ‚ç‚¹ç»„ä¸­é»˜è®¤åŒ…å«ï¼Œä½†å¦‚æœä½¿ç”¨è‡ªå®šä¹‰ AMIï¼Œè¯·ç¡®è®¤ SSM Agent å·²å®‰è£…ã€‚
:::

### èµ„æºå‹åŠ›è¯Šæ–­ä¸è§£å†³

```bash
# Check node conditions
kubectl describe node <node-name>
```

| çŠ¶å†µ | é˜ˆå€¼ | è¯Šæ–­å‘½ä»¤ | è§£å†³æ–¹æ¡ˆ |
|-----------|-----------|--------------------|------------|
| **DiskPressure** | å¯ç”¨ç£ç›˜ &lt; 10% | `df -h`ï¼ˆSSM è®¿é—®åï¼‰ | ä½¿ç”¨ `crictl rmi --prune` æ¸…ç†æœªä½¿ç”¨çš„é•œåƒï¼Œä½¿ç”¨ `crictl rm` ç§»é™¤å·²åœæ­¢çš„å®¹å™¨ |
| **MemoryPressure** | å¯ç”¨å†…å­˜ &lt; 100Mi | `free -m`ï¼ˆSSM è®¿é—®åï¼‰ | é©±é€ä½ä¼˜å…ˆçº§ Podï¼Œè°ƒæ•´å†…å­˜ requests/limitsï¼Œæ›¿æ¢èŠ‚ç‚¹ |
| **PIDPressure** | å¯ç”¨ PID &lt; 5% | `ps aux \| wc -l`ï¼ˆSSM è®¿é—®åï¼‰ | å¢å¤§ `kernel.pid_max`ï¼Œå®šä½å¹¶é‡å¯å¯¼è‡´ PID æ³„æ¼çš„å®¹å™¨ |

### Karpenter èŠ‚ç‚¹ä¾›åº”è°ƒè¯•

```bash
# Check Karpenter controller logs
kubectl logs -f deployment/karpenter -n kube-system

# Check NodePool status
kubectl get nodepool
kubectl describe nodepool <nodepool-name>

# Check EC2NodeClass
kubectl get ec2nodeclass
kubectl describe ec2nodeclass <nodeclass-name>

# When provisioning fails, verify:
# 1. NodePool limits have not been exceeded
# 2. EC2NodeClass subnet/security group selectors are correct
# 3. Service Quotas are sufficient for the instance types
# 4. Pod nodeSelector/affinity matches NodePool requirements
```

:::warning Karpenter v1 API å˜æ›´
åœ¨ Karpenter v1.0+ ä¸­ï¼Œ`Provisioner` å·²é‡å‘½åä¸º `NodePool`ï¼Œ`AWSNodeTemplate` å·²é‡å‘½åä¸º `EC2NodeClass`ã€‚å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ v0.x é…ç½®ï¼Œåˆ™éœ€è¦è¿›è¡Œè¿ç§»ã€‚è¯·å°† API group æ›´æ–°ä¸º `karpenter.sh/v1`ã€‚
:::

### æ‰˜ç®¡èŠ‚ç‚¹ç»„é”™è¯¯ä»£ç 

æ£€æŸ¥æ‰˜ç®¡èŠ‚ç‚¹ç»„çš„å¥åº·çŠ¶æ€ä»¥è¯Šæ–­ä¾›åº”å’Œè¿ç»´é—®é¢˜ã€‚

```bash
# Check node group health status
aws eks describe-nodegroup --cluster-name $CLUSTER --nodegroup-name $NODEGROUP \
  --query 'nodegroup.health' --output json
```

<NodeGroupErrorTable />

**AccessDenied é”™è¯¯æ¢å¤ â€” æ£€æŸ¥ eks:node-manager ClusterRoleï¼š**

`AccessDenied` é”™è¯¯é€šå¸¸åœ¨ `eks:node-manager` ClusterRole æˆ– ClusterRoleBinding è¢«åˆ é™¤æˆ–ä¿®æ”¹æ—¶å‘ç”Ÿã€‚

```bash
# Check eks:node-manager ClusterRole
kubectl get clusterrole eks:node-manager
kubectl get clusterrolebinding eks:node-manager
```

:::danger AccessDenied æ¢å¤
å¦‚æœ `eks:node-manager` ClusterRole/ClusterRoleBinding ç¼ºå¤±ï¼ŒEKS **ä¸ä¼šè‡ªåŠ¨æ¢å¤å®ƒä»¬**ã€‚æ‚¨å¿…é¡»ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¹‹ä¸€æ‰‹åŠ¨æ¢å¤ï¼š

**æ–¹æ³• 1ï¼šæ‰‹åŠ¨é‡æ–°åˆ›å»ºï¼ˆæ¨èï¼‰**

```yaml
# eks-node-manager-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: eks:node-manager
rules:
  - apiGroups: ['']
    resources: [pods]
    verbs: [get, list, watch, delete]
  - apiGroups: ['']
    resources: [nodes]
    verbs: [get, list, watch, patch]
  - apiGroups: ['']
    resources: [pods/eviction]
    verbs: [create]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: eks:node-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: eks:node-manager
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: eks:node-manager
```

```bash
kubectl auth reconcile -f eks-node-manager-role.yaml
```

**æ–¹æ³• 2ï¼šé‡æ–°åˆ›å»ºèŠ‚ç‚¹ç»„**

```bash
# RBAC resources are created together when creating a new node group
eksctl create nodegroup --cluster=<cluster-name> --name=<new-nodegroup-name>
```

**æ–¹æ³• 3ï¼šå‡çº§èŠ‚ç‚¹ç»„**

```bash
# Upgrade process may trigger RBAC re-setup
eksctl upgrade nodegroup --cluster=<cluster-name> --name=<nodegroup-name>
```

> **æ³¨æ„**ï¼šKubernetes é»˜è®¤ç³»ç»Ÿ ClusterRoleï¼ˆ`system:*`ï¼‰ç”± API server è‡ªåŠ¨åè°ƒï¼Œä½† EKS ç‰¹æœ‰çš„ ClusterRoleï¼ˆ`eks:*`ï¼‰ä¸ä¼šè‡ªåŠ¨æ¢å¤ã€‚åˆ é™¤å‰è¯·åŠ¡å¿…å¤‡ä»½ RBAC èµ„æºã€‚
:::

### ä½¿ç”¨ Node Readiness Controller è°ƒè¯•èŠ‚ç‚¹å¼•å¯¼

:::info æ–° Kubernetes ç‰¹æ€§ï¼ˆ2026 å¹´ 2 æœˆï¼‰
[Node Readiness Controller](https://github.com/kubernetes-sigs/node-readiness-controller) æ˜¯ Kubernetes å®˜æ–¹åšå®¢å…¬å¸ƒçš„æ–°é¡¹ç›®ï¼Œå®ƒä»¥å£°æ˜å¼æ–¹å¼è§£å†³èŠ‚ç‚¹å¼•å¯¼è¿‡ç¨‹ä¸­çš„è¿‡æ—©è°ƒåº¦é—®é¢˜ã€‚
:::

#### é—®é¢˜åœºæ™¯

åœ¨æ ‡å‡† Kubernetes ä¸­ï¼Œä¸€æ—¦èŠ‚ç‚¹è¾¾åˆ° `Ready` çŠ¶æ€ï¼Œå·¥ä½œè´Ÿè½½å°±ä¼šè¢«è°ƒåº¦ã€‚ç„¶è€Œï¼ŒèŠ‚ç‚¹å¯èƒ½å®é™…ä¸Šå°šæœªå®Œå…¨å‡†å¤‡å°±ç»ªï¼š

| æœªå®Œæˆçš„ç»„ä»¶ | ç—‡çŠ¶ | å½±å“ |
|---|---|---|
| GPU é©±åŠ¨/å›ºä»¶åŠ è½½ä¸­ | `nvidia-smi` å¤±è´¥ï¼ŒPod `CrashLoopBackOff` | GPU å·¥ä½œè´Ÿè½½å¤±è´¥ |
| CNI æ’ä»¶åˆå§‹åŒ–ä¸­ | Pod IP æœªåˆ†é…ï¼Œ`NetworkNotReady` | ç½‘ç»œé€šä¿¡å¤±è´¥ |
| CSI é©±åŠ¨æœªæ³¨å†Œ | PVC `Pending`ï¼Œå·æŒ‚è½½å¤±è´¥ | å­˜å‚¨ä¸å¯è®¿é—® |
| å®‰å…¨ä»£ç†æœªå®‰è£… | åˆè§„æ€§è¿è§„ | å®‰å…¨ç­–ç•¥æœªæ»¡è¶³ |

#### Node Readiness Controller å·¥ä½œåŸç†

Node Readiness Controller **ä»¥å£°æ˜å¼æ–¹å¼ç®¡ç†è‡ªå®šä¹‰ taint**ï¼Œå»¶è¿Ÿå·¥ä½œè´Ÿè½½è°ƒåº¦ç›´åˆ°æ‰€æœ‰åŸºç¡€è®¾æ–½è¦æ±‚éƒ½å¾—åˆ°æ»¡è¶³ï¼š

```mermaid
flowchart LR
    A[Node Provisioned] --> B[kubelet Ready]
    B --> C[Custom Taints Applied<br/>node.readiness/gpu=NotReady:NoSchedule<br/>node.readiness/cni=NotReady:NoSchedule]
    C --> D{Check Health Signals}
    D -->|GPU Ready| E[Remove GPU Taint]
    D -->|CNI Ready| F[Remove CNI Taint]
    E --> G{All Taints Removed?}
    F --> G
    G -->|Yes| H[Start Workload Scheduling]
    G -->|No| D
```

#### è°ƒè¯•æ£€æŸ¥æ¸…å•

å½“èŠ‚ç‚¹å¤„äº `Ready` çŠ¶æ€ä½† Pod æœªè¢«è°ƒåº¦æ—¶ï¼š

```bash
# 1. Check custom readiness taints on the node
kubectl get node <node-name> -o jsonpath='{.spec.taints}' | jq .

# 2. Filter for node.readiness related taints
kubectl get nodes -o json | jq '
  .items[] |
  select(.spec.taints // [] | any(.key | startswith("node.readiness"))) |
  {name: .metadata.name, taints: [.spec.taints[] | select(.key | startswith("node.readiness"))]}
'

# 3. Check Pod tolerations vs node taint mismatch
kubectl describe pod <pending-pod> | grep -A 20 "Events:"
```

#### ç›¸å…³ç‰¹æ€§ï¼šPod Scheduling Readinessï¼ˆK8s 1.30 GAï¼‰

`schedulingGates` å…è®¸ä» Pod ä¾§æ§åˆ¶è°ƒåº¦å°±ç»ªçŠ¶æ€ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gated-pod
spec:
  schedulingGates:
    - name: "example.com/gpu-validation"  # Scheduling waits until this gate is removed
  containers:
    - name: app
      image: app:latest
```

```bash
# Find Pods with schedulingGates
kubectl get pods -o json | jq '
  .items[] |
  select(.spec.schedulingGates != null and (.spec.schedulingGates | length > 0)) |
  {name: .metadata.name, namespace: .metadata.namespace, gates: .spec.schedulingGates}
'
```

#### ç›¸å…³ç‰¹æ€§ï¼šPod Readiness Gatesï¼ˆAWS LB Controllerï¼‰

AWS Load Balancer Controller ä½¿ç”¨ `elbv2.k8s.aws/pod-readiness-gate-inject` æ³¨è§£æ¥å»¶è¿Ÿ Pod `Ready` çŠ¶æ€è½¬æ¢ï¼Œç›´åˆ° ALB/NLB ç›®æ ‡æ³¨å†Œå®Œæˆï¼š

```bash
# Check Readiness Gate status
kubectl get pod <pod-name> -o jsonpath='{.status.conditions}' | jq '
  [.[] | select(.type | contains("target-health"))]
'

# Check if readiness gate injection is enabled for namespace
kubectl get namespace <ns> -o jsonpath='{.metadata.labels.elbv2\.k8s\.aws/pod-readiness-gate-inject}'
```

:::tip Readiness ç‰¹æ€§å¯¹æ¯”

| ç‰¹æ€§ | ç›®æ ‡ | æ§åˆ¶æœºåˆ¶ | çŠ¶æ€ |
|---------|--------|-------------------|--------|
| **Node Readiness Controller** | Node | åŸºäº Taint | æ–°ç‰¹æ€§ï¼ˆ2026 å¹´ 2 æœˆï¼‰ |
| **Pod Scheduling Readiness** | Pod | schedulingGates | GAï¼ˆK8s 1.30ï¼‰ |
| **Pod Readiness Gates** | Pod | Readiness Conditions | GAï¼ˆAWS LB Controllerï¼‰ |
:::

### ä½¿ç”¨ eks-node-viewer

[eks-node-viewer](https://github.com/awslabs/eks-node-viewer) æ˜¯ä¸€æ¬¾åœ¨ç»ˆç«¯ä¸­å®æ—¶å¯è§†åŒ–èŠ‚ç‚¹èµ„æºåˆ©ç”¨ç‡çš„å·¥å…·ã€‚

```bash
# Basic usage (CPU-based)
eks-node-viewer

# View both CPU and memory
eks-node-viewer --resources cpu,memory

# View a specific NodePool only
eks-node-viewer --node-selector karpenter.sh/nodepool=<nodepool-name>
```

---

## 5. å·¥ä½œè´Ÿè½½è°ƒè¯•

### Pod çŠ¶æ€è°ƒè¯•æµç¨‹å›¾

```mermaid
flowchart TD
    START["`**Pod Anomaly Detected**`"] --> STATUS{"`Check Pod status
    kubectl get pod`"}

    STATUS -->|Pending| PENDING{"`Can it be
    scheduled?`"}
    PENDING -->|Insufficient resources| PEND_RES["`Check Node capacity
    kubectl describe node
    Inspect Karpenter NodePool`"]
    PENDING -->|nodeSelector/affinity mismatch| PEND_LABEL["`Check Node labels
    Fix toleration/affinity`"]
    PENDING -->|Waiting for PVC binding| PEND_PVC["`Check PVC status
    â†’ Section 7 Storage`"]

    STATUS -->|ImagePullBackOff| IMG{"`Image issue`"}
    IMG --> IMG_FIX["`Verify image name/tag
    Check registry access permissions
    Verify imagePullSecrets`"]

    STATUS -->|CrashLoopBackOff| CRASH{"`Container crash`"}
    CRASH --> CRASH_FIX["`kubectl logs --previous
    Check resource limits
    Check liveness probe
    Inspect app config/dependencies`"]

    STATUS -->|OOMKilled| OOM{"`Memory exceeded`"}
    OOM --> OOM_FIX["`Increase memory limits
    Investigate app memory leak
    Check JVM heap settings`"]

    STATUS -->|Running but not Ready| READY{"`Readiness failure`"}
    READY --> READY_FIX["`Check readinessProbe config
    Inspect health check endpoint
    Verify dependent service status`"]

    STATUS -->|Terminating| TERM{"`Termination delay`"}
    TERM --> TERM_FIX["`Check Finalizers
    Inspect preStop hook
    Force delete:
    kubectl delete pod --force
    --grace-period=0`"]

    style START fill:#ff4444,stroke:#cc3636,color:#fff
    style PEND_RES fill:#34a853,stroke:#2a8642,color:#fff
    style PEND_LABEL fill:#34a853,stroke:#2a8642,color:#fff
    style PEND_PVC fill:#34a853,stroke:#2a8642,color:#fff
    style IMG_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style CRASH_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style OOM_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style READY_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style TERM_FIX fill:#34a853,stroke:#2a8642,color:#fff
```

### åŸºæœ¬è°ƒè¯•å‘½ä»¤

```bash
# Check Pod status
kubectl get pods -n <namespace>
kubectl describe pod <pod-name> -n <namespace>

# Check current/previous container logs
kubectl logs <pod-name> -n <namespace>
kubectl logs <pod-name> -n <namespace> --previous

# Check namespace events
kubectl get events -n <namespace> --sort-by='.lastTimestamp'

# Check resource usage
kubectl top pods -n <namespace>
```

### ä½¿ç”¨ kubectl debug

#### Ephemeral Containerï¼ˆå‘è¿è¡Œä¸­çš„ Pod æ·»åŠ è°ƒè¯•å®¹å™¨ï¼‰

```bash
# Basic ephemeral container
kubectl debug <pod-name> -it --image=busybox --target=<container-name>

# Image with network debugging tools
kubectl debug <pod-name> -it --image=nicolaka/netshoot --target=<container-name>
```

#### Pod Copyï¼ˆå…‹éš† Pod è¿›è¡Œè°ƒè¯•ï¼‰

```bash
# Clone a Pod and start with a different image
kubectl debug <pod-name> --copy-to=debug-pod --image=ubuntu

# Change the command when cloning a Pod
kubectl debug <pod-name> --copy-to=debug-pod --container=<container-name> -- sh
```

#### Node è°ƒè¯•ï¼ˆç›´æ¥è®¿é—®èŠ‚ç‚¹ï¼‰

```bash
# Node debugging (host filesystem is mounted at /host)
kubectl debug node/<node-name> -it --image=ubuntu
```

:::tip kubectl debug ä¸ SSM å¯¹æ¯”
`kubectl debug node/` å³ä½¿åœ¨æœªå®‰è£… SSM Agent çš„èŠ‚ç‚¹ä¸Šä¹Ÿå¯ä½¿ç”¨ã€‚ä½†æ˜¯ï¼Œè¦è®¿é—®å®¿ä¸»æœºç½‘ç»œå‘½åç©ºé—´ï¼Œéœ€è¦æ·»åŠ  `--profile=sysadmin` é€‰é¡¹ã€‚
:::

### Deployment Rollout è°ƒè¯•

```bash
# Check rollout status
kubectl rollout status deployment/<name>

# Rollout history
kubectl rollout history deployment/<name>

# Roll back to the previous version
kubectl rollout undo deployment/<name>

# Roll back to a specific revision
kubectl rollout undo deployment/<name> --to-revision=2

# Restart Deployment (Rolling restart)
kubectl rollout restart deployment/<name>
```

### HPA / VPA è°ƒè¯•

```bash
# Check HPA status
kubectl get hpa
kubectl describe hpa <hpa-name>

# Verify metrics-server is running
kubectl get deployment metrics-server -n kube-system
kubectl top pods  # If this command fails, metrics-server has an issue

# Check scaling failure reasons in HPA events
kubectl describe hpa <hpa-name> | grep -A 5 "Events"
```

**HPA æ‰©ç¼©å®¹å¤±è´¥åˆ†æï¼š**

| ç—‡çŠ¶ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|-------|------------|
| `unable to get metrics` | metrics-server æœªå®‰è£…æˆ–æ•…éšœ | æ£€æŸ¥ metrics-server Pod çŠ¶æ€å¹¶é‡å¯ |
| `current metrics unknown` | ä»ç›®æ ‡ Pod æ”¶é›†æŒ‡æ ‡å¤±è´¥ | ç¡®è®¤ Pod å·²è®¾ç½® resource requests |
| `target not found` | scaleTargetRef ä¸åŒ¹é… | ç¡®è®¤ Deployment/StatefulSet åç§°å’Œ apiVersion |
| æ‰©å®¹åç«‹å³ç¼©å®¹ | æœªé…ç½® stabilizationWindow | è®¾ç½® `behavior.scaleDown.stabilizationWindowSeconds` |

### Probe è°ƒè¯•ä¸æœ€ä½³å®è·µ

```yaml
# Recommended Probe configuration example
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  template:
    spec:
      containers:
      - name: app
        image: my-app:latest
        ports:
        - containerPort: 8080
        # Startup Probe: Confirms app startup completion (essential for slow-starting apps)
        startupProbe:
          httpGet:
            path: /healthz
            port: 8080
          failureThreshold: 30    # Wait up to 300 seconds (30 x 10s)
          periodSeconds: 10
        # Liveness Probe: Checks if the app is alive (deadlock detection)
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
          successThreshold: 1
        # Readiness Probe: Checks if the app can receive traffic
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
          successThreshold: 1
```

:::danger Probe é…ç½®æ³¨æ„äº‹é¡¹

- **ä¸è¦åœ¨ Liveness Probe ä¸­åŒ…å«å¤–éƒ¨ä¾èµ–**ï¼ˆä¾‹å¦‚æ•°æ®åº“è¿æ¥æ£€æŸ¥ï¼‰ã€‚å½“å¤–éƒ¨æœåŠ¡å®•æœºæ—¶ï¼Œè¿™å¯èƒ½è§¦å‘çº§è”æ•…éšœï¼Œå¯¼è‡´æ‰€æœ‰ Pod é‡å¯ã€‚
- **ä¸è¦åœ¨æ²¡æœ‰ startupProbe çš„æƒ…å†µä¸‹è®¾ç½®è¿‡é«˜çš„ initialDelaySeconds**ã€‚ç”±äº liveness/readiness probe åœ¨ startupProbe æˆåŠŸä¹‹å‰ä¸ä¼šå¯ç”¨ï¼Œå¯¹äºå¯åŠ¨ç¼“æ…¢çš„åº”ç”¨ç¨‹åºåº”ä½¿ç”¨ startupProbeã€‚
- Readiness Probe å¤±è´¥ä¸ä¼šé‡å¯ Podï¼›å®ƒåªä¼šå°† Pod ä» Service Endpoints ä¸­ç§»é™¤ã€‚
:::

---
## 6. ç½‘ç»œè°ƒè¯•

### ç½‘ç»œè°ƒè¯•å·¥ä½œæµ

```mermaid
flowchart TD
    NET_ISSUE["`**Network Issue Detected**`"] --> CHECK_CNI{"`VPC CNI
    functioning normally?`"}

    CHECK_CNI -->|Pod IP not assigned| CNI_DEBUG["`**VPC CNI Debugging**
    IP exhaustion, ENI limits
    Prefix Delegation`"]

    CHECK_CNI -->|Normal| CHECK_DNS{"`DNS resolution
    working?`"}

    CHECK_DNS -->|Failing| DNS_DEBUG["`**DNS Debugging**
    Check CoreDNS logs
    Inspect ndots configuration`"]

    CHECK_DNS -->|Normal| CHECK_SVC{"`Service
    accessible?`"}

    CHECK_SVC -->|Failing| SVC_DEBUG["`**Service Debugging**
    Verify Selector match
    Check Endpoints`"]

    CHECK_SVC -->|Normal| CHECK_NP{"`NetworkPolicy
    blocking?`"}

    CHECK_NP -->|Yes| NP_DEBUG["`**NetworkPolicy Debugging**
    Verify AND/OR selectors
    Validate policy rules`"]

    CHECK_NP -->|No| CHECK_LB{"`Ingress / LB
    issue?`"}

    CHECK_LB --> LB_DEBUG["`**Ingress/LB Debugging**
    Check Target Group health
    Verify Security Groups`"]

    style NET_ISSUE fill:#ff4444,stroke:#cc3636,color:#fff
    style CNI_DEBUG fill:#4286f4,stroke:#2a6acf,color:#fff
    style DNS_DEBUG fill:#4286f4,stroke:#2a6acf,color:#fff
    style SVC_DEBUG fill:#4286f4,stroke:#2a6acf,color:#fff
    style NP_DEBUG fill:#fbbc04,stroke:#c99603,color:#000
    style LB_DEBUG fill:#ff9900,stroke:#cc7a00,color:#fff
```

### VPC CNI è°ƒè¯•

```bash
# Check VPC CNI Pod status
kubectl get pods -n kube-system -l k8s-app=aws-node

# Check VPC CNI logs
kubectl logs -n kube-system -l k8s-app=aws-node --tail=50

# Check current VPC CNI version
kubectl describe daemonset aws-node -n kube-system | grep Image
```

**è§£å†³ IP è€—å°½é—®é¢˜ï¼š**

```bash
# Check available IPs per subnet
aws ec2 describe-subnets --subnet-ids <subnet-id> \
  --query 'Subnets[].{ID:SubnetId,AZ:AvailabilityZone,Available:AvailableIpAddressCount}'

# Enable Prefix Delegation (16x IP capacity increase)
kubectl set env daemonset aws-node -n kube-system ENABLE_PREFIX_DELEGATION=true
```

**ENI é™åˆ¶å’Œ IP é…é¢ï¼š**

æ¯ç§ EC2 å®ä¾‹ç±»å‹éƒ½æœ‰å¯é™„åŠ çš„ ENI æ•°é‡é™åˆ¶ä»¥åŠæ¯ä¸ª ENI çš„ IP æ•°é‡é™åˆ¶ã€‚å¯ç”¨ Prefix Delegation å¯ä»¥æ˜¾è‘—å¢åŠ æ¯ä¸ª ENI çš„ IP åˆ†é…é‡ã€‚

### DNS æ•…éšœæ’æŸ¥

```bash
# Check CoreDNS Pod status
kubectl get pods -n kube-system -l k8s-app=kube-dns

# Check CoreDNS logs
kubectl logs -n kube-system -l k8s-app=kube-dns --tail=50

# Test DNS resolution
kubectl run -it --rm debug --image=busybox --restart=Never -- nslookup kubernetes.default

# Check CoreDNS configuration
kubectl get configmap coredns -n kube-system -o yaml

# Restart CoreDNS
kubectl rollout restart deployment coredns -n kube-system
```

:::warning ndots é—®é¢˜
åœ¨ Kubernetes é»˜è®¤çš„ `resolv.conf` é…ç½®ä¸­ï¼Œ`ndots:5` ä¼šå¯¼è‡´å¯¹å°‘äº 5 ä¸ªç‚¹çš„åŸŸåä¼˜å…ˆå°è¯•é›†ç¾¤å†…éƒ¨ DNS åç¼€ã€‚å½“è®¿é—®å¤–éƒ¨åŸŸåæ—¶ï¼Œè¿™ä¼šå¯¼è‡´ 4 æ¬¡é¢å¤–çš„ä¸å¿…è¦ DNS æŸ¥è¯¢ï¼Œå¢åŠ å»¶è¿Ÿã€‚

è§£å†³æ–¹æ¡ˆï¼šé€šè¿‡ Pod spec ä¸­çš„ `dnsConfig.options` è®¾ç½® `ndots:2`ï¼Œæˆ–åœ¨å¤–éƒ¨åŸŸåæœ«å°¾æ·»åŠ  `.`ï¼ˆä¾‹å¦‚ `api.example.com.`ï¼‰ã€‚

æ³¨æ„ï¼šVPC DNS é™æµé˜ˆå€¼ä¸º**æ¯ä¸ª ENI 1,024 åŒ…/ç§’**ã€‚
:::

### Service è°ƒè¯•

```bash
# Check Service status
kubectl get svc <service-name>

# Check Endpoints (whether backend Pods are connected)
kubectl get endpoints <service-name>

# Detailed Service information (verify selector)
kubectl describe svc <service-name>

# Check Selector
kubectl get svc <service-name> -o jsonpath='{.spec.selector}'

# Find Pods matching the Selector
kubectl get pods -l <key>=<value>
```

**å¸¸è§ Service é—®é¢˜ï¼š**

| ç—‡çŠ¶ | æ£€æŸ¥é¡¹ | è§£å†³æ–¹æ¡ˆ |
|------|--------|----------|
| Endpoints ä¸ºç©º | Service selector ä¸ Pod label ä¸åŒ¹é… | ä¿®å¤ labels |
| ClusterIP ä¸å¯è¾¾ | kube-proxy æ˜¯å¦æ­£å¸¸è¿è¡Œ | `kubectl logs -n kube-system -l k8s-app=kube-proxy` |
| NodePort ä¸å¯è¾¾ | Security Group æ˜¯å¦å…è®¸ 30000-32767 ç«¯å£ | æ·»åŠ  SG å…¥ç«™è§„åˆ™ |
| LoadBalancer å¤„äº Pending çŠ¶æ€ | æ˜¯å¦å®‰è£…äº† AWS Load Balancer Controller | å®‰è£… controller å¹¶éªŒè¯ IAM æƒé™ |

### NetworkPolicy è°ƒè¯•

NetworkPolicy ä¸­æœ€å¸¸è§çš„é”™è¯¯æ˜¯æ··æ·† **AND ä¸ OR é€‰æ‹©å™¨**ã€‚

```yaml
# AND logic (combining two selectors within the same from entry)
# Allow only "Pods with role client in the alice namespace"
- from:
  - namespaceSelector:
      matchLabels:
        user: alice
    podSelector:
      matchLabels:
        role: client

# OR logic (separating into distinct from entries)
# Allow "all Pods in the alice namespace" OR "Pods with role client in any namespace"
- from:
  - namespaceSelector:
      matchLabels:
        user: alice
  - podSelector:
      matchLabels:
        role: client
```

:::danger AND ä¸ OR æ³¨æ„äº‹é¡¹
ä¸Šè¿°ä¸¤ä¸ª YAML ç¤ºä¾‹ä»…ç›¸å·®ä¸€ä¸ªç¼©è¿›çº§åˆ«ï¼Œä½†ä¼šäº§ç”Ÿå®Œå…¨ä¸åŒçš„å®‰å…¨ç­–ç•¥ã€‚åœ¨ AND é€»è¾‘ä¸­ï¼Œ`namespaceSelector` å’Œ `podSelector` ä½äº**åŒä¸€ä¸ª `- from` æ¡ç›®**å†…ï¼Œè€Œåœ¨ OR é€»è¾‘ä¸­ï¼Œå®ƒä»¬æ˜¯**åˆ†å¼€çš„ `- from` æ¡ç›®**ã€‚
:::

### ä½¿ç”¨ netshoot

[netshoot](https://github.com/nicolaka/netshoot) æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç½‘ç»œè°ƒè¯•æ‰€éœ€å·¥å…·çš„å®¹å™¨é•œåƒã€‚

```bash
# Add as an ephemeral container to an existing Pod
kubectl debug <pod-name> -it --image=nicolaka/netshoot

# Run a standalone debugging Pod
kubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot

# Available tools inside include:
# - curl, wget: HTTP testing
# - dig, nslookup: DNS testing
# - tcpdump: Packet capture
# - iperf3: Bandwidth testing
# - ss, netstat: Socket status inspection
# - traceroute, mtr: Route tracing
```

**å®é™…è°ƒè¯•åœºæ™¯ï¼šéªŒè¯ Pod é—´é€šä¿¡**

```bash
# Test connectivity to another Service from a netshoot Pod
kubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- bash

# Verify DNS resolution
dig <service-name>.<namespace>.svc.cluster.local

# Test TCP connectivity
curl -v http://<service-name>.<namespace>.svc.cluster.local:<port>/health

# Capture packets (traffic to a specific Pod IP)
tcpdump -i any host <pod-ip> -n
```

---

## 7. å­˜å‚¨è°ƒè¯•

### å­˜å‚¨è°ƒè¯•å†³ç­–æ ‘

```mermaid
flowchart TD
    STOR_ISSUE["`**Storage Issue Detected**`"] --> PVC_STATUS{"`Check PVC status
    kubectl get pvc`"}

    PVC_STATUS -->|Pending| PVC_PENDING{"`StorageClass
    exists?`"}
    PVC_PENDING -->|No| SC_CREATE["`Create StorageClass
    or fix the name`"]
    PVC_PENDING -->|Yes| PROVISION{"`Provisioning
    failure cause`"}
    PROVISION -->|IAM permissions| IAM_FIX["`Check EBS CSI Driver
    IRSA permissions`"]
    PROVISION -->|AZ mismatch| AZ_FIX["`Use WaitForFirstConsumer
    volumeBindingMode`"]

    PVC_STATUS -->|Bound| MOUNT_ISSUE{"`Can Pod
    mount it?`"}
    MOUNT_ISSUE -->|Attach failure| ATTACH_FIX["`Attached to another node
    â†’ Delete previous Pod
    Wait for volume detach (~6 min)`"]
    MOUNT_ISSUE -->|Mount failure| MOUNT_FIX["`Check filesystem
    Security Group (EFS)
    Mount target (EFS)`"]

    PVC_STATUS -->|Terminating| FINALIZER_FIX["`Check Finalizers
    Inspect PV reclaimPolicy
    Manually remove finalizer if needed`"]

    style STOR_ISSUE fill:#ff4444,stroke:#cc3636,color:#fff
    style SC_CREATE fill:#34a853,stroke:#2a8642,color:#fff
    style IAM_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style AZ_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style ATTACH_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style MOUNT_FIX fill:#34a853,stroke:#2a8642,color:#fff
    style FINALIZER_FIX fill:#34a853,stroke:#2a8642,color:#fff
```

### EBS CSI Driver è°ƒè¯•

```bash
# Check EBS CSI Driver Pod status
kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver

# Check Controller logs
kubectl logs -n kube-system -l app=ebs-csi-controller -c ebs-plugin --tail=100

# Check Node logs
kubectl logs -n kube-system -l app=ebs-csi-node -c ebs-plugin --tail=100

# Verify IRSA ServiceAccount
kubectl describe sa ebs-csi-controller-sa -n kube-system
```

**EBS CSI Driver é”™è¯¯æ¨¡å¼ï¼š**

| é”™è¯¯ä¿¡æ¯ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|----------|------|----------|
| `could not create volume` | IAM æƒé™ä¸è¶³ | å‘ IRSA Role æ·»åŠ  `ec2:CreateVolume`ã€`ec2:AttachVolume` ç­‰æƒé™ |
| `volume is already attached to another node` | æœªä»å‰ä¸€ä¸ªèŠ‚ç‚¹åˆ†ç¦» | æ¸…ç†ä¹‹å‰çš„ Pod/èŠ‚ç‚¹ï¼Œç­‰å¾… EBS å·åˆ†ç¦»ï¼ˆçº¦ 6 åˆ†é’Ÿï¼‰ |
| `could not attach volume: already at max` | å®ä¾‹ EBS å·æ•°é‡ä¸Šé™å·²è¾¾åˆ° | ä½¿ç”¨æ›´å¤§çš„å®ä¾‹ç±»å‹ï¼ˆNitro å®ä¾‹ï¼šæœ€å¤š 128 ä¸ªå·ï¼‰ |
| `failed to provision volume with StorageClass` | StorageClass ä¸å­˜åœ¨æˆ–é…ç½®é”™è¯¯ | éªŒè¯ StorageClass åç§°å’Œå‚æ•° |

**æ¨è StorageClass é…ç½®ï¼š**

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: topology-aware-ebs
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  encrypted: "true"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
```

:::tip WaitForFirstConsumer
ä½¿ç”¨ `volumeBindingMode: WaitForFirstConsumer` å¯ä»¥å°† PVC ç»‘å®šå»¶è¿Ÿåˆ° Pod è°ƒåº¦æ—¶è¿›è¡Œã€‚è¿™ç¡®ä¿äº†**å·åœ¨ Pod è¢«è°ƒåº¦åˆ°çš„åŒä¸€å¯ç”¨åŒºä¸­åˆ›å»º**ï¼Œä»è€Œé¿å…å¯ç”¨åŒºä¸åŒ¹é…é—®é¢˜ã€‚
:::

### EFS CSI Driver è°ƒè¯•

```bash
# Check EFS CSI Driver Pod status
kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-efs-csi-driver

# Check Controller logs
kubectl logs -n kube-system -l app=efs-csi-controller -c efs-plugin --tail=100

# Check EFS filesystem status
aws efs describe-file-systems --file-system-id <fs-id>

# Verify Mount Targets (must exist in each AZ)
aws efs describe-mount-targets --file-system-id <fs-id>
```

**EFS æ£€æŸ¥æ¸…å•ï¼š**

- éªŒè¯ Pod è¿è¡Œçš„æ‰€æœ‰å¯ç”¨åŒºå­ç½‘ä¸­æ˜¯å¦å­˜åœ¨ Mount Target
- éªŒè¯ Mount Target çš„ Security Group æ˜¯å¦å…è®¸ **TCP 2049 (NFS)** ç«¯å£
- éªŒè¯èŠ‚ç‚¹çš„ Security Group æ˜¯å¦å…è®¸åˆ° EFS Mount Target çš„ TCP 2049 å‡ºç«™æµé‡

### PV/PVC çŠ¶æ€æ£€æŸ¥ä¸å¡ä½é—®é¢˜è§£å†³

```bash
# Check PVC status
kubectl get pvc -n <namespace>

# Check PV status
kubectl get pv

# If PVC is stuck in Terminating (remove finalizer)
kubectl patch pvc <pvc-name> -n <namespace> -p '{"metadata":{"finalizers":null}}'

# Change PV from Released to Available (for reuse)
kubectl patch pv <pv-name> -p '{"spec":{"claimRef":null}}'
```

:::danger æ‰‹åŠ¨ç§»é™¤ Finalizer è­¦å‘Š
æ‰‹åŠ¨ç§»é™¤ finalizer å¯èƒ½å¯¼è‡´å…³è”çš„å­˜å‚¨èµ„æºï¼ˆå¦‚ EBS å·ï¼‰æœªè¢«æ¸…ç†ã€‚è¯·å…ˆç¡®è®¤å·æœªåœ¨ä½¿ç”¨ä¸­ï¼Œå¹¶æ£€æŸ¥ AWS æ§åˆ¶å°ä»¥ç¡®ä¿ä¸ä¼šäº§ç”Ÿå­¤ç«‹å·ã€‚
:::

---

## 8. å¯è§‚æµ‹æ€§ä¸ç›‘æ§

### å¯è§‚æµ‹æ€§æŠ€æœ¯æ ˆæ¶æ„

```mermaid
flowchart TB
    subgraph "Data Sources"
        APPS["`Applications
        (Metrics / Logs / Traces)`"]
        K8S["`Kubernetes
        (Events / Metrics)`"]
        NODES["`Nodes
        (System Metrics)`"]
    end

    subgraph "Collection Layer"
        ADOT["`ADOT Collector
        (OpenTelemetry)`"]
        CWA["`CloudWatch Agent
        (Container Insights)`"]
        PROM["`Prometheus
        (kube-state-metrics)`"]
    end

    subgraph "Storage and Analysis"
        CW["`CloudWatch
        Logs & Metrics`"]
        AMP["`Amazon Managed
        Prometheus`"]
        GRAF["`Grafana
        (Dashboards)`"]
    end

    subgraph "Alerting"
        ALARM["`CloudWatch Alarms`"]
        AM["`Alertmanager`"]
        SNS["`SNS / PagerDuty
        / Slack`"]
    end

    APPS --> ADOT
    APPS --> CWA
    K8S --> PROM
    NODES --> CWA

    ADOT --> CW
    ADOT --> AMP
    CWA --> CW
    PROM --> AMP

    AMP --> GRAF
    CW --> GRAF

    CW --> ALARM
    AMP --> AM
    ALARM --> SNS
    AM --> SNS

    style ADOT fill:#ff9900,stroke:#cc7a00,color:#fff
    style CW fill:#ff9900,stroke:#cc7a00,color:#fff
    style AMP fill:#ff9900,stroke:#cc7a00,color:#fff
    style PROM fill:#4286f4,stroke:#2a6acf,color:#fff
    style GRAF fill:#34a853,stroke:#2a8642,color:#fff
```

### Container Insights è®¾ç½®

```bash
# Install Container Insights Add-on
aws eks create-addon \
  --cluster-name <cluster-name> \
  --addon-name amazon-cloudwatch-observability

# Verify installation
kubectl get pods -n amazon-cloudwatch
```

### æŒ‡æ ‡è°ƒè¯•ï¼šPromQL æŸ¥è¯¢

#### CPU é™æµæ£€æµ‹

```promql
sum(rate(container_cpu_cfs_throttled_periods_total{namespace="production"}[5m]))
/ sum(rate(container_cpu_cfs_periods_total{namespace="production"}[5m])) > 0.25
```

:::info CPU é™æµé˜ˆå€¼
é™æµè¶…è¿‡ 25% ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¯·è€ƒè™‘ç§»é™¤æˆ–å¢åŠ  CPU limitsã€‚è®¸å¤šç»„ç»‡é‡‡ç”¨ä»…è®¾ç½® CPU requests è€Œä¸è®¾ç½® CPU limits çš„ç­–ç•¥ã€‚
:::

#### OOMKilled æ£€æµ‹

```promql
kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} > 0
```

#### Pod é‡å¯ç‡

```promql
sum(rate(kube_pod_container_status_restarts_total[15m])) by (namespace, pod) > 0
```

#### èŠ‚ç‚¹ CPU ä½¿ç”¨ç‡ï¼ˆè¶…è¿‡ 80% å‘Šè­¦ï¼‰

```promql
100 - (avg by(instance)(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
```

#### èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡ï¼ˆè¶…è¿‡ 85% å‘Šè­¦ï¼‰

```promql
(1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 85
```

### æ—¥å¿—è°ƒè¯•ï¼šCloudWatch Logs Insights

#### é”™è¯¯æ—¥å¿—åˆ†æ

```sql
fields @timestamp, @message, kubernetes.container_name, kubernetes.pod_name
| filter @message like /ERROR|FATAL|Exception/
| sort @timestamp desc
| limit 50
```

#### å»¶è¿Ÿåˆ†æ

```sql
fields @timestamp, @message
| filter @message like /latency|duration|elapsed/
| parse @message /latency[=:]\s*(?<latency_ms>\d+)/
| stats avg(latency_ms), max(latency_ms), p99(latency_ms) by bin(5m)
```

#### ç‰¹å®š Pod çš„é”™è¯¯æ¨¡å¼åˆ†æ

```sql
fields @timestamp, @message
| filter kubernetes.pod_name like /api-server/
| filter @message like /error|Error|ERROR/
| stats count() by bin(1m)
| sort bin asc
```

#### OOMKilled äº‹ä»¶è·Ÿè¸ª

```sql
fields @timestamp, @message
| filter @message like /OOMKilled|oom-kill|Out of memory/
| sort @timestamp desc
| limit 20
```

#### å®¹å™¨é‡å¯äº‹ä»¶

```sql
fields @timestamp, @message, kubernetes.pod_name
| filter @message like /Back-off restarting failed container|CrashLoopBackOff/
| stats count() by kubernetes.pod_name
| sort count desc
```

### å‘Šè­¦è§„åˆ™ï¼šPrometheusRule ç¤ºä¾‹

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubernetes-alerts
spec:
  groups:
  - name: kubernetes-pods
    rules:
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 0
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
        description: "Pod {{ $labels.pod }} has been restarting over the last 15 minutes."

    - alert: PodOOMKilled
      expr: kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} OOMKilled"
        description: "Pod {{ $labels.pod }} was terminated due to out of memory. Memory limits adjustment is required."

  - name: kubernetes-nodes
    rules:
    - alert: NodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.node }} is NotReady"

    - alert: NodeHighCPU
      expr: 100 - (avg by(instance)(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.instance }} CPU usage above 80%"

    - alert: NodeHighMemory
      expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 85
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.instance }} memory usage above 85%"
```

### ADOT (AWS Distro for OpenTelemetry) è°ƒè¯•

ADOT æ˜¯ AWS æ‰˜ç®¡çš„ OpenTelemetry å‘è¡Œç‰ˆï¼Œç”¨äºæ”¶é›† tracesã€metrics å’Œ logs å¹¶å°†å…¶å‘é€åˆ°å„ç§ AWS æœåŠ¡ï¼ˆX-Rayã€CloudWatchã€AMP ç­‰ï¼‰ã€‚

```bash
# Check ADOT Add-on status
aws eks describe-addon --cluster-name $CLUSTER \
  --addon-name adot --query 'addon.{status:status,version:addonVersion}'

# Check ADOT Collector Pods
kubectl get pods -n opentelemetry-operator-system
kubectl logs -n opentelemetry-operator-system -l app.kubernetes.io/name=opentelemetry-operator --tail=50

# Check OpenTelemetryCollector CR
kubectl get otelcol -A
kubectl describe otelcol -n $NAMESPACE $COLLECTOR_NAME
```

**å¸¸è§ ADOT é—®é¢˜ï¼š**

| ç—‡çŠ¶ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| Operator Pod `CrashLoopBackOff` | æœªå®‰è£… CertManager | ADOT operator webhook è¯ä¹¦ç®¡ç†éœ€è¦ CertManagerã€‚`kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml` |
| Collector æ— æ³•å‘é€åˆ° AMP | IAM æƒé™ä¸è¶³ | å‘ IRSA/Pod Identity æ·»åŠ  `aps:RemoteWrite` æƒé™ |
| X-Ray traces æœªæ¥æ”¶åˆ° | IAM æƒé™ä¸è¶³ | å‘ IRSA/Pod Identity æ·»åŠ  `xray:PutTraceSegments`ã€`xray:PutTelemetryRecords` æƒé™ |
| CloudWatch metrics æœªæ¥æ”¶åˆ° | IAM æƒé™ä¸è¶³ | å‘ IRSA/Pod Identity æ·»åŠ  `cloudwatch:PutMetricData` æƒé™ |
| Collector Pod `OOMKilled` | èµ„æºä¸è¶³ | åœ¨æ”¶é›†å¤§é‡ traces/metrics æ—¶å¢åŠ  Collector çš„ resources.limits.memory |

:::warning ADOT æƒé™åˆ†ç¦»
AMP remote writeã€X-Ray å’Œ CloudWatch å„éœ€è¦ä¸åŒçš„ IAM æƒé™ã€‚å¦‚æœ Collector å‘å¤šä¸ªåç«¯å‘é€æ•°æ®ï¼Œè¯·éªŒè¯ IAM Role ä¸­åŒ…å«æ‰€æœ‰å¿…éœ€çš„æƒé™ã€‚
:::

---

## 9. äº‹ä»¶æ£€æµ‹æœºåˆ¶ä¸æ—¥å¿—æ¶æ„

### 9.1 äº‹ä»¶æ£€æµ‹ç­–ç•¥æ¦‚è¿°

è¦åœ¨ EKS ç¯å¢ƒä¸­å¿«é€Ÿæ£€æµ‹äº‹ä»¶ï¼Œå¿…é¡»ç³»ç»Ÿæ€§åœ°æ„å»º 4 å±‚ç®¡é“ï¼š**æ•°æ®æº -> æ”¶é›† -> åˆ†æä¸æ£€æµ‹ -> å‘Šè­¦ä¸å“åº”**ã€‚å„å±‚ä¹‹é—´å¿…é¡»æœ‰æœºè¿æ¥ï¼Œä»¥æœ€å°åŒ– MTTDï¼ˆå¹³å‡æ£€æµ‹æ—¶é—´ï¼‰ã€‚

```mermaid
flowchart TB
    subgraph Sources["Data Sources"]
        CP["`**Control Plane Logs**
        API Server, Audit,
        Authenticator`"]
        DP["`**Data Plane Logs**
        kubelet, containerd,
        Application`"]
        MT["`**Metrics**
        Prometheus, CloudWatch,
        Custom Metrics`"]
        TR["`**Traces**
        X-Ray, ADOT,
        Jaeger`"]
    end

    subgraph Collection["Collection Layer"]
        FB["`**Fluent Bit**
        DaemonSet`"]
        CWA["`**CloudWatch Agent**
        Container Insights`"]
        ADOT["`**ADOT Collector**
        OpenTelemetry`"]
    end

    subgraph Analysis["Analysis & Detection"]
        CWL["`**CloudWatch Logs**
        Logs Insights`"]
        AMP["`**Amazon Managed
        Prometheus**`"]
        OS["`**OpenSearch**
        Log Analytics`"]
        CWAD["`**CloudWatch
        Anomaly Detection**`"]
    end

    subgraph Alert["Alerting & Response"]
        CWA2["`**CloudWatch Alarms**
        Composite Alarms`"]
        AM["`**Alertmanager**
        Routing & Silencing`"]
        SNS["`**SNS â†’ Lambda**
        Auto-remediation`"]
        PD["`**PagerDuty / Slack**
        On-call Notification`"]
    end

    CP --> FB
    DP --> FB
    MT --> CWA
    MT --> ADOT
    TR --> ADOT
    FB --> CWL
    FB --> OS
    CWA --> CWL
    ADOT --> AMP
    ADOT --> CWL
    CWL --> CWAD
    AMP --> AM
    CWAD --> CWA2
    CWA2 --> SNS
    AM --> PD
    SNS --> PD

    style CP fill:#4286f4,stroke:#2a6acf,color:#fff
    style DP fill:#4286f4,stroke:#2a6acf,color:#fff
    style MT fill:#34a853,stroke:#2a8642,color:#fff
    style TR fill:#34a853,stroke:#2a8642,color:#fff
    style FB fill:#ff9900,stroke:#cc7a00,color:#fff
    style CWA fill:#ff9900,stroke:#cc7a00,color:#fff
    style ADOT fill:#ff9900,stroke:#cc7a00,color:#fff
    style CWL fill:#4286f4,stroke:#2a6acf,color:#fff
    style AMP fill:#4286f4,stroke:#2a6acf,color:#fff
    style OS fill:#4286f4,stroke:#2a6acf,color:#fff
    style CWAD fill:#fbbc04,stroke:#c99603,color:#000
    style CWA2 fill:#ff4444,stroke:#cc3636,color:#fff
    style AM fill:#ff4444,stroke:#cc3636,color:#fff
    style SNS fill:#ff4444,stroke:#cc3636,color:#fff
    style PD fill:#ff4444,stroke:#cc3636,color:#fff
```

**4 å±‚æ¶æ„è¯´æ˜ï¼š**

| å±‚çº§ | èŒè´£ | å…³é”®ç»„ä»¶ |
|------|------|----------|
| **æ•°æ®æº** | ä»é›†ç¾¤ç”Ÿæˆæ‰€æœ‰å¯è§‚æµ‹ä¿¡å· | Control Plane Logsã€Data Plane Logsã€Metricsã€Traces |
| **æ”¶é›†å±‚** | æ ‡å‡†åŒ–å¹¶è½¬å‘æ¥è‡ªå„ç§æ¥æºçš„æ•°æ®åˆ°ä¸­å¿ƒä½ç½® | Fluent Bitã€CloudWatch Agentã€ADOT Collector |
| **åˆ†æä¸æ£€æµ‹** | åˆ†ææ”¶é›†çš„æ•°æ®å¹¶æ£€æµ‹å¼‚å¸¸ | CloudWatch Logs Insightsã€AMPã€OpenSearchã€Anomaly Detection |
| **å‘Šè­¦ä¸å“åº”** | é€šè¿‡é€‚å½“çš„æ¸ é“é€šçŸ¥æ£€æµ‹åˆ°çš„äº‹ä»¶å¹¶æ‰§è¡Œè‡ªåŠ¨ä¿®å¤ | CloudWatch Alarmsã€Alertmanagerã€SNS -> Lambdaã€PagerDuty/Slack |

### 9.2 æ¨èæ—¥å¿—æ¶æ„

#### æ–¹æ¡ˆ Aï¼šAWS åŸç”ŸæŠ€æœ¯æ ˆï¼ˆä¸­å°å‹é›†ç¾¤ï¼‰

ä»¥ AWS æ‰˜ç®¡æœåŠ¡ä¸ºæ ¸å¿ƒçš„æ¶æ„ï¼Œæœ€å¤§é™åº¦å‡å°‘è¿ç»´å¼€é”€ã€‚

| å±‚çº§ | ç»„ä»¶ | ç”¨é€” |
|------|------|------|
| æ”¶é›† | Fluent Bit (DaemonSet) | èŠ‚ç‚¹/å®¹å™¨æ—¥å¿—æ”¶é›† |
| ä¼ è¾“ | CloudWatch Logs | ä¸­å¿ƒæ—¥å¿—å­˜å‚¨ |
| åˆ†æ | CloudWatch Logs Insights | åŸºäºæŸ¥è¯¢çš„åˆ†æ |
| æ£€æµ‹ | CloudWatch Anomaly Detection | åŸºäº ML çš„å¼‚å¸¸æ£€æµ‹ |
| å‘Šè­¦ | CloudWatch Alarms -> SNS | åŸºäºé˜ˆå€¼/å¼‚å¸¸çš„å‘Šè­¦ |

**Fluent Bit DaemonSet éƒ¨ç½²ç¤ºä¾‹ï¼š**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: amazon-cloudwatch
  labels:
    app.kubernetes.io/name: fluent-bit
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: fluent-bit
  template:
    metadata:
      labels:
        app.kubernetes.io/name: fluent-bit
    spec:
      serviceAccountName: fluent-bit
      containers:
        - name: fluent-bit
          image: public.ecr.aws/aws-observability/aws-for-fluent-bit:2.32.0
          resources:
            limits:
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
          volumeMounts:
            - name: varlog
              mountPath: /var/log
              readOnly: true
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
            - name: fluent-bit-config
              mountPath: /fluent-bit/etc/
      volumes:
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: fluent-bit-config
          configMap:
            name: fluent-bit-config
```

:::tip Fluent Bit ä¸ Fluentd å¯¹æ¯”
Fluent Bit çš„å†…å­˜ä½¿ç”¨é‡æ¯” Fluentd å°‘ 10 å€ä»¥ä¸Šï¼ˆçº¦ 10MB vs çº¦ 100MBï¼‰ã€‚åœ¨ EKS ç¯å¢ƒä¸­ï¼Œå°† Fluent Bit éƒ¨ç½²ä¸º DaemonSet æ˜¯æ ‡å‡†æ¨¡å¼ã€‚ä½¿ç”¨ `amazon-cloudwatch-observability` Add-on ä¼šè‡ªåŠ¨å®‰è£… Fluent Bitã€‚
:::

#### æ–¹æ¡ˆ Bï¼šå¼€æºæŠ€æœ¯æ ˆï¼ˆå¤§è§„æ¨¡/å¤šé›†ç¾¤ï¼‰

ç»“åˆå¼€æºå·¥å…·ä¸ AWS æ‰˜ç®¡æœåŠ¡çš„æ¶æ„ï¼Œåœ¨å¤§è§„æ¨¡ç¯å¢ƒä¸­ç¡®ä¿å¯æ‰©å±•æ€§å’Œçµæ´»æ€§ã€‚

| å±‚çº§ | ç»„ä»¶ | ç”¨é€” |
|------|------|------|
| æ”¶é›† | Fluent Bit + ADOT Collector | ç»Ÿä¸€çš„æ—¥å¿—/æŒ‡æ ‡/è¿½è¸ªæ”¶é›† |
| æŒ‡æ ‡ | Amazon Managed Prometheus (AMP) | æ—¶åºæŒ‡æ ‡å­˜å‚¨ |
| æ—¥å¿— | Amazon OpenSearch Service | å¤§è§„æ¨¡æ—¥å¿—åˆ†æ |
| è¿½è¸ª | AWS X-Ray / Jaeger | åˆ†å¸ƒå¼è¿½è¸ª |
| å¯è§†åŒ– | Amazon Managed Grafana | ç»Ÿä¸€ä»ªè¡¨æ¿ |
| å‘Šè­¦ | Alertmanager + PagerDuty/Slack | é«˜çº§è·¯ç”±ã€åˆ†ç»„ã€é™é»˜ |

:::info å¤šé›†ç¾¤æ¶æ„
åœ¨å¤šé›†ç¾¤ç¯å¢ƒä¸­ï¼Œæ¨èé‡‡ç”¨ hub-and-spoke æ¶æ„ï¼Œæ¯ä¸ªé›†ç¾¤ä¸­çš„ ADOT Collector å°†æŒ‡æ ‡å‘é€åˆ°ä¸­å¿ƒ AMP å·¥ä½œåŒºã€‚Grafana å¯ä»¥ä»å•ä¸ªä»ªè¡¨æ¿ç›‘æ§æ‰€æœ‰é›†ç¾¤ã€‚
:::

### 9.3 äº‹ä»¶æ£€æµ‹æ¨¡å¼

#### æ¨¡å¼ 1ï¼šåŸºäºé˜ˆå€¼çš„æ£€æµ‹

æœ€åŸºç¡€çš„æ£€æµ‹æ–¹æ³•ã€‚å½“è¶…è¿‡é¢„å®šä¹‰çš„é˜ˆå€¼æ—¶è§¦å‘å‘Šè­¦ã€‚

```yaml
# PrometheusRule - Threshold-based alert example
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: eks-threshold-alerts
  namespace: monitoring
spec:
  groups:
    - name: eks-thresholds
      rules:
        - alert: HighPodRestartRate
          expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} restart count increasing"
            description: "{{ $value }} restarts detected within 1 hour"

        - alert: NodeMemoryPressure
          expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) > 0.85
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Node {{ $labels.instance }} memory usage above 85%"

        - alert: PVCNearlyFull
          expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.9
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "PVC {{ $labels.persistentvolumeclaim }} capacity above 90%"
```

#### æ¨¡å¼ 2ï¼šå¼‚å¸¸æ£€æµ‹

ä½¿ç”¨ ML å­¦ä¹ æ­£å¸¸æ¨¡å¼å¹¶æ£€æµ‹åå·®ã€‚åœ¨éš¾ä»¥é¢„å®šä¹‰é˜ˆå€¼æ—¶éå¸¸æœ‰ç”¨ã€‚

```bash
# CloudWatch Anomaly Detection setup
aws cloudwatch put-anomaly-detector \
  --single-metric-anomaly-detector '{
    "Namespace": "ContainerInsights",
    "MetricName": "pod_cpu_utilization",
    "Dimensions": [
      {"Name": "ClusterName", "Value": "'$CLUSTER'"},
      {"Name": "Namespace", "Value": "production"}
    ],
    "Stat": "Average"
  }'

# Create alarm based on Anomaly Detection
aws cloudwatch put-metric-alarm \
  --alarm-name "eks-cpu-anomaly" \
  --alarm-description "EKS CPU utilization anomaly detected" \
  --evaluation-periods 3 \
  --comparison-operator LessThanLowerOrGreaterThanUpperThreshold \
  --threshold-metric-id ad1 \
  --metrics '[
    {
      "Id": "m1",
      "MetricStat": {
        "Metric": {
          "Namespace": "ContainerInsights",
          "MetricName": "pod_cpu_utilization",
          "Dimensions": [
            {"Name": "ClusterName", "Value": "'$CLUSTER'"}
          ]
        },
        "Period": 300,
        "Stat": "Average"
      }
    },
    {
      "Id": "ad1",
      "Expression": "ANOMALY_DETECTION_BAND(m1, 2)"
    }
  ]' \
  --alarm-actions $SNS_TOPIC_ARN
```

:::warning å¼‚å¸¸æ£€æµ‹å­¦ä¹ æœŸ
å¼‚å¸¸æ£€æµ‹éœ€è¦è‡³å°‘ 2 å‘¨çš„å­¦ä¹ æœŸã€‚åœ¨éƒ¨ç½²æ–°æœåŠ¡åï¼Œè¯·åŒæ—¶ä½¿ç”¨åŸºäºé˜ˆå€¼çš„å‘Šè­¦ä½œä¸ºå¹¶è¡Œæ‰‹æ®µã€‚
:::

#### æ¨¡å¼ 3ï¼šå¤åˆå‘Šè­¦

é€»è¾‘ç»„åˆå¤šä¸ªç‹¬ç«‹å‘Šè­¦ï¼Œä»¥å‡å°‘å™ªå£°å¹¶å‡†ç¡®æ£€æµ‹å®é™…äº‹ä»¶ã€‚

```bash
# Combine individual alarms with AND/OR
aws cloudwatch put-composite-alarm \
  --alarm-name "eks-service-degradation" \
  --alarm-rule 'ALARM("high-error-rate") AND (ALARM("high-latency") OR ALARM("pod-restart-spike"))' \
  --alarm-actions $SNS_TOPIC_ARN \
  --alarm-description "Service degradation detected: error rate increase + latency increase or Pod restart spike"
```

:::tip å¤åˆå‘Šè­¦æŠ€å·§
å•ç‹¬çš„å‘Šè­¦å¾€å¾€ä¼šäº§ç”Ÿå¤§é‡è¯¯æŠ¥ï¼ˆFalse Positiveï¼‰ã€‚é€šè¿‡å¤åˆå‘Šè­¦ç»„åˆå¤šä¸ªä¿¡å·å¯ä»¥å‡†ç¡®æ£€æµ‹çœŸå®äº‹ä»¶ã€‚ä¾‹å¦‚ï¼š"é”™è¯¯ç‡ä¸Šå‡ AND å»¶è¿Ÿå¢åŠ "è¡¨ç¤ºæœåŠ¡ä¸­æ–­ï¼Œè€Œ"é”™è¯¯ç‡ä¸Šå‡ AND Pod é‡å¯"è¡¨ç¤ºåº”ç”¨å´©æºƒã€‚
:::

#### æ¨¡å¼ 4ï¼šåŸºäºæ—¥å¿—çš„æŒ‡æ ‡è¿‡æ»¤å™¨

æ£€æµ‹ CloudWatch Logs ä¸­çš„ç‰¹å®šæ¨¡å¼ï¼Œå°†å…¶è½¬æ¢ä¸ºæŒ‡æ ‡å¹¶è®¾ç½®å‘Šè­¦ã€‚

```bash
# Convert OOMKilled events to metrics
aws logs put-metric-filter \
  --log-group-name "/aws/eks/$CLUSTER/cluster" \
  --filter-name "OOMKilledEvents" \
  --filter-pattern '{ $.reason = "OOMKilled" || $.reason = "OOMKilling" }' \
  --metric-transformations \
    metricName=OOMKilledCount,metricNamespace=EKS/Custom,metricValue=1,defaultValue=0

# Detect 403 Forbidden events (security threat)
aws logs put-metric-filter \
  --log-group-name "/aws/eks/$CLUSTER/cluster" \
  --filter-name "UnauthorizedAccess" \
  --filter-pattern '{ $.responseStatus.code = 403 }' \
  --metric-transformations \
    metricName=ForbiddenAccessCount,metricNamespace=EKS/Security,metricValue=1,defaultValue=0
```

### 9.4 äº‹ä»¶æ£€æµ‹æˆç†Ÿåº¦æ¨¡å‹

å°†ç»„ç»‡çš„äº‹ä»¶æ£€æµ‹èƒ½åŠ›åˆ†ä¸º 4 ä¸ªçº§åˆ«ï¼Œä»¥è¯Šæ–­å½“å‰çŠ¶æ€å¹¶æä¾›å‘ä¸‹ä¸€çº§åˆ«æˆé•¿çš„è·¯çº¿å›¾ã€‚

| çº§åˆ« | é˜¶æ®µ | æ£€æµ‹æ–¹æ³• | å·¥å…· | ç›®æ ‡ MTTD |
|------|------|----------|------|-----------|
| Level 1 | åŸºç¡€ | äººå·¥ç›‘æ§ + åŸºç¡€å‘Šè­¦ | CloudWatch Alarms | &lt; 30 åˆ†é’Ÿ |
| Level 2 | æ ‡å‡† | é˜ˆå€¼ + æ—¥å¿—æŒ‡æ ‡è¿‡æ»¤å™¨ | CloudWatch + Prometheus | &lt; 10 åˆ†é’Ÿ |
| Level 3 | é«˜çº§ | å¼‚å¸¸æ£€æµ‹ + å¤åˆå‘Šè­¦ | Anomaly Detection + AMP | &lt; 5 åˆ†é’Ÿ |
| Level 4 | è‡ªåŠ¨åŒ– | è‡ªåŠ¨æ£€æµ‹ + è‡ªåŠ¨ä¿®å¤ | Lambda + EventBridge + FIS | &lt; 1 åˆ†é’Ÿ |

:::info MTTDï¼ˆå¹³å‡æ£€æµ‹æ—¶é—´ï¼‰
ä»äº‹ä»¶å‘ç”Ÿåˆ°è¢«æ£€æµ‹åˆ°çš„å¹³å‡æ—¶é—´ã€‚ç›®æ ‡æ˜¯åœ¨ä» Level 1 å‘ Level 4 æˆé•¿çš„è¿‡ç¨‹ä¸­æŒç»­é™ä½ MTTDã€‚è¯·æ ¹æ®ç»„ç»‡çš„ SLO é€‰æ‹©åˆé€‚çš„çº§åˆ«ã€‚
:::

### 9.5 è‡ªåŠ¨ä¿®å¤æ¨¡å¼

å½“æ£€æµ‹åˆ°ç‰¹å®šäº‹ä»¶æ—¶ï¼Œä½¿ç”¨ EventBridge å’Œ Lambda è‡ªåŠ¨æ‰§è¡Œæ¢å¤æ“ä½œçš„æ¨¡å¼ã€‚

```bash
# EventBridge rule: Detect Pod OOMKilled â†’ trigger Lambda
aws events put-rule \
  --name "eks-oom-auto-remediation" \
  --event-pattern '{
    "source": ["aws.cloudwatch"],
    "detail-type": ["CloudWatch Alarm State Change"],
    "detail": {
      "alarmName": ["eks-oom-killed-alarm"],
      "state": {"value": ["ALARM"]}
    }
  }'
```

:::danger è‡ªåŠ¨ä¿®å¤æ³¨æ„äº‹é¡¹
ä»…åœ¨å……åˆ†æµ‹è¯•åæ‰å°†è‡ªåŠ¨ä¿®å¤åº”ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚é”™è¯¯çš„è‡ªåŠ¨ä¿®å¤é€»è¾‘å¯èƒ½ä¼šä½¿äº‹ä»¶æ¶åŒ–ã€‚é¦–å…ˆåœ¨ `DRY_RUN` æ¨¡å¼ä¸‹éªŒè¯æ¢å¤é€»è¾‘ï¼ˆä»…æ¥æ”¶é€šçŸ¥ï¼‰ï¼Œç„¶åé€æ­¥æ‰©å¤§è‡ªåŠ¨åŒ–èŒƒå›´ã€‚
:::

### 9.6 æ¨èå‘Šè­¦æ¸ é“çŸ©é˜µ

æ ¹æ®äº‹ä»¶ä¸¥é‡ç¨‹åº¦è®¾ç½®é€‚å½“çš„å‘Šè­¦æ¸ é“å’Œå“åº” SLAï¼Œä»¥é˜²æ­¢å‘Šè­¦ç–²åŠ³ï¼ˆAlert Fatigueï¼‰å¹¶ä¸“æ³¨äºå…³é”®äº‹ä»¶ã€‚

| ä¸¥é‡ç¨‹åº¦ | å‘Šè­¦æ¸ é“ | å“åº” SLA | ç¤ºä¾‹ |
|----------|----------|----------|------|
| P1ï¼ˆä¸¥é‡ï¼‰ | PagerDuty + ç”µè¯å‘¼å« | 15 åˆ†é’Ÿå†… | æœåŠ¡å®Œå…¨ä¸­æ–­ã€æ•°æ®ä¸¢å¤±é£é™© |
| P2ï¼ˆé«˜ï¼‰ | Slack DM + PagerDuty | 30 åˆ†é’Ÿå†… | éƒ¨åˆ†æœåŠ¡ä¸­æ–­ã€ä¸¥é‡æ€§èƒ½ä¸‹é™ |
| P3ï¼ˆä¸­ï¼‰ | Slack é¢‘é“ | 4 å°æ—¶å†… | Pod é‡å¯å¢åŠ ã€èµ„æºä½¿ç”¨ç‡å‘Šè­¦ |
| P4ï¼ˆä½ï¼‰ | é‚®ä»¶ / Jira å·¥å• | ä¸‹ä¸€ä¸ªå·¥ä½œæ—¥ | ç£ç›˜ä½¿ç”¨é‡å¢é•¿ã€è¯ä¹¦å³å°†åˆ°æœŸ |

:::warning å‘Šè­¦ç–²åŠ³æ³¨æ„äº‹é¡¹
è¿‡å¤šçš„å‘Šè­¦ä¼šå¯¼è‡´è¿ç»´å›¢é˜Ÿå¿½ç•¥å‘Šè­¦ï¼ˆAlert Fatigueï¼‰ã€‚P3/P4 å‘Šè­¦ä»…å‘é€åˆ° Slack é¢‘é“ï¼Œåªæœ‰çœŸæ­£çš„äº‹ä»¶ï¼ˆP1/P2ï¼‰æ‰å‘é€åˆ° PagerDutyã€‚å®šæœŸå®¡æŸ¥å‘Šè­¦è§„åˆ™å¹¶æ¶ˆé™¤è¯¯æŠ¥ï¼ˆFalse Positiveï¼‰éå¸¸é‡è¦ã€‚
:::

---

## 10. è°ƒè¯•å¿«é€Ÿå‚è€ƒ

### é”™è¯¯æ¨¡å¼ -> åŸå›  -> è§£å†³æ–¹æ¡ˆå¿«é€Ÿå‚è€ƒè¡¨

<ErrorQuickRefTable />

### å¸¸ç”¨ kubectl å‘½ä»¤é€ŸæŸ¥è¡¨

#### æ£€æŸ¥ä¸è¯Šæ–­

```bash
# View all resource status at a glance
kubectl get all -n <namespace>

# Filter only unhealthy Pods
kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded

# Detailed Pod information (including events)
kubectl describe pod <pod-name> -n <namespace>

# Namespace events (most recent first)
kubectl get events -n <namespace> --sort-by='.lastTimestamp'

# Resource usage
kubectl top nodes
kubectl top pods -n <namespace> --sort-by=memory
```

#### æ—¥å¿—æ£€æŸ¥

```bash
# Current container logs
kubectl logs <pod-name> -n <namespace>

# Previous (crashed) container logs
kubectl logs <pod-name> -n <namespace> --previous

# Specific container in a multi-container Pod
kubectl logs <pod-name> -n <namespace> -c <container-name>

# Real-time log streaming
kubectl logs -f <pod-name> -n <namespace>

# View logs from multiple Pods by label
kubectl logs -l app=<app-name> -n <namespace> --tail=50
```

#### è°ƒè¯•

```bash
# Debug with an ephemeral container
kubectl debug <pod-name> -it --image=nicolaka/netshoot --target=<container-name>

# Node debugging
kubectl debug node/<node-name> -it --image=ubuntu

# Execute a command inside a Pod
kubectl exec -it <pod-name> -n <namespace> -- <command>
```

#### Deployment ç®¡ç†

```bash
# Rollout status/history/rollback
kubectl rollout status deployment/<name>
kubectl rollout history deployment/<name>
kubectl rollout undo deployment/<name>

# Restart Deployment
kubectl rollout restart deployment/<name>

# Node maintenance (drain)
kubectl cordon <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
kubectl uncordon <node-name>
```

### æ¨èå·¥å…·çŸ©é˜µ

| åœºæ™¯ | å·¥å…· | è¯´æ˜ |
|------|------|------|
| ç½‘ç»œè°ƒè¯• | [netshoot](https://github.com/nicolaka/netshoot) | åŒ…å«å®Œæ•´ç½‘ç»œå·¥å…·é›†çš„å®¹å™¨ |
| èŠ‚ç‚¹èµ„æºå¯è§†åŒ– | [eks-node-viewer](https://github.com/awslabs/eks-node-viewer) | åŸºäºç»ˆç«¯çš„èŠ‚ç‚¹èµ„æºç›‘æ§ |
| å®¹å™¨è¿è¡Œæ—¶è°ƒè¯• | [crictl](https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/) | containerd è°ƒè¯• CLI |
| æ—¥å¿—åˆ†æ | CloudWatch Logs Insights | AWS åŸç”Ÿæ—¥å¿—æŸ¥è¯¢æœåŠ¡ |
| æŒ‡æ ‡æŸ¥è¯¢ | Prometheus / Grafana | åŸºäº PromQL çš„æŒ‡æ ‡åˆ†æ |
| åˆ†å¸ƒå¼è¿½è¸ª | [ADOT](https://aws-otel.github.io/docs/introduction) / [OpenTelemetry](https://opentelemetry.io/docs/) | è¯·æ±‚è·¯å¾„è¿½è¸ª |
| é›†ç¾¤å®‰å…¨å®¡è®¡ | kube-bench | åŸºäº CIS Benchmark çš„å®‰å…¨æ‰«æ |
| YAML æ¸…å•éªŒè¯ | kubeval / kubeconform | éƒ¨ç½²å‰æ¸…å•éªŒè¯ |
| Karpenter è°ƒè¯• | Karpenter controller logs | èŠ‚ç‚¹ä¾›ç»™é—®é¢˜è¯Šæ–­ |
| IAM è°ƒè¯• | AWS IAM Policy Simulator | IAM æƒé™éªŒè¯ |

### EKS Log Collector

EKS Log Collector æ˜¯ AWS æä¾›çš„è„šæœ¬ï¼Œå¯è‡ªåŠ¨ä» EKS å·¥ä½œèŠ‚ç‚¹æ”¶é›†è°ƒè¯•æ‰€éœ€çš„æ—¥å¿—ï¼Œå¹¶ç”Ÿæˆå¯æäº¤ç»™ AWS Support çš„å½’æ¡£æ–‡ä»¶ã€‚

**å®‰è£…ä¸æ‰§è¡Œï¼š**

```bash
# Download and run the script (on the node after SSM access)
curl -O https://raw.githubusercontent.com/awslabs/amazon-eks-ami/master/log-collector-script/linux/eks-log-collector.sh
sudo bash eks-log-collector.sh
```

**æ”¶é›†é¡¹ç›®ï¼š**

- kubelet æ—¥å¿—
- containerd æ—¥å¿—
- iptables è§„åˆ™
- CNI é…ç½®ï¼ˆVPC CNI è®¾ç½®ï¼‰
- cloud-init æ—¥å¿—
- dmesgï¼ˆå†…æ ¸æ¶ˆæ¯ï¼‰
- systemd units çŠ¶æ€

**è¾“å‡ºï¼š**

æ”¶é›†çš„æ—¥å¿—ä¼šè¢«å‹ç¼©å¹¶ä¿å­˜ä¸º `/var/log/eks_i-xxxx_yyyy-mm-dd_HH-MM-SS.tar.gz` æ ¼å¼ã€‚

**ä¸Šä¼ åˆ° S3ï¼š**

```bash
# Upload collected logs directly to S3
sudo bash eks-log-collector.sh --upload s3://my-bucket/
```

:::tip åˆ©ç”¨ AWS Support
åœ¨æäº¤ AWS Support å·¥å•æ—¶é™„ä¸Šæ­¤æ—¥å¿—æ–‡ä»¶ï¼Œå¯ä»¥è®©æ”¯æŒå·¥ç¨‹å¸ˆå¿«é€Ÿè¯„ä¼°èŠ‚ç‚¹çŠ¶æ€ï¼Œæ˜¾è‘—ç¼©çŸ­è§£å†³æ—¶é—´ã€‚åœ¨æŠ¥å‘ŠèŠ‚ç‚¹åŠ å…¥å¤±è´¥ã€kubelet æ•…éšœã€ç½‘ç»œé—®é¢˜ç­‰æƒ…å†µæ—¶ï¼Œè¯·åŠ¡å¿…é™„ä¸Šæ­¤æ–‡ä»¶ã€‚
:::

### ç›¸å…³æ–‡æ¡£

- [EKS é«˜å¯ç”¨æ¶æ„æŒ‡å—](./eks-resiliency-guide.md) - æ¶æ„çº§æ•…éšœæ¢å¤ç­–ç•¥
- [åŸºäº GitOps çš„ EKS é›†ç¾¤è¿ç»´](./gitops-cluster-operation.md) - GitOps éƒ¨ç½²ä¸è¿ç»´è‡ªåŠ¨åŒ–
- [ä½¿ç”¨ Karpenter å®ç°è¶…å¿«é€Ÿè‡ªåŠ¨æ‰©ç¼©](/docs/infrastructure-optimization/karpenter-autoscaling) - åŸºäº Karpenter çš„èŠ‚ç‚¹ä¾›ç»™ä¼˜åŒ–
- [èŠ‚ç‚¹ç›‘æ§ä»£ç†](./node-monitoring-agent.md) - èŠ‚ç‚¹çº§ç›‘æ§

### å‚è€ƒèµ„æ–™

- [EKS å®˜æ–¹æ•…éšœæ’æŸ¥æŒ‡å—](https://docs.aws.amazon.com/eks/latest/userguide/troubleshooting.html)
- [EKS æœ€ä½³å®è·µ - å®¡è®¡ä¸æ—¥å¿—](https://docs.aws.amazon.com/eks/latest/best-practices/auditing-and-logging.html)
- [EKS æœ€ä½³å®è·µ - ç½‘ç»œ](https://docs.aws.amazon.com/eks/latest/best-practices/networking.html)
- [EKS æœ€ä½³å®è·µ - å¯é æ€§](https://docs.aws.amazon.com/eks/latest/best-practices/reliability.html)
- [Kubernetes å®˜æ–¹è°ƒè¯•æŒ‡å— - Pods](https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/)
- [Kubernetes å®˜æ–¹è°ƒè¯•æŒ‡å— - Services](https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/)
- [Kubernetes DNS è°ƒè¯•](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/)
- [VPC CNI æ•…éšœæ’æŸ¥](https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/troubleshooting.md)
- [EBS CSI Driver FAQ](https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/faq.md)
- [EKS Zonal Shift æ–‡æ¡£](https://docs.aws.amazon.com/eks/latest/userguide/zone-shift.html)
