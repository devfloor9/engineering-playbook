---
title: "é¢„æµ‹æ€§æ‰©ç¼©å®¹ä¸è‡ªåŠ¨æ¢å¤æ¨¡å¼"
sidebar_label: "4. é¢„æµ‹æ€§æ‰©ç¼©å®¹ä¸è‡ªåŠ¨æ¢å¤"
description: "åŸºäºMLçš„é¢„æµ‹æ€§è‡ªåŠ¨æ‰©ç¼©å®¹ã€Karpenter+AIå…ˆå‘åˆ¶äººé¢„é…ç½®ã€AI Agentè‡ªä¸»äº‹ä»¶å“åº”ã€Kiroç¨‹åºåŒ–è°ƒè¯•æ¨¡å¼"
sidebar_position: 4
category: "aiops-aidlc"
tags: [aiops, predictive-scaling, auto-remediation, karpenter, self-healing, eks, kiro, mcp, ai-agent, chaos-engineering]
last_update:
  date: 2026-02-14
  author: devfloor9
---

import { ScalingComparison, ResponsePatterns, MaturityTable, EvolutionStages, MLModelComparison, AnomalyMetrics, RightSizingResults, ChaosExperiments, DashboardPanels } from '@site/src/components/PredictiveOpsTables';

# é¢„æµ‹æ€§æ‰©ç¼©å®¹ä¸è‡ªåŠ¨æ¢å¤æ¨¡å¼

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2026-02-12 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦29åˆ†é’Ÿ

---

## 1. æ¦‚è¿°

### 1.1 ä»å“åº”å¼åˆ°è‡ªä¸»å¼

EKSè¿è¥çš„æ¼”è¿›åˆ†ä¸º **å“åº”å¼ â†’ é¢„æµ‹å¼ â†’ è‡ªä¸»å¼** ä¸‰ä¸ªé˜¶æ®µã€‚

<EvolutionStages />

:::info æœ¬æ–‡æ¡£çš„èŒƒå›´
è¶…è¶Šå“åº”å¼æ‰©ç¼©å®¹çš„å±€é™ï¼Œæ¶µç›–åŸºäºMLçš„é¢„æµ‹æ€§æ‰©ç¼©å®¹å’Œé€šè¿‡AI Agentå®ç°çš„è‡ªä¸»æ¢å¤æ¨¡å¼ã€‚ç‰¹åˆ«ä»¥Kiro+MCPä¸ºåŸºç¡€çš„ **ç¨‹åºåŒ–è°ƒè¯•** å’ŒKagent/Strandsä¸ºåŸºç¡€çš„ **è‡ªåŠ¨äº‹ä»¶å“åº”** ä¸ºæ ¸å¿ƒè¿›è¡Œè¯´æ˜ã€‚
:::

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦é¢„æµ‹æ€§è¿è¥

- **HPAçš„å±€é™æ€§**: æŒ‡æ ‡è¶…è¿‡é˜ˆå€¼åæ‰å“åº” â†’ ç”¨æˆ·ä½“éªŒå·²å—å½±å“
- **å†·å¯åŠ¨é—®é¢˜**: æ–°Podå¯åŠ¨éœ€è¦30ç§’-2åˆ†é’Ÿ â†’ æµé‡çªå¢æ—¶æ— æ³•åº”å¯¹
- **èŠ‚ç‚¹é¢„é…ç½®å»¶è¿Ÿ**: å³ä½¿æ˜¯Karpenterï¼ŒèŠ‚ç‚¹å¯åŠ¨ä¹Ÿéœ€è¦1-3åˆ†é’Ÿ
- **å¤åˆæ•…éšœ**: å•ä¸€æŒ‡æ ‡æ— æ³•æ£€æµ‹çš„å¤šå› ç´ æ•…éšœæ—¥ç›Šå¢å¤š
- **æˆæœ¬ä½æ•ˆ**: è¿‡åº¦é¢„ç•™å†—ä½™èµ„æº â†’ æˆæœ¬æµªè´¹

---

## 2. åŸºäºMLçš„é¢„æµ‹æ€§æ‰©ç¼©å®¹

### 2.1 HPAçš„å±€é™æ€§

HPA(Horizontal Pod Autoscaler)åŸºäº **å½“å‰æŒ‡æ ‡** è¿›è¡Œå“åº”ï¼Œå› æ­¤å­˜åœ¨ç»“æ„æ€§å±€é™ã€‚

<ScalingComparison />

```
[HPAçš„å“åº”å¼æ‰©ç¼©å®¹]

æµé‡ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
                      â†‘ è¶…è¿‡é˜ˆå€¼
                      |
Podæ•°  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                  â†‘ å¼€å§‹æ‰©å®¹
                  |  (å»¶è¿Ÿå‘ç”Ÿ)
ç”¨æˆ·   âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ—âœ—âœ—âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
ä½“éªŒ              â†‘ æ€§èƒ½ä¸‹é™åŒºé—´

[MLé¢„æµ‹æ€§æ‰©ç¼©å®¹]

æµé‡ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
             â†‘ é¢„æµ‹æ—¶ç‚¹ (30åˆ†é’Ÿå‰)
             |
Podæ•°  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
             â†‘ æå‰æ‰©å®¹
             |
ç”¨æˆ·   âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
ä½“éªŒ     (æ— æ€§èƒ½ä¸‹é™)
```

### 2.2 æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹

ç”¨äºé¢„æµ‹EKSå·¥ä½œè´Ÿè½½æµé‡æ¨¡å¼çš„ä»£è¡¨æ€§MLæ¨¡å‹ï¼š

<MLModelComparison />

### 2.3 åŸºäºProphetçš„é¢„æµ‹æ€§æ‰©ç¼©å®¹å®ç°

```python
# åŸºäºProphetçš„EKSæµé‡é¢„æµ‹
import boto3
from prophet import Prophet
import pandas as pd
from datetime import datetime, timedelta

def fetch_metrics_from_amp(workspace_id, query, hours=168):
    """ä»AMPæŸ¥è¯¢è¿‡å»7å¤©çš„æŒ‡æ ‡"""
    client = boto3.client('amp', region_name='ap-northeast-2')
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=hours)

    response = client.query_range(
        workspaceId=workspace_id,
        query=query,
        startTime=start_time,
        endTime=end_time,
        step='5m'
    )
    return response

def predict_scaling(metrics_df, forecast_hours=2):
    """ä½¿ç”¨Propheté¢„æµ‹æœªæ¥æµé‡"""
    # è½¬æ¢ä¸ºProphetæ ¼å¼
    df = metrics_df.rename(columns={
        'timestamp': 'ds',
        'value': 'y'
    })

    model = Prophet(
        changepoint_prior_scale=0.05,
        seasonality_mode='multiplicative',
        daily_seasonality=True,
        weekly_seasonality=True,
    )
    model.fit(df)

    # é¢„æµ‹æœªæ¥forecast_hours
    future = model.make_future_dataframe(
        periods=forecast_hours * 12,  # 5åˆ†é’Ÿé—´éš”
        freq='5min'
    )
    forecast = model.predict(future)

    return forecast[['ds', 'yhat', 'yhat_upper', 'yhat_lower']]

def calculate_required_pods(predicted_rps, pod_capacity_rps=100):
    """åŸºäºé¢„æµ‹RPSè®¡ç®—æ‰€éœ€Podæ•°"""
    # ä½¿ç”¨ä¸Šé™å€¼(yhat_upper)ç¡®ä¿å®‰å…¨ä½™é‡
    required = int(predicted_rps / pod_capacity_rps) + 1
    return max(required, 2)  # æœ€å°‘ç»´æŒ2ä¸ª

def apply_scaling(namespace, deployment, target_replicas):
    """é€šè¿‡kubectlåº”ç”¨æ‰©ç¼©å®¹"""
    import subprocess
    cmd = f"kubectl scale deployment/{deployment} -n {namespace} --replicas={target_replicas}"
    subprocess.run(cmd.split(), check=True)
    print(f"Scaled {deployment} to {target_replicas} replicas")
```

### 2.4 åŸºäºCronJobçš„é¢„æµ‹æ€§æ‰©ç¼©å®¹è‡ªåŠ¨åŒ–

```yaml
# å®šæœŸæ‰§è¡Œé¢„æµ‹æ€§æ‰©ç¼©å®¹çš„CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: predictive-scaler
  namespace: scaling
spec:
  schedule: "*/15 * * * *"  # æ¯15åˆ†é’Ÿæ‰§è¡Œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: predictive-scaler
          containers:
            - name: scaler
              image: my-registry/predictive-scaler:latest
              env:
                - name: AMP_WORKSPACE_ID
                  value: "ws-xxxxx"
                - name: TARGET_NAMESPACE
                  value: "payment"
                - name: TARGET_DEPLOYMENT
                  value: "payment-service"
                - name: FORECAST_HOURS
                  value: "2"
              resources:
                requests:
                  cpu: 500m
                  memory: 1Gi
                limits:
                  cpu: "1"
                  memory: 2Gi
          restartPolicy: OnFailure
```

### 2.5 ç½‘ç»œæ€§èƒ½é¢„æµ‹åŠMLæ¨ç†å·¥ä½œè´Ÿè½½ä¼˜åŒ–

EKSçš„ **Container Network Observability** å¯ä»¥ç²¾ç»†ç›‘æ§Pod-to-Podé€šä¿¡æ¨¡å¼ï¼Œæå‰é¢„æµ‹ç½‘ç»œç“¶é¢ˆå¹¶ä¼˜åŒ–MLæ¨ç†å·¥ä½œè´Ÿè½½çš„æ€§èƒ½ã€‚

#### Container Network Observabilityæ•°æ®åº”ç”¨

**1. Pod-to-Podé€šä¿¡æ¨¡å¼ â†’ ç½‘ç»œç“¶é¢ˆé¢„æµ‹**

```python
# åŸºäºContainer Network ObservabilityæŒ‡æ ‡çš„ç“¶é¢ˆé¢„æµ‹
import boto3
from prophet import Prophet
import pandas as pd

def predict_network_bottleneck(cluster_name, namespace):
    """
    é¢„æµ‹Pod-to-Podç½‘ç»œå»¶è¿Ÿï¼Œåˆ¤æ–­ç“¶é¢ˆå¯èƒ½æ€§ã€‚
    """
    cloudwatch = boto3.client('cloudwatch')

    # æŸ¥è¯¢Container Network ObservabilityæŒ‡æ ‡
    metrics = cloudwatch.get_metric_data(
        MetricDataQueries=[
            {
                'Id': 'rx_latency',
                'MetricStat': {
                    'Metric': {
                        'Namespace': 'ContainerInsights',
                        'MetricName': 'pod_network_rx_latency_ms',
                        'Dimensions': [
                            {'Name': 'ClusterName', 'Value': cluster_name},
                            {'Name': 'Namespace', 'Value': namespace}
                        ]
                    },
                    'Period': 300,
                    'Stat': 'Average'
                }
            },
            {
                'Id': 'tx_bytes',
                'MetricStat': {
                    'Metric': {
                        'Namespace': 'ContainerInsights',
                        'MetricName': 'pod_network_tx_bytes',
                        'Dimensions': [
                            {'Name': 'ClusterName', 'Value': cluster_name},
                            {'Name': 'Namespace', 'Value': namespace}
                        ]
                    },
                    'Period': 300,
                    'Stat': 'Sum'
                }
            }
        ],
        StartTime=datetime.utcnow() - timedelta(days=7),
        EndTime=datetime.utcnow()
    )

    # ä½¿ç”¨Prophetæ¨¡å‹é¢„æµ‹æœªæ¥2å°æ—¶
    df = pd.DataFrame({
        'ds': [d['Timestamp'] for d in metrics['MetricDataResults'][0]['Timestamps']],
        'y': [d for d in metrics['MetricDataResults'][0]['Values']]
    })

    model = Prophet(changepoint_prior_scale=0.05)
    model.fit(df)

    future = model.make_future_dataframe(periods=24, freq='5min')
    forecast = model.predict(future)

    # ç“¶é¢ˆé¢„æµ‹ï¼šé¢„è®¡å»¶è¿Ÿå°†æ¯”å¹³æ—¶å¢åŠ 2å€ä»¥ä¸Š
    baseline = df['y'].mean()
    predicted_peak = forecast['yhat'].iloc[-1]

    if predicted_peak > baseline * 2:
        return {
            'bottleneck_risk': 'HIGH',
            'predicted_latency_ms': predicted_peak,
            'baseline_latency_ms': baseline,
            'action': 'consider_network_policy_optimization'
        }
    return {'bottleneck_risk': 'LOW'}
```

**2. è·¨AZæµé‡è¶‹åŠ¿ â†’ æˆæœ¬ä¼˜åŒ–é¢„æµ‹**

```promql
# è·¨AZç½‘ç»œæµé‡æˆæœ¬è¿½è¸ª
sum(rate(pod_network_tx_bytes{
  source_az!="", dest_az!="",
  source_az!=dest_az
}[5m])) by (source_az, dest_az)
* 0.01 / 1024 / 1024 / 1024  # $0.01/GB
```

**æˆæœ¬ä¼˜åŒ–ç­–ç•¥**ï¼š

- **æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦**: åˆ©ç”¨Kubernetes Topology Aware Hintsä¼˜å…ˆé€‰æ‹©åŒä¸€AZå†…é€šä¿¡
- **æœåŠ¡ç½‘æ ¼ä¼˜åŒ–**: é€šè¿‡Istio locality load balancingæœ€å°åŒ–è·¨AZæµé‡
- **åŸºäºé¢„æµ‹çš„éƒ¨ç½²**: MLæ¨¡å‹å­¦ä¹ é€šä¿¡æ¨¡å¼å¹¶å»ºè®®æœ€ä¼˜AZéƒ¨ç½²æ–¹æ¡ˆ

```yaml
# å¯ç”¨Topology Aware Hints
apiVersion: v1
kind: Service
metadata:
  name: ml-inference-service
  annotations:
    service.kubernetes.io/topology-mode: Auto
spec:
  selector:
    app: ml-inference
  ports:
    - port: 8080
  type: ClusterIP
```

#### MLæ¨ç†å·¥ä½œè´Ÿè½½æ€§èƒ½é¢„æµ‹

**1. Rayã€vLLMã€Tritonã€PyTorchå·¥ä½œè´Ÿè½½ç½‘ç»œæ€§èƒ½ç›‘æ§**

```yaml
# vLLMæ¨ç†æœåŠ¡ç½‘ç»œç›‘æ§
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-network-monitoring
data:
  metrics.yaml: |
    # Container Network ObservabilityæŒ‡æ ‡
    metrics:
      - pod_network_rx_bytes
      - pod_network_tx_bytes
      - pod_network_rx_latency_ms
      - pod_network_rx_errors_total

    # é¢å¤–è‡ªå®šä¹‰æŒ‡æ ‡
    custom_metrics:
      - name: vllm_inference_network_throughput_mbps
        query: |
          sum(rate(pod_network_rx_bytes{app="vllm-inference"}[1m]))
          / 1024 / 1024

      - name: vllm_model_load_network_time_ms
        query: |
          histogram_quantile(0.99,
            rate(pod_network_rx_latency_bucket{
              app="vllm-inference",
              operation="model_load"
            }[5m])
          )
```

**Rayåˆ†å¸ƒå¼æ¨ç†ç½‘ç»œæ¨¡å¼**ï¼š

```python
# Rayé›†ç¾¤çš„ç½‘ç»œç“¶é¢ˆæ£€æµ‹
import ray
from ray import serve

@serve.deployment
class LLMInferenceDeployment:
    def __init__(self):
        self.model = load_model()
        self.network_monitor = NetworkMonitor()

    async def __call__(self, request):
        # ç½‘ç»œå»¶è¿Ÿè¿½è¸ª
        start_time = time.time()

        # Rayçš„åˆ†å¸ƒå¼æ¨ç†è°ƒç”¨
        result = await self.model.generate(request.prompt)

        network_latency = time.time() - start_time

        # å‘é€è‡ªå®šä¹‰æŒ‡æ ‡åˆ°CloudWatch
        self.network_monitor.record_latency(network_latency)

        # æ£€æµ‹åˆ°ç½‘ç»œç“¶é¢ˆæ—¶è§¦å‘æ‰©å®¹
        if network_latency > 200:  # è¶…è¿‡200ms
            trigger_scale_out()

        return result
```

**2. æ¨ç†å»¶è¿Ÿ â†’ æ‰©å®¹è§¦å‘é¢„æµ‹**

```python
# åŸºäºMLæ¨ç†å»¶è¿Ÿçš„é¢„æµ‹æ€§æ‰©ç¼©å®¹
def predict_inference_scaling(service_name, forecast_hours=2):
    """
    å­¦ä¹ æ¨ç†å»¶è¿Ÿæ¨¡å¼ï¼Œé¢„æµ‹éœ€è¦æ‰©å®¹çš„æ—¶é—´ç‚¹ã€‚
    """
    # æ”¶é›†è¿‡å»7å¤©çš„æ¨ç†å»¶è¿Ÿæ•°æ®
    latency_data = fetch_inference_latency_from_cloudwatch(
        service_name=service_name,
        days=7
    )

    # æ”¶é›†è¯·æ±‚é‡æ•°æ®
    request_volume = fetch_request_volume(service_name, days=7)

    # åˆ†æå»¶è¿Ÿä¸è¯·æ±‚é‡çš„ç›¸å…³æ€§
    df = pd.DataFrame({
        'timestamp': latency_data['timestamps'],
        'latency_p99': latency_data['p99'],
        'request_rate': request_volume['rate']
    })

    # è®¡ç®—é˜ˆå€¼ï¼šP99å»¶è¿Ÿ > 500msæ—¶çš„è¯·æ±‚é‡
    threshold_requests = df[df['latency_p99'] > 500]['request_rate'].min()

    # ä½¿ç”¨Propheté¢„æµ‹æœªæ¥è¯·æ±‚é‡
    prophet_df = df[['timestamp', 'request_rate']].rename(
        columns={'timestamp': 'ds', 'request_rate': 'y'}
    )

    model = Prophet()
    model.fit(prophet_df)

    future = model.make_future_dataframe(
        periods=forecast_hours * 12,  # 5åˆ†é’Ÿé—´éš”
        freq='5min'
    )
    forecast = model.predict(future)

    # é¢„æµ‹éœ€è¦æ‰©å®¹çš„æ—¶é—´ç‚¹
    scale_out_needed = forecast[
        forecast['yhat'] > threshold_requests
    ]['ds'].min()

    if pd.notna(scale_out_needed):
        # åœ¨é¢„æµ‹æ—¶é—´30åˆ†é’Ÿå‰å…ˆå‘åˆ¶äººåœ°æ‰©å®¹
        preemptive_time = scale_out_needed - timedelta(minutes=30)

        return {
            'scale_out_recommended': True,
            'recommended_time': preemptive_time,
            'predicted_request_rate': forecast.iloc[-1]['yhat'],
            'threshold': threshold_requests,
            'current_replicas': get_current_replicas(service_name),
            'recommended_replicas': calculate_required_replicas(
                forecast.iloc[-1]['yhat'],
                threshold_requests
            )
        }

    return {'scale_out_recommended': False}
```

**3. GPUåˆ©ç”¨ç‡ + ç½‘ç»œå¸¦å®½ç›¸å…³æ€§åˆ†æ**

```promql
# GPUåˆ©ç”¨ç‡ä¸ç½‘ç»œå¸¦å®½çš„ç›¸å…³æ€§
# (NVIDIA DCGM ExporteræŒ‡æ ‡ + Container Network Observability)

# GPUåˆ©ç”¨ç‡
DCGM_FI_DEV_GPU_UTIL{
  namespace="ml-inference",
  pod=~"vllm-.*"
}

# åŒæ—¶ç½‘ç»œæ¥æ”¶å¸¦å®½
sum(rate(pod_network_rx_bytes{
  namespace="ml-inference",
  pod=~"vllm-.*"
}[1m])) by (pod)

# ç›¸å…³æ€§åˆ†æï¼šGPUåˆ©ç”¨ç‡ < 50% && ç½‘ç»œå¸¦å®½ > 100MB/s
# â†’ ç½‘ç»œç“¶é¢ˆæ­£åœ¨é˜»ç¢GPUåˆ©ç”¨ç‡
```

**ä¼˜åŒ–ç­–ç•¥**ï¼š

```yaml
# è§£å†³ç½‘ç»œç“¶é¢ˆï¼šå¯ç”¨Enhanced Networkingå’ŒENA Express
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: ml-inference-pool
spec:
  template:
    spec:
      requirements:
        - key: karpenter.k8s.aws/instance-family
          operator: In
          values: ["p5", "p4d"]  # æœ€æ–°GPUå®ä¾‹ (æ”¯æŒENA Express)
        - key: karpenter.k8s.aws/instance-size
          operator: In
          values: ["24xlarge", "48xlarge"]
      nodeClassRef:
        name: ml-inference-class
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: ml-inference-class
spec:
  amiSelectorTerms:
    - alias: al2023@latest
  userData: |
    #!/bin/bash
    # å¯ç”¨ENA Express (100Gbpsç½‘ç»œæ€§èƒ½)
    ethtool -K eth0 ena-express on

    # TCP BBRæ‹¥å¡æ§åˆ¶ (é«˜å¸¦å®½ä¼˜åŒ–)
    echo "net.ipv4.tcp_congestion_control=bbr" >> /etc/sysctl.conf
    sysctl -p
```

#### EKS Auto Modeè‡ªåŠ¨æ¢å¤/è‡ªæ„ˆ

**EKS Auto Mode** è‡ªåŠ¨æ£€æµ‹å’Œæ¢å¤èŠ‚ç‚¹æ•…éšœï¼Œå¤§å¹…æå‡ **MTTRï¼ˆå¹³å‡æ¢å¤æ—¶é—´ï¼‰**ã€‚

**1. è‡ªåŠ¨èŠ‚ç‚¹æ•…éšœæ£€æµ‹ä¸æ›¿æ¢**

```mermaid
graph TD
    A[èŠ‚ç‚¹å¥åº·æ£€æŸ¥] -->|å¤±è´¥| B[Auto Modeæ£€æµ‹]
    B --> C{æ•…éšœç±»å‹åˆ†æ}
    C -->|ç¡¬ä»¶æ•…éšœ| D[ç«‹å³é¢„é…ç½®æ–°èŠ‚ç‚¹]
    C -->|ç½‘ç»œæ•…éšœ| E[å°è¯•ç½‘ç»œé‡æ–°é…ç½®]
    C -->|è½¯ä»¶æ•…éšœ| F[è‡ªåŠ¨é‡å¯]
    D --> G[Podè‡ªåŠ¨è¿ç§»]
    E --> G
    F --> G
    G --> H[æœåŠ¡æ¢å¤éªŒè¯]
    H -->|å¤±è´¥| B
    H -->|æˆåŠŸ| I[æ¢å¤å®Œæˆ]
```

**è‡ªåŠ¨æ¢å¤è§¦å‘æ¡ä»¶**ï¼š

- **NodeNotReady**: èŠ‚ç‚¹å¤„äºNotReadyçŠ¶æ€è¶…è¿‡5åˆ†é’Ÿ
- **NetworkUnavailable**: ç½‘ç»œæ’ä»¶æ•…éšœ
- **MemoryPressure/DiskPressure**: èµ„æºä¸è¶³
- **Unschedulable**: èŠ‚ç‚¹å¤„äºä¸å¯è°ƒåº¦çŠ¶æ€

**2. OSè¡¥ä¸è‡ªåŠ¨åŒ–**

Auto Modeè‡ªåŠ¨æ‰§è¡Œ **é›¶åœæœºOSè¡¥ä¸**ï¼š

```yaml
# Auto ModeèŠ‚ç‚¹è‡ªåŠ¨æ›´æ–°ç­–ç•¥ (æ— éœ€ç”¨æˆ·é…ç½®)
# AWSè‡ªåŠ¨ç®¡ç†çš„å†…éƒ¨ç­–ç•¥ç¤ºä¾‹
nodeMaintenance:
  autoUpdate: true
  maintenanceWindow:
    preferredDays: ["Sunday", "Wednesday"]
    preferredHours: ["02:00-06:00"]  # UTC
  strategy:
    type: RollingUpdate
    maxUnavailable: 1
    respectPodDisruptionBudget: true
```

**è¡¥ä¸æµç¨‹**ï¼š

1. **é¢„é…ç½®æ–°èŠ‚ç‚¹**: ä½¿ç”¨æœ€æ–°AL2023 AMIåˆ›å»ºæ–°èŠ‚ç‚¹
2. **Podå®‰å…¨è¿ç§»**: éµå®ˆPDBï¼Œä»æ—§èŠ‚ç‚¹è¿ç§»Podåˆ°æ–°èŠ‚ç‚¹
3. **ç§»é™¤æ—§èŠ‚ç‚¹**: æ‰€æœ‰Podè¿ç§»å®Œæˆåç»ˆæ­¢æ—§èŠ‚ç‚¹
4. **éªŒè¯**: ç¡®è®¤æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡

**3. å®‰å…¨æœåŠ¡é›†æˆ**

Auto Modeä¸AWSå®‰å…¨æœåŠ¡è‡ªåŠ¨é›†æˆï¼Œæ”¯æŒ **å®‰å…¨äº‹ä»¶è‡ªåŠ¨å“åº”**ï¼š

```
GuardDuty Extended Threat Detection
  â†“ (æ£€æµ‹åˆ°åŠ å¯†è´§å¸æŒ–çŸ¿)
Auto Modeè‡ªåŠ¨å“åº”
  â†“
1. éš”ç¦»å—å½±å“çš„èŠ‚ç‚¹ (Taint: NoSchedule)
2. é¢„é…ç½®æ–°èŠ‚ç‚¹
3. å°†Podè¿ç§»åˆ°å¹²å‡€çš„èŠ‚ç‚¹
4. ç»ˆæ­¢å—æ„ŸæŸ“èŠ‚ç‚¹å¹¶æ”¶é›†å–è¯æ•°æ®
5. åœ¨CloudWatch Logsä¸­è®°å½•äº‹ä»¶
```

**4. é¢„æµ‹æ€§è§†è§’ï¼šAuto Modeçš„MTTRæ”¹å–„**

**ä¼ ç»Ÿæ‰‹åŠ¨è¿ç»´ vs Auto Modeå¯¹æ¯”**ï¼š

| æ•…éšœåœºæ™¯ | æ‰‹åŠ¨è¿ç»´MTTR | Auto Mode MTTR | æ”¹å–„ç‡ |
|--------------|----------------|----------------|--------|
| èŠ‚ç‚¹ç¡¬ä»¶æ•…éšœ | 15-30åˆ†é’Ÿ | 2-5åˆ†é’Ÿ | **ç¼©çŸ­83%** |
| OSå®‰å…¨è¡¥ä¸ | æ•°å°æ—¶ (è®¡åˆ’åœæœº) | 0åˆ†é’Ÿ (é›¶åœæœº) | **æ”¹å–„100%** |
| ç½‘ç»œæ’ä»¶æ•…éšœ | 10-20åˆ†é’Ÿ | 1-3åˆ†é’Ÿ | **ç¼©çŸ­85%** |
| æ¶æ„è½¯ä»¶æ„ŸæŸ“ | 30åˆ†é’Ÿ-1å°æ—¶ | 5-10åˆ†é’Ÿ | **ç¼©çŸ­80%** |

**é¢„æµ‹æ€§è¿ç»´è§†è§’ä¸‹Auto Modeçš„ä»·å€¼**ï¼š

- **å…ˆå‘åˆ¶äººæ›¿æ¢**: æ£€æµ‹åˆ°èŠ‚ç‚¹æ€§èƒ½ä¸‹é™ååœ¨æ•…éšœå‘ç”Ÿå‰è¿›è¡Œæ›¿æ¢
- **è‡ªåŠ¨å®¹é‡ç®¡ç†**: å­¦ä¹ å·¥ä½œè´Ÿè½½æ¨¡å¼è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹ç±»å‹
- **æ— ä¸­æ–­ç»´æŠ¤**: æ— éœ€ç”¨æˆ·ä»‹å…¥è‡ªåŠ¨æ‰§è¡Œå®‰å…¨è¡¥ä¸å’Œå‡çº§
- **æˆæœ¬ä¼˜åŒ–**: Spotå®ä¾‹ä¸­æ–­æ—¶è‡ªåŠ¨æ•…éšœè½¬ç§»åˆ°On-Demand

:::tip Auto Mode + é¢„æµ‹æ€§è¿ç»´çš„ååŒæ•ˆåº”
Auto Modeçš„è‡ªåŠ¨æ¢å¤åŠŸèƒ½æ˜¯ **å“åº”å¼(Reactive)** çš„ï¼Œä½†ä¸Container Network Observabilityæ•°æ®ç»“åˆåå¯å®ç° **é¢„æµ‹å¼(Predictive)** è¿ç»´ã€‚é€šè¿‡æ£€æµ‹ç½‘ç»œæ€§èƒ½ä¸‹é™æ¨¡å¼ï¼Œå¯ä»¥åœ¨æ•…éšœå‘ç”Ÿå‰æ›¿æ¢èŠ‚ç‚¹ï¼Œæˆ–æå‰æ¶ˆé™¤MLæ¨ç†å·¥ä½œè´Ÿè½½çš„ç½‘ç»œç“¶é¢ˆã€‚
:::

---

## 3. Karpenter + AIé¢„æµ‹

### 3.1 KarpenteråŸºæœ¬å·¥ä½œåŸç†

Karpenteræ£€æµ‹åˆ°Pending Podå **è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å®ä¾‹ç±»å‹** å¹¶è¿›è¡Œé¢„é…ç½®ã€‚

```yaml
# Karpenter NodePoolé…ç½®
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: default
spec:
  template:
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64", "arm64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: karpenter.k8s.aws/instance-family
          operator: In
          values: ["m7g", "m7i", "c7g", "c7i", "r7g"]
        - key: karpenter.k8s.aws/instance-size
          operator: In
          values: ["medium", "large", "xlarge", "2xlarge"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default
  limits:
    cpu: "100"
    memory: 400Gi
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: default
spec:
  role: KarpenterNodeRole
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: my-cluster
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: my-cluster
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 100Gi
        volumeType: gp3
        iops: 3000
        throughput: 125
```

### 3.2 åŸºäºAIé¢„æµ‹çš„å…ˆå‘åˆ¶äººé¢„é…ç½®

Karpenteræœ¬èº«å¯¹Pending Podåšå‡ºå“åº”ï¼Œä½† **ä¸AIé¢„æµ‹ç»“åˆ** åå¯ä»¥å…ˆå‘åˆ¶äººåœ°é¢„é…ç½®èŠ‚ç‚¹ã€‚

```mermaid
graph TD
    subgraph Prediction["ğŸ§  é¢„æµ‹å±‚"]
        CW_M["CloudWatch<br/>Metrics"]
        ML["MLæ¨¡å‹<br/>(Prophet/ARIMA)"]
        PRED["æµé‡<br/>é¢„æµ‹ç»“æœ"]
    end

    subgraph Preemptive["âš¡ å…ˆå‘åˆ¶äººæªæ–½"]
        WARM["Warm Pool<br/>Podåˆ›å»º"]
        PAUSE["Pause<br/>Containers"]
        KARP["Karpenter<br/>èŠ‚ç‚¹é¢„é…ç½®"]
    end

    subgraph Runtime["ğŸš€ å®é™…æµé‡"]
        TRAFFIC["æµé‡<br/>æ¶Œå…¥"]
        HPA2["HPA<br/>å³æ—¶æ‰©å®¹"]
        READY["Pod<br/>å³æ—¶æœåŠ¡"]
    end

    CW_M --> ML --> PRED
    PRED --> WARM --> KARP
    PRED --> PAUSE --> KARP
    TRAFFIC --> HPA2 --> READY
    KARP -.->|èŠ‚ç‚¹å‡†å¤‡å®Œæˆ| READY
```

**å…ˆå‘åˆ¶äººé¢„é…ç½®ç­–ç•¥**ï¼š

```yaml
# ä½¿ç”¨Placeholder Podå…ˆå‘åˆ¶äººç¡®ä¿èŠ‚ç‚¹
apiVersion: apps/v1
kind: Deployment
metadata:
  name: capacity-reservation
  namespace: scaling
spec:
  replicas: 0  # é¢„æµ‹æ‰©ç¼©å™¨åŠ¨æ€è°ƒæ•´
  selector:
    matchLabels:
      app: capacity-reservation
  template:
    metadata:
      labels:
        app: capacity-reservation
    spec:
      priorityClassName: capacity-reservation  # ä½ä¼˜å…ˆçº§
      terminationGracePeriodSeconds: 0
      containers:
        - name: pause
          image: registry.k8s.io/pause:3.9
          resources:
            requests:
              cpu: "1"
              memory: 2Gi
---
# ä½ä¼˜å…ˆçº§ç±» (è¢«å®é™…å·¥ä½œè´Ÿè½½é©±é€)
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: capacity-reservation
value: -10
globalDefault: false
description: "ç”¨äºKarpenterèŠ‚ç‚¹å…ˆå‘åˆ¶äººé¢„é…ç½®"
```

:::tip å…ˆå‘åˆ¶äººé¢„é…ç½®çš„åŸç†

1. MLæ¨¡å‹é¢„æµ‹30åˆ†é’Ÿåæµé‡å°†å¢åŠ 
2. å¢åŠ Placeholder Pod(pause container)çš„replicas
3. Karpenteræ£€æµ‹åˆ°Pending Podå¹¶é¢„é…ç½®èŠ‚ç‚¹
4. å®é™…æµé‡åˆ°æ¥æ—¶HPAåˆ›å»ºå®é™…Pod
5. Placeholder Podå› ä½ä¼˜å…ˆçº§è¢«ç«‹å³é©±é€
6. ç”±äºèŠ‚ç‚¹å·²å‡†å¤‡å°±ç»ªï¼ŒPodå¯ä»¥ç«‹å³è°ƒåº¦
:::

### 3.5 ARC + Karpenteré›†æˆè‡ªåŠ¨AZç–æ•£

**ARC(Application Recovery Controller)** æ˜¯AWSçš„é«˜å¯ç”¨æ€§æœåŠ¡ï¼Œè‡ªåŠ¨æ£€æµ‹AZæ•…éšœå¹¶å°†æµé‡è½¬ç§»åˆ°å¥åº·çš„AZã€‚ä¸Karpenteré›†æˆåå¯å®ç° **èŠ‚ç‚¹çº§åˆ«çš„è‡ªåŠ¨æ¢å¤**ã€‚

#### ARCæ¦‚è¿°

Application Recovery Controlleræä¾›ä»¥ä¸‹3ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼š

- **Readiness Check**: æŒç»­ç›‘æ§åº”ç”¨ç¨‹åºå¥åº·çŠ¶æ€
- **Routing Control**: é€šè¿‡Route 53æˆ–ALBæ§åˆ¶æµé‡è·¯ç”±
- **Zonal Shift**: æŒ‰AZå•ä½è‡ªåŠ¨æˆ–æ‰‹åŠ¨è½¬ç§»æµé‡

#### Karpenteré›†æˆæ¨¡å¼

```yaml
# æ£€æµ‹ARC Zonal Shiftä¿¡å·çš„Controller
apiVersion: v1
kind: ConfigMap
metadata:
  name: arc-karpenter-controller
  namespace: kube-system
data:
  config.yaml: |
    arcCluster: arn:aws:route53-recovery-control::ACCOUNT:cluster/CLUSTER_ID
    routingControls:
      - name: az-a-routing
        arn: arn:aws:route53-recovery-control::ACCOUNT:controlpanel/PANEL/routingcontrol/CONTROL_A
      - name: az-b-routing
        arn: arn:aws:route53-recovery-control::ACCOUNT:controlpanel/PANEL/routingcontrol/CONTROL_B
      - name: az-c-routing
        arn: arn:aws:route53-recovery-control::ACCOUNT:controlpanel/PANEL/routingcontrol/CONTROL_C
    karpenterNodePools:
      - default
      - gpu-pool
```

#### AZæ•…éšœè‡ªåŠ¨æ¢å¤åºåˆ—

```mermaid
sequenceDiagram
    participant AZ_A as AZ-A (æ•…éšœ)
    participant ARC as ARC Controller
    participant R53 as Route 53
    participant K8s as Kubernetes API
    participant Karp as Karpenter
    participant AZ_B as AZ-B (å¥åº·)
    participant Pod as Workload Pods

    AZ_A->>ARC: Readiness Checkå¤±è´¥
    ARC->>ARC: Gray Failureæ¨¡å¼æ£€æµ‹
    ARC->>R53: Zonal Shiftå¼€å§‹ (AZ-A OUT)
    ARC->>K8s: æ·»åŠ Node Taint (NoSchedule)
    K8s->>Karp: Pending Podäº‹ä»¶å‘ç”Ÿ
    Karp->>AZ_B: é¢„é…ç½®æ›¿ä»£èŠ‚ç‚¹
    AZ_B-->>K8s: æ–°èŠ‚ç‚¹æ³¨å†Œå®Œæˆ
    K8s->>Pod: Podå®‰å…¨è¿ç§» (éµå®ˆPDB)
    Pod-->>AZ_B: æœåŠ¡æ¢å¤å®Œæˆ
    ARC->>K8s: AZ-AèŠ‚ç‚¹Drainå¼€å§‹
    K8s->>AZ_A: èŠ‚ç‚¹æ¸…ç†ä¸ç§»é™¤
```

#### Gray Failureå¤„ç†

**Gray Failure** æŒ‡çš„æ˜¯ä¸å®Œå…¨æ•…éšœè€Œæ˜¯æ€§èƒ½ä¸‹é™çš„çŠ¶æ€ã€‚ARCæ£€æµ‹ä»¥ä¸‹æ¨¡å¼ï¼š

- **ç½‘ç»œå»¶è¿Ÿå¢åŠ **: å¹³æ—¶5ms â†’ è¶…è¿‡50ms
- **é—´æ­‡æ€§è¶…æ—¶**: 1-5%çš„è¯·æ±‚å¤±è´¥
- **èµ„æºäº‰ç”¨**: CPU steal timeå¢åŠ ã€ç½‘ç»œä¸¢åŒ…

```python
# Gray Failureæ£€æµ‹Lambdaå‡½æ•°ç¤ºä¾‹
import boto3
from datetime import datetime, timedelta

def detect_gray_failure(event, context):
    """
    åŸºäºContainer Network Observabilityæ•°æ®
    æ£€æµ‹Gray Failureæ¨¡å¼ã€‚
    """
    cloudwatch = boto3.client('cloudwatch')

    # æŸ¥è¯¢æŒ‰AZçš„ç½‘ç»œå»¶è¿ŸæŒ‡æ ‡
    response = cloudwatch.get_metric_statistics(
        Namespace='ContainerInsights',
        MetricName='pod_network_rx_latency_ms',
        Dimensions=[
            {'Name': 'ClusterName', 'Value': 'my-cluster'},
            {'Name': 'AvailabilityZone', 'Value': 'ap-northeast-2a'}
        ],
        StartTime=datetime.utcnow() - timedelta(minutes=15),
        EndTime=datetime.utcnow(),
        Period=60,
        Statistics=['Average', 'Maximum']
    )

    # Gray Failureé˜ˆå€¼æ£€æŸ¥
    datapoints = response['Datapoints']
    if len(datapoints) < 10:
        return {'status': 'insufficient_data'}

    avg_latency = sum(d['Average'] for d in datapoints) / len(datapoints)
    max_latency = max(d['Maximum'] for d in datapoints)

    # åŸºå‡†ï¼šå¹³å‡å»¶è¿Ÿ > 50ms æˆ–æœ€å¤§å»¶è¿Ÿ > 200ms
    if avg_latency > 50 or max_latency > 200:
        trigger_zonal_shift('ap-northeast-2a')
        return {'status': 'gray_failure_detected', 'action': 'zonal_shift'}

    return {'status': 'healthy'}

def trigger_zonal_shift(az):
    """è§¦å‘ARC Zonal Shiftã€‚"""
    arc = boto3.client('route53-recovery-cluster')
    arc.update_routing_control_state(
        RoutingControlArn='arn:aws:route53-recovery-control::ACCOUNT:...',
        RoutingControlState='Off'  # é˜»æ–­AZ-Aæµé‡
    )
```

#### Istioé›†æˆç«¯åˆ°ç«¯æ¢å¤

ä½¿ç”¨IstioæœåŠ¡ç½‘æ ¼å¯å®ç° **L7å±‚çº§çš„æµé‡æ§åˆ¶**ï¼š

```yaml
# Istio DestinationRuleï¼šAZæ•…éšœæ—¶è‡ªåŠ¨æ•…éšœè½¬ç§»
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: payment-service-dr
spec:
  host: payment-service
  trafficPolicy:
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
    loadBalancer:
      localityLbSetting:
        enabled: true
        failover:
          - from: ap-northeast-2a
            to: ap-northeast-2c
```

**ç«¯åˆ°ç«¯æ¢å¤æµç¨‹**ï¼š

1. **ARC Readiness Checkå¤±è´¥** â†’ Zonal Shiftå¼€å§‹
2. **Route 53** â†’ é˜»æ–­å‘å¾€AZ-Açš„å¤–éƒ¨æµé‡
3. **Istio Envoy** â†’ é˜»æ–­å‘å¾€AZ-Aå†…éƒ¨Podçš„East-Westæµé‡
4. **Karpenter** â†’ åœ¨AZ-Cé¢„é…ç½®æ›¿ä»£èŠ‚ç‚¹
5. **Kubernetes** â†’ éµå®ˆPDBå®‰å…¨è¿ç§»Pod
6. **Istio** â†’ è‡ªåŠ¨è·¯ç”±æµé‡åˆ°æ–°Pod

#### é¢„æµ‹æ€§AZç®¡ç†

åˆ©ç”¨Container Network Observabilityæ•°æ® **å…ˆå‘åˆ¶äººåœ°æ£€æµ‹AZæ€§èƒ½å¼‚å¸¸**ï¼š

```promql
# æŒ‰AZçš„ç½‘ç»œé”™è¯¯ç‡è¶‹åŠ¿
sum(rate(pod_network_rx_errors_total[5m])) by (availability_zone)
/ sum(rate(pod_network_rx_packets_total[5m])) by (availability_zone)
* 100

# æŒ‰AZçš„å¹³å‡Pod-to-Podå»¶è¿Ÿ
histogram_quantile(0.99,
  sum(rate(pod_network_latency_bucket[5m])) by (availability_zone, le)
)
```

**é¢„æµ‹æ€§AZç®¡ç†ç­–ç•¥**ï¼š

- **è¶‹åŠ¿åˆ†æ**: å­¦ä¹ è¿‡å»7å¤©å„AZçš„æ€§èƒ½æ¨¡å¼
- **é¢„è­¦**: æ€§èƒ½æ¯”åŸºå‡†ä¸‹é™20%æ—¶å‘å‡ºè­¦æŠ¥
- **å…ˆå‘åˆ¶äººè½¬ç§»**: ä¸‹é™30%æ—¶è€ƒè™‘è‡ªåŠ¨Zonal Shift
- **æˆæœ¬ä¼˜åŒ–**: è€ƒè™‘è·¨AZæµé‡æˆæœ¬çš„æœ€ä¼˜éƒ¨ç½²

:::warning ARC + Karpenteré›†æˆæ³¨æ„äº‹é¡¹
ARC + Karpenteré›†æˆä»…åœ¨PDBæ­£ç¡®é…ç½®æ—¶æ‰èƒ½ä¿è¯å®‰å…¨çš„Podè¿ç§»ã€‚è¯·ä¸ºæ‰€æœ‰ç”Ÿäº§å·¥ä½œè´Ÿè½½é…ç½®PDBã€‚

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: payment-service-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: payment-service
```
:::

---

## 4. CloudWatchå¼‚å¸¸æ£€æµ‹

### 4.1 å¼‚å¸¸æ£€æµ‹é¢‘å¸¦

CloudWatch Anomaly Detectionä½¿ç”¨MLè‡ªåŠ¨å­¦ä¹ æŒ‡æ ‡çš„ **æ­£å¸¸èŒƒå›´é¢‘å¸¦**ï¼Œæ£€æµ‹è¶…å‡ºé¢‘å¸¦çš„å¼‚å¸¸ã€‚

```bash
# åˆ›å»ºAnomaly Detectionæ¨¡å‹
aws cloudwatch put-anomaly-detector \
  --namespace "ContainerInsights" \
  --metric-name "pod_cpu_utilization" \
  --dimensions Name=ClusterName,Value=my-cluster \
  --stat "Average" \
  --configuration '{
    "ExcludedTimeRanges": [
      {
        "StartTime": "2026-01-01T00:00:00Z",
        "EndTime": "2026-01-02T00:00:00Z"
      }
    ],
    "MetricTimezone": "Asia/Seoul"
  }'
```

### 4.2 EKSæŒ‡æ ‡åº”ç”¨

åº”ç”¨Anomaly Detectionçš„æ ¸å¿ƒEKSæŒ‡æ ‡ï¼š

<AnomalyMetrics />

### 4.3 åŸºäºAnomaly Detectionçš„å‘Šè­¦

```bash
# åŸºäºAnomaly Detectionçš„CloudWatch Alarm
aws cloudwatch put-metric-alarm \
  --alarm-name "EKS-CPU-Anomaly" \
  --comparison-operator GreaterThanUpperThreshold \
  --threshold-metric-id ad1 \
  --evaluation-periods 3 \
  --datapoints-to-alarm 2 \
  --metrics '[
    {
      "Id": "m1",
      "MetricStat": {
        "Metric": {
          "Namespace": "ContainerInsights",
          "MetricName": "pod_cpu_utilization",
          "Dimensions": [
            {"Name": "ClusterName", "Value": "my-cluster"}
          ]
        },
        "Period": 300,
        "Stat": "Average"
      }
    },
    {
      "Id": "ad1",
      "Expression": "ANOMALY_DETECTION_BAND(m1, 2)"
    }
  ]' \
  --alarm-actions "arn:aws:sns:ap-northeast-2:ACCOUNT_ID:ops-alerts"
```

---

## 5. AI Agentè‡ªåŠ¨äº‹ä»¶å“åº”

### 5.1 ç°æœ‰è‡ªåŠ¨åŒ–çš„å±€é™æ€§

åŸºäºEventBridge + Lambdaçš„è‡ªåŠ¨åŒ–æ˜¯ **è§„åˆ™å¼** çš„ï¼Œå› æ­¤å­˜åœ¨å±€é™æ€§ï¼š

```
[ç°æœ‰æ–¹å¼ï¼šè§„åˆ™å¼è‡ªåŠ¨åŒ–]
CloudWatch Alarm â†’ EventBridge Rule â†’ Lambda â†’ å›ºå®šæ“ä½œ

é—®é¢˜ï¼š
  âœ— "CPU > 80%å°±æ‰©å®¹" â€” åŸå› å¯èƒ½æ˜¯å†…å­˜æ³„æ¼
  âœ— "Podé‡å¯ > 5æ¬¡å°±å‘Šè­¦" â€” ä¸åŒåŸå› éœ€è¦ä¸åŒåº”å¯¹
  âœ— æ— æ³•åº”å¯¹å¤åˆæ•…éšœ
  âœ— æ— æ³•é€‚åº”æ–°æ¨¡å¼
```

### 5.2 åŸºäºAI Agentçš„è‡ªä¸»å“åº”

<ResponsePatterns />

AI AgentåŸºäº **ä¸Šä¸‹æ–‡åˆ¤æ–­** è¿›è¡Œè‡ªä¸»å“åº”ã€‚

```mermaid
graph TD
    subgraph Trigger["ğŸ”” è§¦å‘å™¨"]
        CWA["CloudWatch<br/>Alarm"]
        DGA["DevOps Guru<br/>Insight"]
        K8SE["K8s<br/>Event"]
    end

    subgraph Agent["ğŸ¤– AI Agent"]
        COLLECT["æ•°æ®æ”¶é›†<br/>(MCPé›†æˆ)"]
        ANALYZE["AIåˆ†æ<br/>(æ ¹æœ¬åŸå› )"]
        DECIDE["åˆ¤æ–­<br/>(è‡ªåŠ¨/æ‰‹åŠ¨)"]
        ACT["æ‰§è¡Œ<br/>(å®‰å…¨æ“ä½œ)"]
        REPORT["æŠ¥å‘Š<br/>(Slack/Jira)"]
    end

    subgraph Actions["âš¡ å“åº”æªæ–½"]
        SCALE["æ‰©ç¼©å®¹<br/>è°ƒæ•´"]
        RESTART["Pod<br/>é‡å¯"]
        ROLLBACK["éƒ¨ç½²<br/>å›æ»š"]
        ESCALATE["äººå·¥<br/>å‡çº§å¤„ç†"]
    end

    CWA --> COLLECT
    DGA --> COLLECT
    K8SE --> COLLECT
    COLLECT --> ANALYZE --> DECIDE
    DECIDE --> ACT --> REPORT
    ACT --> SCALE
    ACT --> RESTART
    ACT --> ROLLBACK
    ACT --> ESCALATE
```

### 5.3 Kagentè‡ªåŠ¨äº‹ä»¶å“åº”

```yaml
# Kagentï¼šè‡ªåŠ¨äº‹ä»¶å“åº”Agent
apiVersion: kagent.dev/v1alpha1
kind: Agent
metadata:
  name: incident-responder
  namespace: kagent-system
spec:
  description: "EKSäº‹ä»¶è‡ªåŠ¨å“åº”Agent"
  modelConfig:
    provider: bedrock
    model: anthropic.claude-sonnet
    region: ap-northeast-2
  systemPrompt: |
    ä½ æ˜¯ä¸€ä¸ªEKSäº‹ä»¶å“åº”Agentã€‚

    ## å“åº”åŸåˆ™
    1. å®‰å…¨ä¼˜å…ˆï¼šå±é™©å˜æ›´å‡çº§ç»™äººå·¥å¤„ç†
    2. æ ¹æœ¬åŸå› ä¼˜å…ˆï¼šé’ˆå¯¹åŸå› è€Œéç—‡çŠ¶è¿›è¡Œå“åº”
    3. æœ€å°å¹²é¢„ï¼šä»…æ‰§è¡Œæœ€å¿…è¦çš„æ“ä½œ
    4. æ‰€æœ‰æ“ä½œè®°å½•ï¼šè‡ªåŠ¨æŠ¥å‘Šåˆ°Slackå’ŒJIRA

    ## è‡ªåŠ¨æ“ä½œå…è®¸èŒƒå›´
    - Podé‡å¯ (CrashLoopBackOff, 5æ¬¡ä»¥ä¸Š)
    - HPA min/maxè°ƒæ•´ (å½“å‰å€¼çš„Â±50%èŒƒå›´)
    - Deploymentå›æ»š (åˆ°å‰ä¸€ä¸ªç‰ˆæœ¬)
    - èŠ‚ç‚¹drain (MemoryPressure/DiskPressure)

    ## å‡çº§å¤„ç†å¯¹è±¡
    - å¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±çš„æ“ä½œ
    - å½±å“50%ä»¥ä¸Šreplicas
    - StatefulSetç›¸å…³å˜æ›´
    - ç½‘ç»œç­–ç•¥å˜æ›´

  tools:
    - name: kubectl
      type: kmcp
      config:
        allowedVerbs: ["get", "describe", "logs", "top", "rollout", "scale", "delete"]
        deniedResources: ["secrets", "configmaps"]
    - name: cloudwatch
      type: kmcp
      config:
        actions: ["GetMetricData", "DescribeAlarms", "GetInsight"]
    - name: slack
      type: mcp
      config:
        webhook_url: "${SLACK_WEBHOOK}"
        channel: "#incidents"

  triggers:
    - type: cloudwatch-alarm
      filter:
        severity: ["CRITICAL", "HIGH"]
    - type: kubernetes-event
      filter:
        reason: ["CrashLoopBackOff", "OOMKilled", "FailedScheduling"]
```

### 5.4 Strands Agent SOPï¼šå¤åˆæ•…éšœå“åº”

```python
# Strands Agentï¼šå¤åˆæ•…éšœè‡ªåŠ¨å“åº”
from strands import Agent
from strands.tools import eks_tool, cloudwatch_tool, slack_tool, jira_tool

incident_agent = Agent(
    name="complex-incident-handler",
    model="bedrock/anthropic.claude-sonnet",
    tools=[eks_tool, cloudwatch_tool, slack_tool, jira_tool],
    sop="""
    ## å¤åˆæ•…éšœå“åº”SOP

    ### Phase 1ï¼šæƒ…å†µè¯„ä¼° (30ç§’å†…)
    1. æŸ¥è¯¢CloudWatchå‘Šè­¦å’ŒDevOps Guruæ´å¯Ÿ
    2. ç¡®è®¤ç›¸å…³æœåŠ¡çš„PodçŠ¶æ€
    3. ç¡®è®¤èŠ‚ç‚¹çŠ¶æ€å’Œèµ„æºåˆ©ç”¨ç‡
    4. ç¡®è®¤æœ€è¿‘éƒ¨ç½²å†å² (10åˆ†é’Ÿå†…å˜æ›´)

    ### Phase 2ï¼šæ ¹æœ¬åŸå› åˆ†æ (2åˆ†é’Ÿå†…)
    1. ä»æ—¥å¿—ä¸­æå–é”™è¯¯æ¨¡å¼
    2. æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ (CPU, Memory, Network, Disk)
    3. åˆ†æä¸éƒ¨ç½²å˜æ›´çš„æ—¶é—´ç›¸å…³æ€§
    4. ç¡®è®¤ä¾èµ–æœåŠ¡çŠ¶æ€

    ### Phase 3ï¼šè‡ªåŠ¨å“åº”
    æŒ‰åŸå› è‡ªåŠ¨å¤„ç†ï¼š

    **éƒ¨ç½²ç›¸å…³æ•…éšœï¼š**
    - æœ€è¿‘10åˆ†é’Ÿå†…å­˜åœ¨éƒ¨ç½² â†’ è‡ªåŠ¨å›æ»š
    - å›æ»šåç¡®è®¤çŠ¶æ€ â†’ æ¢å¤æ­£å¸¸åˆ™å®Œæˆ

    **èµ„æºä¸è¶³ï¼š**
    - CPU/Memory > 90% â†’ è°ƒæ•´HPAæˆ–Karpenteræ·»åŠ èŠ‚ç‚¹
    - Disk > 85% â†’ æ¸…ç†ä¸å¿…è¦çš„æ—¥å¿—/é•œåƒ

    **ä¾èµ–æœåŠ¡æ•…éšœï¼š**
    - RDSè¿æ¥å¤±è´¥ â†’ ç¡®è®¤è¿æ¥æ± è®¾ç½®ï¼Œå¿…è¦æ—¶é‡å¯
    - SQSå»¶è¿Ÿ â†’ æ£€æŸ¥DLQï¼Œæ¶ˆè´¹è€…æ‰©å®¹

    **åŸå› ä¸æ˜ï¼š**
    - å‡çº§ç»™äººå·¥å¤„ç†
    - åœ¨Slackä¸­åˆ†äº«æ‰€æœ‰æ”¶é›†çš„æ•°æ®

    ### Phase 4ï¼šäº‹åå¤„ç†
    1. åˆ›å»ºäº‹ä»¶æ—¶é—´çº¿
    2. åˆ›å»ºJIRAäº‹ä»¶å·¥å•
    3. åœ¨Slack #incidentsé¢‘é“å‘å¸ƒæŠ¥å‘Š
    4. ä¿å­˜ä¸ºå­¦ä¹ æ•°æ® (åé¦ˆå¾ªç¯)
    """
)
```

:::info AI Agentçš„æ ¸å¿ƒä»·å€¼
è¶…è¶ŠEventBridge+Lambdaï¼Œå®ç°åŸºäºAIä¸Šä¸‹æ–‡çš„è‡ªä¸»å“åº”ã€‚é€šè¿‡ **MCPé›†æˆæŸ¥è¯¢** å„ç§æ•°æ®æºï¼ˆCloudWatchã€EKS APIã€X-Rayã€éƒ¨ç½²å†å²ï¼‰ï¼Œå³ä½¿æ˜¯è§„åˆ™æ— æ³•åº”å¯¹çš„å¤åˆæ•…éšœï¼Œä¹Ÿèƒ½åˆ†ææ ¹æœ¬åŸå› å¹¶è‡ªåŠ¨æ‰§è¡Œé€‚å½“çš„æ“ä½œã€‚
:::

### 5.5 CloudWatch Investigations â€” åŸºäºAIçš„è‡ªåŠ¨æ ¹æœ¬åŸå› åˆ†æ

**CloudWatch Investigations** æ˜¯åŸºäºAWS 17å¹´è¿è¥ç»éªŒæ„å»ºçš„ **ç”Ÿæˆå¼AIè‡ªåŠ¨è°ƒæŸ¥ç³»ç»Ÿ**ã€‚äº‹ä»¶å‘ç”Ÿæ—¶ï¼ŒAIè‡ªåŠ¨ç”Ÿæˆå‡è®¾ã€æ”¶é›†æ•°æ®å¹¶æ‰§è¡ŒéªŒè¯è°ƒæŸ¥å·¥ä½œæµã€‚

#### CloudWatch Investigationsæ¦‚è¿°

```mermaid
graph TD
    subgraph Trigger["ğŸ”” äº‹ä»¶æ£€æµ‹"]
        ALARM["CloudWatch<br/>Alarm"]
        SIGNAL["Application<br/>Signals"]
    end

    subgraph Investigation["ğŸ” AIè°ƒæŸ¥æµç¨‹"]
        HYPO["å‡è®¾ç”Ÿæˆ<br/>(AI)"]
        COLLECT["æ•°æ®æ”¶é›†<br/>(è‡ªåŠ¨)"]
        ANALYZE["ç›¸å…³æ€§åˆ†æ<br/>(AI)"]
        ROOT["æ ¹æœ¬åŸå› <br/>æ¨æ–­"]
    end

    subgraph Evidence["ğŸ“Š è¯æ®æ”¶é›†"]
        METRICS["ç›¸å…³æŒ‡æ ‡"]
        LOGS["ç›¸å…³æ—¥å¿—"]
        TRACES["ç›¸å…³è¿½è¸ª"]
        DEPLOY["éƒ¨ç½²å†å²"]
    end

    subgraph Output["ğŸ“ ç»“æœä¸æªæ–½"]
        SUMMARY["è°ƒæŸ¥ç»“æœ<br/>æ‘˜è¦"]
        REMEDIATION["æ¢å¤å»ºè®®<br/>(Runbook)"]
        REPORT["è¯¦ç»†æŠ¥å‘Š"]
    end

    ALARM --> HYPO
    SIGNAL --> HYPO
    HYPO --> COLLECT
    COLLECT --> METRICS
    COLLECT --> LOGS
    COLLECT --> TRACES
    COLLECT --> DEPLOY
    METRICS --> ANALYZE
    LOGS --> ANALYZE
    TRACES --> ANALYZE
    DEPLOY --> ANALYZE
    ANALYZE --> ROOT
    ROOT --> SUMMARY
    ROOT --> REMEDIATION
    ROOT --> REPORT
```

#### æ ¸å¿ƒåŠŸèƒ½

**1. Application Signalsé›†æˆï¼šåŸºäºæœåŠ¡æ‹“æ‰‘çš„å½±å“åº¦è‡ªåŠ¨åˆ†æ**

CloudWatch Investigationsåˆ©ç”¨Application Signalsè‡ªåŠ¨ç”Ÿæˆçš„æœåŠ¡æ‹“æ‰‘æ¥è¿½è¸ª **æ•…éšœä¼ æ’­è·¯å¾„**ï¼š

```yaml
# Application Signalsè‡ªåŠ¨æœåŠ¡æ‹“æ‰‘ç¤ºä¾‹
payment-gateway (é”™è¯¯ç‡å¢åŠ 25%)
  â””â”€> payment-service (å»¶è¿Ÿå¢åŠ 300%)
       â”œâ”€> postgres-db (è¿æ¥æ± è€—å°½)
       â””â”€> redis-cache (æ­£å¸¸)
            â””â”€> dynamodb (æ­£å¸¸)
```

Investigationsåˆ†ææ­¤æ‹“æ‰‘ï¼š
- **Root Cause**: `postgres-db` è¿æ¥æ± è€—å°½
- **Impacted Services**: `payment-service`, `payment-gateway`
- **Propagation Path**: DB â†’ Service â†’ Gateway

**2. ç›¸å…³æŒ‡æ ‡/æ—¥å¿—/è¿½è¸ªè‡ªåŠ¨ç›¸å…³æ€§åˆ†æ**

```python
# Investigationsæ‰§è¡Œçš„è‡ªåŠ¨ç›¸å…³æ€§åˆ†æç¤ºä¾‹

# æ—¶é—´ç›¸å…³æ€§
payment_service_errors.spike_at = "2026-02-12 14:23:00"
db_connection_pool.exhausted_at = "2026-02-12 14:22:55"
# â†’ ç›¸å·®5ç§’ï¼šDBé—®é¢˜å…ˆäºæœåŠ¡é”™è¯¯å‘ç”Ÿ

# æŒ‡æ ‡ç›¸å…³æ€§
db_active_connections = 100 (è¾¾åˆ°max_connections)
payment_service_response_time = 5000ms (æ¯”å¹³æ—¶50msé«˜100å€)
# â†’ å¼ºç›¸å…³æ€§ï¼šDBè¿æ¥è€—å°½ â†’ æœåŠ¡å»¶è¿Ÿ

# æ—¥å¿—æ¨¡å¼åˆ†æ
logs.error_pattern = "CannotGetJdbcConnectionException"
logs.frequency = 1,234 occurrences in last 5 minutes
# â†’ æ˜ç¡®è¯æ®ï¼šDBè¿æ¥ä¸å¯ç”¨é”™è¯¯
```

**3. åŸºäºå‡è®¾çš„æ ¹æœ¬åŸå› æ¨æ–­**

Investigationsè‡ªåŠ¨ç”Ÿæˆå¹¶éªŒè¯ä»¥ä¸‹å‡è®¾ï¼š

| å‡è®¾ | éªŒè¯æ–¹æ³• | ç»“æœ |
|------|----------|------|
| DBè¿æ¥æ± è€—å°½ | ç¡®è®¤`db_connections`æŒ‡æ ‡ | âœ“ å·²ç¡®è®¤ |
| ç½‘ç»œå»¶è¿Ÿ | åˆ†æVPC Flow Logs | âœ— æ­£å¸¸ |
| OOM(å†…å­˜ä¸è¶³) | ç¡®è®¤å®¹å™¨å†…å­˜æŒ‡æ ‡ | âœ— æ­£å¸¸ |
| éƒ¨ç½²åBug | æŸ¥è¯¢æœ€è¿‘éƒ¨ç½²å†å² | âœ“ ç¡®è®¤10åˆ†é’Ÿå‰æœ‰éƒ¨ç½² |

**æœ€ç»ˆç»“è®º**: æœ€è¿‘éƒ¨ç½²ä¸­DBè¿æ¥æ± é…ç½®è¢«é”™è¯¯åœ°ä»`maxPoolSize=50`æ”¹ä¸º`maxPoolSize=10`ã€‚

**4. è°ƒæŸ¥ç»“æœæ‘˜è¦ä¸æ¢å¤å»ºè®®**

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  CloudWatch Investigationsç»“æœæ‘˜è¦
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”´ æ ¹æœ¬åŸå›  (Root Cause):
   payment-serviceçš„DBè¿æ¥æ± é…ç½®é”™è¯¯
   (maxPoolSize: 50 â†’ 10è¢«é”™è¯¯ä¿®æ”¹)

ğŸ“Š å½±å“åº¦ (Impact):
   - payment-gateway: é”™è¯¯ç‡å¢åŠ 25%
   - payment-service: å»¶è¿Ÿå¢åŠ 300%
   - å—å½±å“è¯·æ±‚ï¼šçº¦15,000ä»¶

â±ï¸ æ—¶é—´çº¿:
   14:10 - éƒ¨ç½²å¼€å§‹ (v1.2.3 â†’ v1.2.4)
   14:22 - DBè¿æ¥æ± å¼€å§‹è€—å°½
   14:23 - æœåŠ¡é”™è¯¯æ€¥å¢å‘Šè­¦è§¦å‘
   14:25 - Investigationsè‡ªåŠ¨å¼€å§‹

ğŸ’¡ å»ºè®®æªæ–½:
   1. ç«‹å³å›æ»š: kubectl rollout undo deployment/payment-service
   2. æ¢å¤DBè¿æ¥æ± é…ç½®: maxPoolSize=50
   3. æ·»åŠ éƒ¨ç½²å‰ç¯å¢ƒå˜é‡éªŒè¯æ­¥éª¤
   4. åº”ç”¨ConfigMapå˜æ›´æ—¶çš„è‡ªåŠ¨éªŒè¯è„šæœ¬

ğŸ“‹ ç›¸å…³èµ„æº:
   - Runbook: https://wiki/db-connection-pool-issue
   - æ—¥å¿—: CloudWatch Logs InsightsæŸ¥è¯¢é“¾æ¥
   - æŒ‡æ ‡: CloudWatch Dashboardé“¾æ¥
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

#### ä¸DevOps Agentçš„åŒºåˆ«

| æ–¹é¢ | CloudWatch Investigations | Kagent / Strands Agent |
|------|--------------------------|------------------------|
| **è¿è¥æ–¹å¼** | AWSæ‰˜ç®¡ (æ— éœ€é…ç½®) | ç”¨æˆ·å®‰è£…Â·è¿ç»´ |
| **åˆ†æèŒƒå›´** | AWSå…¨å±€æ•°æ®è‡ªåŠ¨æ”¶é›† | ä»…é…ç½®çš„æ•°æ®æº |
| **æ ¹æœ¬åŸå› åˆ†æ** | AIè‡ªåŠ¨å‡è®¾ç”ŸæˆÂ·éªŒè¯ | åŸºäºSOPè§„åˆ™æ‰§è¡Œ |
| **è‡ªå®šä¹‰** | æœ‰é™ (AWSé¢„è®¾) | é«˜ (å®Œå…¨è‡ªç”±åº¦) |
| **è‡ªåŠ¨æ¢å¤** | ä»…æä¾›å»ºè®® (ä¸æ‰§è¡Œ) | å¯è‡ªåŠ¨æ‰§è¡Œ |
| **æˆæœ¬** | åŸºäºCloudWatchä½¿ç”¨é‡ | ä»…åŸºç¡€è®¾æ–½æˆæœ¬ |
| **å­¦ä¹ æ›²çº¿** | æ—  (å³æ—¶å¯ç”¨) | ä¸­ç­‰ (éœ€ç¼–å†™YAML) |

**æ¨èé›†æˆæ¨¡å¼**ï¼š

```mermaid
graph LR
    A[CloudWatch Alarm] --> B[Investigations]
    B --> C{æ ¹æœ¬åŸå› è¯†åˆ«}
    C -->|æ˜ç¡®åŸå› | D[EventBridge]
    C -->|ä¸æ˜ç¡®| E[äººå·¥å‡çº§å¤„ç†]
    D --> F[Kagentè‡ªåŠ¨æ¢å¤]
    F --> G[æ¢å¤å®Œæˆ]
    G --> H[Investigationsé‡æ–°éªŒè¯]
```

**é›†æˆç¤ºä¾‹ï¼šEventBridge Rule**

```json
{
  "source": ["aws.cloudwatch"],
  "detail-type": ["CloudWatch Investigation Complete"],
  "detail": {
    "conclusion": {
      "rootCauseType": ["Configuration Error", "Resource Exhaustion"]
    }
  }
}
```

```python
# EventBridge â†’ Kagentè‡ªåŠ¨æ¢å¤Lambda
def lambda_handler(event, context):
    """
    æ¥æ”¶CloudWatch Investigationsç»“æœ
    é€šè¿‡Kagentè§¦å‘è‡ªåŠ¨æ¢å¤ã€‚
    """
    investigation = event['detail']
    root_cause = investigation['conclusion']['rootCauseType']

    if root_cause == "Configuration Error":
        # å‘Kagentè¯·æ±‚ConfigMapå›æ»š
        trigger_kagent_task(
            task_type="rollback_config",
            resource=investigation['affectedResources'][0],
            reason=investigation['conclusion']['summary']
        )
    elif root_cause == "Resource Exhaustion":
        # å‘Kagentè¯·æ±‚æ‰©ç¼©å®¹
        trigger_kagent_task(
            task_type="scale_up",
            resource=investigation['affectedResources'][0],
            target_replicas=calculate_required_replicas()
        )
```

:::tip CloudWatch Investigationsæ´»ç”¨ç­–ç•¥
CloudWatch Investigationsæ˜¯æ— éœ€é…ç½®å³å¯ç›´æ¥ä½¿ç”¨çš„æ‰˜ç®¡AIåˆ†æã€‚éœ€è¦è‡ªå®šä¹‰è‡ªåŠ¨åŒ–æ—¶è¯·ä¸Kagent/Strands Agenté…åˆä½¿ç”¨ã€‚

**æ¨èå·¥ä½œæµ**ï¼š
1. **ä¸€æ¬¡åˆ†æ**: CloudWatch Investigationsè‡ªåŠ¨è¯†åˆ«æ ¹æœ¬åŸå› 
2. **äºŒæ¬¡å“åº”**: åŸå› æ˜ç¡®æ—¶ â†’ Kagent/Strandsè‡ªåŠ¨æ¢å¤
3. **å‡çº§å¤„ç†**: åŸå› ä¸æ˜ç¡®æ—¶ â†’ å°†è°ƒæŸ¥ç»“æœä¼ é€’ç»™äººå·¥
:::

#### å®æˆ˜åœºæ™¯ï¼šEKS Pod OOMKilledè°ƒæŸ¥

```
[äº‹ä»¶] 14:45 - payment-service Pod OOMKilled

[Investigationsè‡ªåŠ¨è°ƒæŸ¥]

æ­¥éª¤1ï¼šå‡è®¾ç”Ÿæˆ
  - å‡è®¾Aï¼šå†…å­˜æ³„æ¼
  - å‡è®¾Bï¼šæµé‡çªå¢å¯¼è‡´çš„æ­£å¸¸å†…å­˜å¢é•¿
  - å‡è®¾Cï¼šå†…å­˜limitsé…ç½®é”™è¯¯

æ­¥éª¤2ï¼šæ•°æ®æ”¶é›†
  - Podå†…å­˜ä½¿ç”¨è¶‹åŠ¿ï¼š100Mi â†’ 512Mi (4å°æ—¶)
  - æµé‡è¶‹åŠ¿ï¼šæ— å˜åŒ– (ç¨³å®š)
  - Heap dumpåˆ†æï¼šRedisè¿æ¥å¯¹è±¡ç´¯ç§¯10,000ä¸ª

æ­¥éª¤3ï¼šæ ¹æœ¬åŸå› è¯†åˆ«
  âœ“ å‡è®¾Aç¡®è®¤ï¼šå†…å­˜æ³„æ¼ (Redisè¿æ¥æœªé‡Šæ”¾)
  âœ— å‡è®¾Bæ’é™¤ï¼šæµé‡æ— å˜åŒ–
  âœ— å‡è®¾Cæ’é™¤ï¼šlimitsè®¾ç½®é€‚å½“ (512Mi)

æ­¥éª¤4ï¼šæ¢å¤å»ºè®®
  å³æ—¶æªæ–½ï¼š
    - kubectl rollout restart deployment/payment-service
    - ä¸´æ—¶å°†å†…å­˜limitså¢åŠ åˆ°1Gi

  æ ¹æœ¬è§£å†³ï¼š
    - ä¿®æ”¹Rediså®¢æˆ·ç«¯ä»£ç  (æ­£ç¡®å…³é—­è¿æ¥æ± )
    - æ·»åŠ å†…å­˜åˆ†æå·¥å…·
    - è®¾ç½®å†…å­˜æ³„æ¼ç›‘æ§å‘Šè­¦

  ç›¸å…³ä»£ç ï¼š
    æ–‡ä»¶: src/cache/redis_client.go
    é—®é¢˜: ç¼ºå°‘defer conn.Close()
    ä¿®å¤PR: https://github.com/...
```

### 5.6 Amazon Q DeveloperåŸºäºè‡ªç„¶è¯­è¨€çš„è¿è¥è‡ªåŠ¨åŒ–

**Amazon Q Developer** æ˜¯AWSçš„ä¸‹ä¸€ä»£AIåŠ©æ‰‹ï¼Œé€šè¿‡ **è‡ªç„¶è¯­è¨€æ¥å£** é©å‘½æ€§åœ°ç®€åŒ–EKSè¿è¥ã€‚æ— éœ€æ§åˆ¶å°æµè§ˆæˆ–å¤æ‚å‘½ä»¤ï¼Œå¯ä»¥ç”¨æ—¥å¸¸è¯­è¨€æé—®å’Œè¯·æ±‚æ“ä½œã€‚

#### åŸºäºè‡ªç„¶è¯­è¨€æ¥å£çš„EKSæ•…éšœæ’æŸ¥

Amazon Q Developerå°† **è‡ªç„¶è¯­è¨€æŸ¥è¯¢** è‡ªåŠ¨è½¬æ¢ä¸ºAWS APIè°ƒç”¨å’Œkubectlå‘½ä»¤å¹¶æ‰§è¡Œï¼š

**åœºæ™¯1ï¼šæŸ¥æ‰¾OOM Pod**

```
[ç”¨æˆ·æé—®]
"å¸®æˆ‘æ‰¾åˆ°è¿™ä¸ªé›†ç¾¤ä¸­å‘ç”ŸOOMçš„Pod"

[Amazon Q Developerå“åº”]
æ­£åœ¨åˆ†æ...

å‘ç°çš„OOMäº‹ä»¶ï¼š
- payment-service-7d8f9c4b-xyz (namespace: payment)
  â””â”€ æœ€è¿‘3æ¬¡OOMKilled (è¿‡å»1å°æ—¶)
  â””â”€ å†…å­˜limits: 512Mi, å®é™…ä½¿ç”¨: 520Mi
  â””â”€ å»ºè®®: å°†memory limitså¢åŠ åˆ°1Gi

- cart-service-5a6b2d1c-abc (namespace: shopping)
  â””â”€ æœ€è¿‘1æ¬¡OOMKilled (è¿‡å»30åˆ†é’Ÿ)
  â””â”€ å†…å­˜limits: 256Mi, å®é™…ä½¿ç”¨: 280Mi
  â””â”€ å»ºè®®: å°†memory limitså¢åŠ åˆ°512Mi

æ‰§è¡Œçš„å‘½ä»¤ï¼š
$ kubectl get events --all-namespaces --field-selector reason=OOMKilled
$ kubectl top pod -n payment payment-service-7d8f9c4b-xyz
$ kubectl describe pod -n payment payment-service-7d8f9c4b-xyz

éœ€è¦æ‰§è¡Œä¸‹ä¸€æ­¥æ“ä½œå—ï¼Ÿ
1. è‡ªåŠ¨è°ƒæ•´memory limits (åº”ç”¨VPA)
2. å¼€å§‹è¯¦ç»†å†…å­˜åˆ†æ
3. åˆ†æå…¨éƒ¨ç›¸å…³æ—¥å¿—
```

**åœºæ™¯2ï¼šè¯†åˆ«é«˜è´Ÿè½½èŠ‚ç‚¹**

```
[ç”¨æˆ·æé—®]
"è¿‡å»1å°æ—¶å†…CPUä½¿ç”¨ç‡è¶…è¿‡80%çš„èŠ‚ç‚¹æœ‰å“ªäº›ï¼Ÿ"

[Amazon Q Developerå“åº”]
æ­£åœ¨æŸ¥è¯¢CloudWatch Container Insightsæ•°æ®...

é«˜è´Ÿè½½èŠ‚ç‚¹åˆ—è¡¨ï¼š

1. ip-10-0-1-45.ap-northeast-2.compute.internal
   - å¹³å‡CPU: 87.3% (è¿‡å»1å°æ—¶)
   - æœ€å¤§CPU: 94.1%
   - å®ä¾‹ç±»å‹: m5.xlarge (4 vCPU)
   - è¿è¡Œä¸­çš„Pod: 23ä¸ª
   - å»ºè®®æ“ä½œ: é€šè¿‡Karpenteræ·»åŠ èŠ‚ç‚¹æˆ–é‡æ–°åˆ†é…å·¥ä½œè´Ÿè½½

2. ip-10-0-2-78.ap-northeast-2.compute.internal
   - å¹³å‡CPU: 82.5% (è¿‡å»1å°æ—¶)
   - æœ€å¤§CPU: 89.7%
   - å®ä¾‹ç±»å‹: c5.2xlarge (8 vCPU)
   - è¿è¡Œä¸­çš„Pod: 31ä¸ª
   - å»ºè®®æ“ä½œ: å°†CPUå¯†é›†å‹Podè¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹

æ‰§è¡Œçš„CloudWatchæŸ¥è¯¢ï¼š
namespace: ContainerInsights
metric: node_cpu_utilization
period: 1h
threshold: > 80%

è¦æ‰§è¡Œä¸‹ä¸€æ­¥æ“ä½œå—ï¼Ÿ
1. é€šè¿‡Karpenterè‡ªåŠ¨é¢„é…ç½®æ–°èŠ‚ç‚¹
2. æ‰‹åŠ¨è¿ç§»é«˜è´Ÿè½½Podåˆ°å…¶ä»–èŠ‚ç‚¹
3. æ£€æŸ¥å’Œè°ƒæ•´HPAè®¾ç½®
```

**åœºæ™¯3ï¼šDeploymentå›æ»š**

```
[ç”¨æˆ·æé—®]
"å¸®æˆ‘å›æ»šè¿™ä¸ªDeployment"

[Amazon Q Developerå“åº”]
æ­£åœ¨å‡†å¤‡payment-service Deploymentçš„å›æ»š...

å½“å‰éƒ¨ç½²çŠ¶æ€ï¼š
- Revision: 15
- éƒ¨ç½²æ—¶é—´: 2026-02-12 14:30:00 (15åˆ†é’Ÿå‰)
- é”™è¯¯ç‡: 12.3% (æ¯”å¹³æ—¶0.5%æ€¥å‰§å¢åŠ )
- ä¸Šä¸€ä¸ªç¨³å®šç‰ˆæœ¬: Revision 14 (é”™è¯¯ç‡0.4%)

å›æ»šç›®æ ‡ï¼š
- å½“å‰: payment-service:v2.3.1 (Revision 15)
- å›æ»š: payment-service:v2.3.0 (Revision 14)

PDBç¡®è®¤ï¼š
- minAvailable: 2
- å½“å‰Running: 5
- å›æ»šæœŸé—´æœ€ä½ä¿è¯: 2ä¸ªç»´æŒ âœ“

æ‰§è¡Œå‘½ä»¤ï¼š
$ kubectl rollout undo deployment/payment-service -n payment --to-revision=14

æ‰§è¡Œå›æ»šå—ï¼Ÿ (yes/no)
```

**åœºæ™¯4ï¼šç½‘ç»œè¿æ¥é—®é¢˜è¯Šæ–­**

```
[ç”¨æˆ·æé—®]
"å¸®æˆ‘æ‰¾å‡ºpayment-serviceæ— æ³•è¿æ¥databaseçš„åŸå› "

[Amazon Q Developerå“åº”]
æ­£åœ¨è¯Šæ–­ç½‘ç»œè¿æ¥é—®é¢˜...

è¯Šæ–­ç»“æœï¼š

1. DNSç¡®è®¤: âœ“ æ­£å¸¸
   - database.payment.svc.cluster.local è§£ææˆåŠŸ
   - IP: 10.100.15.234

2. ç½‘ç»œç­–ç•¥ç¡®è®¤: âœ— å‘ç°é—®é¢˜
   - payment-service â†’ database è¿æ¥è¢«é˜»æ–­
   - åŸå› : NetworkPolicy "database-access-policy"ä¸­
     payment-serviceçš„æ ‡ç­¾ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­

3. SecurityGroupç¡®è®¤: âœ“ æ­£å¸¸
   - RDS SecurityGroupå…è®¸EKSèŠ‚ç‚¹

4. æ—¥å¿—åˆ†æ:
   - é”™è¯¯: "dial tcp 10.100.15.234:5432: i/o timeout"
   - å‘ç”Ÿé¢‘ç‡: è¿‡å»10åˆ†é’Ÿ1,247æ¬¡

è§£å†³æ–¹æ³•:
$ kubectl edit networkpolicy database-access-policy -n payment

åœ¨ingress.from.podSelectorä¸­æ·»åŠ ä»¥ä¸‹æ ‡ç­¾:
  matchLabels:
    app: payment-service  # â† æ·»åŠ æ­¤æ ‡ç­¾

ä¿®æ”¹åçš„YAMLé¢„è§ˆ:
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-access-policy
  namespace: payment
spec:
  podSelector:
    matchLabels:
      app: database
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: payment-service  # â† å·²æ·»åŠ 
        - podSelector:
            matchLabels:
              app: order-service
---

è‡ªåŠ¨ä¿®å¤å—ï¼Ÿ (yes/no)
```

#### Cost Exploreré›†æˆ

Amazon Q Developerä¸ **AWS Cost Explorer** é›†æˆï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€æˆæœ¬æŸ¥è¯¢ï¼š

**åœºæ™¯1ï¼šæˆæœ¬å¢é•¿åŸå› åˆ†æ**

```
[ç”¨æˆ·æé—®]
"è¿™ä¸ªæœˆEKSæˆæœ¬ä¸ºä»€ä¹ˆå¢åŠ äº†ï¼Ÿ"

[Amazon Q Developerå“åº”]
æ­£åœ¨åˆ†æCost Exploreræ•°æ®...

æˆæœ¬å¢é•¿åˆ†æ (2026å¹´2æœˆ):

æ€»EKSæˆæœ¬: $12,450 (ç¯æ¯”+45%)
å¢é•¿åŸå› ï¼š

1. è®¡ç®—æˆæœ¬å¢é•¿: +$3,200 (è´¡çŒ®52%)
   - m5.4xlargeå®ä¾‹ä½¿ç”¨å¢åŠ 
   - åŸå› : MLæ¨ç†å·¥ä½œè´Ÿè½½å¢åŠ  (vLLMéƒ¨ç½²)
   - å»ºè®®: è¿ç§»åˆ°Graviton (m7g)å¯èŠ‚çœ40%

2. æ•°æ®ä¼ è¾“æˆæœ¬å¢é•¿: +$1,800 (è´¡çŒ®29%)
   - è·¨AZæµé‡å¢åŠ  (150GB â†’ 450GB)
   - åŸå› : æ–°å¾®æœåŠ¡é—´çš„é€šä¿¡æ¨¡å¼
   - å»ºè®®: é€šè¿‡Topology Aware Hintsä¼˜å…ˆåŒAZé€šä¿¡

3. EBSå­˜å‚¨æˆæœ¬å¢é•¿: +$950 (è´¡çŒ®15%)
   - gp3å·ä½¿ç”¨é‡å¢åŠ  (500GB â†’ 1,200GB)
   - åŸå› : PrometheusæŒ‡æ ‡ä¿ç•™æœŸå»¶é•¿ (7å¤© â†’ 30å¤©)
   - å»ºè®®: å°†é•¿æœŸä¿å­˜æ•°æ®è¿ç§»åˆ°S3

4. è´Ÿè½½å‡è¡¡å™¨æˆæœ¬å¢é•¿: +$250 (è´¡çŒ®4%)
   - ALB LCUä½¿ç”¨å¢åŠ 
   - åŸå› : APIæµé‡å¢åŠ 

è¯¦ç»†æˆæœ¬ä»ªè¡¨ç›˜: (é“¾æ¥)
æ­£åœ¨ç”Ÿæˆæˆæœ¬ä¼˜åŒ–å»ºè®®æŠ¥å‘Š...
```

**åœºæ™¯2ï¼šSpot vs On-Demandæˆæœ¬å¯¹æ¯”**

```
[ç”¨æˆ·æé—®]
"åˆ‡æ¢åˆ°Spotå®ä¾‹å¯ä»¥èŠ‚çœå¤šå°‘ï¼Ÿ"

[Amazon Q Developerå“åº”]
æ­£åœ¨è¿è¡ŒSpotèŠ‚çœæ¨¡æ‹Ÿ...

å½“å‰é›†ç¾¤é…ç½®ï¼š
- On-DemandèŠ‚ç‚¹: 45ä¸ª
- å®ä¾‹ç±»å‹: m5.xlarge, m5.2xlarge, c5.2xlarge
- æœˆè´¹ç”¨: $8,500

Spotè½¬æ¢æ¨¡æ‹Ÿ (70% Spot, 30% On-Demand):

åœºæ™¯A: ä¿å®ˆè½¬æ¢ (ä»…Statelesså·¥ä½œè´Ÿè½½)
- SpotèŠ‚ç‚¹: 32ä¸ª (70%)
- On-DemandèŠ‚ç‚¹: 13ä¸ª (30%)
- é¢„ä¼°æœˆè´¹ç”¨: $3,200
- èŠ‚çœé¢: $5,300/æœˆ (èŠ‚çœ62%)
- é£é™©åº¦: ä½ (Statefulå·¥ä½œè´Ÿè½½ä¿æŒOn-Demand)

åœºæ™¯B: ç§¯æè½¬æ¢ (å¤§éƒ¨åˆ†Spot)
- SpotèŠ‚ç‚¹: 40ä¸ª (89%)
- On-DemandèŠ‚ç‚¹: 5ä¸ª (11%)
- é¢„ä¼°æœˆè´¹ç”¨: $1,900
- èŠ‚çœé¢: $6,600/æœˆ (èŠ‚çœ78%)
- é£é™©åº¦: ä¸­ç­‰ (Spotä¸­æ–­æ—¶éƒ¨åˆ†æœåŠ¡å—å½±å“)

Spotä¸­æ–­åº”å¯¹ï¼š
- Karpenterè‡ªåŠ¨é‡æ–°é¢„é…ç½®: å¹³å‡90ç§’
- Spot Instance Interruption Warning: æå‰2åˆ†é’Ÿé€šçŸ¥
- PDBä¿è¯æœ€ä½å¯ç”¨æ€§

å»ºè®®ï¼š
1. ä»åœºæ™¯Aå¼€å§‹ (ç¨³å®šæ€§ä¼˜å…ˆ)
2. ç›‘æ§2å‘¨
3. ç¨³å®šåæ‰©å±•åˆ°åœºæ™¯B

æ­£åœ¨ç”ŸæˆSpotè½¬æ¢è®¡åˆ’ä¹¦...
```

#### MCPæœåŠ¡å™¨é›†æˆä½ä»£ç AIOps

Amazon Q Developeré€šè¿‡ **Model Context Protocol(MCP)** ä¸å„ç§AWSæœåŠ¡é›†æˆï¼Œå¯ä»¥æ„å»º **ä½ä»£ç AIOpsè§£å†³æ–¹æ¡ˆ**ï¼š

**MCPæœåŠ¡å™¨é›†æˆæ¶æ„**ï¼š

```mermaid
graph TD
    subgraph User["ğŸ‘¤ è¿ç»´äººå‘˜"]
        NL["è‡ªç„¶è¯­è¨€æé—®"]
    end

    subgraph QDev["ğŸ¤– Amazon Q Developer"]
        INTENT["æ„å›¾åˆ†æ"]
        ORCHESTRATE["MCPæœåŠ¡å™¨ç¼–æ’"]
        SYNTHESIZE["å“åº”åˆæˆ"]
    end

    subgraph MCP["ğŸ“¡ MCPæœåŠ¡å™¨"]
        EKS_MCP["EKS MCP<br/>(kubectl)"]
        CW_MCP["CloudWatch MCP<br/>(æŒ‡æ ‡/æ—¥å¿—)"]
        CE_MCP["Cost Explorer MCP<br/>(æˆæœ¬)"]
        BEDROCK_MCP["Bedrock MCP<br/>(AIåˆ†æ)"]
    end

    subgraph AWS["â˜ï¸ AWSæœåŠ¡"]
        EKS["Amazon EKS"]
        CW["CloudWatch"]
        CE["Cost Explorer"]
        BEDROCK["Bedrock"]
    end

    NL --> INTENT
    INTENT --> ORCHESTRATE
    ORCHESTRATE --> EKS_MCP
    ORCHESTRATE --> CW_MCP
    ORCHESTRATE --> CE_MCP
    ORCHESTRATE --> BEDROCK_MCP
    EKS_MCP --> EKS
    CW_MCP --> CW
    CE_MCP --> CE
    BEDROCK_MCP --> BEDROCK
    EKS --> SYNTHESIZE
    CW --> SYNTHESIZE
    CE --> SYNTHESIZE
    BEDROCK --> SYNTHESIZE
    SYNTHESIZE --> NL
```


**ç»¼åˆè¿è¥è§†å›¾ç¤ºä¾‹**ï¼š

```
[ç”¨æˆ·æé—®]
"è¯·ç»¼åˆå±•ç¤º payment-service çš„å½“å‰çŠ¶æ€"

[Amazon Q Developer å›å¤]
payment-service ç»¼åˆçŠ¶æ€ä»ªè¡¨æ¿ï¼š

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  PAYMENT-SERVICE çŠ¶æ€ (2026-02-12 15:30)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Kubernetes çŠ¶æ€ (EKS MCP)
   - Deployment: payment-service
   - Replicas: 5/5 Running
   - Image: payment-service:v2.3.1
   - æœ€è¿‘é‡å¯ï¼šæ—  (ç¨³å®š)
   - PDB: minAvailable=2 (æ»¡è¶³)

2. æ€§èƒ½æŒ‡æ ‡ (CloudWatch MCP)
   - Request Rate: 1,234 req/s
   - P50 Latency: 45ms
   - P99 Latency: 280ms (SLO: 500ms) âœ“
   - Error Rate: 0.3% (SLO: 1%) âœ“
   - CPU ä½¿ç”¨ç‡: 65% (å¹³å‡)
   - Memory ä½¿ç”¨ç‡: 72% (å¹³å‡)

3. æˆæœ¬ (Cost Explorer MCP)
   - æœ¬æœˆè´¹ç”¨: $1,245
   - ç¯æ¯”: +12%
   - ä¸»è¦è´¹ç”¨: è®¡ç®— (m5.xlarge x 5)
   - ä¼˜åŒ–æœºä¼š: åˆ‡æ¢è‡³ Spot å¯èŠ‚çœ $750

4. AI åˆ†æ (Bedrock MCP)
   - çŠ¶æ€: æ­£å¸¸
   - è¶‹åŠ¿: æµé‡å‘ˆå¢é•¿è¶‹åŠ¿ (å‘¨å¢ +15%)
   - é¢„æµ‹: 2å‘¨å†…éœ€è¦æ‰©å®¹
   - å»ºè®®: å°† HPA maxReplicas å¢åŠ è‡³ 10

5. è¿‘æœŸäº‹ä»¶ (CloudWatch Logs MCP)
   - éƒ¨ç½²: 2å¤©å‰ (v2.3.0 â†’ v2.3.1) æˆåŠŸ
   - å‘Šè­¦: æ— 
   - é”™è¯¯æ—¥å¿—: 15æ¡ (è½»å¾®ï¼ŒDB è¿æ¥è¶…æ—¶)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ç»¼åˆè¯„ä¼°: ğŸŸ¢ æ­£å¸¸
ä¸‹ä¸€æ­¥æ“ä½œ: æ— éœ€
æŒç»­è§‚å¯Ÿ: CPU/Memory è¶‹åŠ¿ç›‘æ§

æƒ³è¦æŸ¥çœ‹å“ªä¸ªé¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯ï¼Ÿ(1-5)
```

**è‡ªç„¶è¯­è¨€ â†’ MCP è°ƒç”¨ â†’ ç»“æœåˆ†æ â†’ æ“ä½œå»ºè®®çš„è‡ªåŠ¨å¾ªç¯**ï¼š

```python
# Amazon Q Developer çš„å†…éƒ¨è¿ä½œ (æ¦‚å¿µæ€§)
class QDeveloperAIOpsLoop:
    def process_query(self, user_query: str):
        """å¤„ç†è‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„è‡ªåŠ¨å¾ªç¯"""

        # 1. æ„å›¾åˆ†æ
        intent = self.analyze_intent(user_query)
        # ä¾‹: "payment-service çŠ¶æ€" â†’ intents: ["k8s_status", "metrics", "cost"]

        # 2. è¯†åˆ«æ‰€éœ€ MCP æœåŠ¡å™¨
        required_mcps = self.identify_mcps(intent)
        # ä¾‹: ["eks-mcp", "cloudwatch-mcp", "cost-explorer-mcp"]

        # 3. MCP è°ƒç”¨ (å¹¶è¡Œ)
        results = await asyncio.gather(
            self.eks_mcp.get_deployment_status("payment-service"),
            self.cloudwatch_mcp.get_metrics("payment-service", period="1h"),
            self.cost_explorer_mcp.get_service_cost("payment-service")
        )

        # 4. ç»“æœç»¼åˆåˆ†æ (ä½¿ç”¨ Bedrock Claude)
        analysis = self.bedrock_mcp.analyze(
            prompt=f"è¯·åˆ†æä»¥ä¸‹æ•°æ®ï¼Œè¯„ä¼°ç»¼åˆçŠ¶æ€å¹¶æå‡ºæ“ä½œå»ºè®®:\n{results}",
            model="anthropic.claude-sonnet-4.0"
        )

        # 5. ç”Ÿæˆæ“ä½œå»ºè®®
        actions = self.generate_actions(analysis)
        # ä¾‹: ["HPA è°ƒæ•´", "è€ƒè™‘ Spot åˆ‡æ¢", "åŠ å¼ºæ—¥å¿—ç›‘æ§"]

        # 6. å‘ç”¨æˆ·å“åº”
        return self.format_response(analysis, actions)
```

**MCP æœåŠ¡å™¨ç»„åˆç¤ºä¾‹**ï¼š

| é—®é¢˜ç±»å‹ | ä½¿ç”¨çš„ MCP æœåŠ¡å™¨ | ç»¼åˆåˆ†æ |
|----------|----------------|----------|
| "Pod ä¸ºä»€ä¹ˆé‡å¯ï¼Ÿ" | EKS MCP + CloudWatch Logs MCP | äº‹ä»¶ + æ—¥å¿—å…³è”åˆ†æ |
| "è´¹ç”¨ä¸ºä»€ä¹ˆå¢åŠ äº†ï¼Ÿ" | Cost Explorer MCP + EKS MCP | è´¹ç”¨å¢é•¿ + èµ„æºå˜æ›´å…³è”åˆ†æ |
| "æœ‰ç½‘ç»œå»¶è¿Ÿå—ï¼Ÿ" | CloudWatch MCP + EKS MCP | æŒ‡æ ‡ + ç½‘ç»œç­–ç•¥åˆ†æ |
| "æœ‰å®‰å…¨å¨èƒå—ï¼Ÿ" | GuardDuty MCP + EKS MCP | å¨èƒæ£€æµ‹ + Pod çŠ¶æ€åˆ†æ |

#### Kagent/Strands çš„åŒºåˆ«

| æ–¹é¢ | Amazon Q Developer | Kagent / Strands |
|------|-------------------|------------------|
| **è¿è¥æ–¹å¼** | å¯¹è¯å¼å·¥å…· (Interactive) | è‡ªåŠ¨åŒ–ä»£ç† (Autonomous) |
| **è§¦å‘æ–¹å¼** | ç”¨æˆ·æé—® (On-demand) | äº‹ä»¶é©±åŠ¨ (Event-driven) |
| **ä¸»è¦ç”¨é€”** | æ‰‹åŠ¨è°ƒæŸ¥å’Œåˆ†æ | è‡ªåŠ¨å“åº”å’Œæ¢å¤ |
| **æ‰§è¡Œæƒé™** | ä»¥è¯»å–ä¸ºä¸» (éƒ¨åˆ†å†™å…¥) | éœ€è¦å†™å…¥æƒé™ (è‡ªåŠ¨æ“ä½œ) |
| **é…ç½®å¤æ‚åº¦** | ä½ (å³å¼€å³ç”¨) | ä¸­ç­‰ (éœ€è¦ YAML é…ç½®) |
| **è‡ªå®šä¹‰** | æœ‰é™ (AWS é¢„è®¾) | é«˜ (åŸºäº SOP å®Œå…¨æ§åˆ¶) |
| **è´¹ç”¨** | Q Developer è®¢é˜…è´¹ | ä»…åŸºç¡€è®¾æ–½è´¹ç”¨ |
| **å­¦ä¹ æ›²çº¿** | æ—  (è‡ªç„¶è¯­è¨€) | ä¸­ç­‰ (éœ€è¦ Kubernetes çŸ¥è¯†) |

**æ¨èç»„åˆæ¨¡å¼**ï¼š

```
[åœºæ™¯ 1: äº‹ä»¶å‘ç”Ÿ]

1. Kagent/Strands (è‡ªåŠ¨å“åº”)
   - å‘Šè­¦æ£€æµ‹ â†’ ç«‹å³å¼€å§‹è‡ªåŠ¨æ“ä½œ
   - ä¾‹: Pod é‡å¯ã€æ‰©å®¹ã€å›æ»š

2. Amazon Q Developer (æ‰‹åŠ¨è°ƒæŸ¥)
   - éœ€è¦å¤æ‚æ ¹å› åˆ†ææ—¶
   - ä¾‹: "è¿™ä¸ª Pod ä¸ºä»€ä¹ˆä¸€ç›´é‡å¯ï¼Ÿ"

[åœºæ™¯ 2: å®šæœŸæ£€æŸ¥]

1. Amazon Q Developer (æ‰‹åŠ¨è°ƒæŸ¥)
   - "åˆ†æä¸€ä¸‹æœ¬å‘¨è´¹ç”¨å¢åŠ çš„åŸå› "
   - "æ‰¾å‡ºæ€§èƒ½ä¸‹é™çš„æœåŠ¡"

2. Kagent/Strands (è‡ªåŠ¨å“åº”)
   - æ¥æ”¶ Q Developer çš„å»ºè®®å¹¶è‡ªåŠ¨åº”ç”¨
   - ä¾‹: VPA è°ƒæ•´ã€HPA é…ç½®å˜æ›´

[åœºæ™¯ 3: é¢„æµ‹æ€§è¿è¥]

1. CloudWatch Anomaly Detection
   - è‡ªåŠ¨æ£€æµ‹å¼‚å¸¸å¾å…†

2. Amazon Q Developer (åˆ†æ)
   - "è¿™ä¸ªå¼‚å¸¸å¾å…†æ„å‘³ç€ä»€ä¹ˆï¼Ÿ"
   - "è¿‡å»æ˜¯å¦æœ‰ç±»ä¼¼çš„æ¨¡å¼ï¼Ÿ"

3. Kagent/Strands (å…ˆå‘åˆ¶äºº)
   - é’ˆå¯¹é¢„æµ‹çš„é—®é¢˜è¿›è¡Œå…ˆå‘åˆ¶äººçš„æ‰©å®¹
```

**é›†æˆå·¥ä½œæµç¤ºä¾‹**ï¼š

```yaml
# Kagent Agent: è‡ªåŠ¨æ‰§è¡Œ Amazon Q Developer çš„å»ºè®®
apiVersion: kagent.dev/v1alpha1
kind: Agent
metadata:
  name: q-developer-executor
spec:
  description: "è‡ªåŠ¨æ‰§è¡Œ Amazon Q Developer çš„å»ºè®®"
  triggers:
    - type: slack-command
      filter:
        command: "/q-execute"
  tools:
    - name: kubectl
      type: kmcp
    - name: amazon-q
      type: custom
      config:
        endpoint: "https://q.aws.amazon.com/api"
  workflow: |
    ## Q Developer å»ºè®®è‡ªåŠ¨æ‰§è¡Œå·¥ä½œæµ

    1. åœ¨ Slack ä¸­å‘ Q Developer æé—®
       ä¾‹: "@q è¯·å»ºè®® payment-service çš„ä¼˜åŒ–æ–¹æ¡ˆ"

    2. Q Developer ç”Ÿæˆå»ºè®®
       ä¾‹: "å°† HPA maxReplicas å¢åŠ è‡³ 10ï¼Œåº”ç”¨ VPA"

    3. ç”¨æˆ·æ‰¹å‡†
       å‘½ä»¤: "/q-execute å»ºè®®ç¼–å·"

    4. Kagent è‡ªåŠ¨æ‰§è¡Œ
       - å˜æ›´ HPA é…ç½®
       - åˆ›å»ºå¹¶åº”ç”¨ VPA
       - åœ¨ Slack ä¸­æŠ¥å‘Šæ‰§è¡Œç»“æœ
```

:::tip Amazon Q Developer çš„æ ¸å¿ƒä»·å€¼
Amazon Q Developer é€šè¿‡**è‡ªç„¶è¯­è¨€æ¥å£**å¤§å¹…é™ä½äº† EKS è¿è¥çš„å…¥é—¨é—¨æ§›ã€‚å³ä½¿ä¸äº†è§£ kubectl å‘½ä»¤æˆ– CloudWatch æŸ¥è¯¢è¯­æ³•ï¼Œä¹Ÿå¯ä»¥ç”¨æ—¥å¸¸è¯­è¨€æé—®å’Œè¯·æ±‚æ“ä½œã€‚é€šè¿‡ **MCP æœåŠ¡å™¨é›†æˆ**ï¼Œè‡ªåŠ¨ç»„åˆå¤šä¸ªæ•°æ®æºï¼Œå¯ä»¥æ„å»º**ä½ä»£ç  AIOps è§£å†³æ–¹æ¡ˆ**ã€‚

**æ¨èä½¿ç”¨åœºæ™¯**ï¼š
1. **æ‰‹åŠ¨è°ƒæŸ¥**ï¼šå¤æ‚é—®é¢˜çš„æ ¹æœ¬åŸå› åˆ†æ
2. **æˆæœ¬ä¼˜åŒ–**ï¼šä¸ Cost Explorer è”åŠ¨çš„æˆæœ¬æ´å¯Ÿ
3. **å­¦ä¹ å·¥å…·**ï¼šæ–°å›¢é˜Ÿæˆå‘˜çš„ EKS è¿è¥å­¦ä¹ 
4. **Kagent/Strands ç»„åˆ**ï¼šQ Developer(è°ƒæŸ¥) + Kagent(è‡ªåŠ¨å“åº”)
:::

### 5.7 åŸºäº Bedrock AgentCore çš„è‡ªä¸»è¿è¥

**Amazon Bedrock AgentCore** æ˜¯ Bedrock Agents çš„æ ¸å¿ƒå¼•æ“ï¼Œå¯ä»¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ„å»º**å®Œå…¨è‡ªä¸»è¿è¥ä»£ç†**ã€‚å¦‚æœè¯´ Kagent/Strands æ˜¯ Kubernetes åŸç”Ÿæ–¹æ³•ï¼Œé‚£ä¹ˆ Bedrock AgentCore å°±æ˜¯ AWS åŸç”Ÿæ–¹æ³•ï¼Œé€šè¿‡ **guardrails** å’Œ **action groups** æ˜ç¡®æ§åˆ¶å®‰å…¨çš„è‡ªåŠ¨åŒ–èŒƒå›´ã€‚

#### 5.6.1 Bedrock AgentCore æ¶æ„

```mermaid
graph TD
    subgraph Trigger["ğŸ”” äº‹ä»¶æ£€æµ‹"]
        EVB["EventBridge<br/>(Alarm/Insight)"]
        CWA["CloudWatch<br/>Alarm"]
    end

    subgraph AgentCore["ğŸ¤– Bedrock AgentCore"]
        AGENT["Agent<br/>(claude-sonnet)"]
        KB["Knowledge Base<br/>(Runbook)"]
        AG["Action Groups<br/>(Lambda)"]
        GR["Guardrails<br/>(å®‰å…¨èŒƒå›´)"]
    end

    subgraph Actions["âš¡ æ¢å¤æ“ä½œ"]
        EKS_A["EKS API<br/>(kubectl)"]
        AWS_A["AWS API<br/>(RDS/SQS)"]
        NOTIFY["Slack/JIRA<br/>(é€šçŸ¥)"]
    end

    EVB --> AGENT
    CWA --> AGENT
    AGENT --> KB
    AGENT --> AG
    AG --> GR
    GR --> EKS_A
    GR --> AWS_A
    GR --> NOTIFY

    style AgentCore fill:#fff3cd,stroke:#ff9800
```

#### 5.6.2 Bedrock Agent å®šä¹‰ â€” äº‹ä»¶è‡ªä¸»æ¢å¤

```python
# Bedrock Agent åˆ›å»º â€” äº‹ä»¶è‡ªåŠ¨å“åº”
import boto3

bedrock = boto3.client('bedrock-agent', region_name='ap-northeast-2')

response = bedrock.create_agent(
    agentName='incident-auto-remediation',
    foundationModel='anthropic.claude-sonnet-v3',
    instruction="""
    ä½ æ˜¯ EKS äº‹ä»¶è‡ªåŠ¨æ¢å¤ä»£ç†ã€‚

    ## æ ¸å¿ƒåŸåˆ™
    1. å®‰å…¨ä¼˜å…ˆ: ä»…åœ¨ guardrails èŒƒå›´å†…æ‰§è¡Œæ“ä½œ
    2. æ ¹æœ¬åŸå› åˆ†æ: è§£å†³åŸå› è€Œéç—‡çŠ¶
    3. æœ€å°å¹²é¢„: ä»…æ‰§è¡Œå¿…è¦çš„æœ€å°å˜æ›´
    4. å®Œå…¨é€æ˜: æ‰€æœ‰æ“ä½œç«‹å³æŠ¥å‘Šåˆ° Slack å’Œ JIRA

    ## è‡ªåŠ¨æ¢å¤å·¥ä½œæµ
    Phase 1: æ£€æµ‹ (30ç§’å†…)
    - CloudWatch Alarm åˆ†æ
    - DevOps Guru Insight æ”¶é›†
    - ç›¸å…³ EKS èµ„æºçŠ¶æ€æŸ¥è¯¢

    Phase 2: è¯Šæ–­ (2åˆ†é’Ÿå†…)
    - Pod æ—¥å¿—å’Œäº‹ä»¶åˆ†æ
    - æŒ‡æ ‡å…³è”åˆ†æ (CPU/Memory/Network)
    - éƒ¨ç½²å†å²ç¡®è®¤ (æœ€è¿‘10åˆ†é’Ÿå˜æ›´)
    - åœ¨ Knowledge Base ä¸­æœç´¢ç±»ä¼¼æ¡ˆä¾‹

    Phase 3: è‡ªåŠ¨æ¢å¤ (5åˆ†é’Ÿå†…)
    - éƒ¨ç½²æ•…éšœ â†’ è‡ªåŠ¨å›æ»š (to previous stable revision)
    - èµ„æºä¸è¶³ â†’ HPA è°ƒæ•´æˆ– Pod é‡å¯
    - ä¾èµ–æœåŠ¡æ•…éšœ â†’ é‡å¯æˆ–é‡ç½®è¿æ¥
    - åŸå› ä¸æ˜ â†’ å‡çº§è‡³äººå·¥å¤„ç†

    Phase 4: éªŒè¯å’ŒæŠ¥å‘Š
    - æ¢å¤åçŠ¶æ€ç¡®è®¤ (ç¡®è®¤æŒ‡æ ‡æ¢å¤æ­£å¸¸)
    - åˆ›å»ºäº‹ä»¶æ—¶é—´çº¿
    - Slack/JIRA è‡ªåŠ¨æŠ¥å‘Š
    """,
    idleSessionTTLInSeconds=600,
    agentResourceRoleArn='arn:aws:iam::ACCOUNT_ID:role/BedrockAgentRole'
)

agent_id = response['agent']['agentId']
```

#### 5.6.3 Action Groups â€” å®‰å…¨çš„æ¢å¤æ“ä½œèŒƒå›´

```python
# Action Group 1: EKS åªè¯»æŸ¥è¯¢
bedrock.create_agent_action_group(
    agentId=agent_id,
    agentVersion='DRAFT',
    actionGroupName='eks-read-actions',
    actionGroupExecutor={
        'lambda': 'arn:aws:lambda:ap-northeast-2:ACCOUNT_ID:function:eks-read-handler'
    },
    apiSchema={
        'payload': '''
        {
          "openapi": "3.0.0",
          "info": {"title": "EKS Read API", "version": "1.0.0"},
          "paths": {
            "/pods": {
              "get": {
                "summary": "Get Pod list",
                "parameters": [
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}}
                ],
                "responses": {"200": {"description": "Pod list"}}
              }
            },
            "/pods/{name}/logs": {
              "get": {
                "summary": "Get Pod logs",
                "parameters": [
                  {"name": "name", "in": "path", "required": true, "schema": {"type": "string"}},
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}}
                ],
                "responses": {"200": {"description": "Pod logs"}}
              }
            },
            "/deployments/{name}/revisions": {
              "get": {
                "summary": "Get deployment revision history",
                "parameters": [
                  {"name": "name", "in": "path", "required": true, "schema": {"type": "string"}},
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}}
                ],
                "responses": {"200": {"description": "Revision list"}}
              }
            }
          }
        }
        '''
    }
)

# Action Group 2: EKS æ¢å¤æ“ä½œ (åº”ç”¨ guardrails)
bedrock.create_agent_action_group(
    agentId=agent_id,
    agentVersion='DRAFT',
    actionGroupName='eks-remediation-actions',
    actionGroupExecutor={
        'lambda': 'arn:aws:lambda:ap-northeast-2:ACCOUNT_ID:function:eks-remediation-handler'
    },
    apiSchema={
        'payload': '''
        {
          "openapi": "3.0.0",
          "info": {"title": "EKS Remediation API", "version": "1.0.0"},
          "paths": {
            "/deployments/{name}/rollback": {
              "post": {
                "summary": "Rollback deployment to previous revision",
                "parameters": [
                  {"name": "name", "in": "path", "required": true, "schema": {"type": "string"}},
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}},
                  {"name": "to_revision", "in": "query", "schema": {"type": "integer"}}
                ],
                "responses": {"200": {"description": "Rollback initiated"}}
              }
            },
            "/pods/{name}/restart": {
              "post": {
                "summary": "Restart Pod (delete and let controller recreate)",
                "parameters": [
                  {"name": "name", "in": "path", "required": true, "schema": {"type": "string"}},
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}}
                ],
                "responses": {"200": {"description": "Pod restarted"}}
              }
            },
            "/hpa/{name}/adjust": {
              "post": {
                "summary": "Adjust HPA min/max replicas",
                "parameters": [
                  {"name": "name", "in": "path", "required": true, "schema": {"type": "string"}},
                  {"name": "namespace", "in": "query", "schema": {"type": "string"}},
                  {"name": "min_replicas", "in": "query", "schema": {"type": "integer"}},
                  {"name": "max_replicas", "in": "query", "schema": {"type": "integer"}}
                ],
                "responses": {"200": {"description": "HPA adjusted"}}
              }
            }
          }
        }
        '''
    }
)

# Action Group 3: é€šçŸ¥å’ŒæŠ¥å‘Š
bedrock.create_agent_action_group(
    agentId=agent_id,
    agentVersion='DRAFT',
    actionGroupName='notification-actions',
    actionGroupExecutor={
        'lambda': 'arn:aws:lambda:ap-northeast-2:ACCOUNT_ID:function:notification-handler'
    },
    apiSchema={
        'payload': '''
        {
          "openapi": "3.0.0",
          "info": {"title": "Notification API", "version": "1.0.0"},
          "paths": {
            "/slack/send": {
              "post": {
                "summary": "Send Slack notification",
                "requestBody": {
                  "required": true,
                  "content": {
                    "application/json": {
                      "schema": {
                        "type": "object",
                        "properties": {
                          "channel": {"type": "string"},
                          "message": {"type": "string"},
                          "severity": {"type": "string", "enum": ["info", "warning", "critical"]}
                        }
                      }
                    }
                  }
                },
                "responses": {"200": {"description": "Message sent"}}
              }
            },
            "/jira/create-incident": {
              "post": {
                "summary": "Create JIRA incident ticket",
                "requestBody": {
                  "required": true,
                  "content": {
                    "application/json": {
                      "schema": {
                        "type": "object",
                        "properties": {
                          "title": {"type": "string"},
                          "description": {"type": "string"},
                          "severity": {"type": "string"}
                        }
                      }
                    }
                  }
                },
                "responses": {"200": {"description": "Ticket created"}}
              }
            }
          }
        }
        '''
    }
)
```

#### 5.6.4 Guardrails â€” å®‰å…¨èŒƒå›´é™åˆ¶

```python
# Guardrails å®šä¹‰ â€” å®‰å…¨è‡ªåŠ¨åŒ–èŒƒå›´é™åˆ¶
bedrock_guardrails = boto3.client('bedrock', region_name='ap-northeast-2')

guardrail_response = bedrock_guardrails.create_guardrail(
    name='incident-remediation-guardrails',
    description='äº‹ä»¶è‡ªåŠ¨æ¢å¤å®‰å…¨èŒƒå›´é™åˆ¶',
    topicPolicyConfig={
        'topicsConfig': [
            {
                'name': 'data-deletion',
                'definition': 'Any action that deletes persistent data, such as PV, StatefulSet, or database',
                'type': 'DENY'
            },
            {
                'name': 'security-policy-change',
                'definition': 'Changes to SecurityGroup, NetworkPolicy, RBAC, or IAM roles',
                'type': 'DENY'
            },
            {
                'name': 'namespace-critical',
                'definition': 'Actions on kube-system or critical infrastructure namespaces',
                'type': 'DENY'
            }
        ]
    },
    contentPolicyConfig={
        'filtersConfig': [
            {'type': 'HATE', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},
            {'type': 'VIOLENCE', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'}
        ]
    },
    wordPolicyConfig={
        'wordsConfig': [
            {'text': 'delete pv'},
            {'text': 'delete statefulset'},
            {'text': 'drop database'},
            {'text': 'rm -rf'},
            {'text': 'delete namespace kube-system'}
        ],
        'managedWordListsConfig': [
            {'type': 'PROFANITY'}
        ]
    }
)

# å°† Guardrails å…³è”åˆ° Agent
bedrock.associate_agent_guardrail(
    agentId=agent_id,
    agentVersion='DRAFT',
    guardrailIdentifier=guardrail_response['guardrailId'],
    guardrailVersion='DRAFT'
)
```

#### 5.6.5 Knowledge Base é›†æˆ â€” Runbook è‡ªåŠ¨å‚è€ƒ

```python
# Knowledge Base åˆ›å»º â€” Runbook å­˜å‚¨åº“
bedrock.create_knowledge_base(
    name='incident-runbook-kb',
    description='äº‹ä»¶å“åº” Runbook å­˜å‚¨åº“',
    roleArn='arn:aws:iam::ACCOUNT_ID:role/BedrockKBRole',
    knowledgeBaseConfiguration={
        'type': 'VECTOR',
        'vectorKnowledgeBaseConfiguration': {
            'embeddingModelArn': 'arn:aws:bedrock:ap-northeast-2::foundation-model/amazon.titan-embed-text-v1'
        }
    },
    storageConfiguration={
        'type': 'OPENSEARCH_SERVERLESS',
        'opensearchServerlessConfiguration': {
            'collectionArn': 'arn:aws:aoss:ap-northeast-2:ACCOUNT_ID:collection/runbook-kb',
            'vectorIndexName': 'runbook-index',
            'fieldMapping': {
                'vectorField': 'embedding',
                'textField': 'text',
                'metadataField': 'metadata'
            }
        }
    }
)

# å°† Knowledge Base å…³è”åˆ° Agent
bedrock.associate_agent_knowledge_base(
    agentId=agent_id,
    agentVersion='DRAFT',
    knowledgeBaseId='KB_ID',
    description='äº‹ä»¶å“åº” Runbook è‡ªåŠ¨å‚è€ƒ',
    knowledgeBaseState='ENABLED'
)
```

**Runbook ç¤ºä¾‹ (å­˜å‚¨åœ¨ Knowledge Base ä¸­)**ï¼š

```markdown
# Runbook: OOMKilled Pod æ¢å¤

## ç—‡çŠ¶
- Pod Status: OOMKilled
- Event Reason: OOMKilled
- Container Exit Code: 137

## æ ¹æœ¬åŸå› åˆ†æ
1. æ£€æŸ¥å†…å­˜ä½¿ç”¨é‡è¶‹åŠ¿ (è¿‡å»24å°æ—¶)
2. æ£€æŸ¥å†…å­˜æ³„æ¼æ¨¡å¼ (æ¸è¿›å¢é•¿ vs çªå¢)
3. åœ¨æ—¥å¿—ä¸­æ£€æŸ¥å¤§æ•°æ®é‡å¤„ç†

## è‡ªåŠ¨æ¢å¤æ“ä½œ
1. ä¸´æ—¶æªæ–½: memory limits å¢åŠ 2å€ (æœ€å¤§ 4Gi)
2. Pod é‡å¯
3. å†…å­˜ä½¿ç”¨é‡ç›‘æ§ (30åˆ†é’Ÿ)

## æ ¹æœ¬åŸå› è§£å†³
1. ç–‘ä¼¼å†…å­˜æ³„æ¼: å‡çº§è‡³å¼€å‘å›¢é˜Ÿ
2. æ•°æ®é‡å¢é•¿: å»ºè®®åº”ç”¨ VPA
3. limits è®¾ç½®ä¸å½“: å»ºè®® Right-sizing
```

#### 5.6.6 EventBridge é›†æˆ â€” è‡ªåŠ¨è§¦å‘

```json
{
  "source": ["aws.cloudwatch"],
  "detail-type": ["CloudWatch Alarm State Change"],
  "detail": {
    "alarmName": [{"prefix": "EKS-"}],
    "state": {
      "value": ["ALARM"]
    }
  }
}
```

**Lambda å‡½æ•° â€” è°ƒç”¨ Bedrock Agent**ï¼š

```python
import boto3
import json

bedrock_runtime = boto3.client('bedrock-agent-runtime', region_name='ap-northeast-2')

def lambda_handler(event, context):
    alarm_name = event['detail']['alarmName']
    alarm_description = event['detail']['alarmDescription']

    # è°ƒç”¨ Bedrock Agent
    response = bedrock_runtime.invoke_agent(
        agentId='AGENT_ID',
        agentAliasId='PROD',
        sessionId=f"incident-{alarm_name}-{event['time']}",
        inputText=f"""
        CloudWatch å‘Šè­¦å·²è§¦å‘ã€‚

        å‘Šè­¦åç§°: {alarm_name}
        æè¿°: {alarm_description}
        å‘ç”Ÿæ—¶é—´: {event['time']}

        è¯·è‡ªåŠ¨è¯Šæ–­å¹¶æ¢å¤æ­¤äº‹ä»¶ã€‚
        æ‰€æœ‰æ“ä½œè¯·æŠ¥å‘Šåˆ° Slack #incidents é¢‘é“ã€‚
        """
    )

    return {
        'statusCode': 200,
        'body': json.dumps('Agent invoked successfully')
    }
```

#### 5.6.7 Kagent + Bedrock Agent æ··åˆæ¨¡å¼

ç»“åˆ Kagentï¼ˆK8s åŸç”Ÿï¼‰å’Œ Bedrock Agentï¼ˆAWS åŸç”Ÿï¼‰ï¼Œå¯ä»¥å®ç°æœ€ä½³çš„è‡ªä¸»è¿è¥ã€‚

| æ–¹é¢ | Kagent | Bedrock Agent | æ¨èç”¨é€” |
|------|--------|---------------|----------|
| **éƒ¨ç½²æ–¹å¼** | Kubernetes CRD | AWS æœåŠ¡ | Kagent: é›†ç¾¤å†…æ“ä½œ<br/>Bedrock: AWS èµ„æºæ“ä½œ |
| **æƒé™æ§åˆ¶** | RBAC | IAM + Guardrails | Kagent: Pod/Deployment<br/>Bedrock: RDS/SQS/Lambda |
| **ä¸Šä¸‹æ–‡** | ç›´æ¥è®¿é—® K8s API | é€šè¿‡ Action Groups è®¿é—® | Kagent: K8s äº‹ä»¶ä¼˜å…ˆ<br/>Bedrock: CloudWatch ä¼˜å…ˆ |
| **å®‰å…¨æœºåˆ¶** | RBAC + NetworkPolicy | Guardrails + Word Policy | ä¸¤è€…åŒæ—¶ä½¿ç”¨ |
| **Knowledge Base** | ConfigMap/Custom | OpenSearch Serverless | Bedrock: å¤§è§„æ¨¡ Runbook |
| **è´¹ç”¨** | ä»…åŸºç¡€è®¾æ–½è´¹ç”¨ | Bedrock API è°ƒç”¨è´¹ç”¨ | Kagent: é¢‘ç¹æ“ä½œ<br/>Bedrock: å¤æ‚åˆ†æ |

**æ··åˆæ¨¡å¼ç¤ºä¾‹**ï¼š

```yaml
# Kagent: K8s èµ„æºè‡ªåŠ¨æ¢å¤
apiVersion: kagent.dev/v1alpha1
kind: Agent
metadata:
  name: k8s-remediation
spec:
  triggers:
    - type: kubernetes-event
      filter:
        reason: ["OOMKilled", "CrashLoopBackOff"]
  tools:
    - name: kubectl
      type: kmcp
  workflow: |
    ## K8s èµ„æºè‡ªåŠ¨æ¢å¤
    1. Pod é‡å¯
    2. HPA è°ƒæ•´
    3. VPA åº”ç”¨
    4. è°ƒç”¨ Bedrock Agent (éœ€è¦ AWS èµ„æºæ“ä½œæ—¶)
---
# EventBridge Rule: CloudWatch â†’ Bedrock Agent
{
  "source": ["aws.cloudwatch"],
  "detail-type": ["CloudWatch Alarm State Change"],
  "detail": {
    "alarmName": [{"prefix": "RDS-"}, {"prefix": "SQS-"}]
  }
}
```

**é›†æˆå·¥ä½œæµ**ï¼š

```
[äº‹ä»¶å‘ç”Ÿ]
      â†“
[K8s Event?]  YES â†’ Kagent è‡ªåŠ¨å“åº” (Pod/Deployment æ“ä½œ)
      â†“ NO
[CloudWatch Alarm?]  YES â†’ è°ƒç”¨ Bedrock Agent (AWS èµ„æºæ“ä½œ)
      â†“
[éœ€è¦å¤æ‚æ ¹å› åˆ†æ?]
      â†“ YES
Bedrock Agent å‚è€ƒ Knowledge Base â†’ è‡ªåŠ¨åº”ç”¨ Runbook
      â†“
[Kagent + Bedrock Agent åä½œ]
Kagent: K8s èµ„æºæ¢å¤
Bedrock Agent: RDS/SQS/Lambda è°ƒæ•´ + Slack æŠ¥å‘Š
```

:::info Bedrock AgentCore çš„æ ¸å¿ƒä»·å€¼
Bedrock AgentCore é€šè¿‡ **guardrails** å’Œ **action groups**ï¼Œå¯ä»¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®‰å…¨åœ°å®ç°å®Œå…¨è‡ªä¸»è¿è¥ã€‚å¦‚æœè¯´ Kagent/Strands æ˜¯ K8s åŸç”Ÿæ–¹æ³•ï¼Œé‚£ä¹ˆ Bedrock AgentCore å°±æ˜¯ AWS åŸç”Ÿæ–¹æ³•ï¼Œå¯ä»¥**å°† AWS èµ„æºï¼ˆRDSã€SQSã€Lambdaï¼‰çº³å…¥ç»Ÿä¸€è‡ªåŠ¨åŒ–**ã€‚é€šè¿‡ **Knowledge Base é›†æˆ**ï¼Œè‡ªåŠ¨å‚è€ƒå†å² Runbookï¼Œå­¦ä¹ å¹¶å¤ç°äººç±»è¿è¥äººå‘˜çš„å†³ç­–æ¨¡å¼ã€‚
:::

#### 5.7.1 Node Readiness Controller ä¸é¢„æµ‹æ€§èŠ‚ç‚¹ç®¡ç†

**Node Readiness Controller(NRC)** æ˜¯ Kubernetes 1.33+ æä¾›çš„èŠ‚ç‚¹å°±ç»ªçŠ¶æ€è‡ªåŠ¨ç®¡ç†å·¥å…·ã€‚å®ƒæ£€æµ‹èŠ‚ç‚¹æ¡ä»¶ (Node Condition) å˜åŒ–å¹¶è‡ªåŠ¨æ‰§è¡Œ taint/cordon æ“ä½œï¼Œæ˜¯**ä»å“åº”å¼è¿è¥è½¬å‘é¢„æµ‹å¼è¿è¥**çš„æ ¸å¿ƒè¦ç´ ã€‚

**NRC åœ¨é¢„æµ‹æ€§è¿è¥ä¸­çš„è§’è‰²**ï¼š

```
[å“åº”å¼è¿è¥]
èŠ‚ç‚¹æ•…éšœå‘ç”Ÿ â†’ æ‰‹åŠ¨ kubectl cordon â†’ æ‰‹åŠ¨ drain â†’ æ‰‹åŠ¨æ¢å¤
â€¢ æ£€æµ‹å»¶è¿Ÿ: 5-10åˆ†é’Ÿ
â€¢ æ‰‹åŠ¨å¹²é¢„: å¿…éœ€
â€¢ MTTR: 20-30åˆ†é’Ÿ

[åŸºäº NRC çš„åŠè‡ªåŠ¨è¿è¥]
Node Condition å˜åŒ– â†’ NRC è‡ªåŠ¨åº”ç”¨ taint â†’ é˜»æ­¢æ–° Pod è°ƒåº¦
â€¢ æ£€æµ‹å»¶è¿Ÿ: 30ç§’
â€¢ æ‰‹åŠ¨å¹²é¢„: ä»…æ¢å¤æ—¶éœ€è¦
â€¢ MTTR: 5-10åˆ†é’Ÿ

[AI + NRC é¢„æµ‹æ€§è¿è¥]
AI é¢„æµ‹æ•…éšœ â†’ é¢„å…ˆæ›´æ–° Node Condition â†’ NRC åº”ç”¨ proactive taint
â€¢ æ£€æµ‹å»¶è¿Ÿ: 0åˆ†é’Ÿ (é¢„æµ‹)
â€¢ æ‰‹åŠ¨å¹²é¢„: æ— 
â€¢ MTTR: 2-5åˆ†é’Ÿ (é¢„å…ˆç–æ•£)
```

**Continuous æ¨¡å¼å’Œè‡ªåŠ¨æ¢å¤å¾ªç¯**ï¼š

NRC æ”¯æŒ **Continuous æ¨¡å¼**ï¼Œå½“ Node Condition æ¢å¤æ—¶è‡ªåŠ¨ç§»é™¤ taintã€‚

```yaml
apiVersion: nrc.k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: gpu-driver-health
spec:
  mode: Continuous  # æ ¸å¿ƒ: è‡ªåŠ¨æ¢å¤
  conditions:
    - type: GPUDriverHealthy
      status: "False"
  action:
    taint:
      key: gpu-driver-unhealthy
      effect: NoSchedule
```

**è‡ªåŠ¨æ¢å¤åºåˆ—**ï¼š

```mermaid
graph LR
    A[GPU é©±åŠ¨å´©æºƒ] --> B[NPD è®¾ç½® Condition False]
    B --> C[NRC åº”ç”¨ NoSchedule taint]
    C --> D[é˜»æ­¢æ–° Pod è°ƒåº¦]
    D --> E[é©±åŠ¨è‡ªåŠ¨é‡å¯]
    E --> F[NPD è®¾ç½® Condition True]
    F --> G[NRC ç§»é™¤ taint]
    G --> H[æœåŠ¡æ¢å¤æ­£å¸¸]
```

**å®é™…åœºæ™¯: GPU èŠ‚ç‚¹è‡ªåŠ¨æ¢å¤**ï¼š

```bash
# 1. æ•…éšœæ£€æµ‹ (NPD æ£€æµ‹åˆ° GPU é©±åŠ¨å´©æºƒ)
kubectl get node gpu-node-1 -o jsonpath='{.status.conditions[?(@.type=="GPUDriverHealthy")]}'
# Output: {"type":"GPUDriverHealthy","status":"False","reason":"DriverCrash"}

# 2. NRC è‡ªåŠ¨åº”ç”¨ taint (30ç§’å†…)
kubectl describe node gpu-node-1 | grep Taints
# Output: gpu-driver-unhealthy:NoSchedule

# 3. é©±åŠ¨è‡ªåŠ¨æ¢å¤ (DaemonSet watchdog)
kubectl logs -n kube-system nvidia-driver-watchdog-xxx
# Output: "Restarting nvidia-driver.service..."

# 4. NPD æ£€æµ‹åˆ°æ¢å¤
kubectl get node gpu-node-1 -o jsonpath='{.status.conditions[?(@.type=="GPUDriverHealthy")]}'
# Output: {"type":"GPUDriverHealthy","status":"True","reason":"DriverHealthy"}

# 5. NRC è‡ªåŠ¨ç§»é™¤ taint
kubectl describe node gpu-node-1 | grep Taints
# Output: <none>
```

**æ ¸å¿ƒ: æ— éœ€æ‰‹åŠ¨å¹²é¢„çš„å®Œå…¨è‡ªåŠ¨æ¢å¤**ã€‚

**Chaos Engineering é›†æˆ**ï¼š

NRC ä¸ Chaos Engineering ç»“åˆï¼Œå¯ä»¥**é¢„å…ˆéªŒè¯æ•…éšœå“åº”èƒ½åŠ›**ã€‚

```yaml
# AWS FIS Experiment: èŠ‚ç‚¹æ•…éšœæ¨¡æ‹Ÿ
apiVersion: fis.aws.amazon.com/v1
kind: ExperimentTemplate
metadata:
  name: nrc-response-test
spec:
  description: "æµ‹é‡ NRC çš„è‡ªåŠ¨ taint å“åº”é€Ÿåº¦"
  actions:
    - name: inject-node-condition-failure
      actionId: aws:eks:inject-node-condition
      parameters:
        nodeSelector: gpu=true
        conditionType: GPUDriverHealthy
        conditionStatus: "False"
        duration: PT5M
  stopConditions:
    - source: aws:cloudwatch:alarm
      value: arn:aws:cloudwatch:...:alarm/pod-eviction-rate-high
  targets:
    - resourceType: aws:eks:node
      selectionMode: COUNT(1)
      resourceTags:
        gpu: "true"
```

**NRC dry-run æ¨¡å¼é¢„å…ˆè¯„ä¼°å½±å“èŒƒå›´**ï¼š

```yaml
apiVersion: nrc.k8s.io/v1alpha1
kind: NodeReadinessRule
metadata:
  name: memory-pressure-dryrun
spec:
  mode: DryRun  # ä¸å®é™…åº”ç”¨ taintï¼Œä»…è®°å½•æ—¥å¿—
  conditions:
    - type: MemoryPressure
      status: "True"
  action:
    taint:
      key: memory-pressure
      effect: NoExecute  # æ¨¡æ‹Ÿå¼ºåˆ¶ Pod ç»ˆæ­¢
```

```bash
# DryRun æ¨¡å¼ç»“æœåˆ†æ
kubectl logs -n kube-system node-readiness-controller | grep "DryRun"
# Output:
# [DryRun] Would apply taint to node-1: memory-pressure:NoExecute
# [DryRun] 15 pods would be evicted: [payment-service-xxx, order-service-yyy, ...]
# [DryRun] Estimated MTTR: 45 seconds
```

**AI å­¦ä¹ å†å² NRC äº‹ä»¶æ¨¡å¼ â†’ æ”¹è¿›æ•…éšœé¢„æµ‹æ¨¡å‹**ï¼š

```python
# CloudWatch Logs Insights: NRC taint æ¨¡å¼åˆ†æ
query = """
fields @timestamp, node_name, condition_type, taint_key, pods_affected
| filter action = "taint_applied"
| stats count() by condition_type, bin(1h)
"""

# AI å­¦ä¹ æ•°æ®é›†ç”Ÿæˆ
import pandas as pd

nrc_events = cloudwatch_logs.query(query)
df = pd.DataFrame(nrc_events)

# æ•…éšœé¢„æµ‹æ¨¡å‹è¾“å…¥ç‰¹å¾
features = [
    'condition_type',           # GPUDriverHealthy, MemoryPressure, DiskPressure
    'taint_frequency_1h',       # è¿‡å»1å°æ—¶ taint é¢‘ç‡
    'node_age_days',            # èŠ‚ç‚¹åˆ›å»ºåç»è¿‡çš„å¤©æ•°
    'pods_affected_avg',        # å¹³å‡å—å½±å“çš„ Pod æ•°é‡
]

# åŸºäº Prophet çš„æ•…éšœé¢„æµ‹
model = Prophet()
model.fit(df[['timestamp', 'taint_frequency_1h']].rename(columns={'timestamp': 'ds', 'taint_frequency_1h': 'y'}))
forecast = model.predict(future)

# é¢„æµ‹ç»“æœ â†’ é¢„å…ˆæ›´æ–° Node Condition
if forecast['yhat'].iloc[-1] > threshold:
    k8s.patch_node_condition(
        node_name='gpu-node-1',
        condition_type='GPUDriverHealthy',
        status='False',
        reason='PredictedFailure'
    )
    # NRC è‡ªåŠ¨åº”ç”¨ proactive taint
```

**Karpenter + NRC è‡ªä¸»èŠ‚ç‚¹ç®¡ç†**ï¼š

ç»“åˆ NRC å’Œ Karpenterï¼Œå¯ä»¥å®ç°**å®Œå…¨è‡ªä¸»çš„èŠ‚ç‚¹ç”Ÿå‘½å‘¨æœŸç®¡ç†**ã€‚

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-pool
spec:
  disruption:
    consolidationPolicy: WhenEmpty
    budgets:
      - nodes: "1"
        schedule: "* * * * *"  # æ¯åˆ†é’Ÿæ£€æŸ¥
  template:
    metadata:
      labels:
        workload-type: gpu-inference
    spec:
      nodeClassRef:
        name: gpu-class
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["g5.xlarge", "g5.2xlarge"]
      taints:
        - key: gpu-not-ready
          effect: NoSchedule
          # NRC åœ¨ GPU å‡†å¤‡å®Œæˆåç§»é™¤
```

**è‡ªä¸»èŠ‚ç‚¹æ›¿æ¢åºåˆ—**ï¼š

```
1. NRC å¯¹ gpu-node-1 åº”ç”¨ taint (GPU é©±åŠ¨æ•…éšœ)
2. Karpenter è‡ªåŠ¨é¢„é…ç½®æ›¿ä»£èŠ‚ç‚¹ (gpu-node-2)
3. å¯¹ gpu-node-2 åº”ç”¨ NRC bootstrap è§„åˆ™
   â†’ GPU é©±åŠ¨åˆå§‹åŒ–å®Œæˆå‰ gpu-not-ready:NoSchedule
4. NPD ç¡®è®¤ GPU å‡†å¤‡å®Œæˆ â†’ Condition True
5. NRC ç§»é™¤ gpu-not-ready taint
6. Scheduler å°†å·¥ä½œè´Ÿè½½è¿ç§»åˆ° gpu-node-2
7. gpu-node-1 æ‰€æœ‰ Pod ç»ˆæ­¢å Karpenter åˆ é™¤èŠ‚ç‚¹
```

**å…¨è¿‡ç¨‹è‡ªåŠ¨: æ£€æµ‹ â†’ éš”ç¦» â†’ æ›¿ä»£ â†’ æ¢å¤ â†’ æ¸…ç†**

:::tip NRC + AI çš„æ ¸å¿ƒä»·å€¼
Node Readiness Controller æä¾›äº†**å“åº”å¼è‡ªåŠ¨åŒ–**ï¼Œä½†ä¸ AI ç»“åˆåè¿›åŒ–ä¸º**é¢„æµ‹å¼è‡ªåŠ¨åŒ–**ã€‚AI å­¦ä¹ å†å² NRC äº‹ä»¶æ¨¡å¼æ¥é¢„æµ‹æ•…éšœï¼ŒNRC é¢„å…ˆåº”ç”¨ taintï¼Œåœ¨**æ•…éšœå‘ç”Ÿå‰ç–æ•£å·¥ä½œè´Ÿè½½**ã€‚ä¸ Karpenter é›†æˆï¼Œå¯ä»¥å®Œå…¨è‡ªä¸»åŒ–æ•´ä¸ªèŠ‚ç‚¹ç”Ÿå‘½å‘¨æœŸã€‚
:::

**å‚è€ƒ**: [Introducing Node Readiness Controller](https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/)

---

## 6. Kiro ç¨‹åºåŒ–è°ƒè¯•

### 6.1 æŒ‡ä»¤å¼ vs ç¨‹åºåŒ–å“åº”å¯¹æ¯”

```
[æŒ‡ä»¤å¼å“åº”] â€” æ‰‹åŠ¨ã€é‡å¤ã€æˆæœ¬é«˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  è¿ç»´äººå‘˜: "payment-service å‡ºç° 500 é”™è¯¯"
  AI:       "åœ¨å“ªä¸ª Pod ä¸Šå‘ç”Ÿçš„ï¼Ÿ"
  è¿ç»´äººå‘˜: "payment-xxx Pod"
  AI:       "è¯·ç»™æˆ‘çœ‹æ—¥å¿—"
  è¿ç»´äººå‘˜: (æ‰§è¡Œ kubectl logs åå¤åˆ¶ç²˜è´´)
  AI:       "çœ‹èµ·æ¥æ˜¯ DB è¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ RDS çŠ¶æ€"
  è¿ç»´äººå‘˜: (åœ¨ AWS æ§åˆ¶å°æ£€æŸ¥ RDS)
  ...é‡å¤...

  æ€»è€—æ—¶: 15-30åˆ†é’Ÿï¼Œå¤§é‡æ‰‹åŠ¨æ“ä½œ

[ç¨‹åºåŒ–å“åº”] â€” è‡ªåŠ¨ã€ç³»ç»ŸåŒ–ã€æˆæœ¬é«˜æ•ˆ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  å‘Šè­¦: "payment-service å‡ºç° 500 é”™è¯¯"

  Kiro Spec:
    1. é€šè¿‡ EKS MCP æŸ¥è¯¢ Pod çŠ¶æ€
    2. æ”¶é›†å¹¶åˆ†æé”™è¯¯æ—¥å¿—
    3. æ£€æŸ¥ç›¸å…³ AWS æœåŠ¡ (RDS, SQS) çŠ¶æ€
    4. è¯Šæ–­æ ¹æœ¬åŸå› 
    5. è‡ªåŠ¨ç”Ÿæˆä¿®å¤ä»£ç 
    6. åˆ›å»º PR å¹¶éªŒè¯

  æ€»è€—æ—¶: 2-5åˆ†é’Ÿï¼Œè‡ªåŠ¨åŒ–
```

### 6.2 Kiro + MCP è°ƒè¯•å·¥ä½œæµ

```mermaid
graph TD
    subgraph Trigger2["ğŸ”” é—®é¢˜æ£€æµ‹"]
        ALERT["CloudWatch<br/>Alarm"]
        GURU["DevOps Guru<br/>Insight"]
    end

    subgraph Kiro2["ğŸ¤– Kiro (ç¨‹åºåŒ–)"]
        SPEC["åŸºäº Spec çš„<br/>è¯Šæ–­è®¡åˆ’"]
        MCP_Q["MCP é›†æˆ<br/>æ•°æ®æ”¶é›†"]
        ANALYZE2["AI åˆ†æ<br/>æ ¹æœ¬åŸå› "]
        FIX["ä¿®å¤ä»£ç <br/>è‡ªåŠ¨ç”Ÿæˆ"]
        PR["åˆ›å»º PR<br/>+ éªŒè¯"]
    end

    subgraph Deploy2["ğŸš€ éƒ¨ç½²"]
        REVIEW["AI å®¡æŸ¥<br/>+ æ‰¹å‡†"]
        ARGO2["Argo CD<br/>è‡ªåŠ¨éƒ¨ç½²"]
        VERIFY["éƒ¨ç½²å<br/>éªŒè¯"]
    end

    ALERT --> SPEC
    GURU --> SPEC
    SPEC --> MCP_Q --> ANALYZE2 --> FIX --> PR
    PR --> REVIEW --> ARGO2 --> VERIFY
    VERIFY -.->|é—®é¢˜æŒç»­| SPEC

    style Kiro2 fill:#e3f2fd,stroke:#2196f3
```

### 6.3 å…·ä½“åœºæ™¯: OOMKilled è‡ªåŠ¨å“åº”

```
[Kiro ç¨‹åºåŒ–è°ƒè¯•: OOMKilled]

1. æ£€æµ‹: payment-service Pod OOMKilled äº‹ä»¶

2. Kiro Spec æ‰§è¡Œ:
   â†’ EKS MCP: get_events(namespace="payment", reason="OOMKilled")
   â†’ EKS MCP: get_pod_logs(pod="payment-xxx", previous=true)
   â†’ CloudWatch MCP: query_metrics("pod_memory_utilization", last="1h")

3. AI åˆ†æ:
   "payment-service çš„å†…å­˜ä½¿ç”¨é‡åœ¨å¯åŠ¨åæ¯2å°æ—¶å¢åŠ  256Miï¼Œ
    æ£€æµ‹åˆ°å†…å­˜æ³„æ¼æ¨¡å¼ã€‚
    æ—¥å¿—ä¸­ç¡®è®¤ Redis è¿æ¥æœªæ­£ç¡®å…³é—­ã€‚"

4. è‡ªåŠ¨ä¿®å¤:
   - memory limits 256Mi â†’ 512Mi (ä¸´æ—¶æªæ–½)
   - Redis è¿æ¥æ± æ¸…ç†ä»£ç è¡¥ä¸ç”Ÿæˆ
   - æ·»åŠ å†…å­˜åˆ†æé…ç½®

5. PR åˆ›å»º:
   Title: "fix: payment-service Redis connection leak"
   - deployment.yaml: memory limits è°ƒæ•´
   - redis_client.go: æ·»åŠ  defer conn.Close()
   - monitoring: æ·»åŠ å†…å­˜ä½¿ç”¨é‡ä»ªè¡¨æ¿
```

:::tip ç¨‹åºåŒ–è°ƒè¯•çš„æ ¸å¿ƒ
é€šè¿‡ Kiro + EKS MCP **ä»¥ç¨‹åºåŒ–æ–¹å¼åˆ†æå’Œè§£å†³**é—®é¢˜ã€‚ç›¸æ¯”æŒ‡ä»¤å¼çš„æ‰‹åŠ¨å“åº”ï¼Œå¯ä»¥å®ç°**æˆæœ¬é«˜æ•ˆä¸”å¿«é€Ÿçš„è‡ªåŠ¨åŒ–**ï¼Œå½“ç›¸åŒé—®é¢˜é‡å¤å‡ºç°æ—¶å¯ä»¥å¤ç”¨å·²å­¦ä¹ çš„ Specã€‚
:::

---

## 7. AI Right-Sizing

### 7.1 åŸºäº Container Insights çš„æ¨è

CloudWatch Container Insights åˆ†æ Pod çš„å®é™…èµ„æºä½¿ç”¨æ¨¡å¼ï¼Œæ¨èé€‚å½“çš„å¤§å°ã€‚

```promql
# å®é™… CPU ä½¿ç”¨é‡ vs requests å¯¹æ¯”
avg(rate(container_cpu_usage_seconds_total{namespace="payment"}[1h]))
  by (pod)
/ avg(kube_pod_container_resource_requests{resource="cpu", namespace="payment"})
  by (pod)
* 100

# å®é™… Memory ä½¿ç”¨é‡ vs requests å¯¹æ¯”
avg(container_memory_working_set_bytes{namespace="payment"})
  by (pod)
/ avg(kube_pod_container_resource_requests{resource="memory", namespace="payment"})
  by (pod)
* 100
```

### 7.2 VPA + ML è‡ªåŠ¨ Right-Sizing

```yaml
# VPA (Vertical Pod Autoscaler) é…ç½®
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: payment-service-vpa
  namespace: payment
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: payment-service
  updatePolicy:
    updateMode: "Auto"  # Off, Initial, Auto
  resourcePolicy:
    containerPolicies:
      - containerName: app
        minAllowed:
          cpu: 100m
          memory: 128Mi
        maxAllowed:
          cpu: "2"
          memory: 4Gi
        controlledResources: ["cpu", "memory"]
```

### 7.3 Right-Sizing æ•ˆæœ

<RightSizingResults />

:::tip K8s 1.35: In-Place Pod Resource Updates
ä» K8s 1.35ï¼ˆ2026.01ï¼ŒEKS æ”¯æŒï¼‰èµ·ï¼Œå¼•å…¥äº† **In-Place Pod Resource Updates** åŠŸèƒ½ï¼Œå¯ä»¥åœ¨ä¸é‡å¯ Pod çš„æƒ…å†µä¸‹åŠ¨æ€è°ƒæ•´ CPU å’Œå†…å­˜ã€‚è¿™è§£å†³äº† VPA æœ€å¤§çš„é™åˆ¶â€”â€”"èµ„æºå˜æ›´æ—¶ Pod é‡å¯"çš„é—®é¢˜ã€‚å³ä½¿æ˜¯ StatefulSet æˆ–å¯¹é‡å¯æ•æ„Ÿçš„å·¥ä½œè´Ÿè½½ï¼Œä¹Ÿå¯ä»¥å®‰å…¨åœ°è¿›è¡Œå‚ç›´æ‰©ç¼©å®¹ã€‚
:::

:::warning VPA æ³¨æ„äº‹é¡¹ (K8s 1.34 åŠä»¥ä¸‹)
åœ¨ K8s 1.34 åŠä»¥ä¸‹ç‰ˆæœ¬ä¸­ï¼ŒVPA `Auto` æ¨¡å¼é€šè¿‡é‡å¯ Pod æ¥è°ƒæ•´èµ„æºã€‚å¯¹äº StatefulSet æˆ–å¯¹é‡å¯æ•æ„Ÿçš„å·¥ä½œè´Ÿè½½ï¼Œå»ºè®®ä½¿ç”¨ `Off` æ¨¡å¼ä»…æŸ¥çœ‹æ¨èå€¼ï¼Œæ‰‹åŠ¨åº”ç”¨æ›´å®‰å…¨ã€‚åŒæ—¶ä½¿ç”¨ç›¸åŒæŒ‡æ ‡(CPU/Memory)çš„ VPA å’Œ HPA å¯èƒ½ä¼šäº§ç”Ÿå†²çªã€‚
:::

### 7.4 In-Place Pod Vertical Scaling (K8s 1.33+)

ä» Kubernetes 1.33 èµ·ï¼Œ**In-Place Pod Vertical Scaling** è¿›å…¥ Beta é˜¶æ®µï¼ŒVPA æœ€å¤§çš„ç¼ºç‚¹â€”â€”**Pod é‡å¯é—®é¢˜**å¾—ä»¥è§£å†³ã€‚ç°åœ¨å¯ä»¥åœ¨ä¸é‡å¯çš„æƒ…å†µä¸‹åŠ¨æ€è°ƒæ•´è¿è¡Œä¸­ Pod çš„ CPU å’Œå†…å­˜ã€‚

#### In-Place Pod Resize æ¦‚è¿°

ç°æœ‰ VPA çš„é—®é¢˜ï¼š
- Pod èµ„æºå˜æ›´æ—¶**å¿…é¡»é‡å¯**
- åœ¨ StatefulSetã€æ•°æ®åº“ã€ç¼“å­˜ç­‰**éœ€è¦ä¿æŒçŠ¶æ€çš„å·¥ä½œè´Ÿè½½**ä¸­éš¾ä»¥ä½¿ç”¨
- é‡å¯æœŸé—´å¯èƒ½å¯¼è‡´æœåŠ¡ä¸­æ–­
- ä¸ PDBï¼ˆPod Disruption Budgetï¼‰å†²çª

In-Place Resize çš„è§£å†³æ–¹æ¡ˆï¼š
- **åŠ¨æ€è°ƒæ•´è¿è¡Œä¸­ Pod çš„èµ„æº**
- å®æ—¶å˜æ›´ cgroup é™åˆ¶
- æ— éœ€é‡å¯å³å¯å¢å‡èµ„æº
- **ä¿æŒ QoS Class æ—¶**æ— éœ€é‡å¯

#### Kubernetes ç‰ˆæœ¬çŠ¶æ€

| Kubernetes ç‰ˆæœ¬ | çŠ¶æ€ | Feature Gate | å¤‡æ³¨ |
|----------------|------|--------------|------|
| 1.27 | Alpha | `InPlacePodVerticalScaling` | å®éªŒæ€§åŠŸèƒ½ |
| 1.33 | Beta | é»˜è®¤å¯ç”¨ | æ¨èç”Ÿäº§æµ‹è¯• |
| 1.35+ (é¢„è®¡) | Stable | é»˜è®¤å¯ç”¨ | ç”Ÿäº§å®‰å…¨ä½¿ç”¨ |

**EKS æ”¯æŒçŠ¶æ€**ï¼š
- **EKS 1.33**ï¼ˆ2026å¹´4æœˆé¢„è®¡ï¼‰ï¼šå¯å¯ç”¨ Beta åŠŸèƒ½
- **EKS 1.35**ï¼ˆ2026å¹´11æœˆé¢„è®¡ï¼‰ï¼šæ”¯æŒ Stable ç‰ˆæœ¬

EKS ä¸­å¯ç”¨ Feature Gate çš„æ–¹æ³• (1.33 Beta)ï¼š
```bash
# EKS é›†ç¾¤åˆ›å»ºæ—¶å¯ç”¨ Feature Gate (è®¡åˆ’ä¸­)
aws eks create-cluster \
  --name my-cluster \
  --kubernetes-version 1.33 \
  --kubernetes-network-config '{"serviceIpv4Cidr":"10.100.0.0/16"}' \
  --role-arn arn:aws:iam::ACCOUNT_ID:role/EKSClusterRole \
  --resources-vpc-config subnetIds=subnet-xxx,subnet-yyy \
  --feature-gates InPlacePodVerticalScaling=true
```

:::info EKS Feature Gate æ”¯æŒ
EKS åœ¨ Kubernetes ç‰ˆæœ¬ GA åä¸€æ®µæ—¶é—´æ‰æ”¯æŒ Feature Gateã€‚1.33 Beta åŠŸèƒ½å¯èƒ½ä¸ä¼šä¸ EKS 1.33 åŒæ—¶å¯ç”¨ï¼Œè¯·æŸ¥çœ‹ AWS å®˜æ–¹æ–‡æ¡£ã€‚
:::

#### å·¥ä½œåŸç†

In-Place Resize é€šè¿‡ **`resize` subresource** å˜æ›´è¿è¡Œä¸­ Pod çš„èµ„æºï¼š

```yaml
# Pod çš„ resize çŠ¶æ€ç¡®è®¤
apiVersion: v1
kind: Pod
metadata:
  name: payment-service-abc123
spec:
  containers:
    - name: app
      resources:
        requests:
          cpu: "1"
          memory: 2Gi
        limits:
          cpu: "2"
          memory: 4Gi
status:
  resize: InProgress  # Proposed, InProgress, Deferred, Infeasible
  containerStatuses:
    - name: app
      allocatedResources:
        cpu: "1"
        memory: 2Gi
      resources:
        requests:
          cpu: "1.5"  # æ–°çš„è¯·æ±‚å€¼
          memory: 3Gi
```

**Resize çŠ¶æ€è½¬æ¢**ï¼š

```
Proposed (å·²æè®®)
  â†“
InProgress (è¿›è¡Œä¸­) â€” kubelet å˜æ›´ cgroup é™åˆ¶
  â†“
[æˆåŠŸ] Pod.spec.resources == Pod.status.allocatedResources
  æˆ–
[å¤±è´¥] Deferred (å·²å»¶è¿Ÿ) â€” èµ„æºä¸è¶³ï¼Œç¨åé‡è¯•
  æˆ–
[å¤±è´¥] Infeasible (ä¸å¯è¡Œ) â€” éœ€è¦å˜æ›´ QoS Classï¼Œéœ€è¦é‡å¯
```

#### VPA Auto æ¨¡å¼é›†æˆ

å½“ In-Place Resize å¯ç”¨æ—¶ï¼ŒVPA **è‡ªåŠ¨åœ¨ä¸é‡å¯çš„æƒ…å†µä¸‹è°ƒæ•´èµ„æº**ï¼š

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: payment-service-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: payment-service
  updatePolicy:
    updateMode: "Auto"  # æ”¯æŒ In-Place Resize æ—¶æ— éœ€é‡å¯å³å¯è°ƒæ•´
  resourcePolicy:
    containerPolicies:
      - containerName: app
        minAllowed:
          cpu: 100m
          memory: 128Mi
        maxAllowed:
          cpu: "4"
          memory: 8Gi
        controlledResources: ["cpu", "memory"]
        mode: Auto  # è‡ªåŠ¨åº”ç”¨ In-Place Resize
```

**VPA åŠ¨ä½œæµç¨‹**ï¼š

```mermaid
graph TD
    A[VPA Recommender] -->|èµ„æºæ¨è| B{In-Place Resize å¯è¡Œ?}
    B -->|Yes| C[æ›´æ–° resize subresource]
    B -->|No| D[é‡æ–°åˆ›å»º Pod]
    C --> E[kubelet å˜æ›´ cgroup]
    E --> F[Pod ç»§ç»­è¿è¡Œ]
    D --> G[Pod é‡å¯]
```

#### çº¦æŸæ¡ä»¶

1. **CPU å¯è‡ªç”± resize**
   - CPU sharesã€CPU quota å¯åŠ¨æ€å˜æ›´
   - cgroup CPU æ§åˆ¶å™¨æ”¯æŒå®æ—¶å˜æ›´

2. **Memory åªèƒ½å¢åŠ ï¼Œä¸èƒ½å‡å°‘**
   - ç”±äº Linux cgroup v1/v2 é™åˆ¶ï¼Œå†…å­˜ limit **å‡å°‘æ—¶éœ€è¦é‡å¯**
   - å†…å­˜å¢åŠ æ˜¯å¯ä»¥çš„ï¼ˆcgroup memory.limit_in_bytes å¢åŠ ï¼‰
   - å†…å­˜å‡å°‘è½¬æ¢ä¸º Infeasible çŠ¶æ€ â†’ éœ€è¦é‡æ–°åˆ›å»º Pod

```yaml
# Memory å¢åŠ : In-Place Resize å¯è¡Œ âœ…
resources:
  requests:
    memory: 2Gi â†’ 4Gi  # OKï¼Œæ— éœ€é‡å¯

# Memory å‡å°‘: In-Place Resize ä¸å¯è¡Œ âŒ
resources:
  requests:
    memory: 4Gi â†’ 2Gi  # Infeasibleï¼Œéœ€è¦é‡æ–°åˆ›å»º Pod
```

3. **QoS Class å˜æ›´æ—¶éœ€è¦é‡å¯**

QoS Class å†³å®š Pod çš„èµ„æºä¿éšœçº§åˆ«ï¼Œå˜æ›´æ—¶éœ€è¦é‡å¯ï¼š

| åŸ QoS | æ–° QoS | In-Place Resize å¯è¡Œ? |
|----------|------------|---------------------|
| Guaranteed | Guaranteed | âœ… å¯è¡Œ (ä¿æŒ requests == limits) |
| Burstable | Burstable | âœ… å¯è¡Œ |
| BestEffort | BestEffort | âœ… å¯è¡Œ |
| Guaranteed | Burstable | âŒ ä¸å¯è¡Œ (éœ€è¦é‡å¯) |
| Burstable | Guaranteed | âŒ ä¸å¯è¡Œ (éœ€è¦é‡å¯) |

```yaml
# QoS Class ä¿æŒ: In-Place Resize å¯è¡Œ âœ…
# Guaranteed â†’ Guaranteed
resources:
  requests:
    cpu: "1"
    memory: 2Gi
  limits:
    cpu: "1"    # ä¿æŒ requests == limits
    memory: 2Gi
# â†’ (å˜æ›´å)
resources:
  requests:
    cpu: "2"
    memory: 4Gi
  limits:
    cpu: "2"    # ä¿æŒ requests == limits
    memory: 4Gi

# QoS Class å˜æ›´: In-Place Resize ä¸å¯è¡Œ âŒ
# Guaranteed â†’ Burstable
resources:
  requests:
    cpu: "1"
    memory: 2Gi
  limits:
    cpu: "1"
    memory: 2Gi
# â†’ (å˜æ›´å)
resources:
  requests:
    cpu: "1"
    memory: 2Gi
  limits:
    cpu: "2"    # requests != limits â†’ QoS å˜æ›´
    memory: 4Gi
# â†’ Infeasibleï¼Œéœ€è¦é‡æ–°åˆ›å»º Pod
```

#### StatefulSet çš„å®‰å…¨å‚ç›´æ‰©ç¼©å®¹æ¨¡å¼

StatefulSet éœ€è¦ä¿æŒçŠ¶æ€ï¼Œå› æ­¤å¿…é¡»åº”ç”¨åŸºäº In-Place Resize çš„å®‰å…¨æ¨¡å¼ï¼š

**æ¨¡å¼ 1: ä¿æŒ Guaranteed QoS**

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  replicas: 3
  template:
    spec:
      containers:
        - name: postgres
          image: postgres:15
          resources:
            requests:
              cpu: "2"
              memory: 4Gi
            limits:
              cpu: "2"    # requests == limits (Guaranteed QoS)
              memory: 4Gi
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: postgres-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: postgres
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: postgres
        minAllowed:
          cpu: "1"
          memory: 2Gi
        maxAllowed:
          cpu: "4"
          memory: 8Gi
        controlledResources: ["cpu", "memory"]
        controlledValues: RequestsAndLimits  # åŒæ—¶è°ƒæ•´ requests å’Œ limits
```

**æ¨¡å¼ 2: æ¸è¿›å¼å†…å­˜å¢åŠ  (é˜²æ­¢å‡å°‘)**

```python
# ç›‘æ§ VPA æ¨èå€¼é˜²æ­¢å†…å­˜å‡å°‘
import boto3
from kubernetes import client, config

def safe_vpa_update(namespace, statefulset_name):
    """
    æ£€æŸ¥ VPA æ¨èå€¼ï¼Œå¦‚æœéœ€è¦å‡å°‘å†…å­˜åˆ™ä»…å‘é€å‘Šè­¦ï¼Œ
    å¦‚æœéœ€è¦å¢åŠ å†…å­˜åˆ™æ‰§è¡Œ In-Place Resize
    """
    config.load_kube_config()
    v1 = client.CoreV1Api()

    # æŸ¥è¯¢å½“å‰ Pod çš„å†…å­˜ä½¿ç”¨é‡
    pods = v1.list_namespaced_pod(
        namespace=namespace,
        label_selector=f"app={statefulset_name}"
    )

    for pod in pods.items:
        current_memory = pod.spec.containers[0].resources.requests['memory']
        vpa_recommendation = get_vpa_recommendation(namespace, statefulset_name)

        if vpa_recommendation['memory'] < current_memory:
            # å†…å­˜å‡å°‘ä»…å‘é€å‘Šè­¦
            send_alert(
                f"[WARNING] {pod.metadata.name}: VPA recommends memory decrease "
                f"({current_memory} â†’ {vpa_recommendation['memory']}). "
                f"Manual Pod restart required for memory decrease."
            )
        elif vpa_recommendation['memory'] > current_memory:
            # å†…å­˜å¢åŠ æ‰§è¡Œ In-Place Resize
            apply_in_place_resize(pod.metadata.name, vpa_recommendation)
```

**æ¨¡å¼ 3: æ»šåŠ¨æ›´æ–°ä¸ In-Place Resize ç»„åˆ**

```yaml
# StatefulSet æ›´æ–°ç­–ç•¥: æ»šåŠ¨æ›´æ–° + In-Place Resize
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
spec:
  replicas: 5
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0  # æ‰€æœ‰ Pod ä½œä¸ºæ›´æ–°ç›®æ ‡
  podManagementPolicy: OrderedReady
  template:
    spec:
      containers:
        - name: cassandra
          resources:
            requests:
              cpu: "4"
              memory: 8Gi
            limits:
              cpu: "4"
              memory: 8Gi
```

**æ›´æ–°åœºæ™¯**ï¼š

1. **CPU å¢åŠ **: é€šè¿‡ In-Place Resize ç«‹å³åº”ç”¨ (æ— éœ€é‡å¯)
2. **Memory å¢åŠ **: é€šè¿‡ In-Place Resize ç«‹å³åº”ç”¨ (æ— éœ€é‡å¯)
3. **Memory å‡å°‘**: é€šè¿‡æ»šåŠ¨æ›´æ–°é€ä¸€é‡å¯ Pod (ä¿æŒ Quorum)

```bash
# å†…å­˜å‡å°‘æ—¶å®‰å…¨æ»šåŠ¨é‡å¯
kubectl rollout restart statefulset/cassandra -n database

# ç›‘æ§æ»šåŠ¨é‡å¯çŠ¶æ€
kubectl rollout status statefulset/cassandra -n database

# ç¡®è®¤é€ä¸ª Pod é‡å¯ (ä¿æŒ Quorum)
# cassandra-4 â†’ cassandra-3 â†’ cassandra-2 â†’ cassandra-1 â†’ cassandra-0
```

#### å®æˆ˜ç¤ºä¾‹: Redis é›†ç¾¤å†…å­˜å¢åŠ 

```yaml
# Redis StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
  namespace: cache
spec:
  replicas: 6
  serviceName: redis-cluster
  template:
    spec:
      containers:
        - name: redis
          image: redis:7
          resources:
            requests:
              cpu: "1"
              memory: 4Gi
            limits:
              cpu: "1"
              memory: 4Gi
---
# VPA è‡ªåŠ¨å†…å­˜å¢åŠ 
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: redis-cluster-vpa
  namespace: cache
spec:
  targetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: redis-cluster
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: redis
        minAllowed:
          memory: 4Gi
        maxAllowed:
          memory: 16Gi
        controlledResources: ["memory"]
        controlledValues: RequestsAndLimits
```

**In-Place Resize æ‰§è¡Œç»“æœ**ï¼š

```bash
# 1. VPA æ£€æµ‹åˆ°å†…å­˜å¢åŠ éœ€æ±‚
$ kubectl describe vpa redis-cluster-vpa -n cache
Recommendation:
  Container Recommendations:
    Container Name:  redis
    Target:
      Memory:  8Gi  # æ¨èä» 4Gi â†’ 8Gi å¢åŠ 

# 2. VPA è‡ªåŠ¨æ‰§è¡Œ In-Place Resize
$ kubectl get pod redis-cluster-0 -n cache -o yaml
status:
  resize: InProgress
  containerStatuses:
    - allocatedResources:
        memory: 4Gi
      resources:
        requests:
          memory: 8Gi  # æ–°çš„è¯·æ±‚å€¼

# 3. Kubelet å®Œæˆ cgroup å˜æ›´
$ kubectl get pod redis-cluster-0 -n cache -o yaml
status:
  resize: ""  # å®Œæˆåæ¸…ç©º
  containerStatuses:
    - allocatedResources:
        memory: 8Gi  # æ–°èµ„æºåˆ†é…å®Œæˆ

# 4. ç¡®è®¤æ— éœ€é‡å¯å³å®Œæˆå†…å­˜å¢åŠ 
$ kubectl exec redis-cluster-0 -n cache -- redis-cli INFO memory
used_memory:8589934592  # 8GB
maxmemory:8589934592

# 5. ç¡®è®¤ Pod æ­£å¸¸è¿è¡Œæ—¶é—´ (æ— é‡å¯)
$ kubectl get pod redis-cluster-0 -n cache
NAME              READY   STATUS    RESTARTS   AGE
redis-cluster-0   1/1     Running   0          15d  # 15å¤©æ— é‡å¯
```

:::warning In-Place Pod Vertical Scaling ä»å¤„äº Beta é˜¶æ®µ
In-Place Pod Vertical Scaling åœ¨ Kubernetes 1.33 è¿›å…¥ Beta é˜¶æ®µã€‚ç”Ÿäº§ç¯å¢ƒå»ºè®®åœ¨ **Kubernetes 1.35+ Stable åå¼•å…¥**ã€‚Beta æœŸé—´ API å¯èƒ½å‘ç”Ÿå˜æ›´ï¼ŒEKS å¯èƒ½åœ¨ Kubernetes GA åä¸€æ®µæ—¶é—´æ‰æ”¯æŒã€‚

**å»ºè®®**ï¼š
- **K8s 1.33-1.34**: åœ¨å¼€å‘/é¢„å‘å¸ƒç¯å¢ƒä¸­æµ‹è¯•
- **K8s 1.35+**: è€ƒè™‘åœ¨ç”Ÿäº§ç¯å¢ƒå¼•å…¥
- **EKS ç”¨æˆ·**: åœ¨ AWS å®˜æ–¹æ–‡æ¡£ä¸­ç¡®è®¤ Feature Gate æ”¯æŒæ—¶é—´
:::

:::tip In-Place Resize çš„æ ¸å¿ƒä»·å€¼
VPA æœ€å¤§çš„ç¼ºç‚¹â€”â€”**Pod é‡å¯é—®é¢˜**å¾—ä»¥è§£å†³ï¼ŒStatefulSetã€æ•°æ®åº“ã€ç¼“å­˜ã€ML æ¨ç†æœåŠ¡ç­‰**éœ€è¦ä¿æŒçŠ¶æ€çš„å·¥ä½œè´Ÿè½½**ä¹Ÿå¯ä»¥å®‰å…¨åœ°åº”ç”¨å‚ç›´æ‰©ç¼©å®¹ã€‚ç‰¹åˆ«æ˜¯å†…å­˜å¢åŠ å¯ä»¥æ— éœ€é‡å¯ç«‹å³ç”Ÿæ•ˆï¼Œå› æ­¤åœ¨æµé‡çªå¢æ—¶å¯ä»¥å¿«é€Ÿå“åº”ã€‚
:::

---

## 8. åé¦ˆå¾ªç¯

### 8.1 é¢„æµ‹ç²¾åº¦æµ‹é‡

```python
# é¢„æµ‹ç²¾åº¦æµ‹é‡åŠæ¨¡å‹é‡è®­ç»ƒ
import numpy as np

def calculate_accuracy(predicted, actual):
    """MAPE (Mean Absolute Percentage Error) è®¡ç®—"""
    mape = np.mean(np.abs((actual - predicted) / actual)) * 100
    return {
        'mape': mape,
        'accuracy': 100 - mape,
        'over_prediction_rate': np.mean(predicted > actual) * 100,
        'under_prediction_rate': np.mean(predicted < actual) * 100
    }

def should_retrain(accuracy_history, threshold=85):
    """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è®­ç»ƒ"""
    recent_accuracy = np.mean(accuracy_history[-10:])
    if recent_accuracy < threshold:
        return True, f"è¿‘æœŸç²¾åº¦ {recent_accuracy:.1f}% < é˜ˆå€¼ {threshold}%"
    return False, f"ç²¾åº¦è‰¯å¥½: {recent_accuracy:.1f}%"
```

### 8.2 è‡ªåŠ¨é‡è®­ç»ƒæµæ°´çº¿

```yaml
# é¢„æµ‹æ¨¡å‹è‡ªåŠ¨é‡è®­ç»ƒ CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: model-retrainer
  namespace: scaling
spec:
  schedule: "0 2 * * 0"  # æ¯å‘¨æ—¥ 02:00
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: retrainer
              image: my-registry/model-retrainer:latest
              env:
                - name: AMP_WORKSPACE_ID
                  value: "ws-xxxxx"
                - name: TRAINING_WEEKS
                  value: "4"
                - name: ACCURACY_THRESHOLD
                  value: "85"
              resources:
                requests:
                  cpu: "2"
                  memory: 4Gi
          restartPolicy: OnFailure
```

### 8.3 A/B æ‰©ç¼©å®¹æµ‹è¯•

```
[A/B æ‰©ç¼©å®¹]

ç»„ A (50% æµé‡): åŸºäº HPA çš„å“åº”å¼æ‰©ç¼©å®¹
ç»„ B (50% æµé‡): åŸºäº ML é¢„æµ‹çš„å…ˆå‘åˆ¶äººæ‰©ç¼©å®¹

æ¯”è¾ƒæŒ‡æ ‡:
  - P99 å»¶è¿Ÿå·®å¼‚
  - æ‰©ç¼©å®¹äº‹ä»¶æ¬¡æ•°
  - èµ„æºä½¿ç”¨æ•ˆç‡
  - æ€§ä»·æ¯”
```

---

## 9. Chaos Engineering + AI

### 9.1 AWS Fault Injection Service (FIS)

```json
{
  "description": "EKS Pod æ•…éšœæ³¨å…¥æµ‹è¯•",
  "targets": {
    "eks-pods": {
      "resourceType": "aws:eks:pod",
      "selectionMode": "COUNT(2)",
      "resourceTags": {
        "app": "payment-service"
      },
      "parameters": {
        "clusterIdentifier": "my-cluster",
        "namespace": "payment"
      }
    }
  },
  "actions": {
    "terminate-pods": {
      "actionId": "aws:eks:terminate-pod",
      "parameters": {},
      "targets": {
        "Pods": "eks-pods"
      }
    }
  },
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentServiceSLO"
    }
  ],
  "roleArn": "arn:aws:iam::ACCOUNT_ID:role/FISRole",
  "tags": {
    "Environment": "staging",
    "Team": "platform"
  }
}
```

### 9.2 åŸºäº AI çš„æ•…éšœæ¨¡å¼å­¦ä¹ 

Chaos Engineering å®éªŒç»“æœç”± AI å­¦ä¹ ï¼Œæå‡åº”å¯¹èƒ½åŠ›ã€‚

<ChaosExperiments />

```python
# FIS å®éªŒå AI å­¦ä¹ æ•°æ®æ”¶é›†
from strands import Agent

chaos_analyzer = Agent(
    name="chaos-pattern-analyzer",
    model="bedrock/anthropic.claude-sonnet",
    sop="""
    ## Chaos Engineering ç»“æœåˆ†æ

    1. FIS å®éªŒç»“æœæ”¶é›†
       - æ³¨å…¥çš„æ•…éšœç±»å‹
       - ç³»ç»Ÿå“åº”æ—¶é—´
       - æ¢å¤æ—¶é—´
       - å½±å“èŒƒå›´

    2. æ¨¡å¼åˆ†æ
       - æ•…éšœä¼ æ’­è·¯å¾„æ˜ å°„
       - è–„å¼±ç‚¹è¯†åˆ«
       - æ¢å¤ç“¶é¢ˆç‚¹å®šä½

    3. åº”å¯¹è§„åˆ™æ›´æ–°
       - å‘ç°æœ‰ SOP æ·»åŠ å­¦ä¹ å†…å®¹
       - ä¸ºæ–°æ¨¡å¼åˆ›å»ºåº”å¯¹è§„åˆ™
       - è°ƒæ•´å‡çº§é˜ˆå€¼

    4. æŠ¥å‘Šç”Ÿæˆ
       - å®éªŒæ‘˜è¦
       - å‘ç°çš„è–„å¼±ç‚¹
       - å»ºè®®æ”¹è¿›äº‹é¡¹
    """
)
```

:::tip Chaos Engineering + AI åé¦ˆå¾ªç¯
é€šè¿‡ FIS æ³¨å…¥æ•…éšœï¼ŒAI å­¦ä¹ ç³»ç»Ÿå“åº”æ¨¡å¼ï¼ŒAI Agent çš„è‡ªåŠ¨åº”å¯¹èƒ½åŠ›å°†æŒç»­æå‡ã€‚"æ•…éšœæ³¨å…¥ -> è§‚å¯Ÿ -> å­¦ä¹  -> åº”å¯¹æ”¹è¿›"çš„åé¦ˆå¾ªç¯æ˜¯è‡ªä¸»è¿ç»´çš„æ ¸å¿ƒã€‚
:::

### 9.4 AWS FIS æœ€æ–°åŠŸèƒ½åŠç”Ÿäº§å®‰å…¨ä¿éšœ

AWS Fault Injection Service(FIS) åœ¨ 2025-2026 å¹´åŸºå‡†ä¸Šæä¾›äº† **EKS ä¸“ç”¨åŠ¨ä½œç±»å‹**å’Œ**è‡ªåŠ¨ä¸­æ–­æœºåˆ¶**ï¼Œä½¿å¾—åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä¹Ÿèƒ½å®‰å…¨åœ°æ‰§è¡Œ Chaos Engineeringã€‚

#### FIS æœ€æ–° EKS åŠ¨ä½œç±»å‹

FIS æä¾›äº†é’ˆå¯¹ EKS å·¥ä½œè´Ÿè½½çš„ä¸“ç”¨æ•…éšœæ³¨å…¥åŠ¨ä½œï¼š

| åŠ¨ä½œç±»å‹ | è¯´æ˜ | é€‚ç”¨å¯¹è±¡ | ä½¿ç”¨åœºæ™¯ |
|----------|------|----------|----------|
| `aws:eks:pod-delete` | åˆ é™¤ç‰¹å®š Pod | Pod | Pod é‡å¯æ¢å¤åŠ›æµ‹è¯• |
| `aws:eks:pod-network-latency` | Pod ç½‘ç»œå»¶è¿Ÿæ³¨å…¥ | Pod | éªŒè¯ç½‘ç»œå»¶è¿Ÿæ—¶åº”ç”¨è¡Œä¸º |
| `aws:eks:pod-network-packet-loss` | Pod ç½‘ç»œä¸¢åŒ…æ³¨å…¥ | Pod | ä¸ç¨³å®šç½‘ç»œç¯å¢ƒæ¨¡æ‹Ÿ |
| `aws:eks:node-drain` | èŠ‚ç‚¹æ’ç©ºï¼ˆå®‰å…¨ Pod è¿ç§»ï¼‰ | Node | èŠ‚ç‚¹ç»´æŠ¤åœºæ™¯æµ‹è¯• |
| `aws:eks:terminate-nodegroup-instances` | ç»ˆæ­¢èŠ‚ç‚¹ç»„å®ä¾‹ | Node Group | å¤§è§„æ¨¡èŠ‚ç‚¹æ•…éšœæ¢å¤æµ‹è¯• |

**Pod åˆ é™¤åŠ¨ä½œè¯¦æƒ…**ï¼š

```json
{
  "actionId": "aws:eks:pod-delete",
  "description": "é€šè¿‡ EKS Pod åˆ é™¤æµ‹è¯•é‡å¯æ¢å¤åŠ›",
  "targets": {
    "Pods": "eks-payment-pods"
  },
  "parameters": {
    "kubernetesServiceAccount": "fis-experiment-role",
    "maxPodsToDelete": "2",
    "podDeletionMode": "all-at-once"
  }
}
```

**ç½‘ç»œå»¶è¿Ÿæ³¨å…¥åŠ¨ä½œ**ï¼š

```json
{
  "actionId": "aws:eks:pod-network-latency",
  "description": "Pod ç½‘ç»œå»¶è¿Ÿ 200ms æ³¨å…¥",
  "targets": {
    "Pods": "eks-payment-pods"
  },
  "parameters": {
    "kubernetesServiceAccount": "fis-experiment-role",
    "duration": "PT5M",
    "delayMilliseconds": "200",
    "jitterMilliseconds": "50",
    "sources": "all",
    "destinations": "all"
  }
}
```

**ä¸¢åŒ…æ³¨å…¥åŠ¨ä½œ**ï¼š

```json
{
  "actionId": "aws:eks:pod-network-packet-loss",
  "description": "5% ä¸¢åŒ…æ³¨å…¥",
  "targets": {
    "Pods": "eks-payment-pods"
  },
  "parameters": {
    "kubernetesServiceAccount": "fis-experiment-role",
    "duration": "PT3M",
    "lossPercent": "5",
    "sources": "all",
    "destinations": "all"
  }
}
```

**èŠ‚ç‚¹æ’ç©ºåŠ¨ä½œ**ï¼š

```json
{
  "actionId": "aws:eks:node-drain",
  "description": "èŠ‚ç‚¹å®‰å…¨æ’ç©ºï¼ˆéµå®ˆ PDBï¼‰",
  "targets": {
    "Nodes": "eks-worker-nodes"
  },
  "parameters": {
    "kubernetesServiceAccount": "fis-experiment-role",
    "gracePeriodSeconds": "300",
    "skipWaitForDeleteTimeout": "false"
  }
}
```

#### åŸºäº stopConditions çš„è‡ªåŠ¨ä¸­æ–­

FIS çš„ **stopConditions** åŠŸèƒ½åœ¨ SLO è¿è§„æ—¶è‡ªåŠ¨ä¸­æ–­å®éªŒï¼Œä¿éšœç”Ÿäº§ç¯å¢ƒå®‰å…¨æ€§ï¼š

```json
{
  "description": "EKS Pod æ•…éšœæ³¨å…¥ with SLO ä¿æŠ¤",
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-ErrorRate-SLO"
    },
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-Latency-P99-SLO"
    },
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-Availability-SLO"
    }
  ]
}
```

**CloudWatch Alarm è®¾ç½®ç¤ºä¾‹**ï¼š

```bash
# Error Rate SLO Alarm (é”™è¯¯ç‡ > 5%)
aws cloudwatch put-metric-alarm \
  --alarm-name "PaymentService-ErrorRate-SLO" \
  --alarm-description "Stop FIS if error rate exceeds 5%" \
  --namespace "AWS/ApplicationELB" \
  --metric-name "HTTPCode_Target_5XX_Count" \
  --dimensions Name=LoadBalancer,Value=app/payment-lb/xxx \
  --statistic Sum \
  --period 60 \
  --evaluation-periods 2 \
  --threshold 50 \
  --comparison-operator GreaterThanThreshold \
  --treat-missing-data notBreaching

# Latency P99 SLO Alarm (P99 > 500ms)
aws cloudwatch put-metric-alarm \
  --alarm-name "PaymentService-Latency-P99-SLO" \
  --alarm-description "Stop FIS if P99 latency exceeds 500ms" \
  --namespace "ContainerInsights" \
  --metric-name "pod_http_request_duration_p99" \
  --dimensions Name=Service,Value=payment-service \
  --statistic Average \
  --period 60 \
  --evaluation-periods 3 \
  --threshold 500 \
  --comparison-operator GreaterThanThreshold

# Availability SLO Alarm (å¯ç”¨æ€§ < 99.9%)
aws cloudwatch put-metric-alarm \
  --alarm-name "PaymentService-Availability-SLO" \
  --alarm-description "Stop FIS if availability drops below 99.9%" \
  --metric-name "AvailabilityRate" \
  --namespace "CustomMetrics" \
  --dimensions Name=Service,Value=payment-service \
  --statistic Average \
  --period 300 \
  --evaluation-periods 1 \
  --threshold 99.9 \
  --comparison-operator LessThanThreshold
```

#### ç”Ÿäº§å®‰å…¨ä¿éšœæ¨¡å¼

**æ¨¡å¼ 1: PDB é›†æˆ -- FIS å®éªŒæœŸé—´ç¡®ä¿éµå®ˆ PDB**

```yaml
# Pod Disruption Budget è®¾ç½®
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: payment-service-pdb
  namespace: payment
spec:
  minAvailable: 2  # æœ€å°‘ 2 ä¸ª Pod å§‹ç»ˆä¿æŒ Running
  selector:
    matchLabels:
      app: payment-service
---
# FIS Experiment Template (è‡ªåŠ¨éµå®ˆ PDB)
{
  "description": "Pod åˆ é™¤å®éªŒï¼ˆéµå®ˆ PDBï¼‰",
  "targets": {
    "eks-payment-pods": {
      "resourceType": "aws:eks:pod",
      "selectionMode": "COUNT(1)",
      "resourceTags": {
        "app": "payment-service"
      },
      "parameters": {
        "clusterIdentifier": "my-cluster",
        "namespace": "payment"
      }
    }
  },
  "actions": {
    "delete-pod-safely": {
      "actionId": "aws:eks:pod-delete",
      "parameters": {
        "kubernetesServiceAccount": "fis-experiment-role",
        "maxPodsToDelete": "1",
        "podDeletionMode": "one-at-a-time"
      },
      "targets": {
        "Pods": "eks-payment-pods"
      }
    }
  }
}
```

**FIS + PDB åŠ¨ä½œæµç¨‹**ï¼š

```mermaid
sequenceDiagram
    participant FIS as AWS FIS
    participant K8s as Kubernetes API
    participant PDB as PodDisruptionBudget
    participant Pod as Payment Pods

    FIS->>K8s: Pod åˆ é™¤è¯·æ±‚ (payment-pod-1)
    K8s->>PDB: ç¡®è®¤æ˜¯å¦å…è®¸ Disruption
    PDB->>K8s: minAvailable=2, å½“å‰ Running=3
    K8s->>PDB: å¯åˆ é™¤ 1 ä¸ª (3-1=2 >= minAvailable)
    PDB->>K8s: å…è®¸ Disruption
    K8s->>Pod: åˆ é™¤ payment-pod-1
    Pod->>Pod: Graceful Shutdown (30s)
    Pod->>K8s: Pod ç»ˆæ­¢å®Œæˆ
    K8s->>K8s: è°ƒåº¦æ–° Pod (payment-pod-4)
    K8s->>FIS: å®éªŒå®Œæˆ
```

**PDB è¿è§„åœºæ™¯**ï¼š

```bash
# å½“å‰ Running Pods: 2 ä¸ªï¼ˆæœ€å°å€¼ï¼‰
$ kubectl get pods -n payment -l app=payment-service
NAME                READY   STATUS    RESTARTS   AGE
payment-pod-2       1/1     Running   0          5m
payment-pod-3       1/1     Running   0          5m

# FIS å°è¯•åˆ é™¤ Pod
$ aws fis start-experiment --experiment-template-id EXT123456

# Kubernetes æ£€æŸ¥ PDB å¹¶æ‹’ç»
# minAvailable=2, å½“å‰=2 -> åˆ é™¤ 1 ä¸ªåä»…å‰© 1 ä¸ª -> PDB è¿è§„
# -> FIS å®éªŒå¤±è´¥ï¼ˆPDB é˜»æ­¢ Disruptionï¼‰

# FIS å®éªŒæ—¥å¿—
{
  "state": "failed",
  "reason": "PodDisruptionBudget prevents pod deletion. Current: 2, Required: 2"
}
```

**æ¨¡å¼ 2: çˆ†ç‚¸åŠå¾„é™åˆ¶ -- é€šè¿‡æ ‡ç­¾/å‘½åç©ºé—´é™åˆ¶å®éªŒèŒƒå›´**

```json
{
  "description": "é™å®šèŒƒå›´çš„ Pod æ•…éšœå®éªŒ",
  "targets": {
    "eks-test-pods": {
      "resourceType": "aws:eks:pod",
      "selectionMode": "PERCENT(25)",
      "resourceTags": {
        "environment": "staging",
        "chaos-experiment": "enabled",
        "team": "payments"
      },
      "filters": [
        {
          "path": "Namespace",
          "values": ["payment-staging"]
        },
        {
          "path": "Labels.version",
          "values": ["canary"]
        }
      ],
      "parameters": {
        "clusterIdentifier": "staging-cluster",
        "namespace": "payment-staging"
      }
    }
  }
}
```

**çˆ†ç‚¸åŠå¾„é™åˆ¶ç­–ç•¥**ï¼š

| é™åˆ¶æ–¹å¼ | è®¾ç½®æ–¹æ³• | ç¤ºä¾‹ |
|----------|----------|------|
| **å‘½åç©ºé—´** | `filters.Namespace` | `payment-staging`ï¼ˆæ’é™¤ç”Ÿäº§ç¯å¢ƒï¼‰ |
| **æ ‡ç­¾é€‰æ‹©** | `filters.Labels` | `version=canary`ï¼ˆä»…é‡‘ä¸é›€éƒ¨ç½²ï¼‰ |
| **åŸºäºæ ‡ç­¾** | `resourceTags` | `chaos-experiment=enabled`ï¼ˆæ˜¾å¼åŠ å…¥ï¼‰ |
| **æ¯”ä¾‹é™åˆ¶** | `selectionMode: PERCENT(N)` | `PERCENT(25)`ï¼ˆæœ€å¤šå½±å“ 25%ï¼‰ |
| **æ•°é‡é™åˆ¶** | `selectionMode: COUNT(N)` | `COUNT(2)`ï¼ˆæœ€å¤š 2 ä¸ªï¼‰ |

**æ¨¡å¼ 3: æ¸è¿›å¼æ‰©å±• -- 1 ä¸ª Pod -> 10% Pod -> 25% Pod åˆ†é˜¶æ®µæ‰©å±•**

```json
{
  "description": "æ¸è¿›å¼ Pod åˆ é™¤å®éªŒ",
  "actions": {
    "phase-1-single-pod": {
      "actionId": "aws:eks:pod-delete",
      "description": "Phase 1: åˆ é™¤ 1 ä¸ª Pod",
      "parameters": {
        "kubernetesServiceAccount": "fis-experiment-role",
        "maxPodsToDelete": "1"
      },
      "targets": {
        "Pods": "eks-payment-pods-phase1"
      }
    },
    "wait-1": {
      "actionId": "aws:fis:wait",
      "parameters": {
        "duration": "PT2M"
      },
      "startAfter": ["phase-1-single-pod"]
    },
    "phase-2-10-percent": {
      "actionId": "aws:eks:pod-delete",
      "description": "Phase 2: åˆ é™¤ 10% Pod",
      "parameters": {
        "kubernetesServiceAccount": "fis-experiment-role",
        "selectionMode": "PERCENT(10)"
      },
      "targets": {
        "Pods": "eks-payment-pods-phase2"
      },
      "startAfter": ["wait-1"]
    },
    "wait-2": {
      "actionId": "aws:fis:wait",
      "parameters": {
        "duration": "PT3M"
      },
      "startAfter": ["phase-2-10-percent"]
    },
    "phase-3-25-percent": {
      "actionId": "aws:eks:pod-delete",
      "description": "Phase 3: åˆ é™¤ 25% Pod",
      "parameters": {
        "kubernetesServiceAccount": "fis-experiment-role",
        "selectionMode": "PERCENT(25)"
      },
      "targets": {
        "Pods": "eks-payment-pods-phase3"
      },
      "startAfter": ["wait-2"]
    }
  },
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-ErrorRate-SLO"
    }
  ]
}
```

**æ¸è¿›å¼æ‰©å±•æµç¨‹**ï¼š

```
Phase 1: åˆ é™¤ 1 ä¸ª Pod
  â†“ (ç­‰å¾… 2 åˆ†é’Ÿ, SLO ç›‘æ§)
Phase 2: åˆ é™¤ 10% Pod
  â†“ (ç­‰å¾… 3 åˆ†é’Ÿ, SLO ç›‘æ§)
Phase 3: åˆ é™¤ 25% Pod
  â†“
[æˆåŠŸ] æ‰€æœ‰é˜¶æ®µé€šè¿‡ -> ç³»ç»Ÿæ¢å¤åŠ›éªŒè¯å®Œæˆ
[å¤±è´¥] SLO è¿è§„ -> è‡ªåŠ¨ä¸­æ–­, å›æ»š
```

**æ¨¡å¼ 4: å›æ»šæ¡ä»¶ -- latency P99 > 500ms æˆ– error rate > 5% æ—¶è‡ªåŠ¨ä¸­æ–­**

```json
{
  "description": "ç½‘ç»œå»¶è¿Ÿå®éªŒ with è‡ªåŠ¨å›æ»š",
  "actions": {
    "inject-latency": {
      "actionId": "aws:eks:pod-network-latency",
      "description": "200ms ç½‘ç»œå»¶è¿Ÿæ³¨å…¥",
      "parameters": {
        "kubernetesServiceAccount": "fis-experiment-role",
        "duration": "PT10M",
        "delayMilliseconds": "200",
        "jitterMilliseconds": "50"
      },
      "targets": {
        "Pods": "eks-payment-pods"
      }
    }
  },
  "stopConditions": [
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-Latency-P99-SLO"
    },
    {
      "source": "aws:cloudwatch:alarm",
      "value": "arn:aws:cloudwatch:ap-northeast-2:ACCOUNT_ID:alarm:PaymentService-ErrorRate-SLO"
    }
  ],
  "roleArn": "arn:aws:iam::ACCOUNT_ID:role/FISExperimentRole",
  "tags": {
    "Environment": "production",
    "Team": "platform",
    "ChaosExperimentType": "network-latency"
  }
}
```

**è‡ªåŠ¨å›æ»šåœºæ™¯**ï¼š

```
[00:00] FIS å®éªŒå¼€å§‹ -- 200ms ç½‘ç»œå»¶è¿Ÿæ³¨å…¥
[00:00] CloudWatch Alarms ç›‘æ§å¼€å§‹
  - Latency P99 SLO: æ­£å¸¸ (250ms < 500ms)
  - Error Rate SLO: æ­£å¸¸ (2% < 5%)
[00:03] Latency P99 å‡é«˜æ£€æµ‹: 450ms
[00:05] Latency P99 SLO è¿è§„: 520ms > 500ms
[00:05] CloudWatch Alarm è§¦å‘: "PaymentService-Latency-P99-SLO"
[00:05] FIS è‡ªåŠ¨ä¸­æ–­ (stopCondition æ»¡è¶³)
[00:05] ç½‘ç»œå»¶è¿Ÿç§»é™¤ï¼ˆè‡ªåŠ¨å›æ»šï¼‰
[00:06] Latency P99 æ¢å¤: 280ms
[00:08] ç³»ç»Ÿæ¢å¤æ­£å¸¸çŠ¶æ€
```

#### FIS Experiment Template YAML ç¤ºä¾‹

```yaml
# FIS Experiment Template: EKS Pod æ•…éšœæ³¨å…¥ + stopConditions
AWSTemplateFormatVersion: '2010-09-09'
Description: 'FIS Experiment Template for EKS Pod Fault Injection'

Resources:
  PaymentServiceFISExperiment:
    Type: AWS::FIS::ExperimentTemplate
    Properties:
      Description: 'EKS Pod åˆ é™¤å®éªŒ with SLO ä¿æŠ¤'
      StopConditions:
        - Source: 'aws:cloudwatch:alarm'
          Value: !GetAtt PaymentServiceErrorRateAlarm.Arn
        - Source: 'aws:cloudwatch:alarm'
          Value: !GetAtt PaymentServiceLatencyAlarm.Arn
      Targets:
        PaymentPods:
          ResourceType: 'aws:eks:pod'
          SelectionMode: 'COUNT(2)'
          ResourceTags:
            app: 'payment-service'
          Parameters:
            clusterIdentifier: !Ref EKSClusterName
            namespace: 'payment'
      Actions:
        DeletePods:
          ActionId: 'aws:eks:pod-delete'
          Parameters:
            kubernetesServiceAccount: !GetAtt FISServiceAccount.Name
            maxPodsToDelete: '2'
            podDeletionMode: 'one-at-a-time'
          Targets:
            Pods: 'PaymentPods'
      RoleArn: !GetAtt FISExperimentRole.Arn
      Tags:
        Environment: 'production'
        Team: 'platform'

  PaymentServiceErrorRateAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: 'PaymentService-ErrorRate-SLO'
      AlarmDescription: 'Stop FIS if error rate exceeds 5%'
      MetricName: 'HTTPCode_Target_5XX_Count'
      Namespace: 'AWS/ApplicationELB'
      Statistic: Sum
      Period: 60
      EvaluationPeriods: 2
      Threshold: 50
      ComparisonOperator: GreaterThanThreshold
      TreatMissingData: notBreaching

  PaymentServiceLatencyAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: 'PaymentService-Latency-P99-SLO'
      AlarmDescription: 'Stop FIS if P99 latency exceeds 500ms'
      MetricName: 'pod_http_request_duration_p99'
      Namespace: 'ContainerInsights'
      Statistic: Average
      Period: 60
      EvaluationPeriods: 3
      Threshold: 500
      ComparisonOperator: GreaterThanThreshold

  FISExperimentRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: fis.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AWSFaultInjectionSimulatorEKSAccess'
      Policies:
        - PolicyName: FISCloudWatchAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'cloudwatch:DescribeAlarms'
                  - 'cloudwatch:GetMetricData'
                Resource: '*'

  FISServiceAccount:
    Type: AWS::EKS::ServiceAccount
    Properties:
      ClusterName: !Ref EKSClusterName
      Name: 'fis-experiment-role'
      Namespace: 'kube-system'
      RoleArn: !GetAtt FISExperimentRole.Arn

Parameters:
  EKSClusterName:
    Type: String
    Description: 'Name of the EKS cluster'
    Default: 'my-cluster'

Outputs:
  ExperimentTemplateId:
    Description: 'FIS Experiment Template ID'
    Value: !GetAtt PaymentServiceFISExperiment.Id
    Export:
      Name: !Sub '${AWS::StackName}-ExperimentTemplateId'
```

:::tip FIS ç”Ÿäº§å®‰å…¨ä¿éšœçš„æ ¸å¿ƒ
AWS FIS çš„ **stopConditions** å’Œ **PDB é›†æˆ**æ˜¯åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®‰å…¨æ‰§è¡Œ Chaos Engineering çš„æ ¸å¿ƒåŠŸèƒ½ã€‚ç»“åˆ SLO è¿è§„æ—¶è‡ªåŠ¨ä¸­æ–­ã€æ¸è¿›å¼æ‰©å±•ã€çˆ†ç‚¸åŠå¾„é™åˆ¶ï¼Œå¯ä»¥åœ¨**ä¸å½±å“ç”¨æˆ·**çš„æƒ…å†µä¸‹éªŒè¯ç³»ç»Ÿæ¢å¤åŠ›ã€‚

**å»ºè®®**ï¼š
1. **å§‹ç»ˆè®¾ç½® stopConditions**: ä¸ CloudWatch Alarm è”åŠ¨ï¼ŒSLO è¿è§„æ—¶è‡ªåŠ¨ä¸­æ–­
2. **å¿…é¡»è®¾ç½® PDB**: æ‰€æœ‰ç”Ÿäº§å·¥ä½œè´Ÿè½½åº”ç”¨ PDB
3. **æ¸è¿›å¼æ‰©å±•**: 1 ä¸ª -> 10% -> 25% åˆ†é˜¶æ®µæ‰©å±•ä»¥ç¡®ä¿å®‰å…¨æ€§
4. **éç”Ÿäº§ç¯å¢ƒä¼˜å…ˆ**: åœ¨é¢„å‘å¸ƒç¯å¢ƒå……åˆ†æµ‹è¯•åå†åº”ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒ
:::

### 9.5 åŸºäº AI çš„é«˜çº§ Chaos Engineering

åˆ©ç”¨ AI å¯ä»¥ä½¿ Chaos Engineering ä»**æ‰‹åŠ¨å®éªŒè®¾è®¡ -> æ™ºèƒ½è‡ªåŠ¨è®¾è®¡**è¿›åŒ–ã€‚é€šè¿‡å­¦ä¹ è¿‡å»çš„æ•…éšœæ¨¡å¼ã€è‡ªåŠ¨å®šä¹‰ Steady State Hypothesisã€GameDay è‡ªåŠ¨åŒ–ï¼Œå¯ä»¥ç³»ç»Ÿæ€§åœ°æå‡ç³»ç»Ÿæ¢å¤åŠ›ã€‚

#### 9.5.1 å­¦ä¹ è¿‡å»æ•…éšœæ¨¡å¼ -> è‡ªåŠ¨å»ºè®®æ–°çš„æ··æ²Œåœºæ™¯

AI å­¦ä¹ è¿‡å»çš„äº‹ä»¶æ•°æ®ï¼Œè‡ªåŠ¨å»ºè®®å®é™…å‘ç”Ÿå¯èƒ½æ€§è¾ƒé«˜çš„æ··æ²Œåœºæ™¯ã€‚

```python
# åŸºäº AI çš„æ··æ²Œåœºæ™¯ç”Ÿæˆå™¨
from strands import Agent
import boto3

fis_client = boto3.client('fis', region_name='ap-northeast-2')
cloudwatch_client = boto3.client('cloudwatch', region_name='ap-northeast-2')

chaos_designer = Agent(
    name="chaos-scenario-designer",
    model="bedrock/anthropic.claude-sonnet",
    sop="""
    ## åŸºäº AI çš„æ··æ²Œåœºæ™¯è‡ªåŠ¨è®¾è®¡

    ### Phase 1: è¿‡å»äº‹ä»¶åˆ†æï¼ˆå­¦ä¹ ï¼‰
    1. é€šè¿‡ CloudWatch Logs Insights æ”¶é›†è¿‡å» 6 ä¸ªæœˆäº‹ä»¶
       - æŒ‰æ•…éšœç±»å‹åˆ†æé¢‘ç‡
       - åˆ†æå½±å“èŒƒå›´å’Œæ¢å¤æ—¶é—´
       - æŒ‰æ ¹æœ¬åŸå› åˆ†ç±»ï¼ˆç½‘ç»œ/èµ„æº/éƒ¨ç½²ï¼‰

    2. æå–äº‹ä»¶æ¨¡å¼
       - è¯†åˆ«é‡å¤å‘ç”Ÿæ¨¡å¼
       - åˆ†æå­£èŠ‚æ€§/æ—¶é—´æ®µæ¨¡å¼
       - åŸºäºä¾èµ–çš„è¿é”æ•…éšœæ¨¡å¼

    ### Phase 2: è‡ªåŠ¨ç”Ÿæˆæ··æ²Œåœºæ™¯
    1. æŒ‰æ•…éšœæ¨¡å¼è‡ªåŠ¨ç”Ÿæˆ FIS å®éªŒæ¨¡æ¿
       - Pod OOMKilled æ¨¡å¼ -> å†…å­˜å‹åŠ›å®éªŒ
       - ç½‘ç»œè¶…æ—¶æ¨¡å¼ -> å»¶è¿Ÿæ³¨å…¥å®éªŒ
       - èŠ‚ç‚¹æ•…éšœæ¨¡å¼ -> èŠ‚ç‚¹ç»ˆæ­¢å®éªŒ

    2. è‡ªåŠ¨å®šä¹‰ Steady State Hypothesis
       - åŸºäºè¿‡å» SLO æ•°æ®å®šä¹‰æ­£å¸¸çŠ¶æ€
       - åŸºäº CloudWatch Alarm è‡ªåŠ¨ç”Ÿæˆä¸­æ–­æ¡ä»¶

    3. å»ºè®®å®éªŒä¼˜å…ˆçº§
       - åŸºäºé¢‘ç‡ x å½±å“åº¦è®¡ç®—ä¼˜å…ˆçº§
       - ä¼˜å…ˆå»ºè®®æœªéªŒè¯çš„æ•…éšœåœºæ™¯

    ### Phase 3: è‡ªåŠ¨æ‰§è¡Œå®éªŒå’Œåˆ†æ
    1. è‡ªåŠ¨æ‰§è¡Œ FIS å®éªŒï¼ˆè°ƒåº¦ï¼‰
    2. è§‚å¯Ÿç³»ç»Ÿå“åº”å¹¶æ”¶é›†æŒ‡æ ‡
    3. å¯¹æ¯”åˆ†æé¢„æœŸä¸å®é™…ç»“æœ
    4. è¯†åˆ«æ¢å¤åŠ›ä¸è¶³çš„é¢†åŸŸå¹¶å»ºè®®æ”¹è¿›
    """
)
```

**å®æˆ˜ç¤ºä¾‹: åŸºäºè¿‡å»äº‹ä»¶çš„æ··æ²Œåœºæ™¯è‡ªåŠ¨ç”Ÿæˆ**

```python
# Step 1: æ”¶é›†è¿‡å»äº‹ä»¶æ•°æ®
import json
from datetime import datetime, timedelta

def analyze_past_incidents():
    """é€šè¿‡ CloudWatch Logs Insights åˆ†æè¿‡å»äº‹ä»¶"""
    logs_client = boto3.client('logs', region_name='ap-northeast-2')

    query = """
    fields @timestamp, detail.alarmName, detail.state.value, detail.state.reason
    | filter detail-type = "CloudWatch Alarm State Change"
    | filter detail.state.value = "ALARM"
    | stats count(*) as incident_count by detail.state.reason as failure_pattern
    | sort incident_count desc
    """

    start_time = int((datetime.now() - timedelta(days=180)).timestamp())
    end_time = int(datetime.now().timestamp())

    response = logs_client.start_query(
        logGroupName='/aws/events/cloudwatch-alarms',
        startTime=start_time,
        endTime=end_time,
        queryString=query
    )

    query_id = response['queryId']

    # ç­‰å¾…æŸ¥è¯¢ç»“æœå¹¶è¿”å›
    import time
    while True:
        result = logs_client.get_query_results(queryId=query_id)
        if result['status'] == 'Complete':
            return result['results']
        time.sleep(2)

# Step 2: AI åŸºäºäº‹ä»¶æ¨¡å¼å»ºè®®æ··æ²Œåœºæ™¯
incident_patterns = analyze_past_incidents()

scenario_prompt = f"""
è¿‡å» 6 ä¸ªæœˆå‘ç”Ÿçš„äº‹ä»¶æ¨¡å¼:
{json.dumps(incident_patterns, indent=2)}

è¯·åŸºäºè¿™äº›æ¨¡å¼æ‰§è¡Œä»¥ä¸‹æ“ä½œ:
1. è¯†åˆ«æœ€é¢‘ç¹çš„æ•…éšœæ¨¡å¼ Top 5
2. ä¸ºæ¯ä¸ªæ¨¡å¼åˆ›å»º AWS FIS å®éªŒæ¨¡æ¿
3. å®šä¹‰ Steady State Hypothesisï¼ˆåŸºäº SLOï¼‰
4. å»ºè®®å®éªŒä¼˜å…ˆçº§ï¼ˆé¢‘ç‡ x å½±å“åº¦ï¼‰
"""

response = chaos_designer.run(scenario_prompt)

# Step 3: è‡ªåŠ¨ç”Ÿæˆ AI å»ºè®®çš„ FIS å®éªŒæ¨¡æ¿
# ç¤ºä¾‹è¾“å‡º:
"""
[AI åˆ†æç»“æœ]

Top 5 æ•…éšœæ¨¡å¼:
1. Pod OOMKilled (37 æ¬¡) -- å†…å­˜ä¸è¶³
2. Network Timeout (24 æ¬¡) -- å¤–éƒ¨ API å»¶è¿Ÿ
3. Node NotReady (18 æ¬¡) -- èŠ‚ç‚¹æ•…éšœ
4. Deployment Failed (12 æ¬¡) -- é•œåƒ Pull å¤±è´¥
5. RDS Connection Timeout (9 æ¬¡) -- æ•°æ®åº“è¿æ¥å¤±è´¥

å»ºè®®çš„æ··æ²Œåœºæ™¯:

[åœºæ™¯ 1: å†…å­˜å‹åŠ›å®éªŒ]
ç›®çš„: éªŒè¯ Pod OOMKilled åº”å¯¹èƒ½åŠ›
FIS åŠ¨ä½œ: aws:eks:inject-pod-memory-stress
ç›®æ ‡: payment-serviceï¼ˆè¿‡å»å‘ç”Ÿ OOMKilled 37 æ¬¡ï¼‰
Steady State: memory_utilization < 85%, pod_restart_count < 5
ä¼˜å…ˆçº§: é«˜ï¼ˆé¢‘ç‡ 37 x å½±å“åº¦ 9 = 333ï¼‰

[åœºæ™¯ 2: ç½‘ç»œå»¶è¿Ÿå®éªŒ]
ç›®çš„: éªŒè¯å¤–éƒ¨ API å»¶è¿Ÿæ—¶çš„è¶…æ—¶å¤„ç†
FIS åŠ¨ä½œ: aws:eks:pod-network-latency
ç›®æ ‡: order-serviceï¼ˆè°ƒç”¨å¤–éƒ¨ payment APIï¼‰
Steady State: p99_latency < 500ms, error_rate < 1%
ä¼˜å…ˆçº§: ä¸­ï¼ˆé¢‘ç‡ 24 x å½±å“åº¦ 7 = 168ï¼‰

[åœºæ™¯ 3: èŠ‚ç‚¹ç»ˆæ­¢å®éªŒ]
ç›®çš„: éªŒè¯èŠ‚ç‚¹æ•…éšœæ—¶ Pod é‡æ–°è°ƒåº¦
FIS åŠ¨ä½œ: aws:eks:terminate-nodegroup-instances
ç›®æ ‡: worker-node-groupï¼ˆç»ˆæ­¢ 25%ï¼‰
Steady State: available_pods >= minAvailable (PDB), scheduling_time < 60s
ä¼˜å…ˆçº§: é«˜ï¼ˆé¢‘ç‡ 18 x å½±å“åº¦ 10 = 180ï¼‰
"""
```

#### 9.5.2 AI è‡ªåŠ¨å®šä¹‰ Steady State Hypothesis

Chaos Engineering çš„æ ¸å¿ƒ **Steady State Hypothesis**ï¼ˆæ­£å¸¸çŠ¶æ€å‡è®¾ï¼‰ç”± AI åŸºäºè¿‡å»çš„æŒ‡æ ‡æ•°æ®è‡ªåŠ¨å®šä¹‰ã€‚

```python
# Steady State Hypothesis è‡ªåŠ¨ç”Ÿæˆ
steady_state_agent = Agent(
    name="steady-state-generator",
    model="bedrock/anthropic.claude-sonnet",
    sop="""
    ## Steady State Hypothesis è‡ªåŠ¨å®šä¹‰

    ### è¾“å…¥æ•°æ®
    1. è¿‡å» 30 å¤© CloudWatch æŒ‡æ ‡ï¼ˆæ­£å¸¸çŠ¶æ€æœŸé—´ï¼‰
       - RPS (Requests Per Second)
       - Error Rate
       - P50/P95/P99 Latency
       - CPU/Memory Utilization
       - Pod Restart Count

    2. å½“å‰ SLO è®¾ç½®
       - Availability SLO: 99.9%
       - Latency SLO: P99 < 500ms
       - Error Budget: 0.1%

    ### æ­£å¸¸çŠ¶æ€å®šä¹‰é€»è¾‘
    1. è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„æ­£å¸¸èŒƒå›´
       - Baseline: è¿‡å» 30 å¤©å¹³å‡å€¼
       - Acceptable Range: å¹³å‡å€¼ +/- 2 sigmaï¼ˆæ ‡å‡†å·®ï¼‰
       - Alert Threshold: å¹³å‡å€¼ + 3 sigma

    2. åŸºäº SLO çš„ä¸Šé™è®¾ç½®
       - Error Rate: max(SLO threshold, å¹³å‡å€¼ + 2 sigma)
       - Latency: min(SLO threshold, å¹³å‡å€¼ + 2 sigma)

    3. è½¬æ¢ä¸º CloudWatch Alarm
       - Steady State è¿è§„æ—¶è‡ªåŠ¨ä¸­æ–­ FIS å®éªŒ

    ### è¾“å‡º
    - Steady State Hypothesis YAML
    - CloudWatch Alarm å®šä¹‰ï¼ˆFIS stopConditionsï¼‰
    """
)
```

**å®æˆ˜ç¤ºä¾‹: Steady State è‡ªåŠ¨ç”Ÿæˆ**

```python
def generate_steady_state_hypothesis(service_name: str, lookback_days: int = 30):
    """åŸºäº AI çš„ Steady State Hypothesis è‡ªåŠ¨ç”Ÿæˆ"""

    # Step 1: æ”¶é›†è¿‡å»æŒ‡æ ‡
    end_time = datetime.now()
    start_time = end_time - timedelta(days=lookback_days)

    metrics = {
        'error_rate': cloudwatch_client.get_metric_statistics(
            Namespace='AWS/ApplicationELB',
            MetricName='HTTPCode_Target_5XX_Count',
            Dimensions=[{'Name': 'LoadBalancer', 'Value': f'app/{service_name}-lb'}],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Average', 'Maximum']
        ),
        'latency_p99': cloudwatch_client.get_metric_statistics(
            Namespace='ContainerInsights',
            MetricName='pod_http_request_duration_p99',
            Dimensions=[{'Name': 'Service', 'Value': service_name}],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Average']
        )
    }

    # Step 2: AI å®šä¹‰æ­£å¸¸çŠ¶æ€
    prompt = f"""
    æœåŠ¡: {service_name}
    è¿‡å» {lookback_days} å¤©æŒ‡æ ‡æ•°æ®:
    {json.dumps(metrics, indent=2, default=str)}

    è¯·ç”Ÿæˆä»¥ä¸‹å†…å®¹:
    1. Steady State Hypothesisï¼ˆæ­£å¸¸çŠ¶æ€åŸºå‡†ï¼‰
    2. FIS stopConditions ç”¨çš„ CloudWatch Alarm å®šä¹‰
    3. å®éªŒæœŸé—´éœ€ç›‘æ§çš„æ ¸å¿ƒæŒ‡æ ‡åˆ—è¡¨
    """

    response = steady_state_agent.run(prompt)

    # ç¤ºä¾‹è¾“å‡º:
    """
    [Steady State Hypothesis: payment-service]

    ## æ­£å¸¸çŠ¶æ€åŸºå‡† (Baseline: è¿‡å» 30 å¤©å¹³å‡å€¼)

    1. Error Rate
       - Baseline: 0.3%
       - Acceptable Range: 0% - 0.8% (å¹³å‡å€¼ +/- 2 sigma)
       - Alert Threshold: 1.2% (å¹³å‡å€¼ + 3 sigma)
       -> FIS stopCondition: error_rate > 1.2%

    2. Latency P99
       - Baseline: 320ms
       - Acceptable Range: 200ms - 440ms
       - Alert Threshold: 560ms
       -> FIS stopCondition: p99_latency > 560ms

    3. Availability
       - Baseline: 99.97%
       - Acceptable Range: 99.9% - 100%
       - Alert Threshold: 99.8%
       -> FIS stopCondition: availability < 99.8%

    4. Pod Restart Count (5 åˆ†é’Ÿçª—å£)
       - Baseline: 0.1 æ¬¡
       - Acceptable Range: 0 - 1 æ¬¡
       - Alert Threshold: 3 æ¬¡
       -> FIS stopCondition: restart_count > 3

    ## CloudWatch Alarm å®šä¹‰ (FIS stopConditions)

    ```yaml
    stopConditions:
      - source: aws:cloudwatch:alarm
        value: arn:aws:cloudwatch:region:account:alarm:payment-ErrorRate-SSH
      - source: aws:cloudwatch:alarm
        value: arn:aws:cloudwatch:region:account:alarm:payment-LatencyP99-SSH
      - source: aws:cloudwatch:alarm
        value: arn:aws:cloudwatch:region:account:alarm:payment-Availability-SSH
      - source: aws:cloudwatch:alarm
        value: arn:aws:cloudwatch:region:account:alarm:payment-RestartCount-SSH
    ```

    ## æ ¸å¿ƒç›‘æ§æŒ‡æ ‡
    1. RPS (æ­£å¸¸èŒƒå›´: 800-1200 req/s)
    2. Active Connections (æ­£å¸¸èŒƒå›´: 50-150)
    3. Database Connection Pool (æ­£å¸¸èŒƒå›´: 10-30)
    """

    return response
```

#### 9.5.3 GameDay è‡ªåŠ¨åŒ– -- AI ç”Ÿæˆåœºæ™¯ + æ‰§è¡Œ + åˆ†æ

**GameDay**ï¼ˆç¾éš¾æ¢å¤æ¼”ç»ƒï¼‰ç”± AI å®Œå…¨è‡ªåŠ¨åŒ–ã€‚ä»åœºæ™¯ç”Ÿæˆåˆ°æ‰§è¡Œã€ç»“æœåˆ†æå…¨éƒ¨è‡ªä¸»å®Œæˆã€‚

```python
# GameDay è‡ªåŠ¨åŒ– Agent
gameday_orchestrator = Agent(
    name="gameday-orchestrator",
    model="bedrock/anthropic.claude-opus",  # å¤æ‚å†³ç­– -> ä½¿ç”¨ Opus
    sop="""
    ## GameDay è‡ªåŠ¨åŒ–å·¥ä½œæµ

    ### Phase 1: äº‹å‰è®¡åˆ’ (D-7)
    1. åˆ†æè¿‡å»äº‹ä»¶ -> ç”Ÿæˆç°å®åœºæ™¯
    2. å®šä¹‰å‚ä¸å›¢é˜ŸåŠè§’è‰²ï¼ˆè‡ªåŠ¨é€šçŸ¥ï¼‰
    3. å®šä¹‰ Steady State Hypothesis
    4. å‡†å¤‡ Rollback Plan

    ### Phase 2: æ‰§è¡Œå‡†å¤‡ (D-1)
    1. ç¡®è®¤é¢„å‘å¸ƒç¯å¢ƒçŠ¶æ€
    2. å‡†å¤‡ Monitoring Dashboard (AMG)
    3. å‘å‚ä¸è€…å‘é€ GameDay ç®€æŠ¥ (Slack)
    4. éªŒè¯ stopConditions

    ### Phase 3: GameDay æ‰§è¡Œ (D-Day)
    1. åœºæ™¯ 1: Pod æ•…éšœæ³¨å…¥ï¼ˆæ‰§è¡Œ FISï¼‰
       - è§‚å¯Ÿæ—¶é—´: 10 åˆ†é’Ÿ
       - éªŒè¯è‡ªåŠ¨æ¢å¤
       - æ”¶é›†æŒ‡æ ‡

    2. åœºæ™¯ 2: ç½‘ç»œå»¶è¿Ÿæ³¨å…¥
       - è§‚å¯Ÿæ—¶é—´: 15 åˆ†é’Ÿ
       - éªŒè¯è¶…æ—¶å¤„ç†
       - åˆ†æç”¨æˆ·å½±å“

    3. åœºæ™¯ 3: æ•°æ®åº“æ•…éšœ
       - è§‚å¯Ÿæ—¶é—´: 20 åˆ†é’Ÿ
       - éªŒè¯ Failover
       - æµ‹é‡æ¢å¤æ—¶é—´

    ### Phase 4: äº‹ååˆ†æ (D+1)
    1. é‡å»ºæ—¶é—´çº¿
    2. åˆ†ææ¢å¤æ—¶é—´ (MTTR)
    3. è¯†åˆ«è–„å¼±ç‚¹å¹¶å»ºè®®æ”¹è¿›
    4. è‡ªåŠ¨ç”Ÿæˆ Post-Mortem æŠ¥å‘Š
    5. åˆ›å»º JIRA å·¥å•ï¼ˆæ”¹è¿›ä»»åŠ¡ï¼‰
    """
)
```

**å®æˆ˜ç¤ºä¾‹: è‡ªåŠ¨åŒ– GameDay æ‰§è¡Œ**

```python
# GameDay åœºæ™¯å®šä¹‰
gameday_scenario = {
    "name": "EKS å¤åˆæ•…éšœåº”å¯¹æ¼”ç»ƒ",
    "date": "2026-02-20",
    "environment": "staging",
    "scenarios": [
        {
            "id": "scenario-1",
            "name": "Pod å¤§é‡ç»ˆæ­¢ (25% åŒæ—¶æ•…éšœ)",
            "fis_template_id": "EXT-pod-termination-25pct",
            "duration": "10m",
            "expected_behavior": "HPA è‡ªåŠ¨æ‰©å®¹, 60 ç§’å†…æ¢å¤",
            "success_criteria": "error_rate < 2%, p99_latency < 800ms"
        },
        {
            "id": "scenario-2",
            "name": "ç½‘ç»œå»¶è¿Ÿ 300ms æ³¨å…¥",
            "fis_template_id": "EXT-network-latency-300ms",
            "duration": "15m",
            "expected_behavior": "Circuit Breaker åŠ¨ä½œ, Fallback å“åº”",
            "success_criteria": "timeout_rate < 5%, fallback_success > 95%"
        },
        {
            "id": "scenario-3",
            "name": "RDS Failover æ¨¡æ‹Ÿ",
            "fis_template_id": "EXT-rds-failover",
            "duration": "20m",
            "expected_behavior": "Connection Pool é‡è¿, æ— æ•°æ®ä¸¢å¤±",
            "success_criteria": "connection_retry_success > 99%, data_consistency = 100%"
        }
    ]
}

# GameDay è‡ªåŠ¨æ‰§è¡Œ
def run_automated_gameday(scenario):
    """åŸºäº AI çš„ GameDay è‡ªåŠ¨æ‰§è¡Œ"""

    # Phase 1: äº‹å‰å‡†å¤‡
    print("[Phase 1] GameDay äº‹å‰å‡†å¤‡å¼€å§‹...")
    gameday_orchestrator.run(f"""
    GameDay åœºæ™¯:
    {json.dumps(scenario, indent=2)}

    è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œ:
    1. å‘å‚ä¸å›¢é˜Ÿå‘é€ Slack é€šçŸ¥ï¼ˆæ—¥æœŸã€åœºæ™¯æ¦‚è¦ï¼‰
    2. åˆ›å»º AMG ä»ªè¡¨æ¿ï¼ˆå®æ—¶ç›‘æ§ï¼‰
    3. éªŒè¯ stopConditions
    """)

    # Phase 2: æŒ‰åœºæ™¯æ‰§è¡Œ
    print("[Phase 2] GameDay åœºæ™¯æ‰§è¡Œå¼€å§‹...")
    results = []

    for scenario_item in scenario['scenarios']:
        print(f"  -> æ‰§è¡Œä¸­: {scenario_item['name']}")

        # å¯åŠ¨ FIS å®éªŒ
        experiment = fis_client.start_experiment(
            experimentTemplateId=scenario_item['fis_template_id']
        )

        experiment_id = experiment['experiment']['id']

        # ç­‰å¾…å®éªŒå®Œæˆ
        import time
        while True:
            status = fis_client.get_experiment(id=experiment_id)
            state = status['experiment']['state']['status']

            if state in ['completed', 'stopped', 'failed']:
                break

            time.sleep(10)

        # æ”¶é›†ç»“æœ
        result = {
            'scenario_id': scenario_item['id'],
            'experiment_id': experiment_id,
            'state': state,
            'metrics': collect_metrics_during_experiment(experiment_id)
        }
        results.append(result)

        # AI åˆ†æ
        analysis_prompt = f"""
        åœºæ™¯: {scenario_item['name']}
        é¢„æœŸè¡Œä¸º: {scenario_item['expected_behavior']}
        æˆåŠŸæ ‡å‡†: {scenario_item['success_criteria']}
        å®é™…ç»“æœ:
        {json.dumps(result, indent=2)}

        è¯·åˆ†æä»¥ä¸‹å†…å®¹:
        1. æ˜¯å¦æ»¡è¶³æˆåŠŸæ ‡å‡†
        2. é¢„æœŸä¸å®é™…è¡Œä¸ºå¯¹æ¯”
        3. å‘ç°çš„è–„å¼±ç‚¹
        4. æ”¹è¿›å»ºè®®
        """

        scenario_analysis = gameday_orchestrator.run(analysis_prompt)
        result['ai_analysis'] = scenario_analysis

    # Phase 3: ç»¼åˆåˆ†æåŠæŠ¥å‘Šç”Ÿæˆ
    print("[Phase 3] GameDay ç»“æœåˆ†æåŠæŠ¥å‘Šç”Ÿæˆ...")

    final_report_prompt = f"""
    GameDay å…¨éƒ¨ç»“æœ:
    {json.dumps(results, indent=2)}

    è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„ Post-Mortem æŠ¥å‘Š:
    1. Executive Summaryï¼ˆç®¡ç†å±‚æ‘˜è¦ï¼‰
    2. å„åœºæ™¯è¯¦ç»†ç»“æœ
    3. æ—¶é—´çº¿é‡å»º
    4. è–„å¼±ç‚¹åŠæ”¹è¿›ä»»åŠ¡ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    5. éœ€åˆ›å»º JIRA å·¥å•çš„æ”¹è¿›ä»»åŠ¡åˆ—è¡¨
    """

    final_report = gameday_orchestrator.run(final_report_prompt)

    # Slack æŠ¥å‘Š
    slack_client = boto3.client('chatbot', region_name='ap-northeast-2')
    slack_client.send_message(
        Channel='#gameday-results',
        Message=final_report
    )

    # è‡ªåŠ¨åˆ›å»º JIRA å·¥å•
    create_jira_tickets_from_report(final_report)

    return final_report

# æ‰§è¡Œ
report = run_automated_gameday(gameday_scenario)
```

**AI ç”Ÿæˆçš„ GameDay æŠ¥å‘Šç¤ºä¾‹**ï¼š

```markdown
# GameDay Post-Mortem æŠ¥å‘Š
Date: 2026-02-20 | Environment: Staging | Duration: 45 åˆ†é’Ÿ

## Executive Summary
æ‰§è¡Œäº† 3 ä¸ªåœºæ™¯, 2 ä¸ªæˆåŠŸ, 1 ä¸ªéƒ¨åˆ†æˆåŠŸã€‚
- Pod å¤§é‡ç»ˆæ­¢: æˆåŠŸï¼ˆæ¢å¤æ—¶é—´ 45 ç§’ï¼‰
- ç½‘ç»œå»¶è¿Ÿ: éƒ¨åˆ†æˆåŠŸï¼ˆTimeout 7% å‘ç”Ÿï¼‰
- RDS Failover: æˆåŠŸï¼ˆFailover æ—¶é—´ 18 ç§’ï¼‰

ä¸»è¦å‘ç°: Circuit Breaker è¶…æ—¶è®¾ç½®ä¸è¶³

## åœºæ™¯ 1: Pod å¤§é‡ç»ˆæ­¢
ç›®æ ‡: éªŒè¯ 25% Pod åŒæ—¶ç»ˆæ­¢æ—¶çš„è‡ªåŠ¨æ¢å¤
ç»“æœ: æˆåŠŸ
- æ¢å¤æ—¶é—´: 45 ç§’ï¼ˆç›®æ ‡: 60 ç§’ä»¥å†…ï¼‰
- Error Rate: 1.2%ï¼ˆç›®æ ‡: < 2%ï¼‰
- P99 Latency: 680msï¼ˆç›®æ ‡: < 800msï¼‰

å‘ç°äº‹é¡¹:
- HPA åœ¨ 40 ç§’å†…å®Œæˆæ–° Pod åˆ›å»º
- PDB é€‚å½“é™åˆ¶äº†åŒæ—¶ç»ˆæ­¢
- æˆåŠŸå°†ç”¨æˆ·å½±å“æœ€å°åŒ–

## åœºæ™¯ 2: ç½‘ç»œå»¶è¿Ÿ
ç›®æ ‡: éªŒè¯ 300ms å»¶è¿Ÿæ³¨å…¥æ—¶ Circuit Breaker åŠ¨ä½œ
ç»“æœ: éƒ¨åˆ†æˆåŠŸ
- Timeout Rate: 7%ï¼ˆç›®æ ‡: < 5%ï¼‰
- Fallback Success: 98%ï¼ˆç›®æ ‡: > 95%ï¼‰

å‘ç°äº‹é¡¹:
- Circuit Breaker åŠ¨ä½œæ­£å¸¸
- ä½†è¶…æ—¶è®¾ç½®è¿‡çŸ­ï¼ˆå½“å‰: 500msï¼‰
- å»ºè®®: å°†è¶…æ—¶å¢åŠ åˆ° 800ms

è–„å¼±ç‚¹:
- order-service çš„ payment-api è°ƒç”¨è¶…æ—¶è®¾ç½®ä¸è¶³
- æ— é‡è¯•é€»è¾‘ï¼ˆç›´æ¥è¿”å› 503 é”™è¯¯ï¼‰

## åœºæ™¯ 3: RDS Failover
ç›®æ ‡: éªŒè¯ RDS Failover æ—¶çš„è¿æ¥é‡è¯•
ç»“æœ: æˆåŠŸ
- Failover æ—¶é—´: 18 ç§’
- Connection Retry Success: 100%
- Data Consistency: 100%

å‘ç°äº‹é¡¹:
- Connection Pool è‡ªåŠ¨é‡è¿æˆåŠŸ
- ä¸­æ–­äº‹åŠ¡çš„è¯·æ±‚è‡ªåŠ¨é‡è¯•æˆåŠŸ

## æ”¹è¿›ä»»åŠ¡ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### P0 (ç´§æ€¥)
- [ ] order-service: payment-api è¶…æ—¶ 500ms -> 800ms å¢åŠ 
- [ ] order-service: æ·»åŠ é‡è¯•é€»è¾‘ (exponential backoff)

### P1 (é«˜)
- [ ] ç¼–å†™ Circuit Breaker è®¾ç½®æ ‡å‡†åŒ–æ–‡æ¡£
- [ ] å…¨å…¬å¸æœåŠ¡è¶…æ—¶è®¾ç½®å®¡æŸ¥

### P2 (ä¸­)
- [ ] æ”¹è¿› GameDay è‡ªåŠ¨åŒ–è„šæœ¬ï¼ˆæ›´å¤šåœºæ™¯ï¼‰
- [ ] åœ¨ Observability ä»ªè¡¨æ¿æ·»åŠ  Circuit Breaker çŠ¶æ€

## JIRA å·¥å•åˆ›å»º
- INFRA-1234: order-service è¶…æ—¶è®¾ç½®æ”¹è¿›
- INFRA-1235: Circuit Breaker è®¾ç½®æ ‡å‡†åŒ–æ–‡æ¡£
- INFRA-1236: å…¨å…¬å¸æœåŠ¡è¶…æ—¶å®¡è®¡
```

:::tip åŸºäº AI çš„é«˜çº§ Chaos Engineering çš„æ ¸å¿ƒ
åˆ©ç”¨ AI å¯ä»¥ä½¿ Chaos Engineering ä»**æ‰‹åŠ¨å®éªŒè®¾è®¡ -> æ™ºèƒ½è‡ªåŠ¨è®¾è®¡**è¿›åŒ–ã€‚é€šè¿‡å­¦ä¹ è¿‡å»çš„æ•…éšœæ¨¡å¼è‡ªåŠ¨å»ºè®®å®é™…å‘ç”Ÿå¯èƒ½æ€§è¾ƒé«˜çš„åœºæ™¯ï¼ŒåŸºäºæ•°æ®å®šä¹‰ Steady State Hypothesisï¼Œå®Œå…¨è‡ªåŠ¨åŒ– GameDayï¼Œä»è€Œç³»ç»Ÿæ€§åœ°æå‡ç³»ç»Ÿæ¢å¤åŠ›ã€‚

**æ ¸å¿ƒä»·å€¼**ï¼š
1. **æ•°æ®é©±åŠ¨åœºæ™¯**: è¿‡å»äº‹ä»¶åˆ†æ -> ç°å®çš„æ··æ²Œåœºæ™¯
2. **è‡ªåŠ¨æ­£å¸¸çŠ¶æ€å®šä¹‰**: åŸºäºæŒ‡æ ‡è‡ªåŠ¨ç”Ÿæˆ Steady State Hypothesis
3. **GameDay è‡ªåŠ¨åŒ–**: åœºæ™¯ç”Ÿæˆ -> æ‰§è¡Œ -> åˆ†æ -> æŠ¥å‘Šç”Ÿæˆå…¨éƒ¨è‡ªåŠ¨åŒ–
4. **æŒç»­æ”¹è¿›**: AI å­¦ä¹ å®éªŒç»“æœ -> æ”¹è¿›ä¸‹æ¬¡å®éªŒ
:::

### 9.6 åŸºäºé¢„æµ‹çš„æˆæœ¬ä¼˜åŒ–

ç»“åˆé¢„æµ‹æ‰©ç¼©å®¹å’Œ AI åˆ†æï¼Œå¯ä»¥åŒæ—¶å®ç°**æ€§èƒ½ä¿æŒ + æˆæœ¬ä¼˜åŒ–**ã€‚é€šè¿‡ç»“åˆæµé‡é¢„æµ‹å’Œ Spot å®ä¾‹ä¸­æ–­é¢„æµ‹ï¼ŒåŠ¨æ€è°ƒæ•´ On-Demand ä¸ Spot çš„æ¯”ä¾‹ï¼Œæå‰é˜²æ­¢é¢„ç®—è¶…æ”¯ã€‚

#### 9.6.1 æµé‡é¢„æµ‹ + Spot ä¸­æ–­é¢„æµ‹ç»“åˆ

ç»“åˆ Karpenter çš„ Spot å®ä¾‹ä½¿ç”¨å’Œæµé‡é¢„æµ‹ï¼Œå‡è¡¡ç»´æŒ**æˆæœ¬æ•ˆç‡å’Œç¨³å®šæ€§**ã€‚

```mermaid
graph TD
    subgraph Prediction["é¢„æµ‹å±‚"]
        TRAFFIC["æµé‡é¢„æµ‹<br/>(Prophet)"]
        SPOT["Spot ä¸­æ–­é¢„æµ‹<br/>(AWS API)"]
    end

    subgraph Decision["AI å†³ç­–"]
        ANALYZE["æˆæœ¬-ç¨³å®šæ€§<br/>æƒè¡¡åˆ†æ"]
        DECIDE["Spot/OnDemand<br/>æ¯”ä¾‹å†³å®š"]
    end

    subgraph Action["è‡ªåŠ¨æ“ä½œ"]
        KARP["Karpenter<br/>NodePool è°ƒæ•´"]
        HPA_ADJ["HPA è®¾ç½®<br/>ä¼˜åŒ–"]
    end

    TRAFFIC --> ANALYZE
    SPOT --> ANALYZE
    ANALYZE --> DECIDE
    DECIDE --> KARP
    DECIDE --> HPA_ADJ

    style Decision fill:#fff3cd,stroke:#ff9800
```

**åŸºäº Spot ä¸­æ–­é¢„æµ‹çš„æ¯”ä¾‹è°ƒæ•´**ï¼š

```python
# Spot ä¸­æ–­é¢„æµ‹ + æµé‡é¢„æµ‹é›†æˆæ‰©ç¼©å™¨
import boto3
from datetime import datetime, timedelta

ec2_client = boto3.client('ec2', region_name='ap-northeast-2')
cloudwatch_client = boto3.client('cloudwatch', region_name='ap-northeast-2')

def predict_spot_interruption_risk(instance_types: list[str], availability_zones: list[str]) -> dict:
    """Spot å®ä¾‹ä¸­æ–­é£é™©é¢„æµ‹"""

    # æŸ¥è¯¢ Spot ä¸­æ–­å»ºè®®ï¼ˆæœ€è¿‘ 5 åˆ†é’Ÿæ•°æ®ï¼‰
    risk_scores = {}

    for az in availability_zones:
        for instance_type in instance_types:
            # ä» CloudWatch æŸ¥è¯¢ Spot ä¸­æ–­é¢‘ç‡
            response = cloudwatch_client.get_metric_statistics(
                Namespace='AWS/EC2Spot',
                MetricName='InterruptionRate',
                Dimensions=[
                    {'Name': 'AvailabilityZone', 'Value': az},
                    {'Name': 'InstanceType', 'Value': instance_type}
                ],
                StartTime=datetime.now() - timedelta(hours=24),
                EndTime=datetime.now(),
                Period=3600,
                Statistics=['Average']
            )

            if response['Datapoints']:
                avg_interruption_rate = sum(dp['Average'] for dp in response['Datapoints']) / len(response['Datapoints'])
                risk_scores[f"{instance_type}/{az}"] = avg_interruption_rate
            else:
                risk_scores[f"{instance_type}/{az}"] = 0.0

    return risk_scores

def calculate_optimal_spot_ratio(traffic_prediction: dict, spot_risk: dict) -> dict:
    """åŸºäºæµé‡é¢„æµ‹ + Spot é£é™©åº¦è®¡ç®—æœ€ä¼˜ Spot æ¯”ä¾‹"""

    predicted_rps = traffic_prediction['predicted_rps']
    prediction_confidence = traffic_prediction['confidence']  # 0.0 - 1.0

    # å¹³å‡ Spot ä¸­æ–­é£é™©åº¦
    avg_spot_risk = sum(spot_risk.values()) / len(spot_risk) if spot_risk else 0.0

    # å†³ç­–é€»è¾‘
    if avg_spot_risk > 0.05:  # 5% ä»¥ä¸Šä¸­æ–­é£é™©
        # é«˜é£é™©: å¢åŠ  On-Demand æ¯”ä¾‹
        spot_ratio = 0.3
        ondemand_ratio = 0.7
        reason = "Spot ä¸­æ–­é£é™©é«˜ (>5%)"

    elif prediction_confidence < 0.7:  # é¢„æµ‹ç½®ä¿¡åº¦ä½
        # ä¸ç¡®å®šæ€§é«˜: å¢åŠ  On-Demand æ¯”ä¾‹ï¼ˆç¨³å®šæ€§ä¼˜å…ˆï¼‰
        spot_ratio = 0.5
        ondemand_ratio = 0.5
        reason = "æµé‡é¢„æµ‹ç½®ä¿¡åº¦ä½ (<70%)"

    elif predicted_rps > 5000:  # é¢„è®¡é«˜æµé‡
        # é«˜å³°æ—¶é—´: å¢åŠ  On-Demand æ¯”ä¾‹ï¼ˆæ€§èƒ½ä¼˜å…ˆï¼‰
        spot_ratio = 0.4
        ondemand_ratio = 0.6
        reason = "é¢„è®¡é«˜æµé‡ (>5000 RPS)"

    else:
        # æ­£å¸¸: æœ€å¤§åŒ– Spot æ¯”ä¾‹ï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰
        spot_ratio = 0.8
        ondemand_ratio = 0.2
        reason = "æ­£å¸¸è¿è¥æ¡ä»¶ï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰"

    return {
        'spot_ratio': spot_ratio,
        'ondemand_ratio': ondemand_ratio,
        'reason': reason,
        'estimated_cost_saving': calculate_cost_saving(spot_ratio)
    }

def calculate_cost_saving(spot_ratio: float) -> float:
    """åŸºäº Spot æ¯”ä¾‹ä¼°ç®—æˆæœ¬èŠ‚çœ"""
    # å‡è®¾: Spot å®ä¾‹æ¯” On-Demand ä¾¿å®œ 70%
    spot_discount = 0.7
    return spot_ratio * spot_discount * 100  # ç™¾åˆ†æ¯”

# æ‰§è¡Œç¤ºä¾‹
spot_risk = predict_spot_interruption_risk(
    instance_types=['c6i.xlarge', 'c5.xlarge'],
    availability_zones=['ap-northeast-2a', 'ap-northeast-2b', 'ap-northeast-2c']
)

traffic_pred = {
    'predicted_rps': 3500,
    'confidence': 0.85
}

optimal_ratio = calculate_optimal_spot_ratio(traffic_pred, spot_risk)

print(f"""
[åŸºäºé¢„æµ‹çš„ Spot æ¯”ä¾‹è°ƒæ•´]
æµé‡é¢„æµ‹: {traffic_pred['predicted_rps']} RPS (ç½®ä¿¡åº¦: {traffic_pred['confidence']:.0%})
Spot ä¸­æ–­é£é™©: {sum(spot_risk.values()) / len(spot_risk):.2%}

å»ºè®®æ¯”ä¾‹:
- Spot: {optimal_ratio['spot_ratio']:.0%}
- On-Demand: {optimal_ratio['ondemand_ratio']:.0%}

ä¾æ®: {optimal_ratio['reason']}
é¢„è®¡æˆæœ¬èŠ‚çœ: {optimal_ratio['estimated_cost_saving']:.1f}%
""")
```

#### 9.6.2 é€šè¿‡é¢„æµ‹æ‰©ç¼©å®¹åŠ¨æ€è°ƒæ•´ On-Demand ä¸ Spot æ¯”ä¾‹

åŠ¨æ€è°ƒæ•´ Karpenter NodePool è®¾ç½®ï¼Œæ ¹æ®é¢„æµ‹çš„æµé‡å’Œ Spot é£é™©åº¦ç»´æŒæœ€ä¼˜æ¯”ä¾‹ã€‚

```yaml
# Karpenter NodePool: åŠ¨æ€ Spot æ¯”ä¾‹è°ƒæ•´
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: dynamic-spot-pool
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["c6i.xlarge", "c5.xlarge", "c6a.xlarge"]

      # Spot æ¯”ä¾‹åŠ¨æ€è°ƒæ•´ï¼ˆé»˜è®¤å€¼: 70% Spot, 30% On-Demandï¼‰
      kubelet:
        systemReserved:
          cpu: 100m
          memory: 100Mi

  # Spot ä¸­æ–­å¤„ç†ç­–ç•¥
  disruption:
    consolidationPolicy: WhenUnderutilized
    expireAfter: 720h  # 30 å¤©

  # åŸºäºæƒé‡çš„æ¯”ä¾‹æ§åˆ¶
  weight: 100
---
# Lambda å‡½æ•°: Karpenter NodePool åŠ¨æ€æ›´æ–°
import boto3
import json

eks_client = boto3.client('eks', region_name='ap-northeast-2')
k8s_client = boto3.client('eks', region_name='ap-northeast-2')  # ä»£æ›¿ kubectl ä½¿ç”¨

def update_karpenter_nodepool_weights(optimal_ratio: dict):
    """æ›´æ–° Karpenter NodePool çš„ Spot/OnDemand æƒé‡"""

    spot_weight = int(optimal_ratio['spot_ratio'] * 100)
    ondemand_weight = int(optimal_ratio['ondemand_ratio'] * 100)

    # NodePool æ›´æ–°ï¼ˆä½¿ç”¨ API ä»£æ›¿ kubectl applyï¼‰
    nodepool_patch = {
        "spec": {
            "template": {
                "spec": {
                    "requirements": [
                        {
                            "key": "karpenter.sh/capacity-type",
                            "operator": "In",
                            "values": ["spot", "on-demand"],
                            "weight": {
                                "spot": spot_weight,
                                "on-demand": ondemand_weight
                            }
                        }
                    ]
                }
            }
        }
    }

    # è®°å½• CloudWatch æŒ‡æ ‡
    cloudwatch_client.put_metric_data(
        Namespace='Karpenter/CostOptimization',
        MetricData=[
            {
                'MetricName': 'SpotRatio',
                'Value': optimal_ratio['spot_ratio'],
                'Unit': 'Percent',
                'Timestamp': datetime.now()
            },
            {
                'MetricName': 'EstimatedCostSaving',
                'Value': optimal_ratio['estimated_cost_saving'],
                'Unit': 'Percent',
                'Timestamp': datetime.now()
            }
        ]
    )

    print(f"Karpenter NodePool æ›´æ–°: Spot {spot_weight}%, OnDemand {ondemand_weight}%")

# EventBridge Rule: æ¯ 5 åˆ†é’Ÿæ‰§è¡Œ
def lambda_handler(event, context):
    # 1. è·å–æµé‡é¢„æµ‹
    traffic_pred = get_traffic_prediction()

    # 2. Spot ä¸­æ–­é£é™©é¢„æµ‹
    spot_risk = predict_spot_interruption_risk(
        instance_types=['c6i.xlarge', 'c5.xlarge'],
        availability_zones=['ap-northeast-2a', 'ap-northeast-2b', 'ap-northeast-2c']
    )

    # 3. è®¡ç®—æœ€ä¼˜æ¯”ä¾‹
    optimal_ratio = calculate_optimal_spot_ratio(traffic_pred, spot_risk)

    # 4. æ›´æ–° Karpenter NodePool
    update_karpenter_nodepool_weights(optimal_ratio)

    # 5. Slack é€šçŸ¥ï¼ˆæ¯”ä¾‹å˜æ›´æ—¶ï¼‰
    if abs(optimal_ratio['spot_ratio'] - 0.7) > 0.1:  # ç›¸æ¯”é»˜è®¤å€¼å˜æ›´è¶…è¿‡ 10%
        send_slack_notification(
            channel='#cost-optimization',
            message=f"""
            Karpenter Spot æ¯”ä¾‹è‡ªåŠ¨è°ƒæ•´

            **è°ƒæ•´ä¾æ®**: {optimal_ratio['reason']}
            **æ–°æ¯”ä¾‹**: Spot {optimal_ratio['spot_ratio']:.0%}, On-Demand {optimal_ratio['ondemand_ratio']:.0%}
            **é¢„è®¡æˆæœ¬èŠ‚çœ**: {optimal_ratio['estimated_cost_saving']:.1f}%

            æµé‡é¢„æµ‹: {traffic_pred['predicted_rps']} RPS (ç½®ä¿¡åº¦ {traffic_pred['confidence']:.0%})
            Spot ä¸­æ–­é£é™©: {sum(spot_risk.values()) / len(spot_risk):.2%}
            """
        )

    return {
        'statusCode': 200,
        'body': json.dumps(optimal_ratio)
    }
```

#### 9.6.3 åŸºäº CloudWatch æŒ‡æ ‡çš„æˆæœ¬å¼‚å¸¸æ£€æµ‹

åˆ©ç”¨ CloudWatch Anomaly Detection æå‰æ£€æµ‹é¢„ç®—è¶…æ”¯å¹¶è‡ªåŠ¨å‘Šè­¦ã€‚

```python
# æˆæœ¬å¼‚å¸¸æ£€æµ‹è®¾ç½®
import boto3

cloudwatch_client = boto3.client('cloudwatch', region_name='ap-northeast-2')
ce_client = boto3.client('ce', region_name='ap-northeast-2')  # Cost Explorer

# Step 1: å°†æ—¥æˆæœ¬æŒ‡æ ‡è®°å½•åˆ° CloudWatch
def record_daily_cost_to_cloudwatch():
    """å°† Cost Explorer æ•°æ®è®°å½•ä¸º CloudWatch Custom Metric"""

    # æŸ¥è¯¢æ˜¨æ—¥æˆæœ¬
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    today = datetime.now().strftime('%Y-%m-%d')

    response = ce_client.get_cost_and_usage(
        TimePeriod={
            'Start': yesterday,
            'End': today
        },
        Granularity='DAILY',
        Metrics=['UnblendedCost'],
        Filter={
            'Dimensions': {
                'Key': 'SERVICE',
                'Values': ['Amazon Elastic Kubernetes Service', 'Amazon EC2']
            }
        }
    )

    total_cost = float(response['ResultsByTime'][0]['Total']['UnblendedCost']['Amount'])

    # è®°å½• CloudWatch æŒ‡æ ‡
    cloudwatch_client.put_metric_data(
        Namespace='AWS/Billing',
        MetricData=[
            {
                'MetricName': 'DailyEKSCost',
                'Value': total_cost,
                'Unit': 'None',
                'Timestamp': datetime.now()
            }
        ]
    )

    return total_cost

# Step 2: è®¾ç½® Anomaly Detection
cloudwatch_client.put_anomaly_detector(
    Namespace='AWS/Billing',
    MetricName='DailyEKSCost',
    Stat='Sum'
)

# Step 3: è®¾ç½®å¼‚å¸¸æˆæœ¬å‘Šè­¦
cloudwatch_client.put_metric_alarm(
    AlarmName='EKS-Cost-Anomaly-Detection',
    AlarmDescription='EKS æ—¥æˆæœ¬å¼‚å¸¸æ£€æµ‹ (Anomaly Detection)',
    ActionsEnabled=True,
    AlarmActions=[
        'arn:aws:sns:ap-northeast-2:ACCOUNT_ID:cost-alerts'
    ],
    MetricName='DailyEKSCost',
    Namespace='AWS/Billing',
    Statistic='Sum',
    Period=86400,  # 24 å°æ—¶
    EvaluationPeriods=1,
    ThresholdMetricId='ad1',
    ComparisonOperator='LessThanLowerOrGreaterThanUpperThreshold',
    Metrics=[
        {
            'Id': 'm1',
            'ReturnData': True,
            'MetricStat': {
                'Metric': {
                    'Namespace': 'AWS/Billing',
                    'MetricName': 'DailyEKSCost'
                },
                'Period': 86400,
                'Stat': 'Sum'
            }
        },
        {
            'Id': 'ad1',
            'Expression': 'ANOMALY_DETECTION_BAND(m1, 2)',  # 2 standard deviations
            'Label': 'DailyEKSCost (expected)'
        }
    ]
)

print("æˆæœ¬å¼‚å¸¸æ£€æµ‹è®¾ç½®å®Œæˆ: CloudWatch Anomaly Detection + Alarm")
```

#### 9.6.4 åŸºäºé¢„æµ‹æ¨¡å‹çš„ Reserved Instances/Savings Plans ä¼˜åŒ–

åˆ©ç”¨ ML æ¨¡å‹é¢„æµ‹æœªæ¥èµ„æºä½¿ç”¨é‡ï¼Œä¼˜åŒ– Reserved Instances æˆ– Savings Plans çš„è´­ä¹°ã€‚

```python
# RI/Savings Plans è´­ä¹°ä¼˜åŒ–
from prophet import Prophet
import pandas as pd

def predict_baseline_capacity(historical_data: pd.DataFrame) -> dict:
    """åŸºäºè¿‡å»èµ„æºä½¿ç”¨é‡é¢„æµ‹ Baseline å®¹é‡"""

    # Prophet æ¨¡å‹è®­ç»ƒ
    model = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False
    )

    # è¿‡å»å®ä¾‹å°æ—¶(instance-hours)æ•°æ®
    df = historical_data[['ds', 'y']].copy()  # ds: æ—¥æœŸ, y: å®ä¾‹å°æ—¶
    model.fit(df)

    # é¢„æµ‹æœªæ¥ 90 å¤©
    future = model.make_future_dataframe(periods=90)
    forecast = model.predict(future)

    # Baseline è®¡ç®—: ä¸‹ä½ 20% percentileï¼ˆå§‹ç»ˆéœ€è¦çš„æœ€ä½å®¹é‡ï¼‰
    baseline_capacity = forecast['yhat'].quantile(0.20)

    # å³°å€¼å®¹é‡: ä¸Šä½ 95% percentile
    peak_capacity = forecast['yhat'].quantile(0.95)

    return {
        'baseline_capacity': baseline_capacity,
        'peak_capacity': peak_capacity,
        'forecast': forecast
    }

# æ‰§è¡Œç¤ºä¾‹
historical_data = pd.DataFrame({
    'ds': pd.date_range(start='2025-08-01', end='2026-02-01', freq='H'),
    'y': [50, 52, 48, 55, 60, 58, 62, ...]  # æ¯å°æ—¶å®ä¾‹æ•°
})

prediction = predict_baseline_capacity(historical_data)

print(f"""
[RI/Savings Plans è´­ä¹°å»ºè®®]

Baseline å®¹é‡ (ä¸‹ä½ 20%): {prediction['baseline_capacity']:.0f} å®ä¾‹
-> å»ºè®®: ä¸º {prediction['baseline_capacity']:.0f} ä¸ªå®ä¾‹è´­ä¹° 1 å¹´ RI

Peak å®¹é‡ (ä¸Šä½ 95%): {prediction['peak_capacity']:.0f} å®ä¾‹
-> Baseline è¶…å‡ºéƒ¨åˆ†: {prediction['peak_capacity'] - prediction['baseline_capacity']:.0f} ä¸ª
-> è¶…å‡ºéƒ¨åˆ†ä½¿ç”¨ Spot + On-Demand ç»„åˆ

é¢„è®¡æˆæœ¬èŠ‚çœ:
- åº”ç”¨ RI æ—¶: 30-40% èŠ‚çœ
- åº”ç”¨ Spot æ—¶: 60-70% èŠ‚çœï¼ˆé«˜å³°æ—¶æ®µï¼‰
- æ€»é¢„è®¡èŠ‚çœ: çº¦ 45%ï¼ˆæ··åˆç­–ç•¥ï¼‰
""")
```

**Cost Explorer é›†æˆ -- å®æ—¶æˆæœ¬è·Ÿè¸ª**

```yaml
# CloudWatch Dashboard: æˆæœ¬ä¼˜åŒ–ç°çŠ¶
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-optimization-dashboard
data:
  dashboard.json: |
    {
      "widgets": [
        {
          "type": "metric",
          "properties": {
            "title": "æ—¥ EKS æˆæœ¬è¶‹åŠ¿",
            "metrics": [
              ["AWS/Billing", "DailyEKSCost", {"stat": "Sum"}],
              [".", ".", {"stat": "Sum", "id": "ad1", "expression": "ANOMALY_DETECTION_BAND(m1, 2)"}]
            ],
            "period": 86400,
            "region": "ap-northeast-2"
          }
        },
        {
          "type": "metric",
          "properties": {
            "title": "Spot vs On-Demand æ¯”ä¾‹",
            "metrics": [
              ["Karpenter/CostOptimization", "SpotRatio"],
              [".", "OnDemandRatio"]
            ],
            "period": 300,
            "region": "ap-northeast-2"
          }
        },
        {
          "type": "metric",
          "properties": {
            "title": "ç´¯è®¡æˆæœ¬èŠ‚çœé¢",
            "metrics": [
              ["Karpenter/CostOptimization", "EstimatedCostSaving"]
            ],
            "period": 86400,
            "stat": "Sum",
            "region": "ap-northeast-2"
          }
        },
        {
          "type": "metric",
          "properties": {
            "title": "Spot ä¸­æ–­é¢‘ç‡",
            "metrics": [
              ["AWS/EC2Spot", "InterruptionRate", {"stat": "Average"}]
            ],
            "period": 3600,
            "region": "ap-northeast-2"
          }
        }
      ]
    }
```

:::info åŸºäºé¢„æµ‹çš„æˆæœ¬ä¼˜åŒ–çš„æ ¸å¿ƒ
ç»“åˆæµé‡é¢„æµ‹å’Œ Spot ä¸­æ–­é¢„æµ‹ï¼Œå¯ä»¥åœ¨**ä¸é™ä½æ€§èƒ½**çš„æƒ…å†µä¸‹å¤§å¹…é™ä½æˆæœ¬ã€‚é€šè¿‡ Karpenter çš„åŠ¨æ€ Spot æ¯”ä¾‹è°ƒæ•´æœ€å¤§åŒ–æˆæœ¬æ•ˆç‡ï¼Œé€šè¿‡ CloudWatch Anomaly Detection æå‰é˜²æ­¢é¢„ç®—è¶…æ”¯ï¼Œé€šè¿‡åŸºäº ML çš„å®¹é‡é¢„æµ‹ä¼˜åŒ– RI/Savings Plans è´­ä¹°ã€‚

**æˆæœ¬èŠ‚çœç­–ç•¥**ï¼š
1. **æœ€å¤§åŒ– Spot æ¯”ä¾‹**: æ­£å¸¸æ—¶æ®µ 80% Spot, é«˜å³°æ—¶æ®µ 40% Spot
2. **Baseline RI è´­ä¹°**: ä¸ºä¸‹ä½ 20% percentile å®¹é‡è´­ä¹° 1 å¹´ RI
3. **å¼‚å¸¸æ£€æµ‹**: é€šè¿‡ CloudWatch Anomaly Detection æå‰é¢„è­¦é¢„ç®—è¶…æ”¯
4. **åŠ¨æ€è°ƒæ•´**: æ¯ 5 åˆ†é’ŸåŸºäºæµé‡é¢„æµ‹ + Spot é£é™©åº¦è°ƒæ•´æ¯”ä¾‹

**é¢„æœŸæ•ˆæœ**ï¼š
- Spot åˆ©ç”¨: 60-70% æˆæœ¬èŠ‚çœï¼ˆå¯¹æ¯” On-Demandï¼‰
- RI åˆ©ç”¨: 30-40% æˆæœ¬èŠ‚çœï¼ˆå¯¹æ¯” On-Demandï¼‰
- æ··åˆç­–ç•¥: æ€»è®¡ 45-50% æˆæœ¬èŠ‚çœï¼ˆåŸºäºé¢„æµ‹ä¼˜åŒ–ï¼‰
:::

---

## 10. é›†æˆè¿ç»´ä»ªè¡¨æ¿

### 10.1 AMG ä»ªè¡¨æ¿é…ç½®

<MaturityTable />

é›†æˆè¿ç»´ä»ªè¡¨æ¿åŒæ—¶æ˜¾ç¤ºé¢„æµ‹æ•°æ®å’Œå®é™…æ•°æ®ã€‚

```json
{
  "dashboard": {
    "title": "EKS é¢„æµ‹è¿ç»´ä»ªè¡¨æ¿",
    "panels": [
      {
        "title": "æµé‡é¢„æµ‹ vs å®é™…",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{namespace='payment'}[5m]))",
            "legendFormat": "å®é™… RPS"
          },
          {
            "expr": "predicted_rps{service='payment'}",
            "legendFormat": "é¢„æµ‹ RPS"
          }
        ]
      },
      {
        "title": "æ‰©ç¼©å®¹äº‹ä»¶",
        "type": "timeseries",
        "targets": [
          {
            "expr": "kube_deployment_spec_replicas{deployment='payment-service'}",
            "legendFormat": "å½“å‰ Replicas"
          },
          {
            "expr": "predicted_replicas{deployment='payment-service'}",
            "legendFormat": "é¢„æµ‹æ‰€éœ€ Replicas"
          }
        ]
      },
      {
        "title": "SLO ç°çŠ¶",
        "type": "gauge",
        "targets": [
          {
            "expr": "1 - (sum(rate(http_requests_total{status=~'5..'}[30d])) / sum(rate(http_requests_total[30d])))",
            "legendFormat": "å¯ç”¨æ€§ SLO"
          }
        ],
        "thresholds": {
          "steps": [
            {"value": 0.999, "color": "green"},
            {"value": 0.995, "color": "yellow"},
            {"value": 0, "color": "red"}
          ]
        }
      },
      {
        "title": "Error Budget å‰©ä½™",
        "type": "stat",
        "targets": [
          {
            "expr": "error_budget_remaining_percent{service='payment'}",
            "legendFormat": "å‰©ä½™ Error Budget"
          }
        ]
      },
      {
        "title": "é¢„æµ‹ç²¾åº¦",
        "type": "stat",
        "targets": [
          {
            "expr": "prediction_accuracy_percent",
            "legendFormat": "ç²¾åº¦"
          }
        ]
      },
      {
        "title": "äº‹ä»¶è‡ªåŠ¨å“åº”ç‡",
        "type": "stat",
        "targets": [
          {
            "expr": "auto_remediation_success_rate",
            "legendFormat": "è‡ªåŠ¨å“åº”æˆåŠŸç‡"
          }
        ]
      }
    ]
  }
}
```

### 10.2 æ ¸å¿ƒä»ªè¡¨æ¿é¢æ¿

<DashboardPanels />

---

## 11. æ€»ç»“

### 11.1 å¼•å…¥è·¯çº¿å›¾

```
Phase 1: æ„å»ºå¯è§‚æµ‹æ€§åŸºç¡€
  â””â”€â”€ AMP/AMG + CloudWatch + Anomaly Detection

Phase 2: é¢„æµ‹æ‰©ç¼©å®¹
  â””â”€â”€ Prophet/ARIMA + Karpenter å…ˆå‘åˆ¶äººé…ç½®

Phase 3: AI Agent æ‰©å±•
  â””â”€â”€ Q Developer + Strands + Kagent + MCP é›†æˆ

Phase 4: Kiro ç¼–ç¨‹å¼è°ƒè¯•
  â””â”€â”€ Kiro Spec -> è‡ªåŠ¨è¯Šæ–­ -> è‡ªåŠ¨ä¿®å¤

Phase 5: Chaos Engineering + åé¦ˆå¾ªç¯
  â””â”€â”€ FIS å®éªŒ -> AI å­¦ä¹  -> è‡ªä¸»è¿ç»´è¿›åŒ–
```

### 11.2 åç»­æ­¥éª¤

- **[1. AIOps æˆ˜ç•¥æŒ‡å—](./aiops-introduction.md)**: é¢„æµ‹è¿ç»´çš„ä¸Šå±‚æˆ˜ç•¥ -- AIOps å…¨å±€
- **[2. æ™ºèƒ½å¯è§‚æµ‹æ€§æ ˆ](./aiops-observability-stack.md)**: é¢„æµ‹è¿ç»´çš„æ•°æ®åŸºç¡€ -- å¯è§‚æµ‹æ€§æ„å»º
- **[3. AIDLC æ¡†æ¶](./aidlc-framework.md)**: åŒ…å«é¢„æµ‹è¿ç»´çš„ AI å¼€å‘ç”Ÿå‘½å‘¨æœŸ

### 11.3 å­¦ä¹ è·¯å¾„

```
[ä¸Šä¸€ç¯‡] 1. AIOps æˆ˜ç•¥æŒ‡å— -- ç†è§£æˆ˜ç•¥å’Œæ–¹å‘æ€§
     â†“
[ä¸Šä¸€ç¯‡] 2. æ™ºèƒ½å¯è§‚æµ‹æ€§æ ˆ -- æ„å»ºæ•°æ®é‡‡é›†Â·åˆ†æåŸºç¡€
     â†“
[ä¸Šä¸€ç¯‡] 3. AIDLC æ¡†æ¶ -- AI é©±åŠ¨å¼€å‘æ–¹æ³•è®º
     â†“
[å½“å‰æ–‡æ¡£] 4. é¢„æµ‹æ‰©ç¼©å®¹åŠè‡ªåŠ¨æ¢å¤ -- å®ç°è‡ªä¸»è¿ç»´
```

:::info ç›¸å…³æ–‡æ¡£

- [1. AIOps æˆ˜ç•¥æŒ‡å—](./aiops-introduction.md) -- AIOps å…¨å±€æˆ˜ç•¥
- [2. æ™ºèƒ½å¯è§‚æµ‹æ€§æ ˆ](./aiops-observability-stack.md) -- å¯è§‚æµ‹æ€§åŸºç¡€è®¾æ–½
- [3. AIDLC æ¡†æ¶](./aidlc-framework.md) -- AI é©±åŠ¨å¼€å‘æ–¹æ³•è®º
:::
