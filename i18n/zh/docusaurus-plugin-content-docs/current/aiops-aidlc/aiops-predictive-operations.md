---
title: "é¢„æµ‹æ€§æ‰©ç¼©å®¹ä¸è‡ªåŠ¨æ¢å¤æ¨¡å¼"
sidebar_label: "4. é¢„æµ‹æ€§æ‰©ç¼©å®¹ä¸è‡ªåŠ¨æ¢å¤"
description: "åŸºäºMLçš„é¢„æµ‹æ€§è‡ªåŠ¨æ‰©ç¼©å®¹ã€Karpenter+AIå…ˆå‘åˆ¶äººé¢„é…ç½®ã€AI Agentè‡ªä¸»äº‹ä»¶å“åº”ã€Kiroç¨‹åºåŒ–è°ƒè¯•æ¨¡å¼"
sidebar_position: 4
category: "aiops-aidlc"
tags: [aiops, predictive-scaling, auto-remediation, karpenter, self-healing, eks, kiro, mcp, ai-agent, chaos-engineering]
last_update:
  date: 2026-02-14
  author: devfloor9
---

import { ScalingComparison, ResponsePatterns, MaturityTable, EvolutionStages, MLModelComparison, AnomalyMetrics, RightSizingResults, ChaosExperiments, DashboardPanels } from '@site/src/components/PredictiveOpsTables';

# é¢„æµ‹æ€§æ‰©ç¼©å®¹ä¸è‡ªåŠ¨æ¢å¤æ¨¡å¼

> ğŸ“… **æ’°å†™æ—¥æœŸ**: 2026-02-12 | **ä¿®æ”¹æ—¥æœŸ**: 2026-02-14 | â±ï¸ **é˜…è¯»æ—¶é—´**: çº¦29åˆ†é’Ÿ

---

## 1. æ¦‚è¿°

### 1.1 ä»å“åº”å¼åˆ°è‡ªä¸»å¼

EKSè¿è¥çš„æ¼”è¿›åˆ†ä¸º **å“åº”å¼ â†’ é¢„æµ‹å¼ â†’ è‡ªä¸»å¼** ä¸‰ä¸ªé˜¶æ®µã€‚

<EvolutionStages />

:::info æœ¬æ–‡æ¡£çš„èŒƒå›´
è¶…è¶Šå“åº”å¼æ‰©ç¼©å®¹çš„å±€é™ï¼Œæ¶µç›–åŸºäºMLçš„é¢„æµ‹æ€§æ‰©ç¼©å®¹å’Œé€šè¿‡AI Agentå®ç°çš„è‡ªä¸»æ¢å¤æ¨¡å¼ã€‚ç‰¹åˆ«ä»¥Kiro+MCPä¸ºåŸºç¡€çš„ **ç¨‹åºåŒ–è°ƒè¯•** å’ŒKagent/Strandsä¸ºåŸºç¡€çš„ **è‡ªåŠ¨äº‹ä»¶å“åº”** ä¸ºæ ¸å¿ƒè¿›è¡Œè¯´æ˜ã€‚
:::

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦é¢„æµ‹æ€§è¿è¥

- **HPAçš„å±€é™æ€§**: æŒ‡æ ‡è¶…è¿‡é˜ˆå€¼åæ‰å“åº” â†’ ç”¨æˆ·ä½“éªŒå·²å—å½±å“
- **å†·å¯åŠ¨é—®é¢˜**: æ–°Podå¯åŠ¨éœ€è¦30ç§’-2åˆ†é’Ÿ â†’ æµé‡çªå¢æ—¶æ— æ³•åº”å¯¹
- **èŠ‚ç‚¹é¢„é…ç½®å»¶è¿Ÿ**: å³ä½¿æ˜¯Karpenterï¼ŒèŠ‚ç‚¹å¯åŠ¨ä¹Ÿéœ€è¦1-3åˆ†é’Ÿ
- **å¤åˆæ•…éšœ**: å•ä¸€æŒ‡æ ‡æ— æ³•æ£€æµ‹çš„å¤šå› ç´ æ•…éšœæ—¥ç›Šå¢å¤š
- **æˆæœ¬ä½æ•ˆ**: è¿‡åº¦é¢„ç•™å†—ä½™èµ„æº â†’ æˆæœ¬æµªè´¹

---

## 2. åŸºäºMLçš„é¢„æµ‹æ€§æ‰©ç¼©å®¹

### 2.1 HPAçš„å±€é™æ€§

HPA(Horizontal Pod Autoscaler)åŸºäº **å½“å‰æŒ‡æ ‡** è¿›è¡Œå“åº”ï¼Œå› æ­¤å­˜åœ¨ç»“æ„æ€§å±€é™ã€‚

<ScalingComparison />

```
[HPAçš„å“åº”å¼æ‰©ç¼©å®¹]

æµé‡ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
                      â†‘ è¶…è¿‡é˜ˆå€¼
                      |
Podæ•°  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                  â†‘ å¼€å§‹æ‰©å®¹
                  |  (å»¶è¿Ÿå‘ç”Ÿ)
ç”¨æˆ·   âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ—âœ—âœ—âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
ä½“éªŒ              â†‘ æ€§èƒ½ä¸‹é™åŒºé—´

[MLé¢„æµ‹æ€§æ‰©ç¼©å®¹]

æµé‡ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
             â†‘ é¢„æµ‹æ—¶ç‚¹ (30åˆ†é’Ÿå‰)
             |
Podæ•°  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
             â†‘ æå‰æ‰©å®¹
             |
ç”¨æˆ·   âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
ä½“éªŒ     (æ— æ€§èƒ½ä¸‹é™)
```

### 2.2 æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹

ç”¨äºé¢„æµ‹EKSå·¥ä½œè´Ÿè½½æµé‡æ¨¡å¼çš„ä»£è¡¨æ€§MLæ¨¡å‹ï¼š

<MLModelComparison />

### 2.3 åŸºäºProphetçš„é¢„æµ‹æ€§æ‰©ç¼©å®¹å®ç°

```python
# åŸºäºProphetçš„EKSæµé‡é¢„æµ‹
import boto3
from prophet import Prophet
import pandas as pd
from datetime import datetime, timedelta

def fetch_metrics_from_amp(workspace_id, query, hours=168):
    """ä»AMPæŸ¥è¯¢è¿‡å»7å¤©çš„æŒ‡æ ‡"""
    client = boto3.client('amp', region_name='ap-northeast-2')
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=hours)

    response = client.query_range(
        workspaceId=workspace_id,
        query=query,
        startTime=start_time,
        endTime=end_time,
        step='5m'
    )
    return response

def predict_scaling(metrics_df, forecast_hours=2):
    """ä½¿ç”¨Propheté¢„æµ‹æœªæ¥æµé‡"""
    # è½¬æ¢ä¸ºProphetæ ¼å¼
    df = metrics_df.rename(columns={
        'timestamp': 'ds',
        'value': 'y'
    })

    model = Prophet(
        changepoint_prior_scale=0.05,
        seasonality_mode='multiplicative',
        daily_seasonality=True,
        weekly_seasonality=True,
    )
    model.fit(df)

    # é¢„æµ‹æœªæ¥forecast_hours
    future = model.make_future_dataframe(
        periods=forecast_hours * 12,  # 5åˆ†é’Ÿé—´éš”
        freq='5min'
    )
    forecast = model.predict(future)

    return forecast[['ds', 'yhat', 'yhat_upper', 'yhat_lower']]

def calculate_required_pods(predicted_rps, pod_capacity_rps=100):
    """åŸºäºé¢„æµ‹RPSè®¡ç®—æ‰€éœ€Podæ•°"""
    # ä½¿ç”¨ä¸Šé™å€¼(yhat_upper)ç¡®ä¿å®‰å…¨ä½™é‡
    required = int(predicted_rps / pod_capacity_rps) + 1
    return max(required, 2)  # æœ€å°‘ç»´æŒ2ä¸ª

def apply_scaling(namespace, deployment, target_replicas):
    """é€šè¿‡kubectlåº”ç”¨æ‰©ç¼©å®¹"""
    import subprocess
    cmd = f"kubectl scale deployment/{deployment} -n {namespace} --replicas={target_replicas}"
    subprocess.run(cmd.split(), check=True)
    print(f"Scaled {deployment} to {target_replicas} replicas")
```

### 2.4 åŸºäºCronJobçš„é¢„æµ‹æ€§æ‰©ç¼©å®¹è‡ªåŠ¨åŒ–

```yaml
# å®šæœŸæ‰§è¡Œé¢„æµ‹æ€§æ‰©ç¼©å®¹çš„CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: predictive-scaler
  namespace: scaling
spec:
  schedule: "*/15 * * * *"  # æ¯15åˆ†é’Ÿæ‰§è¡Œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: predictive-scaler
          containers:
            - name: scaler
              image: my-registry/predictive-scaler:latest
              env:
                - name: AMP_WORKSPACE_ID
                  value: "ws-xxxxx"
                - name: TARGET_NAMESPACE
                  value: "payment"
                - name: TARGET_DEPLOYMENT
                  value: "payment-service"
                - name: FORECAST_HOURS
                  value: "2"
              resources:
                requests:
                  cpu: 500m
                  memory: 1Gi
                limits:
                  cpu: "1"
                  memory: 2Gi
          restartPolicy: OnFailure
```

### 2.5 ç½‘ç»œæ€§èƒ½é¢„æµ‹åŠMLæ¨ç†å·¥ä½œè´Ÿè½½ä¼˜åŒ–

EKSçš„ **Container Network Observability** å¯ä»¥ç²¾ç»†ç›‘æ§Pod-to-Podé€šä¿¡æ¨¡å¼ï¼Œæå‰é¢„æµ‹ç½‘ç»œç“¶é¢ˆå¹¶ä¼˜åŒ–MLæ¨ç†å·¥ä½œè´Ÿè½½çš„æ€§èƒ½ã€‚

#### Container Network Observabilityæ•°æ®åº”ç”¨

**1. Pod-to-Podé€šä¿¡æ¨¡å¼ â†’ ç½‘ç»œç“¶é¢ˆé¢„æµ‹**

```python
# åŸºäºContainer Network ObservabilityæŒ‡æ ‡çš„ç“¶é¢ˆé¢„æµ‹
import boto3
from prophet import Prophet
import pandas as pd

def predict_network_bottleneck(cluster_name, namespace):
    """
    é¢„æµ‹Pod-to-Podç½‘ç»œå»¶è¿Ÿï¼Œåˆ¤æ–­ç“¶é¢ˆå¯èƒ½æ€§ã€‚
    """
    cloudwatch = boto3.client('cloudwatch')

    # æŸ¥è¯¢Container Network ObservabilityæŒ‡æ ‡
    metrics = cloudwatch.get_metric_data(
        MetricDataQueries=[
            {
                'Id': 'rx_latency',
                'MetricStat': {
                    'Metric': {
                        'Namespace': 'ContainerInsights',
                        'MetricName': 'pod_network_rx_latency_ms',
                        'Dimensions': [
                            {'Name': 'ClusterName', 'Value': cluster_name},
                            {'Name': 'Namespace', 'Value': namespace}
                        ]
                    },
                    'Period': 300,
                    'Stat': 'Average'
                }
            },
            {
                'Id': 'tx_bytes',
                'MetricStat': {
                    'Metric': {
                        'Namespace': 'ContainerInsights',
                        'MetricName': 'pod_network_tx_bytes',
                        'Dimensions': [
                            {'Name': 'ClusterName', 'Value': cluster_name},
                            {'Name': 'Namespace', 'Value': namespace}
                        ]
                    },
                    'Period': 300,
                    'Stat': 'Sum'
                }
            }
        ],
        StartTime=datetime.utcnow() - timedelta(days=7),
        EndTime=datetime.utcnow()
    )

    # ä½¿ç”¨Prophetæ¨¡å‹é¢„æµ‹æœªæ¥2å°æ—¶
    df = pd.DataFrame({
        'ds': [d['Timestamp'] for d in metrics['MetricDataResults'][0]['Timestamps']],
        'y': [d for d in metrics['MetricDataResults'][0]['Values']]
    })

    model = Prophet(changepoint_prior_scale=0.05)
    model.fit(df)

    future = model.make_future_dataframe(periods=24, freq='5min')
    forecast = model.predict(future)

    # ç“¶é¢ˆé¢„æµ‹ï¼šé¢„è®¡å»¶è¿Ÿå°†æ¯”å¹³æ—¶å¢åŠ 2å€ä»¥ä¸Š
    baseline = df['y'].mean()
    predicted_peak = forecast['yhat'].iloc[-1]

    if predicted_peak > baseline * 2:
        return {
            'bottleneck_risk': 'HIGH',
            'predicted_latency_ms': predicted_peak,
            'baseline_latency_ms': baseline,
            'action': 'consider_network_policy_optimization'
        }
    return {'bottleneck_risk': 'LOW'}
```

**2. è·¨AZæµé‡è¶‹åŠ¿ â†’ æˆæœ¬ä¼˜åŒ–é¢„æµ‹**

```promql
# è·¨AZç½‘ç»œæµé‡æˆæœ¬è¿½è¸ª
sum(rate(pod_network_tx_bytes{
  source_az!="", dest_az!="",
  source_az!=dest_az
}[5m])) by (source_az, dest_az)
* 0.01 / 1024 / 1024 / 1024  # $0.01/GB
```

**æˆæœ¬ä¼˜åŒ–ç­–ç•¥**ï¼š

- **æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦**: åˆ©ç”¨Kubernetes Topology Aware Hintsä¼˜å…ˆé€‰æ‹©åŒä¸€AZå†…é€šä¿¡
- **æœåŠ¡ç½‘æ ¼ä¼˜åŒ–**: é€šè¿‡Istio locality load balancingæœ€å°åŒ–è·¨AZæµé‡
- **åŸºäºé¢„æµ‹çš„éƒ¨ç½²**: MLæ¨¡å‹å­¦ä¹ é€šä¿¡æ¨¡å¼å¹¶å»ºè®®æœ€ä¼˜AZéƒ¨ç½²æ–¹æ¡ˆ

```yaml
# å¯ç”¨Topology Aware Hints
apiVersion: v1
kind: Service
metadata:
  name: ml-inference-service
  annotations:
    service.kubernetes.io/topology-mode: Auto
spec:
  selector:
    app: ml-inference
  ports:
    - port: 8080
  type: ClusterIP
```

#### MLæ¨ç†å·¥ä½œè´Ÿè½½æ€§èƒ½é¢„æµ‹

**1. Rayã€vLLMã€Tritonã€PyTorchå·¥ä½œè´Ÿè½½ç½‘ç»œæ€§èƒ½ç›‘æ§**

```yaml
# vLLMæ¨ç†æœåŠ¡ç½‘ç»œç›‘æ§
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-network-monitoring
data:
  metrics.yaml: |
    # Container Network ObservabilityæŒ‡æ ‡
    metrics:
      - pod_network_rx_bytes
      - pod_network_tx_bytes
      - pod_network_rx_latency_ms
      - pod_network_rx_errors_total

    # é¢å¤–è‡ªå®šä¹‰æŒ‡æ ‡
    custom_metrics:
      - name: vllm_inference_network_throughput_mbps
        query: |
          sum(rate(pod_network_rx_bytes{app="vllm-inference"}[1m]))
          / 1024 / 1024

      - name: vllm_model_load_network_time_ms
        query: |
          histogram_quantile(0.99,
            rate(pod_network_rx_latency_bucket{
              app="vllm-inference",
              operation="model_load"
            }[5m])
          )
```

**Rayåˆ†å¸ƒå¼æ¨ç†ç½‘ç»œæ¨¡å¼**ï¼š

```python
# Rayé›†ç¾¤çš„ç½‘ç»œç“¶é¢ˆæ£€æµ‹
import ray
from ray import serve

@serve.deployment
class LLMInferenceDeployment:
    def __init__(self):
        self.model = load_model()
        self.network_monitor = NetworkMonitor()

    async def __call__(self, request):
        # ç½‘ç»œå»¶è¿Ÿè¿½è¸ª
        start_time = time.time()

        # Rayçš„åˆ†å¸ƒå¼æ¨ç†è°ƒç”¨
        result = await self.model.generate(request.prompt)

        network_latency = time.time() - start_time

        # å‘é€è‡ªå®šä¹‰æŒ‡æ ‡åˆ°CloudWatch
        self.network_monitor.record_latency(network_latency)

        # æ£€æµ‹åˆ°ç½‘ç»œç“¶é¢ˆæ—¶è§¦å‘æ‰©å®¹
        if network_latency > 200:  # è¶…è¿‡200ms
            trigger_scale_out()

        return result
```

**2. æ¨ç†å»¶è¿Ÿ â†’ æ‰©å®¹è§¦å‘é¢„æµ‹**

```python
# åŸºäºMLæ¨ç†å»¶è¿Ÿçš„é¢„æµ‹æ€§æ‰©ç¼©å®¹
def predict_inference_scaling(service_name, forecast_hours=2):
    """
    å­¦ä¹ æ¨ç†å»¶è¿Ÿæ¨¡å¼ï¼Œé¢„æµ‹éœ€è¦æ‰©å®¹çš„æ—¶é—´ç‚¹ã€‚
    """
    # æ”¶é›†è¿‡å»7å¤©çš„æ¨ç†å»¶è¿Ÿæ•°æ®
    latency_data = fetch_inference_latency_from_cloudwatch(
        service_name=service_name,
        days=7
    )

    # æ”¶é›†è¯·æ±‚é‡æ•°æ®
    request_volume = fetch_request_volume(service_name, days=7)

    # åˆ†æå»¶è¿Ÿä¸è¯·æ±‚é‡çš„ç›¸å…³æ€§
    df = pd.DataFrame({
        'timestamp': latency_data['timestamps'],
        'latency_p99': latency_data['p99'],
        'request_rate': request_volume['rate']
    })

    # è®¡ç®—é˜ˆå€¼ï¼šP99å»¶è¿Ÿ > 500msæ—¶çš„è¯·æ±‚é‡
    threshold_requests = df[df['latency_p99'] > 500]['request_rate'].min()

    # ä½¿ç”¨Propheté¢„æµ‹æœªæ¥è¯·æ±‚é‡
    prophet_df = df[['timestamp', 'request_rate']].rename(
        columns={'timestamp': 'ds', 'request_rate': 'y'}
    )

    model = Prophet()
    model.fit(prophet_df)

    future = model.make_future_dataframe(
        periods=forecast_hours * 12,  # 5åˆ†é’Ÿé—´éš”
        freq='5min'
    )
    forecast = model.predict(future)

    # é¢„æµ‹éœ€è¦æ‰©å®¹çš„æ—¶é—´ç‚¹
    scale_out_needed = forecast[
        forecast['yhat'] > threshold_requests
    ]['ds'].min()

    if pd.notna(scale_out_needed):
        # åœ¨é¢„æµ‹æ—¶é—´30åˆ†é’Ÿå‰å…ˆå‘åˆ¶äººåœ°æ‰©å®¹
        preemptive_time = scale_out_needed - timedelta(minutes=30)

        return {
            'scale_out_recommended': True,
            'recommended_time': preemptive_time,
            'predicted_request_rate': forecast.iloc[-1]['yhat'],
            'threshold': threshold_requests,
            'current_replicas': get_current_replicas(service_name),
            'recommended_replicas': calculate_required_replicas(
                forecast.iloc[-1]['yhat'],
                threshold_requests
            )
        }

    return {'scale_out_recommended': False}
```

**3. GPUåˆ©ç”¨ç‡ + ç½‘ç»œå¸¦å®½ç›¸å…³æ€§åˆ†æ**

```promql
# GPUåˆ©ç”¨ç‡ä¸ç½‘ç»œå¸¦å®½çš„ç›¸å…³æ€§
# (NVIDIA DCGM ExporteræŒ‡æ ‡ + Container Network Observability)

# GPUåˆ©ç”¨ç‡
DCGM_FI_DEV_GPU_UTIL{
  namespace="ml-inference",
  pod=~"vllm-.*"
}

# åŒæ—¶ç½‘ç»œæ¥æ”¶å¸¦å®½
sum(rate(pod_network_rx_bytes{
  namespace="ml-inference",
  pod=~"vllm-.*"
}[1m])) by (pod)

# ç›¸å…³æ€§åˆ†æï¼šGPUåˆ©ç”¨ç‡ < 50% && ç½‘ç»œå¸¦å®½ > 100MB/s
# â†’ ç½‘ç»œç“¶é¢ˆæ­£åœ¨é˜»ç¢GPUåˆ©ç”¨ç‡
```

**ä¼˜åŒ–ç­–ç•¥**ï¼š

```yaml
# è§£å†³ç½‘ç»œç“¶é¢ˆï¼šå¯ç”¨Enhanced Networkingå’ŒENA Express
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: ml-inference-pool
spec:
  template:
    spec:
      requirements:
        - key: karpenter.k8s.aws/instance-family
          operator: In
          values: ["p5", "p4d"]  # æœ€æ–°GPUå®ä¾‹ (æ”¯æŒENA Express)
        - key: karpenter.k8s.aws/instance-size
          operator: In
          values: ["24xlarge", "48xlarge"]
      nodeClassRef:
        name: ml-inference-class
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: ml-inference-class
spec:
  amiSelectorTerms:
    - alias: al2023@latest
  userData: |
    #!/bin/bash
    # å¯ç”¨ENA Express (100Gbpsç½‘ç»œæ€§èƒ½)
    ethtool -K eth0 ena-express on

    # TCP BBRæ‹¥å¡æ§åˆ¶ (é«˜å¸¦å®½ä¼˜åŒ–)
    echo "net.ipv4.tcp_congestion_control=bbr" >> /etc/sysctl.conf
    sysctl -p
```

#### EKS Auto Modeè‡ªåŠ¨æ¢å¤/è‡ªæ„ˆ

**EKS Auto Mode** è‡ªåŠ¨æ£€æµ‹å’Œæ¢å¤èŠ‚ç‚¹æ•…éšœï¼Œå¤§å¹…æå‡ **MTTRï¼ˆå¹³å‡æ¢å¤æ—¶é—´ï¼‰**ã€‚

**1. è‡ªåŠ¨èŠ‚ç‚¹æ•…éšœæ£€æµ‹ä¸æ›¿æ¢**

```mermaid
graph TD
    A[èŠ‚ç‚¹å¥åº·æ£€æŸ¥] -->|å¤±è´¥| B[Auto Modeæ£€æµ‹]
    B --> C{æ•…éšœç±»å‹åˆ†æ}
    C -->|ç¡¬ä»¶æ•…éšœ| D[ç«‹å³é¢„é…ç½®æ–°èŠ‚ç‚¹]
    C -->|ç½‘ç»œæ•…éšœ| E[å°è¯•ç½‘ç»œé‡æ–°é…ç½®]
    C -->|è½¯ä»¶æ•…éšœ| F[è‡ªåŠ¨é‡å¯]
    D --> G[Podè‡ªåŠ¨è¿ç§»]
    E --> G
    F --> G
    G --> H[æœåŠ¡æ¢å¤éªŒè¯]
    H -->|å¤±è´¥| B
    H -->|æˆåŠŸ| I[æ¢å¤å®Œæˆ]
```

**è‡ªåŠ¨æ¢å¤è§¦å‘æ¡ä»¶**ï¼š

- **NodeNotReady**: èŠ‚ç‚¹å¤„äºNotReadyçŠ¶æ€è¶…è¿‡5åˆ†é’Ÿ
- **NetworkUnavailable**: ç½‘ç»œæ’ä»¶æ•…éšœ
- **MemoryPressure/DiskPressure**: èµ„æºä¸è¶³
- **Unschedulable**: èŠ‚ç‚¹å¤„äºä¸å¯è°ƒåº¦çŠ¶æ€

**2. OSè¡¥ä¸è‡ªåŠ¨åŒ–**

Auto Modeè‡ªåŠ¨æ‰§è¡Œ **é›¶åœæœºOSè¡¥ä¸**ï¼š

```yaml
# Auto ModeèŠ‚ç‚¹è‡ªåŠ¨æ›´æ–°ç­–ç•¥ (æ— éœ€ç”¨æˆ·é…ç½®)
# AWSè‡ªåŠ¨ç®¡ç†çš„å†…éƒ¨ç­–ç•¥ç¤ºä¾‹
nodeMaintenance:
  autoUpdate: true
  maintenanceWindow:
    preferredDays: ["Sunday", "Wednesday"]
    preferredHours: ["02:00-06:00"]  # UTC
  strategy:
    type: RollingUpdate
    maxUnavailable: 1
    respectPodDisruptionBudget: true
```

**è¡¥ä¸æµç¨‹**ï¼š

1. **é¢„é…ç½®æ–°èŠ‚ç‚¹**: ä½¿ç”¨æœ€æ–°AL2023 AMIåˆ›å»ºæ–°èŠ‚ç‚¹
2. **Podå®‰å…¨è¿ç§»**: éµå®ˆPDBï¼Œä»æ—§èŠ‚ç‚¹è¿ç§»Podåˆ°æ–°èŠ‚ç‚¹
3. **ç§»é™¤æ—§èŠ‚ç‚¹**: æ‰€æœ‰Podè¿ç§»å®Œæˆåç»ˆæ­¢æ—§èŠ‚ç‚¹
4. **éªŒè¯**: ç¡®è®¤æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡

**3. å®‰å…¨æœåŠ¡é›†æˆ**

Auto Modeä¸AWSå®‰å…¨æœåŠ¡è‡ªåŠ¨é›†æˆï¼Œæ”¯æŒ **å®‰å…¨äº‹ä»¶è‡ªåŠ¨å“åº”**ï¼š

```
GuardDuty Extended Threat Detection
  â†“ (æ£€æµ‹åˆ°åŠ å¯†è´§å¸æŒ–çŸ¿)
Auto Modeè‡ªåŠ¨å“åº”
  â†“
1. éš”ç¦»å—å½±å“çš„èŠ‚ç‚¹ (Taint: NoSchedule)
2. é¢„é…ç½®æ–°èŠ‚ç‚¹
3. å°†Podè¿ç§»åˆ°å¹²å‡€çš„èŠ‚ç‚¹
4. ç»ˆæ­¢å—æ„ŸæŸ“èŠ‚ç‚¹å¹¶æ”¶é›†å–è¯æ•°æ®
5. åœ¨CloudWatch Logsä¸­è®°å½•äº‹ä»¶
```

**4. é¢„æµ‹æ€§è§†è§’ï¼šAuto Modeçš„MTTRæ”¹å–„**

**ä¼ ç»Ÿæ‰‹åŠ¨è¿ç»´ vs Auto Modeå¯¹æ¯”**ï¼š

| æ•…éšœåœºæ™¯ | æ‰‹åŠ¨è¿ç»´MTTR | Auto Mode MTTR | æ”¹å–„ç‡ |
|--------------|----------------|----------------|--------|
| èŠ‚ç‚¹ç¡¬ä»¶æ•…éšœ | 15-30åˆ†é’Ÿ | 2-5åˆ†é’Ÿ | **ç¼©çŸ­83%** |
| OSå®‰å…¨è¡¥ä¸ | æ•°å°æ—¶ (è®¡åˆ’åœæœº) | 0åˆ†é’Ÿ (é›¶åœæœº) | **æ”¹å–„100%** |
| ç½‘ç»œæ’ä»¶æ•…éšœ | 10-20åˆ†é’Ÿ | 1-3åˆ†é’Ÿ | **ç¼©çŸ­85%** |
| æ¶æ„è½¯ä»¶æ„ŸæŸ“ | 30åˆ†é’Ÿ-1å°æ—¶ | 5-10åˆ†é’Ÿ | **ç¼©çŸ­80%** |

**é¢„æµ‹æ€§è¿ç»´è§†è§’ä¸‹Auto Modeçš„ä»·å€¼**ï¼š

- **å…ˆå‘åˆ¶äººæ›¿æ¢**: æ£€æµ‹åˆ°èŠ‚ç‚¹æ€§èƒ½ä¸‹é™ååœ¨æ•…éšœå‘ç”Ÿå‰è¿›è¡Œæ›¿æ¢
- **è‡ªåŠ¨å®¹é‡ç®¡ç†**: å­¦ä¹ å·¥ä½œè´Ÿè½½æ¨¡å¼è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹ç±»å‹
- **æ— ä¸­æ–­ç»´æŠ¤**: æ— éœ€ç”¨æˆ·ä»‹å…¥è‡ªåŠ¨æ‰§è¡Œå®‰å…¨è¡¥ä¸å’Œå‡çº§
- **æˆæœ¬ä¼˜åŒ–**: Spotå®ä¾‹ä¸­æ–­æ—¶è‡ªåŠ¨æ•…éšœè½¬ç§»åˆ°On-Demand

:::tip Auto Mode + é¢„æµ‹æ€§è¿ç»´çš„ååŒæ•ˆåº”
Auto Modeçš„è‡ªåŠ¨æ¢å¤åŠŸèƒ½æ˜¯ **å“åº”å¼(Reactive)** çš„ï¼Œä½†ä¸Container Network Observabilityæ•°æ®ç»“åˆåå¯å®ç° **é¢„æµ‹å¼(Predictive)** è¿ç»´ã€‚é€šè¿‡æ£€æµ‹ç½‘ç»œæ€§èƒ½ä¸‹é™æ¨¡å¼ï¼Œå¯ä»¥åœ¨æ•…éšœå‘ç”Ÿå‰æ›¿æ¢èŠ‚ç‚¹ï¼Œæˆ–æå‰æ¶ˆé™¤MLæ¨ç†å·¥ä½œè´Ÿè½½çš„ç½‘ç»œç“¶é¢ˆã€‚
:::

---

## 3. Karpenter + AIé¢„æµ‹

### 3.1 KarpenteråŸºæœ¬å·¥ä½œåŸç†

Karpenteræ£€æµ‹åˆ°Pending Podå **è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å®ä¾‹ç±»å‹** å¹¶è¿›è¡Œé¢„é…ç½®ã€‚

```yaml
# Karpenter NodePoolé…ç½®
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: default
spec:
  template:
    spec:
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64", "arm64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["on-demand", "spot"]
        - key: karpenter.k8s.aws/instance-family
          operator: In
          values: ["m7g", "m7i", "c7g", "c7i", "r7g"]
        - key: karpenter.k8s.aws/instance-size
          operator: In
          values: ["medium", "large", "xlarge", "2xlarge"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default
  limits:
    cpu: "100"
    memory: 400Gi
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: default
spec:
  role: KarpenterNodeRole
  amiSelectorTerms:
    - alias: al2023@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: my-cluster
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: my-cluster
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 100Gi
        volumeType: gp3
        iops: 3000
        throughput: 125
```

### 3.2 åŸºäºAIé¢„æµ‹çš„å…ˆå‘åˆ¶äººé¢„é…ç½®

Karpenteræœ¬èº«å¯¹Pending Podåšå‡ºå“åº”ï¼Œä½† **ä¸AIé¢„æµ‹ç»“åˆ** åå¯ä»¥å…ˆå‘åˆ¶äººåœ°é¢„é…ç½®èŠ‚ç‚¹ã€‚

```mermaid
graph TD
    subgraph Prediction["ğŸ§  é¢„æµ‹å±‚"]
        CW_M["CloudWatch<br/>Metrics"]
        ML["MLæ¨¡å‹<br/>(Prophet/ARIMA)"]
        PRED["æµé‡<br/>é¢„æµ‹ç»“æœ"]
    end

    subgraph Preemptive["âš¡ å…ˆå‘åˆ¶äººæªæ–½"]
        WARM["Warm Pool<br/>Podåˆ›å»º"]
        PAUSE["Pause<br/>Containers"]
        KARP["Karpenter<br/>èŠ‚ç‚¹é¢„é…ç½®"]
    end

    subgraph Runtime["ğŸš€ å®é™…æµé‡"]
        TRAFFIC["æµé‡<br/>æ¶Œå…¥"]
        HPA2["HPA<br/>å³æ—¶æ‰©å®¹"]
        READY["Pod<br/>å³æ—¶æœåŠ¡"]
    end

    CW_M --> ML --> PRED
    PRED --> WARM --> KARP
    PRED --> PAUSE --> KARP
    TRAFFIC --> HPA2 --> READY
    KARP -.->|èŠ‚ç‚¹å‡†å¤‡å®Œæˆ| READY
```

**å…ˆå‘åˆ¶äººé¢„é…ç½®ç­–ç•¥**ï¼š

```yaml
# ä½¿ç”¨Placeholder Podå…ˆå‘åˆ¶äººç¡®ä¿èŠ‚ç‚¹
apiVersion: apps/v1
kind: Deployment
metadata:
  name: capacity-reservation
  namespace: scaling
spec:
  replicas: 0  # é¢„æµ‹æ‰©ç¼©å™¨åŠ¨æ€è°ƒæ•´
  selector:
    matchLabels:
      app: capacity-reservation
  template:
    metadata:
      labels:
        app: capacity-reservation
    spec:
      priorityClassName: capacity-reservation  # ä½ä¼˜å…ˆçº§
      terminationGracePeriodSeconds: 0
      containers:
        - name: pause
          image: registry.k8s.io/pause:3.9
          resources:
            requests:
              cpu: "1"
              memory: 2Gi
---
# ä½ä¼˜å…ˆçº§ç±» (è¢«å®é™…å·¥ä½œè´Ÿè½½é©±é€)
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: capacity-reservation
value: -10
globalDefault: false
description: "ç”¨äºKarpenterèŠ‚ç‚¹å…ˆå‘åˆ¶äººé¢„é…ç½®"
```

:::tip å…ˆå‘åˆ¶äººé¢„é…ç½®çš„åŸç†

1. MLæ¨¡å‹é¢„æµ‹30åˆ†é’Ÿåæµé‡å°†å¢åŠ 
2. å¢åŠ Placeholder Pod(pause container)çš„replicas
3. Karpenteræ£€æµ‹åˆ°Pending Podå¹¶é¢„é…ç½®èŠ‚ç‚¹
4. å®é™…æµé‡åˆ°æ¥æ—¶HPAåˆ›å»ºå®é™…Pod
5. Placeholder Podå› ä½ä¼˜å…ˆçº§è¢«ç«‹å³é©±é€
6. ç”±äºèŠ‚ç‚¹å·²å‡†å¤‡å°±ç»ªï¼ŒPodå¯ä»¥ç«‹å³è°ƒåº¦
:::

### 3.5 ARC + Karpenteré›†æˆè‡ªåŠ¨AZç–æ•£

**ARC(Application Recovery Controller)** æ˜¯AWSçš„é«˜å¯ç”¨æ€§æœåŠ¡ï¼Œè‡ªåŠ¨æ£€æµ‹AZæ•…éšœå¹¶å°†æµé‡è½¬ç§»åˆ°å¥åº·çš„AZã€‚ä¸Karpenteré›†æˆåå¯å®ç° **èŠ‚ç‚¹çº§åˆ«çš„è‡ªåŠ¨æ¢å¤**ã€‚

#### ARCæ¦‚è¿°

Application Recovery Controlleræä¾›ä»¥ä¸‹3ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼š

- **Readiness Check**: æŒç»­ç›‘æ§åº”ç”¨ç¨‹åºå¥åº·çŠ¶æ€
- **Routing Control**: é€šè¿‡Route 53æˆ–ALBæ§åˆ¶æµé‡è·¯ç”±
- **Zonal Shift**: æŒ‰AZå•ä½è‡ªåŠ¨æˆ–æ‰‹åŠ¨è½¬ç§»æµé‡

#### Karpenteré›†æˆæ¨¡å¼

```yaml
# æ£€æµ‹ARC Zonal Shiftä¿¡å·çš„Controller
apiVersion: v1
kind: ConfigMap
metadata:
  name: arc-karpenter-controller
  namespace: kube-system
data:
  config.yaml: |
    arcCluster: arn:aws:route53-recovery-control::ACCOUNT:cluster/CLUSTER_ID
    routingControls:
      - name: az-a-routing
        arn: arn:aws:route53-recovery-control::ACCOUNT:controlpanel/PANEL/routingcontrol/CONTROL_A
      - name: az-b-routing
        arn: arn:aws:route53-recovery-control::ACCOUNT:controlpanel/PANEL/routingcontrol/CONTROL_B
      - name: az-c-routing
        arn: arn:aws:route53-recovery-control::ACCOUNT:controlpanel/PANEL/routingcontrol/CONTROL_C
    karpenterNodePools:
      - default
      - gpu-pool
```

#### AZæ•…éšœè‡ªåŠ¨æ¢å¤åºåˆ—

```mermaid
sequenceDiagram
    participant AZ_A as AZ-A (æ•…éšœ)
    participant ARC as ARC Controller
    participant R53 as Route 53
    participant K8s as Kubernetes API
    participant Karp as Karpenter
    participant AZ_B as AZ-B (å¥åº·)
    participant Pod as Workload Pods

    AZ_A->>ARC: Readiness Checkå¤±è´¥
    ARC->>ARC: Gray Failureæ¨¡å¼æ£€æµ‹
    ARC->>R53: Zonal Shiftå¼€å§‹ (AZ-A OUT)
    ARC->>K8s: æ·»åŠ Node Taint (NoSchedule)
    K8s->>Karp: Pending Podäº‹ä»¶å‘ç”Ÿ
    Karp->>AZ_B: é¢„é…ç½®æ›¿ä»£èŠ‚ç‚¹
    AZ_B-->>K8s: æ–°èŠ‚ç‚¹æ³¨å†Œå®Œæˆ
    K8s->>Pod: Podå®‰å…¨è¿ç§» (éµå®ˆPDB)
    Pod-->>AZ_B: æœåŠ¡æ¢å¤å®Œæˆ
    ARC->>K8s: AZ-AèŠ‚ç‚¹Drainå¼€å§‹
    K8s->>AZ_A: èŠ‚ç‚¹æ¸…ç†ä¸ç§»é™¤
```

#### Gray Failureå¤„ç†

**Gray Failure** æŒ‡çš„æ˜¯ä¸å®Œå…¨æ•…éšœè€Œæ˜¯æ€§èƒ½ä¸‹é™çš„çŠ¶æ€ã€‚ARCæ£€æµ‹ä»¥ä¸‹æ¨¡å¼ï¼š

- **ç½‘ç»œå»¶è¿Ÿå¢åŠ **: å¹³æ—¶5ms â†’ è¶…è¿‡50ms
- **é—´æ­‡æ€§è¶…æ—¶**: 1-5%çš„è¯·æ±‚å¤±è´¥
- **èµ„æºäº‰ç”¨**: CPU steal timeå¢åŠ ã€ç½‘ç»œä¸¢åŒ…

```python
# Gray Failureæ£€æµ‹Lambdaå‡½æ•°ç¤ºä¾‹
import boto3
from datetime import datetime, timedelta

def detect_gray_failure(event, context):
    """
    åŸºäºContainer Network Observabilityæ•°æ®
    æ£€æµ‹Gray Failureæ¨¡å¼ã€‚
    """
    cloudwatch = boto3.client('cloudwatch')

    # æŸ¥è¯¢æŒ‰AZçš„ç½‘ç»œå»¶è¿ŸæŒ‡æ ‡
    response = cloudwatch.get_metric_statistics(
        Namespace='ContainerInsights',
        MetricName='pod_network_rx_latency_ms',
        Dimensions=[
            {'Name': 'ClusterName', 'Value': 'my-cluster'},
            {'Name': 'AvailabilityZone', 'Value': 'ap-northeast-2a'}
        ],
        StartTime=datetime.utcnow() - timedelta(minutes=15),
        EndTime=datetime.utcnow(),
        Period=60,
        Statistics=['Average', 'Maximum']
    )

    # Gray Failureé˜ˆå€¼æ£€æŸ¥
    datapoints = response['Datapoints']
    if len(datapoints) < 10:
        return {'status': 'insufficient_data'}

    avg_latency = sum(d['Average'] for d in datapoints) / len(datapoints)
    max_latency = max(d['Maximum'] for d in datapoints)

    # åŸºå‡†ï¼šå¹³å‡å»¶è¿Ÿ > 50ms æˆ–æœ€å¤§å»¶è¿Ÿ > 200ms
    if avg_latency > 50 or max_latency > 200:
        trigger_zonal_shift('ap-northeast-2a')
        return {'status': 'gray_failure_detected', 'action': 'zonal_shift'}

    return {'status': 'healthy'}

def trigger_zonal_shift(az):
    """è§¦å‘ARC Zonal Shiftã€‚"""
    arc = boto3.client('route53-recovery-cluster')
    arc.update_routing_control_state(
        RoutingControlArn='arn:aws:route53-recovery-control::ACCOUNT:...',
        RoutingControlState='Off'  # é˜»æ–­AZ-Aæµé‡
    )
```

#### Istioé›†æˆç«¯åˆ°ç«¯æ¢å¤

ä½¿ç”¨IstioæœåŠ¡ç½‘æ ¼å¯å®ç° **L7å±‚çº§çš„æµé‡æ§åˆ¶**ï¼š

```yaml
# Istio DestinationRuleï¼šAZæ•…éšœæ—¶è‡ªåŠ¨æ•…éšœè½¬ç§»
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: payment-service-dr
spec:
  host: payment-service
  trafficPolicy:
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
    loadBalancer:
      localityLbSetting:
        enabled: true
        failover:
          - from: ap-northeast-2a
            to: ap-northeast-2c
```

**ç«¯åˆ°ç«¯æ¢å¤æµç¨‹**ï¼š

1. **ARC Readiness Checkå¤±è´¥** â†’ Zonal Shiftå¼€å§‹
2. **Route 53** â†’ é˜»æ–­å‘å¾€AZ-Açš„å¤–éƒ¨æµé‡
3. **Istio Envoy** â†’ é˜»æ–­å‘å¾€AZ-Aå†…éƒ¨Podçš„East-Westæµé‡
4. **Karpenter** â†’ åœ¨AZ-Cé¢„é…ç½®æ›¿ä»£èŠ‚ç‚¹
5. **Kubernetes** â†’ éµå®ˆPDBå®‰å…¨è¿ç§»Pod
6. **Istio** â†’ è‡ªåŠ¨è·¯ç”±æµé‡åˆ°æ–°Pod

#### é¢„æµ‹æ€§AZç®¡ç†

åˆ©ç”¨Container Network Observabilityæ•°æ® **å…ˆå‘åˆ¶äººåœ°æ£€æµ‹AZæ€§èƒ½å¼‚å¸¸**ï¼š

```promql
# æŒ‰AZçš„ç½‘ç»œé”™è¯¯ç‡è¶‹åŠ¿
sum(rate(pod_network_rx_errors_total[5m])) by (availability_zone)
/ sum(rate(pod_network_rx_packets_total[5m])) by (availability_zone)
* 100

# æŒ‰AZçš„å¹³å‡Pod-to-Podå»¶è¿Ÿ
histogram_quantile(0.99,
  sum(rate(pod_network_latency_bucket[5m])) by (availability_zone, le)
)
```

**é¢„æµ‹æ€§AZç®¡ç†ç­–ç•¥**ï¼š

- **è¶‹åŠ¿åˆ†æ**: å­¦ä¹ è¿‡å»7å¤©å„AZçš„æ€§èƒ½æ¨¡å¼
- **é¢„è­¦**: æ€§èƒ½æ¯”åŸºå‡†ä¸‹é™20%æ—¶å‘å‡ºè­¦æŠ¥
- **å…ˆå‘åˆ¶äººè½¬ç§»**: ä¸‹é™30%æ—¶è€ƒè™‘è‡ªåŠ¨Zonal Shift
- **æˆæœ¬ä¼˜åŒ–**: è€ƒè™‘è·¨AZæµé‡æˆæœ¬çš„æœ€ä¼˜éƒ¨ç½²

:::warning ARC + Karpenteré›†æˆæ³¨æ„äº‹é¡¹
ARC + Karpenteré›†æˆä»…åœ¨PDBæ­£ç¡®é…ç½®æ—¶æ‰èƒ½ä¿è¯å®‰å…¨çš„Podè¿ç§»ã€‚è¯·ä¸ºæ‰€æœ‰ç”Ÿäº§å·¥ä½œè´Ÿè½½é…ç½®PDBã€‚

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: payment-service-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: payment-service
```
:::

---

## 4. CloudWatchå¼‚å¸¸æ£€æµ‹

### 4.1 å¼‚å¸¸æ£€æµ‹é¢‘å¸¦

CloudWatch Anomaly Detectionä½¿ç”¨MLè‡ªåŠ¨å­¦ä¹ æŒ‡æ ‡çš„ **æ­£å¸¸èŒƒå›´é¢‘å¸¦**ï¼Œæ£€æµ‹è¶…å‡ºé¢‘å¸¦çš„å¼‚å¸¸ã€‚

```bash
# åˆ›å»ºAnomaly Detectionæ¨¡å‹
aws cloudwatch put-anomaly-detector \
  --namespace "ContainerInsights" \
  --metric-name "pod_cpu_utilization" \
  --dimensions Name=ClusterName,Value=my-cluster \
  --stat "Average" \
  --configuration '{
    "ExcludedTimeRanges": [
      {
        "StartTime": "2026-01-01T00:00:00Z",
        "EndTime": "2026-01-02T00:00:00Z"
      }
    ],
    "MetricTimezone": "Asia/Seoul"
  }'
```

### 4.2 EKSæŒ‡æ ‡åº”ç”¨

åº”ç”¨Anomaly Detectionçš„æ ¸å¿ƒEKSæŒ‡æ ‡ï¼š

<AnomalyMetrics />

### 4.3 åŸºäºAnomaly Detectionçš„å‘Šè­¦

```bash
# åŸºäºAnomaly Detectionçš„CloudWatch Alarm
aws cloudwatch put-metric-alarm \
  --alarm-name "EKS-CPU-Anomaly" \
  --comparison-operator GreaterThanUpperThreshold \
  --threshold-metric-id ad1 \
  --evaluation-periods 3 \
  --datapoints-to-alarm 2 \
  --metrics '[
    {
      "Id": "m1",
      "MetricStat": {
        "Metric": {
          "Namespace": "ContainerInsights",
          "MetricName": "pod_cpu_utilization",
          "Dimensions": [
            {"Name": "ClusterName", "Value": "my-cluster"}
          ]
        },
        "Period": 300,
        "Stat": "Average"
      }
    },
    {
      "Id": "ad1",
      "Expression": "ANOMALY_DETECTION_BAND(m1, 2)"
    }
  ]' \
  --alarm-actions "arn:aws:sns:ap-northeast-2:ACCOUNT_ID:ops-alerts"
```

---

## 5. AI Agentè‡ªåŠ¨äº‹ä»¶å“åº”

### 5.1 ç°æœ‰è‡ªåŠ¨åŒ–çš„å±€é™æ€§

åŸºäºEventBridge + Lambdaçš„è‡ªåŠ¨åŒ–æ˜¯ **è§„åˆ™å¼** çš„ï¼Œå› æ­¤å­˜åœ¨å±€é™æ€§ï¼š

```
[ç°æœ‰æ–¹å¼ï¼šè§„åˆ™å¼è‡ªåŠ¨åŒ–]
CloudWatch Alarm â†’ EventBridge Rule â†’ Lambda â†’ å›ºå®šæ“ä½œ

é—®é¢˜ï¼š
  âœ— "CPU > 80%å°±æ‰©å®¹" â€” åŸå› å¯èƒ½æ˜¯å†…å­˜æ³„æ¼
  âœ— "Podé‡å¯ > 5æ¬¡å°±å‘Šè­¦" â€” ä¸åŒåŸå› éœ€è¦ä¸åŒåº”å¯¹
  âœ— æ— æ³•åº”å¯¹å¤åˆæ•…éšœ
  âœ— æ— æ³•é€‚åº”æ–°æ¨¡å¼
```

### 5.2 åŸºäºAI Agentçš„è‡ªä¸»å“åº”

<ResponsePatterns />

AI AgentåŸºäº **ä¸Šä¸‹æ–‡åˆ¤æ–­** è¿›è¡Œè‡ªä¸»å“åº”ã€‚

```mermaid
graph TD
    subgraph Trigger["ğŸ”” è§¦å‘å™¨"]
        CWA["CloudWatch<br/>Alarm"]
        DGA["DevOps Guru<br/>Insight"]
        K8SE["K8s<br/>Event"]
    end

    subgraph Agent["ğŸ¤– AI Agent"]
        COLLECT["æ•°æ®æ”¶é›†<br/>(MCPé›†æˆ)"]
        ANALYZE["AIåˆ†æ<br/>(æ ¹æœ¬åŸå› )"]
        DECIDE["åˆ¤æ–­<br/>(è‡ªåŠ¨/æ‰‹åŠ¨)"]
        ACT["æ‰§è¡Œ<br/>(å®‰å…¨æ“ä½œ)"]
        REPORT["æŠ¥å‘Š<br/>(Slack/Jira)"]
    end

    subgraph Actions["âš¡ å“åº”æªæ–½"]
        SCALE["æ‰©ç¼©å®¹<br/>è°ƒæ•´"]
        RESTART["Pod<br/>é‡å¯"]
        ROLLBACK["éƒ¨ç½²<br/>å›æ»š"]
        ESCALATE["äººå·¥<br/>å‡çº§å¤„ç†"]
    end

    CWA --> COLLECT
    DGA --> COLLECT
    K8SE --> COLLECT
    COLLECT --> ANALYZE --> DECIDE
    DECIDE --> ACT --> REPORT
    ACT --> SCALE
    ACT --> RESTART
    ACT --> ROLLBACK
    ACT --> ESCALATE
```

### 5.3 Kagentè‡ªåŠ¨äº‹ä»¶å“åº”

```yaml
# Kagentï¼šè‡ªåŠ¨äº‹ä»¶å“åº”Agent
apiVersion: kagent.dev/v1alpha1
kind: Agent
metadata:
  name: incident-responder
  namespace: kagent-system
spec:
  description: "EKSäº‹ä»¶è‡ªåŠ¨å“åº”Agent"
  modelConfig:
    provider: bedrock
    model: anthropic.claude-sonnet
    region: ap-northeast-2
  systemPrompt: |
    ä½ æ˜¯ä¸€ä¸ªEKSäº‹ä»¶å“åº”Agentã€‚

    ## å“åº”åŸåˆ™
    1. å®‰å…¨ä¼˜å…ˆï¼šå±é™©å˜æ›´å‡çº§ç»™äººå·¥å¤„ç†
    2. æ ¹æœ¬åŸå› ä¼˜å…ˆï¼šé’ˆå¯¹åŸå› è€Œéç—‡çŠ¶è¿›è¡Œå“åº”
    3. æœ€å°å¹²é¢„ï¼šä»…æ‰§è¡Œæœ€å¿…è¦çš„æ“ä½œ
    4. æ‰€æœ‰æ“ä½œè®°å½•ï¼šè‡ªåŠ¨æŠ¥å‘Šåˆ°Slackå’ŒJIRA

    ## è‡ªåŠ¨æ“ä½œå…è®¸èŒƒå›´
    - Podé‡å¯ (CrashLoopBackOff, 5æ¬¡ä»¥ä¸Š)
    - HPA min/maxè°ƒæ•´ (å½“å‰å€¼çš„Â±50%èŒƒå›´)
    - Deploymentå›æ»š (åˆ°å‰ä¸€ä¸ªç‰ˆæœ¬)
    - èŠ‚ç‚¹drain (MemoryPressure/DiskPressure)

    ## å‡çº§å¤„ç†å¯¹è±¡
    - å¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±çš„æ“ä½œ
    - å½±å“50%ä»¥ä¸Šreplicas
    - StatefulSetç›¸å…³å˜æ›´
    - ç½‘ç»œç­–ç•¥å˜æ›´

  tools:
    - name: kubectl
      type: kmcp
      config:
        allowedVerbs: ["get", "describe", "logs", "top", "rollout", "scale", "delete"]
        deniedResources: ["secrets", "configmaps"]
    - name: cloudwatch
      type: kmcp
      config:
        actions: ["GetMetricData", "DescribeAlarms", "GetInsight"]
    - name: slack
      type: mcp
      config:
        webhook_url: "${SLACK_WEBHOOK}"
        channel: "#incidents"

  triggers:
    - type: cloudwatch-alarm
      filter:
        severity: ["CRITICAL", "HIGH"]
    - type: kubernetes-event
      filter:
        reason: ["CrashLoopBackOff", "OOMKilled", "FailedScheduling"]
```

### 5.4 Strands Agent SOPï¼šå¤åˆæ•…éšœå“åº”

```python
# Strands Agentï¼šå¤åˆæ•…éšœè‡ªåŠ¨å“åº”
from strands import Agent
from strands.tools import eks_tool, cloudwatch_tool, slack_tool, jira_tool

incident_agent = Agent(
    name="complex-incident-handler",
    model="bedrock/anthropic.claude-sonnet",
    tools=[eks_tool, cloudwatch_tool, slack_tool, jira_tool],
    sop="""
    ## å¤åˆæ•…éšœå“åº”SOP

    ### Phase 1ï¼šæƒ…å†µè¯„ä¼° (30ç§’å†…)
    1. æŸ¥è¯¢CloudWatchå‘Šè­¦å’ŒDevOps Guruæ´å¯Ÿ
    2. ç¡®è®¤ç›¸å…³æœåŠ¡çš„PodçŠ¶æ€
    3. ç¡®è®¤èŠ‚ç‚¹çŠ¶æ€å’Œèµ„æºåˆ©ç”¨ç‡
    4. ç¡®è®¤æœ€è¿‘éƒ¨ç½²å†å² (10åˆ†é’Ÿå†…å˜æ›´)

    ### Phase 2ï¼šæ ¹æœ¬åŸå› åˆ†æ (2åˆ†é’Ÿå†…)
    1. ä»æ—¥å¿—ä¸­æå–é”™è¯¯æ¨¡å¼
    2. æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ (CPU, Memory, Network, Disk)
    3. åˆ†æä¸éƒ¨ç½²å˜æ›´çš„æ—¶é—´ç›¸å…³æ€§
    4. ç¡®è®¤ä¾èµ–æœåŠ¡çŠ¶æ€

    ### Phase 3ï¼šè‡ªåŠ¨å“åº”
    æŒ‰åŸå› è‡ªåŠ¨å¤„ç†ï¼š

    **éƒ¨ç½²ç›¸å…³æ•…éšœï¼š**
    - æœ€è¿‘10åˆ†é’Ÿå†…å­˜åœ¨éƒ¨ç½² â†’ è‡ªåŠ¨å›æ»š
    - å›æ»šåç¡®è®¤çŠ¶æ€ â†’ æ¢å¤æ­£å¸¸åˆ™å®Œæˆ

    **èµ„æºä¸è¶³ï¼š**
    - CPU/Memory > 90% â†’ è°ƒæ•´HPAæˆ–Karpenteræ·»åŠ èŠ‚ç‚¹
    - Disk > 85% â†’ æ¸…ç†ä¸å¿…è¦çš„æ—¥å¿—/é•œåƒ

    **ä¾èµ–æœåŠ¡æ•…éšœï¼š**
    - RDSè¿æ¥å¤±è´¥ â†’ ç¡®è®¤è¿æ¥æ± è®¾ç½®ï¼Œå¿…è¦æ—¶é‡å¯
    - SQSå»¶è¿Ÿ â†’ æ£€æŸ¥DLQï¼Œæ¶ˆè´¹è€…æ‰©å®¹

    **åŸå› ä¸æ˜ï¼š**
    - å‡çº§ç»™äººå·¥å¤„ç†
    - åœ¨Slackä¸­åˆ†äº«æ‰€æœ‰æ”¶é›†çš„æ•°æ®

    ### Phase 4ï¼šäº‹åå¤„ç†
    1. åˆ›å»ºäº‹ä»¶æ—¶é—´çº¿
    2. åˆ›å»ºJIRAäº‹ä»¶å·¥å•
    3. åœ¨Slack #incidentsé¢‘é“å‘å¸ƒæŠ¥å‘Š
    4. ä¿å­˜ä¸ºå­¦ä¹ æ•°æ® (åé¦ˆå¾ªç¯)
    """
)
```

:::info AI Agentçš„æ ¸å¿ƒä»·å€¼
è¶…è¶ŠEventBridge+Lambdaï¼Œå®ç°åŸºäºAIä¸Šä¸‹æ–‡çš„è‡ªä¸»å“åº”ã€‚é€šè¿‡ **MCPé›†æˆæŸ¥è¯¢** å„ç§æ•°æ®æºï¼ˆCloudWatchã€EKS APIã€X-Rayã€éƒ¨ç½²å†å²ï¼‰ï¼Œå³ä½¿æ˜¯è§„åˆ™æ— æ³•åº”å¯¹çš„å¤åˆæ•…éšœï¼Œä¹Ÿèƒ½åˆ†ææ ¹æœ¬åŸå› å¹¶è‡ªåŠ¨æ‰§è¡Œé€‚å½“çš„æ“ä½œã€‚
:::

### 5.5 CloudWatch Investigations â€” åŸºäºAIçš„è‡ªåŠ¨æ ¹æœ¬åŸå› åˆ†æ

**CloudWatch Investigations** æ˜¯åŸºäºAWS 17å¹´è¿è¥ç»éªŒæ„å»ºçš„ **ç”Ÿæˆå¼AIè‡ªåŠ¨è°ƒæŸ¥ç³»ç»Ÿ**ã€‚äº‹ä»¶å‘ç”Ÿæ—¶ï¼ŒAIè‡ªåŠ¨ç”Ÿæˆå‡è®¾ã€æ”¶é›†æ•°æ®å¹¶æ‰§è¡ŒéªŒè¯è°ƒæŸ¥å·¥ä½œæµã€‚

#### CloudWatch Investigationsæ¦‚è¿°

```mermaid
graph TD
    subgraph Trigger["ğŸ”” äº‹ä»¶æ£€æµ‹"]
        ALARM["CloudWatch<br/>Alarm"]
        SIGNAL["Application<br/>Signals"]
    end

    subgraph Investigation["ğŸ” AIè°ƒæŸ¥æµç¨‹"]
        HYPO["å‡è®¾ç”Ÿæˆ<br/>(AI)"]
        COLLECT["æ•°æ®æ”¶é›†<br/>(è‡ªåŠ¨)"]
        ANALYZE["ç›¸å…³æ€§åˆ†æ<br/>(AI)"]
        ROOT["æ ¹æœ¬åŸå› <br/>æ¨æ–­"]
    end

    subgraph Evidence["ğŸ“Š è¯æ®æ”¶é›†"]
        METRICS["ç›¸å…³æŒ‡æ ‡"]
        LOGS["ç›¸å…³æ—¥å¿—"]
        TRACES["ç›¸å…³è¿½è¸ª"]
        DEPLOY["éƒ¨ç½²å†å²"]
    end

    subgraph Output["ğŸ“ ç»“æœä¸æªæ–½"]
        SUMMARY["è°ƒæŸ¥ç»“æœ<br/>æ‘˜è¦"]
        REMEDIATION["æ¢å¤å»ºè®®<br/>(Runbook)"]
        REPORT["è¯¦ç»†æŠ¥å‘Š"]
    end

    ALARM --> HYPO
    SIGNAL --> HYPO
    HYPO --> COLLECT
    COLLECT --> METRICS
    COLLECT --> LOGS
    COLLECT --> TRACES
    COLLECT --> DEPLOY
    METRICS --> ANALYZE
    LOGS --> ANALYZE
    TRACES --> ANALYZE
    DEPLOY --> ANALYZE
    ANALYZE --> ROOT
    ROOT --> SUMMARY
    ROOT --> REMEDIATION
    ROOT --> REPORT
```

#### æ ¸å¿ƒåŠŸèƒ½

**1. Application Signalsé›†æˆï¼šåŸºäºæœåŠ¡æ‹“æ‰‘çš„å½±å“åº¦è‡ªåŠ¨åˆ†æ**

CloudWatch Investigationsåˆ©ç”¨Application Signalsè‡ªåŠ¨ç”Ÿæˆçš„æœåŠ¡æ‹“æ‰‘æ¥è¿½è¸ª **æ•…éšœä¼ æ’­è·¯å¾„**ï¼š

```yaml
# Application Signalsè‡ªåŠ¨æœåŠ¡æ‹“æ‰‘ç¤ºä¾‹
payment-gateway (é”™è¯¯ç‡å¢åŠ 25%)
  â””â”€> payment-service (å»¶è¿Ÿå¢åŠ 300%)
       â”œâ”€> postgres-db (è¿æ¥æ± è€—å°½)
       â””â”€> redis-cache (æ­£å¸¸)
            â””â”€> dynamodb (æ­£å¸¸)
```

Investigationsåˆ†ææ­¤æ‹“æ‰‘ï¼š
- **Root Cause**: `postgres-db` è¿æ¥æ± è€—å°½
- **Impacted Services**: `payment-service`, `payment-gateway`
- **Propagation Path**: DB â†’ Service â†’ Gateway

**2. ç›¸å…³æŒ‡æ ‡/æ—¥å¿—/è¿½è¸ªè‡ªåŠ¨ç›¸å…³æ€§åˆ†æ**

```python
# Investigationsæ‰§è¡Œçš„è‡ªåŠ¨ç›¸å…³æ€§åˆ†æç¤ºä¾‹

# æ—¶é—´ç›¸å…³æ€§
payment_service_errors.spike_at = "2026-02-12 14:23:00"
db_connection_pool.exhausted_at = "2026-02-12 14:22:55"
# â†’ ç›¸å·®5ç§’ï¼šDBé—®é¢˜å…ˆäºæœåŠ¡é”™è¯¯å‘ç”Ÿ

# æŒ‡æ ‡ç›¸å…³æ€§
db_active_connections = 100 (è¾¾åˆ°max_connections)
payment_service_response_time = 5000ms (æ¯”å¹³æ—¶50msé«˜100å€)
# â†’ å¼ºç›¸å…³æ€§ï¼šDBè¿æ¥è€—å°½ â†’ æœåŠ¡å»¶è¿Ÿ

# æ—¥å¿—æ¨¡å¼åˆ†æ
logs.error_pattern = "CannotGetJdbcConnectionException"
logs.frequency = 1,234 occurrences in last 5 minutes
# â†’ æ˜ç¡®è¯æ®ï¼šDBè¿æ¥ä¸å¯ç”¨é”™è¯¯
```

**3. åŸºäºå‡è®¾çš„æ ¹æœ¬åŸå› æ¨æ–­**

Investigationsè‡ªåŠ¨ç”Ÿæˆå¹¶éªŒè¯ä»¥ä¸‹å‡è®¾ï¼š

| å‡è®¾ | éªŒè¯æ–¹æ³• | ç»“æœ |
|------|----------|------|
| DBè¿æ¥æ± è€—å°½ | ç¡®è®¤`db_connections`æŒ‡æ ‡ | âœ“ å·²ç¡®è®¤ |
| ç½‘ç»œå»¶è¿Ÿ | åˆ†æVPC Flow Logs | âœ— æ­£å¸¸ |
| OOM(å†…å­˜ä¸è¶³) | ç¡®è®¤å®¹å™¨å†…å­˜æŒ‡æ ‡ | âœ— æ­£å¸¸ |
| éƒ¨ç½²åBug | æŸ¥è¯¢æœ€è¿‘éƒ¨ç½²å†å² | âœ“ ç¡®è®¤10åˆ†é’Ÿå‰æœ‰éƒ¨ç½² |

**æœ€ç»ˆç»“è®º**: æœ€è¿‘éƒ¨ç½²ä¸­DBè¿æ¥æ± é…ç½®è¢«é”™è¯¯åœ°ä»`maxPoolSize=50`æ”¹ä¸º`maxPoolSize=10`ã€‚

**4. è°ƒæŸ¥ç»“æœæ‘˜è¦ä¸æ¢å¤å»ºè®®**

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  CloudWatch Investigationsç»“æœæ‘˜è¦
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”´ æ ¹æœ¬åŸå›  (Root Cause):
   payment-serviceçš„DBè¿æ¥æ± é…ç½®é”™è¯¯
   (maxPoolSize: 50 â†’ 10è¢«é”™è¯¯ä¿®æ”¹)

ğŸ“Š å½±å“åº¦ (Impact):
   - payment-gateway: é”™è¯¯ç‡å¢åŠ 25%
   - payment-service: å»¶è¿Ÿå¢åŠ 300%
   - å—å½±å“è¯·æ±‚ï¼šçº¦15,000ä»¶

â±ï¸ æ—¶é—´çº¿:
   14:10 - éƒ¨ç½²å¼€å§‹ (v1.2.3 â†’ v1.2.4)
   14:22 - DBè¿æ¥æ± å¼€å§‹è€—å°½
   14:23 - æœåŠ¡é”™è¯¯æ€¥å¢å‘Šè­¦è§¦å‘
   14:25 - Investigationsè‡ªåŠ¨å¼€å§‹

ğŸ’¡ å»ºè®®æªæ–½:
   1. ç«‹å³å›æ»š: kubectl rollout undo deployment/payment-service
   2. æ¢å¤DBè¿æ¥æ± é…ç½®: maxPoolSize=50
   3. æ·»åŠ éƒ¨ç½²å‰ç¯å¢ƒå˜é‡éªŒè¯æ­¥éª¤
   4. åº”ç”¨ConfigMapå˜æ›´æ—¶çš„è‡ªåŠ¨éªŒè¯è„šæœ¬

ğŸ“‹ ç›¸å…³èµ„æº:
   - Runbook: https://wiki/db-connection-pool-issue
   - æ—¥å¿—: CloudWatch Logs InsightsæŸ¥è¯¢é“¾æ¥
   - æŒ‡æ ‡: CloudWatch Dashboardé“¾æ¥
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

#### ä¸DevOps Agentçš„åŒºåˆ«

| æ–¹é¢ | CloudWatch Investigations | Kagent / Strands Agent |
|------|--------------------------|------------------------|
| **è¿è¥æ–¹å¼** | AWSæ‰˜ç®¡ (æ— éœ€é…ç½®) | ç”¨æˆ·å®‰è£…Â·è¿ç»´ |
| **åˆ†æèŒƒå›´** | AWSå…¨å±€æ•°æ®è‡ªåŠ¨æ”¶é›† | ä»…é…ç½®çš„æ•°æ®æº |
| **æ ¹æœ¬åŸå› åˆ†æ** | AIè‡ªåŠ¨å‡è®¾ç”ŸæˆÂ·éªŒè¯ | åŸºäºSOPè§„åˆ™æ‰§è¡Œ |
| **è‡ªå®šä¹‰** | æœ‰é™ (AWSé¢„è®¾) | é«˜ (å®Œå…¨è‡ªç”±åº¦) |
| **è‡ªåŠ¨æ¢å¤** | ä»…æä¾›å»ºè®® (ä¸æ‰§è¡Œ) | å¯è‡ªåŠ¨æ‰§è¡Œ |
| **æˆæœ¬** | åŸºäºCloudWatchä½¿ç”¨é‡ | ä»…åŸºç¡€è®¾æ–½æˆæœ¬ |
| **å­¦ä¹ æ›²çº¿** | æ—  (å³æ—¶å¯ç”¨) | ä¸­ç­‰ (éœ€ç¼–å†™YAML) |

**æ¨èé›†æˆæ¨¡å¼**ï¼š

```mermaid
graph LR
    A[CloudWatch Alarm] --> B[Investigations]
    B --> C{æ ¹æœ¬åŸå› è¯†åˆ«}
    C -->|æ˜ç¡®åŸå› | D[EventBridge]
    C -->|ä¸æ˜ç¡®| E[äººå·¥å‡çº§å¤„ç†]
    D --> F[Kagentè‡ªåŠ¨æ¢å¤]
    F --> G[æ¢å¤å®Œæˆ]
    G --> H[Investigationsé‡æ–°éªŒè¯]
```

**é›†æˆç¤ºä¾‹ï¼šEventBridge Rule**

```json
{
  "source": ["aws.cloudwatch"],
  "detail-type": ["CloudWatch Investigation Complete"],
  "detail": {
    "conclusion": {
      "rootCauseType": ["Configuration Error", "Resource Exhaustion"]
    }
  }
}
```

```python
# EventBridge â†’ Kagentè‡ªåŠ¨æ¢å¤Lambda
def lambda_handler(event, context):
    """
    æ¥æ”¶CloudWatch Investigationsç»“æœ
    é€šè¿‡Kagentè§¦å‘è‡ªåŠ¨æ¢å¤ã€‚
    """
    investigation = event['detail']
    root_cause = investigation['conclusion']['rootCauseType']

    if root_cause == "Configuration Error":
        # å‘Kagentè¯·æ±‚ConfigMapå›æ»š
        trigger_kagent_task(
            task_type="rollback_config",
            resource=investigation['affectedResources'][0],
            reason=investigation['conclusion']['summary']
        )
    elif root_cause == "Resource Exhaustion":
        # å‘Kagentè¯·æ±‚æ‰©ç¼©å®¹
        trigger_kagent_task(
            task_type="scale_up",
            resource=investigation['affectedResources'][0],
            target_replicas=calculate_required_replicas()
        )
```

:::tip CloudWatch Investigationsæ´»ç”¨ç­–ç•¥
CloudWatch Investigationsæ˜¯æ— éœ€é…ç½®å³å¯ç›´æ¥ä½¿ç”¨çš„æ‰˜ç®¡AIåˆ†æã€‚éœ€è¦è‡ªå®šä¹‰è‡ªåŠ¨åŒ–æ—¶è¯·ä¸Kagent/Strands Agenté…åˆä½¿ç”¨ã€‚

**æ¨èå·¥ä½œæµ**ï¼š
1. **ä¸€æ¬¡åˆ†æ**: CloudWatch Investigationsè‡ªåŠ¨è¯†åˆ«æ ¹æœ¬åŸå› 
2. **äºŒæ¬¡å“åº”**: åŸå› æ˜ç¡®æ—¶ â†’ Kagent/Strandsè‡ªåŠ¨æ¢å¤
3. **å‡çº§å¤„ç†**: åŸå› ä¸æ˜ç¡®æ—¶ â†’ å°†è°ƒæŸ¥ç»“æœä¼ é€’ç»™äººå·¥
:::

#### å®æˆ˜åœºæ™¯ï¼šEKS Pod OOMKilledè°ƒæŸ¥

```
[äº‹ä»¶] 14:45 - payment-service Pod OOMKilled

[Investigationsè‡ªåŠ¨è°ƒæŸ¥]

æ­¥éª¤1ï¼šå‡è®¾ç”Ÿæˆ
  - å‡è®¾Aï¼šå†…å­˜æ³„æ¼
  - å‡è®¾Bï¼šæµé‡çªå¢å¯¼è‡´çš„æ­£å¸¸å†…å­˜å¢é•¿
  - å‡è®¾Cï¼šå†…å­˜limitsé…ç½®é”™è¯¯

æ­¥éª¤2ï¼šæ•°æ®æ”¶é›†
  - Podå†…å­˜ä½¿ç”¨è¶‹åŠ¿ï¼š100Mi â†’ 512Mi (4å°æ—¶)
  - æµé‡è¶‹åŠ¿ï¼šæ— å˜åŒ– (ç¨³å®š)
  - Heap dumpåˆ†æï¼šRedisè¿æ¥å¯¹è±¡ç´¯ç§¯10,000ä¸ª

æ­¥éª¤3ï¼šæ ¹æœ¬åŸå› è¯†åˆ«
  âœ“ å‡è®¾Aç¡®è®¤ï¼šå†…å­˜æ³„æ¼ (Redisè¿æ¥æœªé‡Šæ”¾)
  âœ— å‡è®¾Bæ’é™¤ï¼šæµé‡æ— å˜åŒ–
  âœ— å‡è®¾Cæ’é™¤ï¼šlimitsè®¾ç½®é€‚å½“ (512Mi)

æ­¥éª¤4ï¼šæ¢å¤å»ºè®®
  å³æ—¶æªæ–½ï¼š
    - kubectl rollout restart deployment/payment-service
    - ä¸´æ—¶å°†å†…å­˜limitså¢åŠ åˆ°1Gi

  æ ¹æœ¬è§£å†³ï¼š
    - ä¿®æ”¹Rediså®¢æˆ·ç«¯ä»£ç  (æ­£ç¡®å…³é—­è¿æ¥æ± )
    - æ·»åŠ å†…å­˜åˆ†æå·¥å…·
    - è®¾ç½®å†…å­˜æ³„æ¼ç›‘æ§å‘Šè­¦

  ç›¸å…³ä»£ç ï¼š
    æ–‡ä»¶: src/cache/redis_client.go
    é—®é¢˜: ç¼ºå°‘defer conn.Close()
    ä¿®å¤PR: https://github.com/...
```

### 5.6 Amazon Q DeveloperåŸºäºè‡ªç„¶è¯­è¨€çš„è¿è¥è‡ªåŠ¨åŒ–

**Amazon Q Developer** æ˜¯AWSçš„ä¸‹ä¸€ä»£AIåŠ©æ‰‹ï¼Œé€šè¿‡ **è‡ªç„¶è¯­è¨€æ¥å£** é©å‘½æ€§åœ°ç®€åŒ–EKSè¿è¥ã€‚æ— éœ€æ§åˆ¶å°æµè§ˆæˆ–å¤æ‚å‘½ä»¤ï¼Œå¯ä»¥ç”¨æ—¥å¸¸è¯­è¨€æé—®å’Œè¯·æ±‚æ“ä½œã€‚

#### åŸºäºè‡ªç„¶è¯­è¨€æ¥å£çš„EKSæ•…éšœæ’æŸ¥

Amazon Q Developerå°† **è‡ªç„¶è¯­è¨€æŸ¥è¯¢** è‡ªåŠ¨è½¬æ¢ä¸ºAWS APIè°ƒç”¨å’Œkubectlå‘½ä»¤å¹¶æ‰§è¡Œï¼š

**åœºæ™¯1ï¼šæŸ¥æ‰¾OOM Pod**

```
[ç”¨æˆ·æé—®]
"å¸®æˆ‘æ‰¾åˆ°è¿™ä¸ªé›†ç¾¤ä¸­å‘ç”ŸOOMçš„Pod"

[Amazon Q Developerå“åº”]
æ­£åœ¨åˆ†æ...

å‘ç°çš„OOMäº‹ä»¶ï¼š
- payment-service-7d8f9c4b-xyz (namespace: payment)
  â””â”€ æœ€è¿‘3æ¬¡OOMKilled (è¿‡å»1å°æ—¶)
  â””â”€ å†…å­˜limits: 512Mi, å®é™…ä½¿ç”¨: 520Mi
  â””â”€ å»ºè®®: å°†memory limitså¢åŠ åˆ°1Gi

- cart-service-5a6b2d1c-abc (namespace: shopping)
  â””â”€ æœ€è¿‘1æ¬¡OOMKilled (è¿‡å»30åˆ†é’Ÿ)
  â””â”€ å†…å­˜limits: 256Mi, å®é™…ä½¿ç”¨: 280Mi
  â””â”€ å»ºè®®: å°†memory limitså¢åŠ åˆ°512Mi

æ‰§è¡Œçš„å‘½ä»¤ï¼š
$ kubectl get events --all-namespaces --field-selector reason=OOMKilled
$ kubectl top pod -n payment payment-service-7d8f9c4b-xyz
$ kubectl describe pod -n payment payment-service-7d8f9c4b-xyz

éœ€è¦æ‰§è¡Œä¸‹ä¸€æ­¥æ“ä½œå—ï¼Ÿ
1. è‡ªåŠ¨è°ƒæ•´memory limits (åº”ç”¨VPA)
2. å¼€å§‹è¯¦ç»†å†…å­˜åˆ†æ
3. åˆ†æå…¨éƒ¨ç›¸å…³æ—¥å¿—
```

**åœºæ™¯2ï¼šè¯†åˆ«é«˜è´Ÿè½½èŠ‚ç‚¹**

```
[ç”¨æˆ·æé—®]
"è¿‡å»1å°æ—¶å†…CPUä½¿ç”¨ç‡è¶…è¿‡80%çš„èŠ‚ç‚¹æœ‰å“ªäº›ï¼Ÿ"

[Amazon Q Developerå“åº”]
æ­£åœ¨æŸ¥è¯¢CloudWatch Container Insightsæ•°æ®...

é«˜è´Ÿè½½èŠ‚ç‚¹åˆ—è¡¨ï¼š

1. ip-10-0-1-45.ap-northeast-2.compute.internal
   - å¹³å‡CPU: 87.3% (è¿‡å»1å°æ—¶)
   - æœ€å¤§CPU: 94.1%
   - å®ä¾‹ç±»å‹: m5.xlarge (4 vCPU)
   - è¿è¡Œä¸­çš„Pod: 23ä¸ª
   - å»ºè®®æ“ä½œ: é€šè¿‡Karpenteræ·»åŠ èŠ‚ç‚¹æˆ–é‡æ–°åˆ†é…å·¥ä½œè´Ÿè½½

2. ip-10-0-2-78.ap-northeast-2.compute.internal
   - å¹³å‡CPU: 82.5% (è¿‡å»1å°æ—¶)
   - æœ€å¤§CPU: 89.7%
   - å®ä¾‹ç±»å‹: c5.2xlarge (8 vCPU)
   - è¿è¡Œä¸­çš„Pod: 31ä¸ª
   - å»ºè®®æ“ä½œ: å°†CPUå¯†é›†å‹Podè¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹

æ‰§è¡Œçš„CloudWatchæŸ¥è¯¢ï¼š
namespace: ContainerInsights
metric: node_cpu_utilization
period: 1h
threshold: > 80%

è¦æ‰§è¡Œä¸‹ä¸€æ­¥æ“ä½œå—ï¼Ÿ
1. é€šè¿‡Karpenterè‡ªåŠ¨é¢„é…ç½®æ–°èŠ‚ç‚¹
2. æ‰‹åŠ¨è¿ç§»é«˜è´Ÿè½½Podåˆ°å…¶ä»–èŠ‚ç‚¹
3. æ£€æŸ¥å’Œè°ƒæ•´HPAè®¾ç½®
```

**åœºæ™¯3ï¼šDeploymentå›æ»š**

```
[ç”¨æˆ·æé—®]
"å¸®æˆ‘å›æ»šè¿™ä¸ªDeployment"

[Amazon Q Developerå“åº”]
æ­£åœ¨å‡†å¤‡payment-service Deploymentçš„å›æ»š...

å½“å‰éƒ¨ç½²çŠ¶æ€ï¼š
- Revision: 15
- éƒ¨ç½²æ—¶é—´: 2026-02-12 14:30:00 (15åˆ†é’Ÿå‰)
- é”™è¯¯ç‡: 12.3% (æ¯”å¹³æ—¶0.5%æ€¥å‰§å¢åŠ )
- ä¸Šä¸€ä¸ªç¨³å®šç‰ˆæœ¬: Revision 14 (é”™è¯¯ç‡0.4%)

å›æ»šç›®æ ‡ï¼š
- å½“å‰: payment-service:v2.3.1 (Revision 15)
- å›æ»š: payment-service:v2.3.0 (Revision 14)

PDBç¡®è®¤ï¼š
- minAvailable: 2
- å½“å‰Running: 5
- å›æ»šæœŸé—´æœ€ä½ä¿è¯: 2ä¸ªç»´æŒ âœ“

æ‰§è¡Œå‘½ä»¤ï¼š
$ kubectl rollout undo deployment/payment-service -n payment --to-revision=14

æ‰§è¡Œå›æ»šå—ï¼Ÿ (yes/no)
```

**åœºæ™¯4ï¼šç½‘ç»œè¿æ¥é—®é¢˜è¯Šæ–­**

```
[ç”¨æˆ·æé—®]
"å¸®æˆ‘æ‰¾å‡ºpayment-serviceæ— æ³•è¿æ¥databaseçš„åŸå› "

[Amazon Q Developerå“åº”]
æ­£åœ¨è¯Šæ–­ç½‘ç»œè¿æ¥é—®é¢˜...

è¯Šæ–­ç»“æœï¼š

1. DNSç¡®è®¤: âœ“ æ­£å¸¸
   - database.payment.svc.cluster.local è§£ææˆåŠŸ
   - IP: 10.100.15.234

2. ç½‘ç»œç­–ç•¥ç¡®è®¤: âœ— å‘ç°é—®é¢˜
   - payment-service â†’ database è¿æ¥è¢«é˜»æ–­
   - åŸå› : NetworkPolicy "database-access-policy"ä¸­
     payment-serviceçš„æ ‡ç­¾ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­

3. SecurityGroupç¡®è®¤: âœ“ æ­£å¸¸
   - RDS SecurityGroupå…è®¸EKSèŠ‚ç‚¹

4. æ—¥å¿—åˆ†æ:
   - é”™è¯¯: "dial tcp 10.100.15.234:5432: i/o timeout"
   - å‘ç”Ÿé¢‘ç‡: è¿‡å»10åˆ†é’Ÿ1,247æ¬¡

è§£å†³æ–¹æ³•:
$ kubectl edit networkpolicy database-access-policy -n payment

åœ¨ingress.from.podSelectorä¸­æ·»åŠ ä»¥ä¸‹æ ‡ç­¾:
  matchLabels:
    app: payment-service  # â† æ·»åŠ æ­¤æ ‡ç­¾

ä¿®æ”¹åçš„YAMLé¢„è§ˆ:
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-access-policy
  namespace: payment
spec:
  podSelector:
    matchLabels:
      app: database
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: payment-service  # â† å·²æ·»åŠ 
        - podSelector:
            matchLabels:
              app: order-service
---

è‡ªåŠ¨ä¿®å¤å—ï¼Ÿ (yes/no)
```

#### Cost Exploreré›†æˆ

Amazon Q Developerä¸ **AWS Cost Explorer** é›†æˆï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€æˆæœ¬æŸ¥è¯¢ï¼š

**åœºæ™¯1ï¼šæˆæœ¬å¢é•¿åŸå› åˆ†æ**

```
[ç”¨æˆ·æé—®]
"è¿™ä¸ªæœˆEKSæˆæœ¬ä¸ºä»€ä¹ˆå¢åŠ äº†ï¼Ÿ"

[Amazon Q Developerå“åº”]
æ­£åœ¨åˆ†æCost Exploreræ•°æ®...

æˆæœ¬å¢é•¿åˆ†æ (2026å¹´2æœˆ):

æ€»EKSæˆæœ¬: $12,450 (ç¯æ¯”+45%)
å¢é•¿åŸå› ï¼š

1. è®¡ç®—æˆæœ¬å¢é•¿: +$3,200 (è´¡çŒ®52%)
   - m5.4xlargeå®ä¾‹ä½¿ç”¨å¢åŠ 
   - åŸå› : MLæ¨ç†å·¥ä½œè´Ÿè½½å¢åŠ  (vLLMéƒ¨ç½²)
   - å»ºè®®: è¿ç§»åˆ°Graviton (m7g)å¯èŠ‚çœ40%

2. æ•°æ®ä¼ è¾“æˆæœ¬å¢é•¿: +$1,800 (è´¡çŒ®29%)
   - è·¨AZæµé‡å¢åŠ  (150GB â†’ 450GB)
   - åŸå› : æ–°å¾®æœåŠ¡é—´çš„é€šä¿¡æ¨¡å¼
   - å»ºè®®: é€šè¿‡Topology Aware Hintsä¼˜å…ˆåŒAZé€šä¿¡

3. EBSå­˜å‚¨æˆæœ¬å¢é•¿: +$950 (è´¡çŒ®15%)
   - gp3å·ä½¿ç”¨é‡å¢åŠ  (500GB â†’ 1,200GB)
   - åŸå› : PrometheusæŒ‡æ ‡ä¿ç•™æœŸå»¶é•¿ (7å¤© â†’ 30å¤©)
   - å»ºè®®: å°†é•¿æœŸä¿å­˜æ•°æ®è¿ç§»åˆ°S3

4. è´Ÿè½½å‡è¡¡å™¨æˆæœ¬å¢é•¿: +$250 (è´¡çŒ®4%)
   - ALB LCUä½¿ç”¨å¢åŠ 
   - åŸå› : APIæµé‡å¢åŠ 

è¯¦ç»†æˆæœ¬ä»ªè¡¨ç›˜: (é“¾æ¥)
æ­£åœ¨ç”Ÿæˆæˆæœ¬ä¼˜åŒ–å»ºè®®æŠ¥å‘Š...
```

**åœºæ™¯2ï¼šSpot vs On-Demandæˆæœ¬å¯¹æ¯”**

```
[ç”¨æˆ·æé—®]
"åˆ‡æ¢åˆ°Spotå®ä¾‹å¯ä»¥èŠ‚çœå¤šå°‘ï¼Ÿ"

[Amazon Q Developerå“åº”]
æ­£åœ¨è¿è¡ŒSpotèŠ‚çœæ¨¡æ‹Ÿ...

å½“å‰é›†ç¾¤é…ç½®ï¼š
- On-DemandèŠ‚ç‚¹: 45ä¸ª
- å®ä¾‹ç±»å‹: m5.xlarge, m5.2xlarge, c5.2xlarge
- æœˆè´¹ç”¨: $8,500

Spotè½¬æ¢æ¨¡æ‹Ÿ (70% Spot, 30% On-Demand):

åœºæ™¯A: ä¿å®ˆè½¬æ¢ (ä»…Statelesså·¥ä½œè´Ÿè½½)
- SpotèŠ‚ç‚¹: 32ä¸ª (70%)
- On-DemandèŠ‚ç‚¹: 13ä¸ª (30%)
- é¢„ä¼°æœˆè´¹ç”¨: $3,200
- èŠ‚çœé¢: $5,300/æœˆ (èŠ‚çœ62%)
- é£é™©åº¦: ä½ (Statefulå·¥ä½œè´Ÿè½½ä¿æŒOn-Demand)

åœºæ™¯B: ç§¯æè½¬æ¢ (å¤§éƒ¨åˆ†Spot)
- SpotèŠ‚ç‚¹: 40ä¸ª (89%)
- On-DemandèŠ‚ç‚¹: 5ä¸ª (11%)
- é¢„ä¼°æœˆè´¹ç”¨: $1,900
- èŠ‚çœé¢: $6,600/æœˆ (èŠ‚çœ78%)
- é£é™©åº¦: ä¸­ç­‰ (Spotä¸­æ–­æ—¶éƒ¨åˆ†æœåŠ¡å—å½±å“)

Spotä¸­æ–­åº”å¯¹ï¼š
- Karpenterè‡ªåŠ¨é‡æ–°é¢„é…ç½®: å¹³å‡90ç§’
- Spot Instance Interruption Warning: æå‰2åˆ†é’Ÿé€šçŸ¥
- PDBä¿è¯æœ€ä½å¯ç”¨æ€§

å»ºè®®ï¼š
1. ä»åœºæ™¯Aå¼€å§‹ (ç¨³å®šæ€§ä¼˜å…ˆ)
2. ç›‘æ§2å‘¨
3. ç¨³å®šåæ‰©å±•åˆ°åœºæ™¯B

æ­£åœ¨ç”ŸæˆSpotè½¬æ¢è®¡åˆ’ä¹¦...
```

#### MCPæœåŠ¡å™¨é›†æˆä½ä»£ç AIOps

Amazon Q Developeré€šè¿‡ **Model Context Protocol(MCP)** ä¸å„ç§AWSæœåŠ¡é›†æˆï¼Œå¯ä»¥æ„å»º **ä½ä»£ç AIOpsè§£å†³æ–¹æ¡ˆ**ï¼š

**MCPæœåŠ¡å™¨é›†æˆæ¶æ„**ï¼š

```mermaid
graph TD
    subgraph User["ğŸ‘¤ è¿ç»´äººå‘˜"]
        NL["è‡ªç„¶è¯­è¨€æé—®"]
    end

    subgraph QDev["ğŸ¤– Amazon Q Developer"]
        INTENT["æ„å›¾åˆ†æ"]
        ORCHESTRATE["MCPæœåŠ¡å™¨ç¼–æ’"]
        SYNTHESIZE["å“åº”åˆæˆ"]
    end

    subgraph MCP["ğŸ“¡ MCPæœåŠ¡å™¨"]
        EKS_MCP["EKS MCP<br/>(kubectl)"]
        CW_MCP["CloudWatch MCP<br/>(æŒ‡æ ‡/æ—¥å¿—)"]
        CE_MCP["Cost Explorer MCP<br/>(æˆæœ¬)"]
        BEDROCK_MCP["Bedrock MCP<br/>(AIåˆ†æ)"]
    end

    subgraph AWS["â˜ï¸ AWSæœåŠ¡"]
        EKS["Amazon EKS"]
        CW["CloudWatch"]
        CE["Cost Explorer"]
        BEDROCK["Bedrock"]
    end

    NL --> INTENT
    INTENT --> ORCHESTRATE
    ORCHESTRATE --> EKS_MCP
    ORCHESTRATE --> CW_MCP
    ORCHESTRATE --> CE_MCP
    ORCHESTRATE --> BEDROCK_MCP
    EKS_MCP --> EKS
    CW_MCP --> CW
    CE_MCP --> CE
    BEDROCK_MCP --> BEDROCK
    EKS --> SYNTHESIZE
    CW --> SYNTHESIZE
    CE --> SYNTHESIZE
    BEDROCK --> SYNTHESIZE
    SYNTHESIZE --> NL
```
